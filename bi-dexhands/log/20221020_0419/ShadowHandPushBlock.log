Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-3p0qd2z7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandPushBlock_ppo_20221020041912
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/3p0qd2z7
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:2
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=2, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=2, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:2', seed=None, sim_device='cuda:2', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandPushBlock', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_push_block', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 125, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.5, 'orientation_scale': 0.1, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'block', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandPushBlock', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandPushBlock_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 6808
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:2
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 1489 steps/s (collection: 5.972s, learning 5.025s)
               Value function loss: 8.4861
                    Surrogate loss: 0.0383
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 11.00s
                        Total time: 11.00s
                               ETA: 1099650.3s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 3599 steps/s (collection: 3.572s, learning 0.980s)
               Value function loss: 0.8765
                    Surrogate loss: -0.0282
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 4.55s
                        Total time: 15.55s
                               ETA: 777406.3s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 4831 steps/s (collection: 3.218s, learning 0.173s)
               Value function loss: 0.3168
                    Surrogate loss: -0.0339
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 3.39s
                        Total time: 18.94s
                               ETA: 631299.3s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 4453 steps/s (collection: 3.472s, learning 0.206s)
               Value function loss: 0.2258
                    Surrogate loss: -0.0187
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 3.68s
                        Total time: 22.62s
                               ETA: 565433.0s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 3114 steps/s (collection: 5.088s, learning 0.173s)
               Value function loss: 0.1788
                    Surrogate loss: -0.0164
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 5.26s
                        Total time: 27.88s
                               ETA: 557555.1s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 4226 steps/s (collection: 3.711s, learning 0.166s)
               Value function loss: 0.2110
                    Surrogate loss: -0.0172
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 3.88s
                        Total time: 31.76s
                               ETA: 529231.9s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 3771 steps/s (collection: 4.180s, learning 0.164s)
               Value function loss: 0.3816
                    Surrogate loss: -0.0239
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 4.34s
                        Total time: 36.10s
                               ETA: 515679.4s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 3995 steps/s (collection: 3.929s, learning 0.172s)
               Value function loss: 0.3774
                    Surrogate loss: -0.0245
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 4.10s
                        Total time: 40.20s
                               ETA: 502467.7s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 3839 steps/s (collection: 4.102s, learning 0.165s)
               Value function loss: 0.3187
                    Surrogate loss: -0.0241
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 4.27s
                        Total time: 44.47s
                               ETA: 494041.7s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 3760 steps/s (collection: 4.187s, learning 0.169s)
               Value function loss: 0.3178
                    Surrogate loss: -0.0200
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 4.36s
                        Total time: 48.82s
                               ETA: 488192.7s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 3253 steps/s (collection: 4.865s, learning 0.171s)
               Value function loss: 0.2105
                    Surrogate loss: -0.0236
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 5.04s
                        Total time: 53.86s
                               ETA: 489583.9s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 2845 steps/s (collection: 4.511s, learning 1.246s)
               Value function loss: 0.1156
                    Surrogate loss: -0.0314
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 5.76s
                        Total time: 59.62s
                               ETA: 496753.1s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 3386 steps/s (collection: 4.662s, learning 0.176s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0382
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 4.84s
                        Total time: 64.45s
                               ETA: 495745.7s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 3623 steps/s (collection: 4.352s, learning 0.170s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0210
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 4.52s
                        Total time: 68.98s
                               ETA: 492623.1s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 3473 steps/s (collection: 4.557s, learning 0.161s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0224
             Mean action noise std: 0.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 4.72s
                        Total time: 73.69s
                               ETA: 491222.6s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 2998 steps/s (collection: 5.253s, learning 0.211s)
               Value function loss: 1.1073
                    Surrogate loss: 0.0019
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 5.46s
                        Total time: 79.16s
                               ETA: 494662.3s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 2623 steps/s (collection: 6.029s, learning 0.216s)
               Value function loss: 0.0872
                    Surrogate loss: -0.0265
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 6.25s
                        Total time: 85.40s
                               ETA: 502291.0s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 2938 steps/s (collection: 5.361s, learning 0.214s)
               Value function loss: 0.0693
                    Surrogate loss: -0.0281
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 5.58s
                        Total time: 90.98s
                               ETA: 505352.5s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 2146 steps/s (collection: 7.415s, learning 0.218s)
               Value function loss: 0.0667
                    Surrogate loss: -0.0241
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 7.63s
                        Total time: 98.61s
                               ETA: 518916.1s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.331s, learning 0.237s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0223
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 8.57s
                        Total time: 107.18s
                               ETA: 535799.9s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 1639 steps/s (collection: 9.778s, learning 0.215s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0175
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 9.99s
                        Total time: 117.17s
                               ETA: 557858.1s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.138s, learning 0.173s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0141
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 11.31s
                        Total time: 128.48s
                               ETA: 583898.5s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 1223 steps/s (collection: 13.182s, learning 0.210s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0296
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 13.39s
                        Total time: 141.88s
                               ETA: 616722.1s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 1075 steps/s (collection: 15.017s, learning 0.218s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0395
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 15.24s
                        Total time: 157.11s
                               ETA: 654484.5s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 1039 steps/s (collection: 15.581s, learning 0.181s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0314
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 15.76s
                        Total time: 172.87s
                               ETA: 691328.7s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 1041 steps/s (collection: 15.523s, learning 0.211s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0494
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 15.73s
                        Total time: 188.61s
                               ETA: 725233.1s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 1049 steps/s (collection: 15.434s, learning 0.177s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0503
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 15.61s
                        Total time: 204.22s
                               ETA: 756169.6s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.934s, learning 0.171s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0400
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 16.11s
                        Total time: 220.32s
                               ETA: 786658.7s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.109s, learning 0.273s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0360
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 16.38s
                        Total time: 236.71s
                               ETA: 815996.6s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 1023 steps/s (collection: 15.810s, learning 0.201s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0363
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 16.01s
                        Total time: 252.72s
                               ETA: 842142.3s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 1039 steps/s (collection: 15.580s, learning 0.174s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0400
             Mean action noise std: 0.80
                       Mean reward: 13.39
               Mean episode length: 124.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 15.75s
                        Total time: 268.47s
                               ETA: 865773.9s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 1015 steps/s (collection: 15.968s, learning 0.174s)
               Value function loss: 0.6382
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 16.14s
                        Total time: 284.61s
                               ETA: 889137.4s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.715s, learning 0.239s)
               Value function loss: 0.2179
                    Surrogate loss: -0.0233
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 15.95s
                        Total time: 300.57s
                               ETA: 910513.4s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.151s, learning 0.184s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0448
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 16.34s
                        Total time: 316.90s
                               ETA: 931753.5s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.952s, learning 0.168s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0393
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 16.12s
                        Total time: 333.02s
                               ETA: 951163.4s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 1045 steps/s (collection: 15.500s, learning 0.169s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0332
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 15.67s
                        Total time: 348.69s
                               ETA: 968240.8s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.938s, learning 0.166s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0492
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 16.10s
                        Total time: 364.79s
                               ETA: 985569.3s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.070s, learning 0.165s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0428
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 12.23s
                        Total time: 377.03s
                               ETA: 991808.3s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.844s, learning 0.184s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0437
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 8.03s
                        Total time: 385.05s
                               ETA: 986944.6s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.169s, learning 0.169s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 8.34s
                        Total time: 393.39s
                               ETA: 983098.0s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.150s, learning 0.165s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0507
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 8.31s
                        Total time: 401.71s
                               ETA: 979382.4s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.994s, learning 0.161s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0490
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 8.16s
                        Total time: 409.86s
                               ETA: 975464.5s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.140s, learning 0.163s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0519
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 8.30s
                        Total time: 418.17s
                               ETA: 972068.8s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.953s, learning 0.159s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0500
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 8.11s
                        Total time: 426.28s
                               ETA: 968394.3s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.068s, learning 0.167s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0525
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 8.23s
                        Total time: 434.51s
                               ETA: 965156.8s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.018s, learning 0.161s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0521
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 8.18s
                        Total time: 442.69s
                               ETA: 961937.7s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.184s, learning 0.161s)
               Value function loss: 1.2160
                    Surrogate loss: 0.0207
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 8.35s
                        Total time: 451.04s
                               ETA: 959208.8s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.857s, learning 0.187s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0198
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 8.04s
                        Total time: 459.08s
                               ETA: 955968.0s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.069s, learning 0.230s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0339
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 8.30s
                        Total time: 467.38s
                               ETA: 953377.0s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.165s, learning 0.209s)
               Value function loss: 0.0534
                    Surrogate loss: -0.0347
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 8.37s
                        Total time: 475.75s
                               ETA: 951041.1s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.225s, learning 0.182s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0291
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 8.41s
                        Total time: 484.16s
                               ETA: 948861.8s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.023s, learning 0.167s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0282
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 8.19s
                        Total time: 492.35s
                               ETA: 946346.4s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.232s, learning 0.173s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0327
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 8.40s
                        Total time: 500.76s
                               ETA: 944330.6s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.336s, learning 0.167s)
               Value function loss: 0.0684
                    Surrogate loss: -0.0205
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 8.50s
                        Total time: 509.26s
                               ETA: 942572.1s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.395s, learning 0.159s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0359
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 8.55s
                        Total time: 517.81s
                               ETA: 940970.3s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.075s, learning 0.195s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0253
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 8.27s
                        Total time: 526.08s
                               ETA: 938918.3s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.893s, learning 0.205s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0442
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 8.10s
                        Total time: 534.18s
                               ETA: 936635.3s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.391s, learning 0.156s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0465
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 8.55s
                        Total time: 542.73s
                               ETA: 935206.0s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 2073 steps/s (collection: 7.733s, learning 0.167s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0395
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 7.90s
                        Total time: 550.63s
                               ETA: 932728.5s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.309s, learning 0.249s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0509
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 8.56s
                        Total time: 559.19s
                               ETA: 931428.8s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.348s, learning 0.190s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0452
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 8.54s
                        Total time: 567.73s
                               ETA: 930138.5s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.017s, learning 0.167s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0552
             Mean action noise std: 0.80
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 8.18s
                        Total time: 575.91s
                               ETA: 928318.9s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.915s, learning 0.180s)
               Value function loss: 1.0526
                    Surrogate loss: 0.0238
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 8.09s
                        Total time: 584.00s
                               ETA: 926415.5s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.227s, learning 0.172s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0285
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 8.40s
                        Total time: 592.40s
                               ETA: 925046.1s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.272s, learning 0.191s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0404
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 8.46s
                        Total time: 600.87s
                               ETA: 923818.1s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.350s, learning 0.169s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0425
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 8.52s
                        Total time: 609.39s
                               ETA: 922711.2s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.971s, learning 0.161s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0472
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 8.13s
                        Total time: 617.52s
                               ETA: 921059.7s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.058s, learning 0.162s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0485
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 8.22s
                        Total time: 625.74s
                               ETA: 919586.0s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.014s, learning 0.164s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0541
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 8.18s
                        Total time: 633.92s
                               ETA: 918093.3s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.165s, learning 0.175s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0474
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 8.34s
                        Total time: 642.26s
                               ETA: 916874.9s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 2004 steps/s (collection: 7.989s, learning 0.186s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0515
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 8.18s
                        Total time: 650.43s
                               ETA: 915458.8s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.007s, learning 0.169s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0390
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 8.18s
                        Total time: 658.61s
                               ETA: 914082.8s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.006s, learning 0.190s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0495
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 8.20s
                        Total time: 666.80s
                               ETA: 912771.3s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.980s, learning 0.172s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0554
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 8.15s
                        Total time: 674.96s
                               ETA: 911436.4s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.879s, learning 0.163s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0576
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 8.04s
                        Total time: 683.00s
                               ETA: 909989.2s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.096s, learning 0.166s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0581
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 8.26s
                        Total time: 691.26s
                               ETA: 908869.8s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.163s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0576
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 8.10s
                        Total time: 699.36s
                               ETA: 907575.3s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.130s, learning 0.169s)
               Value function loss: 0.8855
                    Surrogate loss: 0.0378
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 8.30s
                        Total time: 707.66s
                               ETA: 906562.7s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.985s, learning 0.166s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0199
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 8.15s
                        Total time: 715.82s
                               ETA: 905388.2s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.199s, learning 0.210s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0319
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 8.41s
                        Total time: 724.22s
                               ETA: 904564.9s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 2056 steps/s (collection: 7.797s, learning 0.171s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0304
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 7.97s
                        Total time: 732.19s
                               ETA: 903217.3s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.783s, learning 0.168s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0395
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 7.95s
                        Total time: 740.14s
                               ETA: 901881.6s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.156s, learning 0.169s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0454
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 8.33s
                        Total time: 748.47s
                               ETA: 901028.9s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.796s, learning 0.168s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0520
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 7.96s
                        Total time: 756.43s
                               ETA: 899767.2s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.943s, learning 0.162s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0509
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 8.10s
                        Total time: 764.54s
                               ETA: 898699.3s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.136s, learning 0.218s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0515
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 8.35s
                        Total time: 772.89s
                               ETA: 897946.3s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.351s, learning 0.161s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0576
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 8.51s
                        Total time: 781.40s
                               ETA: 897391.8s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 2053 steps/s (collection: 7.812s, learning 0.166s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0524
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 7.98s
                        Total time: 789.38s
                               ETA: 896244.1s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.903s, learning 0.170s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0526
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 8.07s
                        Total time: 797.46s
                               ETA: 895228.4s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.129s, learning 0.253s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0501
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 8.38s
                        Total time: 805.84s
                               ETA: 894577.3s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.122s, learning 0.169s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0577
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 8.29s
                        Total time: 814.13s
                               ETA: 893841.2s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.838s, learning 0.165s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0557
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 8.00s
                        Total time: 822.13s
                               ETA: 892808.6s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.247s, learning 0.186s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0581
             Mean action noise std: 0.80
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 8.43s
                        Total time: 830.56s
                               ETA: 892259.0s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.206s, learning 0.160s)
               Value function loss: 1.4165
                    Surrogate loss: 0.0293
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 8.37s
                        Total time: 838.93s
                               ETA: 891650.4s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.034s, learning 0.158s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0337
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 8.19s
                        Total time: 847.12s
                               ETA: 890871.3s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.999s, learning 0.162s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0429
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 8.16s
                        Total time: 855.28s
                               ETA: 890075.4s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.832s, learning 0.175s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0384
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 8.01s
                        Total time: 863.29s
                               ETA: 889136.8s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.030s, learning 0.166s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0426
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 8.20s
                        Total time: 871.49s
                               ETA: 888409.9s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.225s, learning 0.169s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0442
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 8.39s
                        Total time: 879.88s
                               ETA: 887897.9s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.778s, learning 0.174s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0435
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 7.95s
                        Total time: 887.83s
                               ETA: 886953.3s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1984 steps/s (collection: 7.999s, learning 0.258s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0482
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 8.26s
                        Total time: 896.09s
                               ETA: 886330.2s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.278s, learning 0.189s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0528
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 8.47s
                        Total time: 904.56s
                               ETA: 885924.0s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.012s, learning 0.184s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0540
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 8.20s
                        Total time: 912.75s
                               ETA: 885263.1s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.149s, learning 0.155s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0514
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 8.30s
                        Total time: 921.06s
                               ETA: 884718.3s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1989 steps/s (collection: 7.968s, learning 0.268s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0563
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 8.24s
                        Total time: 929.29s
                               ETA: 884119.5s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.972s, learning 0.164s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0587
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 8.14s
                        Total time: 937.43s
                               ETA: 883437.4s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.019s, learning 0.166s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0554
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 8.18s
                        Total time: 945.61s
                               ETA: 882813.5s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.920s, learning 0.182s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0584
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 8.10s
                        Total time: 953.71s
                               ETA: 882124.3s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.275s, learning 0.180s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0571
             Mean action noise std: 0.80
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 8.46s
                        Total time: 962.17s
                               ETA: 881771.8s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.108s, learning 0.172s)
               Value function loss: 1.3730
                    Surrogate loss: 0.0084
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 8.28s
                        Total time: 970.45s
                               ETA: 881265.5s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.881s, learning 0.168s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0265
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 8.05s
                        Total time: 978.50s
                               ETA: 880560.7s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 2088 steps/s (collection: 7.679s, learning 0.166s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0533
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 7.84s
                        Total time: 986.34s
                               ETA: 879685.7s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.853s, learning 0.169s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0601
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 8.02s
                        Total time: 994.36s
                               ETA: 878983.0s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 2094 steps/s (collection: 7.658s, learning 0.166s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0579
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 7.82s
                        Total time: 1002.19s
                               ETA: 878119.5s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.148s, learning 0.187s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0556
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 8.34s
                        Total time: 1010.52s
                               ETA: 877715.1s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 2075 steps/s (collection: 7.731s, learning 0.163s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0587
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 7.89s
                        Total time: 1018.42s
                               ETA: 876936.8s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 2088 steps/s (collection: 7.681s, learning 0.162s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0603
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 7.84s
                        Total time: 1026.26s
                               ETA: 876129.1s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.743s, learning 0.169s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0568
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 7.91s
                        Total time: 1034.17s
                               ETA: 875393.1s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 2044 steps/s (collection: 7.821s, learning 0.194s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0580
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 8.01s
                        Total time: 1042.19s
                               ETA: 874755.2s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.853s, learning 0.240s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0588
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 8.09s
                        Total time: 1050.28s
                               ETA: 874192.6s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.912s, learning 0.166s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0561
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 8.08s
                        Total time: 1058.36s
                               ETA: 873627.6s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.954s, learning 0.169s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0589
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 8.12s
                        Total time: 1066.48s
                               ETA: 873107.9s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.876s, learning 0.185s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0611
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 8.06s
                        Total time: 1074.54s
                               ETA: 872547.1s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.949s, learning 0.195s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0559
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 8.14s
                        Total time: 1082.69s
                               ETA: 872060.9s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.976s, learning 0.163s)
               Value function loss: 1.4913
                    Surrogate loss: 0.0307
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 8.14s
                        Total time: 1090.83s
                               ETA: 871578.5s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.977s, learning 0.165s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0276
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 8.14s
                        Total time: 1098.97s
                               ETA: 871106.5s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 2049 steps/s (collection: 7.812s, learning 0.182s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0403
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 7.99s
                        Total time: 1106.96s
                               ETA: 870524.8s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.843s, learning 0.167s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0336
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 8.01s
                        Total time: 1114.97s
                               ETA: 869965.2s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.847s, learning 0.206s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0428
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 8.05s
                        Total time: 1123.02s
                               ETA: 869447.3s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.821s, learning 0.189s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0392
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 8.01s
                        Total time: 1131.03s
                               ETA: 868904.6s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 2090 steps/s (collection: 7.649s, learning 0.188s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0517
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 7.84s
                        Total time: 1138.87s
                               ETA: 868237.3s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.837s, learning 0.169s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0502
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 8.01s
                        Total time: 1146.88s
                               ETA: 867708.0s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.805s, learning 0.166s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0496
             Mean action noise std: 0.80
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 7.97s
                        Total time: 1154.85s
                               ETA: 867160.4s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 2075 steps/s (collection: 7.704s, learning 0.190s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0423
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 7.89s
                        Total time: 1162.74s
                               ETA: 866563.4s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.781s, learning 0.160s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0573
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 7.94s
                        Total time: 1170.68s
                               ETA: 866010.5s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 2079 steps/s (collection: 7.724s, learning 0.156s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0569
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 7.88s
                        Total time: 1178.56s
                               ETA: 865420.7s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.853s, learning 0.209s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0596
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 8.06s
                        Total time: 1186.62s
                               ETA: 864971.6s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.016s, learning 0.165s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0573
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 8.18s
                        Total time: 1194.81s
                               ETA: 864615.3s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.973s, learning 0.163s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0571
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 8.14s
                        Total time: 1202.94s
                               ETA: 864232.0s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.725s, learning 0.187s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0586
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 7.91s
                        Total time: 1210.85s
                               ETA: 863693.9s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 2080 steps/s (collection: 7.685s, learning 0.190s)
               Value function loss: 1.8372
                    Surrogate loss: 0.0126
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 7.87s
                        Total time: 1218.73s
                               ETA: 863137.1s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.768s, learning 0.173s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0368
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 7.94s
                        Total time: 1226.67s
                               ETA: 862635.0s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.931s, learning 0.164s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0509
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 8.10s
                        Total time: 1234.77s
                               ETA: 862246.9s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.774s, learning 0.197s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0513
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 7.97s
                        Total time: 1242.74s
                               ETA: 861777.8s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 2067 steps/s (collection: 7.739s, learning 0.184s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0508
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 7.92s
                        Total time: 1250.66s
                               ETA: 861282.4s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.890s, learning 0.189s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 8.08s
                        Total time: 1258.74s
                               ETA: 860900.4s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.862s, learning 0.267s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0575
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 8.13s
                        Total time: 1266.87s
                               ETA: 860557.2s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.778s, learning 0.163s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0580
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 7.94s
                        Total time: 1274.81s
                               ETA: 860091.6s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.800s, learning 0.231s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 8.03s
                        Total time: 1282.84s
                               ETA: 859692.3s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 2124 steps/s (collection: 7.528s, learning 0.184s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0608
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 7.71s
                        Total time: 1290.55s
                               ETA: 859086.3s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 2110 steps/s (collection: 7.575s, learning 0.187s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0559
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 7.76s
                        Total time: 1298.32s
                               ETA: 858521.7s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 2051 steps/s (collection: 7.775s, learning 0.210s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0559
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 7.98s
                        Total time: 1306.30s
                               ETA: 858110.1s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.195s, learning 0.165s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0578
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 8.36s
                        Total time: 1314.66s
                               ETA: 857948.9s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1989 steps/s (collection: 7.983s, learning 0.253s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0590
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 8.24s
                        Total time: 1322.90s
                               ETA: 857709.4s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 2073 steps/s (collection: 7.694s, learning 0.208s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0592
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 7.90s
                        Total time: 1330.80s
                               ETA: 857257.5s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.606s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0562
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 8.77s
                        Total time: 1339.57s
                               ETA: 857366.0s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.977s, learning 0.161s)
               Value function loss: 0.6792
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 8.14s
                        Total time: 1347.71s
                               ETA: 857072.1s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 2044 steps/s (collection: 7.827s, learning 0.186s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0474
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 8.01s
                        Total time: 1355.72s
                               ETA: 856702.4s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.802s, learning 0.160s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0529
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 7.96s
                        Total time: 1363.68s
                               ETA: 856305.6s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.891s, learning 0.213s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0521
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 8.10s
                        Total time: 1371.78s
                               ETA: 856001.8s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.895s, learning 0.170s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0505
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 8.07s
                        Total time: 1379.85s
                               ETA: 855677.8s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 2075 steps/s (collection: 7.626s, learning 0.268s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0489
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 7.89s
                        Total time: 1387.74s
                               ETA: 855252.1s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.791s, learning 0.159s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0422
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 7.95s
                        Total time: 1395.69s
                               ETA: 854866.3s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 2096 steps/s (collection: 7.650s, learning 0.167s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0560
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 7.82s
                        Total time: 1403.51s
                               ETA: 854403.3s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 2030 steps/s (collection: 7.897s, learning 0.173s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0566
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 8.07s
                        Total time: 1411.58s
                               ETA: 854099.7s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.154s, learning 0.201s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0583
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 8.35s
                        Total time: 1419.93s
                               ETA: 853970.4s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 2000 steps/s (collection: 7.999s, learning 0.189s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0552
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 8.19s
                        Total time: 1428.12s
                               ETA: 853743.2s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 2102 steps/s (collection: 7.632s, learning 0.159s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0563
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 7.79s
                        Total time: 1435.91s
                               ETA: 853283.0s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 2051 steps/s (collection: 7.822s, learning 0.164s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0582
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 7.99s
                        Total time: 1443.90s
                               ETA: 852943.2s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 2076 steps/s (collection: 7.729s, learning 0.161s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0530
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 7.89s
                        Total time: 1451.79s
                               ETA: 852550.6s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.124s, learning 0.165s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0558
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 8.29s
                        Total time: 1460.08s
                               ETA: 852395.2s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.214s, learning 0.182s)
               Value function loss: 1.8782
                    Surrogate loss: 0.0179
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 8.40s
                        Total time: 1468.47s
                               ETA: 852304.4s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 2110 steps/s (collection: 7.596s, learning 0.166s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0314
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 7.76s
                        Total time: 1476.24s
                               ETA: 851848.3s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.021s, learning 0.184s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0440
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 8.21s
                        Total time: 1484.44s
                               ETA: 851651.5s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 2059 steps/s (collection: 7.789s, learning 0.164s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0360
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 7.95s
                        Total time: 1492.40s
                               ETA: 851313.4s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.863s, learning 0.266s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0412
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 8.13s
                        Total time: 1500.52s
                               ETA: 851078.6s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.050s, learning 0.165s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0442
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 8.22s
                        Total time: 1508.74s
                               ETA: 850895.1s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.875s, learning 0.210s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0548
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 8.08s
                        Total time: 1516.82s
                               ETA: 850639.9s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.841s, learning 0.167s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0499
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 8.01s
                        Total time: 1524.83s
                               ETA: 850345.3s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 2003 steps/s (collection: 7.982s, learning 0.195s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0500
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 8.18s
                        Total time: 1533.01s
                               ETA: 850147.4s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.780s, learning 0.170s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0565
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 7.95s
                        Total time: 1540.96s
                               ETA: 849826.3s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.140s, learning 0.169s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0519
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 8.31s
                        Total time: 1549.27s
                               ETA: 849705.6s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.941s, learning 0.163s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0418
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 8.10s
                        Total time: 1557.37s
                               ETA: 849474.0s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.782s, learning 0.179s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0498
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 7.96s
                        Total time: 1565.33s
                               ETA: 849167.7s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 2065 steps/s (collection: 7.766s, learning 0.165s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0555
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 7.93s
                        Total time: 1573.26s
                               ETA: 848848.1s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.905s, learning 0.158s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0510
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 8.06s
                        Total time: 1581.33s
                               ETA: 848602.7s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.773s, learning 0.173s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0554
             Mean action noise std: 0.79
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 7.95s
                        Total time: 1589.27s
                               ETA: 848297.7s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 2004 steps/s (collection: 7.990s, learning 0.184s)
               Value function loss: 1.4467
                    Surrogate loss: -0.0010
             Mean action noise std: 0.79
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 8.17s
                        Total time: 1597.45s
                               ETA: 848116.9s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.923s, learning 0.193s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0391
             Mean action noise std: 0.79
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 8.12s
                        Total time: 1605.56s
                               ETA: 847907.2s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 2075 steps/s (collection: 7.726s, learning 0.169s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0269
             Mean action noise std: 0.79
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 7.90s
                        Total time: 1613.46s
                               ETA: 847583.6s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.797s, learning 0.167s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0444
             Mean action noise std: 0.79
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 7.96s
                        Total time: 1621.42s
                               ETA: 847298.9s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.890s, learning 0.165s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0356
             Mean action noise std: 0.79
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 8.05s
                        Total time: 1629.48s
                               ETA: 847064.4s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 2093 steps/s (collection: 7.656s, learning 0.170s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0472
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 7.83s
                        Total time: 1637.30s
                               ETA: 846714.0s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.957s, learning 0.172s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0458
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 8.13s
                        Total time: 1645.43s
                               ETA: 846523.3s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.926s, learning 0.170s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0449
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 8.10s
                        Total time: 1653.53s
                               ETA: 846317.3s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 2069 steps/s (collection: 7.749s, learning 0.166s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0429
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 7.92s
                        Total time: 1661.44s
                               ETA: 846021.4s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.972s, learning 0.174s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0464
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 8.15s
                        Total time: 1669.59s
                               ETA: 845845.5s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 2052 steps/s (collection: 7.818s, learning 0.165s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0477
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 7.98s
                        Total time: 1677.57s
                               ETA: 845588.9s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.965s, learning 0.164s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0502
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 8.13s
                        Total time: 1685.70s
                               ETA: 845408.0s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.972s, learning 0.185s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0450
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 8.16s
                        Total time: 1693.86s
                               ETA: 845242.9s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 2039 steps/s (collection: 7.866s, learning 0.166s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0451
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 8.03s
                        Total time: 1701.89s
                               ETA: 845017.4s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.866s, learning 0.162s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0485
             Mean action noise std: 0.78
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 8.03s
                        Total time: 1709.92s
                               ETA: 844791.7s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.039s, learning 0.164s)
               Value function loss: 1.6720
                    Surrogate loss: 0.0536
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 8.20s
                        Total time: 1718.12s
                               ETA: 844654.4s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1997 steps/s (collection: 7.984s, learning 0.219s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0287
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 8.20s
                        Total time: 1726.32s
                               ETA: 844518.3s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 2090 steps/s (collection: 7.653s, learning 0.183s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0302
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 7.84s
                        Total time: 1734.16s
                               ETA: 844204.7s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.967s, learning 0.201s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0399
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 8.17s
                        Total time: 1742.32s
                               ETA: 844054.7s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.029s, learning 0.186s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0328
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 8.21s
                        Total time: 1750.54s
                               ETA: 843929.0s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.885s, learning 0.189s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0445
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 8.07s
                        Total time: 1758.61s
                               ETA: 843736.9s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.197s, learning 0.165s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0449
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 8.36s
                        Total time: 1766.98s
                               ETA: 843684.3s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 2054 steps/s (collection: 7.807s, learning 0.169s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0501
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 7.98s
                        Total time: 1774.95s
                               ETA: 843448.6s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 2090 steps/s (collection: 7.664s, learning 0.173s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0454
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 7.84s
                        Total time: 1782.79s
                               ETA: 843149.2s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.024s, learning 0.205s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0507
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 8.23s
                        Total time: 1791.02s
                               ETA: 843037.0s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.064s, learning 0.166s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0457
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 8.23s
                        Total time: 1799.25s
                               ETA: 842926.3s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 2051 steps/s (collection: 7.802s, learning 0.184s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0475
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 7.99s
                        Total time: 1807.23s
                               ETA: 842702.9s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.862s, learning 0.197s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0499
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 8.06s
                        Total time: 1815.29s
                               ETA: 842515.4s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.033s, learning 0.192s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0447
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 8.22s
                        Total time: 1823.52s
                               ETA: 842405.7s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.303s, learning 0.164s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0473
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 8.47s
                        Total time: 1831.98s
                               ETA: 842408.3s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.128s, learning 0.168s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0416
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 8.30s
                        Total time: 1840.28s
                               ETA: 842332.9s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.978s, learning 0.161s)
               Value function loss: 1.7563
                    Surrogate loss: 0.0075
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 8.14s
                        Total time: 1848.42s
                               ETA: 842186.7s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 2062 steps/s (collection: 7.737s, learning 0.205s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0226
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 7.94s
                        Total time: 1856.36s
                               ETA: 841952.2s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 2068 steps/s (collection: 7.736s, learning 0.184s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0263
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 7.92s
                        Total time: 1864.28s
                               ETA: 841709.9s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.021s, learning 0.162s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 8.18s
                        Total time: 1872.46s
                               ETA: 841587.9s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 2122 steps/s (collection: 7.559s, learning 0.161s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 7.72s
                        Total time: 1880.18s
                               ETA: 841259.9s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.784s, learning 0.187s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0200
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 7.97s
                        Total time: 1888.16s
                               ETA: 841046.7s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 2071 steps/s (collection: 7.741s, learning 0.167s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0268
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 7.91s
                        Total time: 1896.06s
                               ETA: 840807.2s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.886s, learning 0.165s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0283
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 8.05s
                        Total time: 1904.12s
                               ETA: 840633.1s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.907s, learning 0.171s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0212
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 8.08s
                        Total time: 1912.19s
                               ETA: 840472.1s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.043s, learning 0.172s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0386
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 8.22s
                        Total time: 1920.41s
                               ETA: 840372.5s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.954s, learning 0.188s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0483
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 8.14s
                        Total time: 1928.55s
                               ETA: 840241.8s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.917s, learning 0.166s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0465
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 8.08s
                        Total time: 1936.63s
                               ETA: 840086.5s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.059s, learning 0.161s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0476
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 8.22s
                        Total time: 1944.85s
                               ETA: 839991.9s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.114s, learning 0.167s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0447
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 8.28s
                        Total time: 1953.13s
                               ETA: 839923.8s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.928s, learning 0.167s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0428
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 8.09s
                        Total time: 1961.23s
                               ETA: 839776.7s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.901s, learning 0.171s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0425
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 8.07s
                        Total time: 1969.30s
                               ETA: 839621.0s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 2080 steps/s (collection: 7.718s, learning 0.156s)
               Value function loss: 1.2290
                    Surrogate loss: -0.0014
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 7.87s
                        Total time: 1977.18s
                               ETA: 839382.5s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.860s, learning 0.165s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0372
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 8.02s
                        Total time: 1985.20s
                               ETA: 839209.7s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.842s, learning 0.163s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0412
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 8.01s
                        Total time: 1993.21s
                               ETA: 839030.2s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.862s, learning 0.175s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0415
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 8.04s
                        Total time: 2001.24s
                               ETA: 838865.4s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.860s, learning 0.236s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0411
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 8.10s
                        Total time: 2009.34s
                               ETA: 838726.4s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.899s, learning 0.167s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0421
             Mean action noise std: 0.78
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 8.07s
                        Total time: 2017.40s
                               ETA: 838576.1s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 2049 steps/s (collection: 7.826s, learning 0.167s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0410
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 7.99s
                        Total time: 2025.40s
                               ETA: 838396.5s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.743s, learning 0.170s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0510
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 7.91s
                        Total time: 2033.31s
                               ETA: 838185.7s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.159s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0462
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 8.18s
                        Total time: 2041.49s
                               ETA: 838086.5s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 2044 steps/s (collection: 7.848s, learning 0.165s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0467
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 8.01s
                        Total time: 2049.50s
                               ETA: 837919.6s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 2058 steps/s (collection: 7.770s, learning 0.189s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0473
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 7.96s
                        Total time: 2057.46s
                               ETA: 837731.5s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 2077 steps/s (collection: 7.721s, learning 0.166s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0466
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 7.89s
                        Total time: 2065.35s
                               ETA: 837516.1s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.897s, learning 0.187s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0444
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 8.08s
                        Total time: 2073.43s
                               ETA: 837381.6s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.997s, learning 0.167s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0499
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 8.16s
                        Total time: 2081.60s
                               ETA: 837280.4s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 2065 steps/s (collection: 7.766s, learning 0.168s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0473
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 7.93s
                        Total time: 2089.53s
                               ETA: 837087.5s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.066s, learning 0.163s)
               Value function loss: 2.3826
                    Surrogate loss: 0.0444
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 8.23s
                        Total time: 2097.76s
                               ETA: 837014.2s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 2089 steps/s (collection: 7.679s, learning 0.164s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0266
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 7.84s
                        Total time: 2105.60s
                               ETA: 836787.7s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.840s, learning 0.163s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0339
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 8.00s
                        Total time: 2113.60s
                               ETA: 836626.7s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.823s, learning 0.186s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0349
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 8.01s
                        Total time: 2121.61s
                               ETA: 836469.2s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.771s, learning 0.200s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0394
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 7.97s
                        Total time: 2129.58s
                               ETA: 836297.8s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.852s, learning 0.195s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0379
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 8.05s
                        Total time: 2137.63s
                               ETA: 836157.8s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.912s, learning 0.180s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0384
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 8.09s
                        Total time: 2145.72s
                               ETA: 836036.3s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 2064 steps/s (collection: 7.778s, learning 0.158s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0398
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 7.94s
                        Total time: 2153.66s
                               ETA: 835854.9s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.873s, learning 0.176s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0456
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 8.05s
                        Total time: 2161.71s
                               ETA: 835718.7s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.948s, learning 0.161s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0384
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 8.11s
                        Total time: 2169.82s
                               ETA: 835606.4s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.029s, learning 0.160s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0451
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 8.19s
                        Total time: 2178.01s
                               ETA: 835525.5s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 2066 steps/s (collection: 7.765s, learning 0.165s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0443
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 7.93s
                        Total time: 2185.94s
                               ETA: 835346.2s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.897s, learning 0.213s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0455
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 8.11s
                        Total time: 2194.05s
                               ETA: 835236.9s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 2079 steps/s (collection: 7.710s, learning 0.169s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0483
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 7.88s
                        Total time: 2201.93s
                               ETA: 835040.9s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 2005 steps/s (collection: 7.985s, learning 0.185s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0455
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 8.17s
                        Total time: 2210.10s
                               ETA: 834956.0s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.907s, learning 0.164s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0425
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 8.07s
                        Total time: 2218.17s
                               ETA: 834834.6s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.921s, learning 0.162s)
               Value function loss: 1.8539
                    Surrogate loss: 0.0040
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 8.08s
                        Total time: 2226.25s
                               ETA: 834718.5s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.034s, learning 0.170s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0336
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 8.20s
                        Total time: 2234.45s
                               ETA: 834648.4s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.066s, learning 0.158s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0402
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 8.22s
                        Total time: 2242.68s
                               ETA: 834586.2s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 2088 steps/s (collection: 7.605s, learning 0.238s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0370
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 7.84s
                        Total time: 2250.52s
                               ETA: 834383.2s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.881s, learning 0.193s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0367
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 8.07s
                        Total time: 2258.60s
                               ETA: 834266.9s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 2114 steps/s (collection: 7.514s, learning 0.236s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0353
             Mean action noise std: 0.77
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 7.75s
                        Total time: 2266.35s
                               ETA: 834032.2s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 2066 steps/s (collection: 7.726s, learning 0.202s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0343
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 7.93s
                        Total time: 2274.28s
                               ETA: 833864.7s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 2067 steps/s (collection: 7.756s, learning 0.170s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0404
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 7.93s
                        Total time: 2282.20s
                               ETA: 833697.4s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.849s, learning 0.171s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0420
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 8.02s
                        Total time: 2290.22s
                               ETA: 833565.4s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.073s, learning 0.169s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0421
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 8.24s
                        Total time: 2298.46s
                               ETA: 833514.9s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.248s, learning 0.170s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0421
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 8.42s
                        Total time: 2306.88s
                               ETA: 833528.2s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.187s, learning 0.168s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0445
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 8.36s
                        Total time: 2315.24s
                               ETA: 833518.8s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.260s, learning 0.159s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0417
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 8.42s
                        Total time: 2323.66s
                               ETA: 833532.4s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.094s, learning 0.165s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0392
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 8.26s
                        Total time: 2331.92s
                               ETA: 833488.4s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.931s, learning 0.163s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0442
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 8.09s
                        Total time: 2340.01s
                               ETA: 833386.0s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.914s, learning 0.192s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0425
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 8.11s
                        Total time: 2348.11s
                               ETA: 833288.2s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.799s, learning 0.205s)
               Value function loss: 0.7277
                    Surrogate loss: 0.0016
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 8.00s
                        Total time: 2356.12s
                               ETA: 833155.1s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 2053 steps/s (collection: 7.811s, learning 0.168s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0311
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 7.98s
                        Total time: 2364.10s
                               ETA: 833014.2s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.940s, learning 0.162s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0375
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 8.10s
                        Total time: 2372.20s
                               ETA: 832917.4s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.851s, learning 0.196s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0384
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 8.05s
                        Total time: 2380.25s
                               ETA: 832802.0s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 2058 steps/s (collection: 7.768s, learning 0.191s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0367
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 7.96s
                        Total time: 2388.20s
                               ETA: 832656.8s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.717s, learning 0.197s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 7.91s
                        Total time: 2396.12s
                               ETA: 832496.8s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 2094 steps/s (collection: 7.662s, learning 0.161s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 7.82s
                        Total time: 2403.94s
                               ETA: 832306.3s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 2066 steps/s (collection: 7.734s, learning 0.192s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0360
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 7.93s
                        Total time: 2411.87s
                               ETA: 832152.8s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.031s, learning 0.177s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0385
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 8.21s
                        Total time: 2420.08s
                               ETA: 832097.3s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1994 steps/s (collection: 7.937s, learning 0.278s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0384
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 8.21s
                        Total time: 2428.29s
                               ETA: 832044.3s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.959s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 8.13s
                        Total time: 2436.42s
                               ETA: 831961.4s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 2081 steps/s (collection: 7.709s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0368
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 7.87s
                        Total time: 2444.29s
                               ETA: 831792.3s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 2052 steps/s (collection: 7.816s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0419
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 7.98s
                        Total time: 2452.27s
                               ETA: 831661.2s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.261s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0381
             Mean action noise std: 0.76
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 8.43s
                        Total time: 2460.70s
                               ETA: 831683.0s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.971s, learning 0.174s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0424
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 8.14s
                        Total time: 2468.84s
                               ETA: 831608.5s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.162s, learning 0.163s)
               Value function loss: 1.9501
                    Surrogate loss: 0.0012
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 8.32s
                        Total time: 2477.17s
                               ETA: 831594.6s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 2121 steps/s (collection: 7.568s, learning 0.156s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 7.72s
                        Total time: 2484.89s
                               ETA: 831379.9s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 2062 steps/s (collection: 7.740s, learning 0.202s)
               Value function loss: 0.0488
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 7.94s
                        Total time: 2492.83s
                               ETA: 831239.4s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.078s, learning 0.188s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 8.27s
                        Total time: 2501.10s
                               ETA: 831207.4s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 2141 steps/s (collection: 7.458s, learning 0.193s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0230
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 7.65s
                        Total time: 2508.75s
                               ETA: 830971.7s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 2111 steps/s (collection: 7.603s, learning 0.158s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 7.76s
                        Total time: 2516.51s
                               ETA: 830773.8s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.057s, learning 0.162s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 8.22s
                        Total time: 2524.73s
                               ETA: 830727.9s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.917s, learning 0.205s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0292
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 8.12s
                        Total time: 2532.85s
                               ETA: 830650.6s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.872s, learning 0.171s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 8.04s
                        Total time: 2540.90s
                               ETA: 830547.8s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 2077 steps/s (collection: 7.702s, learning 0.182s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 7.88s
                        Total time: 2548.78s
                               ETA: 830394.1s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.162s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0306
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 8.43s
                        Total time: 2557.21s
                               ETA: 830419.3s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.887s, learning 0.156s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0354
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 8.04s
                        Total time: 2565.25s
                               ETA: 830318.0s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.045s, learning 0.180s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0335
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 8.23s
                        Total time: 2573.48s
                               ETA: 830276.3s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.902s, learning 0.171s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0389
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 8.07s
                        Total time: 2581.55s
                               ETA: 830186.0s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 2065 steps/s (collection: 7.765s, learning 0.168s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0357
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 7.93s
                        Total time: 2589.49s
                               ETA: 830051.2s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.166s, learning 0.166s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0345
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 8.33s
                        Total time: 2597.82s
                               ETA: 830044.7s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 2107 steps/s (collection: 7.611s, learning 0.162s)
               Value function loss: 1.3969
                    Surrogate loss: 0.0137
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 7.77s
                        Total time: 2605.59s
                               ETA: 829860.1s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 2108 steps/s (collection: 7.608s, learning 0.164s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0269
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 7.77s
                        Total time: 2613.36s
                               ETA: 829676.2s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.873s, learning 0.191s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0322
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 8.06s
                        Total time: 2621.43s
                               ETA: 829585.9s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 2043 steps/s (collection: 7.855s, learning 0.161s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 8.02s
                        Total time: 2629.44s
                               ETA: 829481.0s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1996 steps/s (collection: 7.983s, learning 0.224s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0351
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 8.21s
                        Total time: 2637.65s
                               ETA: 829436.8s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.146s, learning 0.165s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0339
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 8.31s
                        Total time: 2645.96s
                               ETA: 829425.6s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.914s, learning 0.162s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 8.08s
                        Total time: 2654.04s
                               ETA: 829340.8s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.841s, learning 0.298s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 8.14s
                        Total time: 2662.18s
                               ETA: 829276.2s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.075s, learning 0.189s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0343
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 8.26s
                        Total time: 2670.44s
                               ETA: 829250.9s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1989 steps/s (collection: 7.999s, learning 0.237s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0376
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 8.24s
                        Total time: 2678.68s
                               ETA: 829216.9s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.159s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0428
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 8.27s
                        Total time: 2686.95s
                               ETA: 829194.1s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.835s, learning 0.312s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0464
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 8.15s
                        Total time: 2695.10s
                               ETA: 829133.1s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.968s, learning 0.161s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0395
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 8.13s
                        Total time: 2703.22s
                               ETA: 829066.5s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.967s, learning 0.192s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0434
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 8.16s
                        Total time: 2711.38s
                               ETA: 829009.8s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 2072 steps/s (collection: 7.741s, learning 0.163s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 7.90s
                        Total time: 2719.29s
                               ETA: 828875.6s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.103s, learning 0.164s)
               Value function loss: 2.1658
                    Surrogate loss: 0.0406
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 8.27s
                        Total time: 2727.56s
                               ETA: 828852.4s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.883s, learning 0.163s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0280
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 8.05s
                        Total time: 2735.60s
                               ETA: 828762.5s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.873s, learning 0.165s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0242
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 8.04s
                        Total time: 2743.64s
                               ETA: 828670.6s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 2043 steps/s (collection: 7.848s, learning 0.171s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0250
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 8.02s
                        Total time: 2751.66s
                               ETA: 828573.6s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.077s, learning 0.171s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0382
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 8.25s
                        Total time: 2759.91s
                               ETA: 828545.8s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.891s, learning 0.157s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 8.05s
                        Total time: 2767.96s
                               ETA: 828458.4s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 2124 steps/s (collection: 7.529s, learning 0.182s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0347
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 7.71s
                        Total time: 2775.67s
                               ETA: 828270.8s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 2081 steps/s (collection: 7.701s, learning 0.171s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0357
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 7.87s
                        Total time: 2783.54s
                               ETA: 828132.0s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 2048 steps/s (collection: 7.838s, learning 0.161s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0343
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 8.00s
                        Total time: 2791.54s
                               ETA: 828031.6s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.125s, learning 0.261s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0433
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 8.39s
                        Total time: 2799.92s
                               ETA: 828046.3s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.133s, learning 0.258s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0403
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 8.39s
                        Total time: 2808.32s
                               ETA: 828062.5s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.133s, learning 0.201s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0449
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 8.33s
                        Total time: 2816.65s
                               ETA: 828061.9s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 2080 steps/s (collection: 7.710s, learning 0.164s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0414
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 7.87s
                        Total time: 2824.52s
                               ETA: 827926.0s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 2059 steps/s (collection: 7.766s, learning 0.188s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0443
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 7.95s
                        Total time: 2832.48s
                               ETA: 827814.4s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.893s, learning 0.187s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0446
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 8.08s
                        Total time: 2840.56s
                               ETA: 827740.2s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 2076 steps/s (collection: 7.727s, learning 0.161s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0459
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 7.89s
                        Total time: 2848.45s
                               ETA: 827610.6s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.276s, learning 0.161s)
               Value function loss: 2.1682
                    Surrogate loss: 0.0259
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 8.44s
                        Total time: 2856.88s
                               ETA: 827640.6s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.726s, learning 0.214s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0325
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 7.94s
                        Total time: 2864.82s
                               ETA: 827527.0s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.895s, learning 0.169s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0329
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 8.06s
                        Total time: 2872.89s
                               ETA: 827449.5s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.155s, learning 0.161s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0361
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 8.32s
                        Total time: 2881.20s
                               ETA: 827445.0s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.973s, learning 0.188s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 8.16s
                        Total time: 2889.36s
                               ETA: 827396.0s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.758s, learning 0.189s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 7.95s
                        Total time: 2897.31s
                               ETA: 827286.0s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 2062 steps/s (collection: 7.765s, learning 0.181s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0400
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 7.95s
                        Total time: 2905.26s
                               ETA: 827176.2s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 2081 steps/s (collection: 7.684s, learning 0.188s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0421
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 7.87s
                        Total time: 2913.13s
                               ETA: 827046.0s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.120s, learning 0.275s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 8.39s
                        Total time: 2921.52s
                               ETA: 827064.6s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.054s, learning 0.192s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0443
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 8.25s
                        Total time: 2929.77s
                               ETA: 827041.0s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 2056 steps/s (collection: 7.805s, learning 0.162s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0406
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 7.97s
                        Total time: 2937.73s
                               ETA: 826939.0s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.965s, learning 0.159s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0456
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 8.12s
                        Total time: 2945.86s
                               ETA: 826881.7s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.974s, learning 0.162s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0428
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 8.14s
                        Total time: 2953.99s
                               ETA: 826827.9s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.108s, learning 0.186s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0423
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 8.29s
                        Total time: 2962.29s
                               ETA: 826818.4s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.911s, learning 0.161s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0375
             Mean action noise std: 0.75
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 8.07s
                        Total time: 2970.36s
                               ETA: 826747.0s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 2075 steps/s (collection: 7.686s, learning 0.209s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0472
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 7.90s
                        Total time: 2978.25s
                               ETA: 826627.2s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.027s, learning 0.158s)
               Value function loss: 1.3966
                    Surrogate loss: 0.0162
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 8.18s
                        Total time: 2986.44s
                               ETA: 826588.2s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.019s, learning 0.185s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0275
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 8.20s
                        Total time: 2994.64s
                               ETA: 826554.5s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 2065 steps/s (collection: 7.771s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0328
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 7.93s
                        Total time: 3002.58s
                               ETA: 826446.7s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 2092 steps/s (collection: 7.671s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 7.83s
                        Total time: 3010.41s
                               ETA: 826311.1s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 2113 steps/s (collection: 7.595s, learning 0.158s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0411
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 7.75s
                        Total time: 3018.16s
                               ETA: 826154.9s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.893s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 8.05s
                        Total time: 3026.21s
                               ETA: 826081.4s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1998 steps/s (collection: 7.981s, learning 0.216s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0413
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 8.20s
                        Total time: 3034.41s
                               ETA: 826047.4s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.973s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0413
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 8.13s
                        Total time: 3042.54s
                               ETA: 825996.4s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.974s, learning 0.157s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0414
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 8.13s
                        Total time: 3050.67s
                               ETA: 825944.8s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.937s, learning 0.157s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 8.09s
                        Total time: 3058.77s
                               ETA: 825883.8s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.932s, learning 0.157s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0420
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 8.09s
                        Total time: 3066.86s
                               ETA: 825821.6s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 2033 steps/s (collection: 7.849s, learning 0.208s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0430
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 8.06s
                        Total time: 3074.91s
                               ETA: 825750.9s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.143s, learning 0.188s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0434
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 8.33s
                        Total time: 3083.24s
                               ETA: 825754.3s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.847s, learning 0.255s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0438
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 8.10s
                        Total time: 3091.35s
                               ETA: 825696.1s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.143s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0440
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 8.30s
                        Total time: 3099.65s
                               ETA: 825691.8s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.869s, learning 0.158s)
               Value function loss: 2.4899
                    Surrogate loss: 0.0498
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 8.03s
                        Total time: 3107.68s
                               ETA: 825614.3s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 2059 steps/s (collection: 7.791s, learning 0.165s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0230
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 7.96s
                        Total time: 3115.63s
                               ETA: 825518.2s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 2065 steps/s (collection: 7.737s, learning 0.194s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0254
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 7.93s
                        Total time: 3123.56s
                               ETA: 825415.9s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 2064 steps/s (collection: 7.727s, learning 0.209s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0316
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 7.94s
                        Total time: 3131.50s
                               ETA: 825315.8s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.778s, learning 0.173s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0298
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 7.95s
                        Total time: 3139.45s
                               ETA: 825220.1s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 2089 steps/s (collection: 7.623s, learning 0.220s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0353
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 7.84s
                        Total time: 3147.29s
                               ETA: 825096.3s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.945s, learning 0.160s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 8.10s
                        Total time: 3155.40s
                               ETA: 825041.6s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 2006 steps/s (collection: 8.007s, learning 0.160s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0366
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 8.17s
                        Total time: 3163.57s
                               ETA: 825003.2s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 2090 steps/s (collection: 7.671s, learning 0.167s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0418
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 7.84s
                        Total time: 3171.40s
                               ETA: 824879.7s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.854s, learning 0.166s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0422
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 8.02s
                        Total time: 3179.42s
                               ETA: 824803.9s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.141s, learning 0.166s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 8.31s
                        Total time: 3187.73s
                               ETA: 824802.7s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.774s, learning 0.166s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0446
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 7.94s
                        Total time: 3195.67s
                               ETA: 824706.6s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.956s, learning 0.203s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0425
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 8.16s
                        Total time: 3203.83s
                               ETA: 824667.7s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.743s, learning 0.171s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0468
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 7.91s
                        Total time: 3211.74s
                               ETA: 824565.7s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 2033 steps/s (collection: 7.872s, learning 0.186s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0436
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 8.06s
                        Total time: 3219.80s
                               ETA: 824501.4s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 2072 steps/s (collection: 7.741s, learning 0.165s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0434
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 7.91s
                        Total time: 3227.71s
                               ETA: 824398.4s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.975s, learning 0.161s)
               Value function loss: 1.8253
                    Surrogate loss: -0.0009
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 8.14s
                        Total time: 3235.85s
                               ETA: 824354.4s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.038s, learning 0.178s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0282
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 8.22s
                        Total time: 3244.06s
                               ETA: 824330.9s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.940s, learning 0.166s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0327
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 8.11s
                        Total time: 3252.17s
                               ETA: 824279.6s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 2118 steps/s (collection: 7.570s, learning 0.163s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0358
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 7.73s
                        Total time: 3259.90s
                               ETA: 824134.1s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.925s, learning 0.162s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0365
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 8.09s
                        Total time: 3267.99s
                               ETA: 824078.5s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 2005 steps/s (collection: 7.878s, learning 0.290s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0331
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 8.17s
                        Total time: 3276.15s
                               ETA: 824043.8s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 2071 steps/s (collection: 7.724s, learning 0.187s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0444
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 7.91s
                        Total time: 3284.07s
                               ETA: 823944.7s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 2107 steps/s (collection: 7.615s, learning 0.161s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0432
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 7.78s
                        Total time: 3291.84s
                               ETA: 823812.1s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.780s, learning 0.159s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0403
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 7.94s
                        Total time: 3299.78s
                               ETA: 823720.8s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.079s, learning 0.157s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0417
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 8.24s
                        Total time: 3308.02s
                               ETA: 823704.2s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.046s, learning 0.172s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0450
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 8.22s
                        Total time: 3316.23s
                               ETA: 823682.9s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.946s, learning 0.185s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0464
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 8.13s
                        Total time: 3324.36s
                               ETA: 823640.1s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.289s, learning 0.170s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0435
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 8.46s
                        Total time: 3332.82s
                               ETA: 823678.6s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.063s, learning 0.223s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0433
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 8.29s
                        Total time: 3341.11s
                               ETA: 823674.1s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.161s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0434
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 8.29s
                        Total time: 3349.40s
                               ETA: 823671.1s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.931s, learning 0.163s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0472
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 8.09s
                        Total time: 3357.49s
                               ETA: 823619.5s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.105s, learning 0.167s)
               Value function loss: 0.7352
                    Surrogate loss: 0.0029
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 8.27s
                        Total time: 3365.77s
                               ETA: 823612.0s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.746s, learning 0.217s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0265
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 7.96s
                        Total time: 3373.73s
                               ETA: 823528.8s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.729s, learning 0.221s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0363
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 7.95s
                        Total time: 3381.68s
                               ETA: 823443.0s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 2002 steps/s (collection: 7.922s, learning 0.258s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0391
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 8.18s
                        Total time: 3389.86s
                               ETA: 823413.4s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.877s, learning 0.190s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 8.07s
                        Total time: 3397.93s
                               ETA: 823356.4s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.100s, learning 0.164s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 8.26s
                        Total time: 3406.19s
                               ETA: 823347.1s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 2123 steps/s (collection: 7.556s, learning 0.161s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0350
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 7.72s
                        Total time: 3413.91s
                               ETA: 823205.9s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.824s, learning 0.186s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 8.01s
                        Total time: 3421.92s
                               ETA: 823135.9s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 2076 steps/s (collection: 7.725s, learning 0.167s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0441
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 7.89s
                        Total time: 3429.81s
                               ETA: 823038.0s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.048s, learning 0.178s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0438
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 8.23s
                        Total time: 3438.03s
                               ETA: 823020.5s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.041s, learning 0.309s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0481
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 8.35s
                        Total time: 3446.38s
                               ETA: 823032.7s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.117s, learning 0.170s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0450
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 8.29s
                        Total time: 3454.67s
                               ETA: 823029.7s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.836s, learning 0.168s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0420
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 8.00s
                        Total time: 3462.67s
                               ETA: 822959.6s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.106s, learning 0.226s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0463
             Mean action noise std: 0.74
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 8.33s
                        Total time: 3471.01s
                               ETA: 822967.5s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.069s, learning 0.169s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0485
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 8.24s
                        Total time: 3479.24s
                               ETA: 822953.0s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.162s, learning 0.155s)
               Value function loss: 1.8694
                    Surrogate loss: 0.0067
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 8.32s
                        Total time: 3487.56s
                               ETA: 822957.1s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 2147 steps/s (collection: 7.460s, learning 0.169s)
               Value function loss: 0.0657
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 7.63s
                        Total time: 3495.19s
                               ETA: 822799.4s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 2084 steps/s (collection: 7.694s, learning 0.167s)
               Value function loss: 0.0549
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 7.86s
                        Total time: 3503.05s
                               ETA: 822696.7s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 2089 steps/s (collection: 7.677s, learning 0.165s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 7.84s
                        Total time: 3510.89s
                               ETA: 822590.1s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 2079 steps/s (collection: 7.713s, learning 0.167s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0247
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 7.88s
                        Total time: 3518.77s
                               ETA: 822492.6s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.953s, learning 0.167s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 8.12s
                        Total time: 3526.89s
                               ETA: 822451.8s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.833s, learning 0.169s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0246
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 8.00s
                        Total time: 3534.90s
                               ETA: 822383.7s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 2048 steps/s (collection: 7.805s, learning 0.193s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0237
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 8.00s
                        Total time: 3542.89s
                               ETA: 822314.9s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.292s, learning 0.164s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0312
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 8.46s
                        Total time: 3551.35s
                               ETA: 822352.2s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.921s, learning 0.170s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 8.09s
                        Total time: 3559.44s
                               ETA: 822305.2s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.870s, learning 0.195s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 8.06s
                        Total time: 3567.51s
                               ETA: 822252.2s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.886s, learning 0.173s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0478
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 8.06s
                        Total time: 3575.56s
                               ETA: 822198.2s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.810s, learning 0.162s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0460
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 7.97s
                        Total time: 3583.54s
                               ETA: 822124.4s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.119s, learning 0.181s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0488
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 8.30s
                        Total time: 3591.84s
                               ETA: 822125.9s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 2049 steps/s (collection: 7.829s, learning 0.165s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0494
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 7.99s
                        Total time: 3599.83s
                               ETA: 822057.6s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.940s, learning 0.160s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0513
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 8.10s
                        Total time: 3607.93s
                               ETA: 822013.8s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 2057 steps/s (collection: 7.778s, learning 0.187s)
               Value function loss: 1.5716
                    Surrogate loss: 0.0161
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 7.96s
                        Total time: 3615.90s
                               ETA: 821939.4s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.875s, learning 0.161s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0353
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 8.04s
                        Total time: 3623.93s
                               ETA: 821881.5s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.973s, learning 0.175s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0381
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 8.15s
                        Total time: 3632.08s
                               ETA: 821849.0s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 2084 steps/s (collection: 7.675s, learning 0.187s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0366
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 7.86s
                        Total time: 3639.94s
                               ETA: 821752.0s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.013s, learning 0.173s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0413
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 8.19s
                        Total time: 3648.13s
                               ETA: 821728.5s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.004s, learning 0.175s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0425
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 8.18s
                        Total time: 3656.31s
                               ETA: 821703.4s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.775s, learning 0.172s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0435
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 7.95s
                        Total time: 3664.25s
                               ETA: 821626.4s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.024s, learning 0.169s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0424
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 8.19s
                        Total time: 3672.45s
                               ETA: 821604.8s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.864s, learning 0.165s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0469
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 8.03s
                        Total time: 3680.48s
                               ETA: 821546.5s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.915s, learning 0.209s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0493
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 8.12s
                        Total time: 3688.60s
                               ETA: 821509.6s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.938s, learning 0.172s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0480
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 8.11s
                        Total time: 3696.71s
                               ETA: 821469.7s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.865s, learning 0.255s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0465
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 8.12s
                        Total time: 3704.83s
                               ETA: 821432.4s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 2119 steps/s (collection: 7.574s, learning 0.158s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0485
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 7.73s
                        Total time: 3712.56s
                               ETA: 821309.1s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.927s, learning 0.159s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0502
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 8.09s
                        Total time: 3720.65s
                               ETA: 821264.5s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 2076 steps/s (collection: 7.705s, learning 0.186s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0461
             Mean action noise std: 0.73
                       Mean reward: 13.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 7.89s
                        Total time: 3728.54s
                               ETA: 821177.2s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.850s, learning 0.188s)
               Value function loss: 2.5347
                    Surrogate loss: 0.0814
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 8.04s
                        Total time: 3736.57s
                               ETA: 821122.5s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 2105 steps/s (collection: 7.616s, learning 0.165s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0308
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 7.78s
                        Total time: 3744.36s
                               ETA: 821011.7s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.161s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0334
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 8.10s
                        Total time: 3752.46s
                               ETA: 820971.9s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.054s, learning 0.212s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 8.27s
                        Total time: 3760.72s
                               ETA: 820967.8s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.907s, learning 0.167s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0425
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 8.07s
                        Total time: 3768.80s
                               ETA: 820921.8s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.950s, learning 0.160s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 8.11s
                        Total time: 3776.91s
                               ETA: 820884.0s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.023s, learning 0.162s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0473
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 8.18s
                        Total time: 3785.09s
                               ETA: 820862.3s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 2051 steps/s (collection: 7.791s, learning 0.195s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0457
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 7.99s
                        Total time: 3793.08s
                               ETA: 820797.7s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.058s, learning 0.169s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0483
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 8.23s
                        Total time: 3801.31s
                               ETA: 820785.4s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.743s, learning 0.168s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0464
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 7.91s
                        Total time: 3809.22s
                               ETA: 820705.1s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 2104 steps/s (collection: 7.627s, learning 0.159s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0475
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 7.79s
                        Total time: 3817.00s
                               ETA: 820598.0s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.910s, learning 0.184s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0481
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 8.09s
                        Total time: 3825.10s
                               ETA: 820557.4s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.027s, learning 0.162s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0477
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 8.19s
                        Total time: 3833.29s
                               ETA: 820537.5s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 2045 steps/s (collection: 7.842s, learning 0.168s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0508
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 8.01s
                        Total time: 3841.30s
                               ETA: 820479.4s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.951s, learning 0.174s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0489
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 8.12s
                        Total time: 3849.42s
                               ETA: 820445.9s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.014s, learning 0.170s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0498
             Mean action noise std: 0.73
                       Mean reward: 13.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 8.18s
                        Total time: 3857.60s
                               ETA: 820425.1s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.804s, learning 0.160s)
               Value function loss: 2.0764
                    Surrogate loss: 0.0208
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 11.96s
                        Total time: 3869.57s
                               ETA: 821206.5s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1045 steps/s (collection: 15.438s, learning 0.234s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0304
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 15.67s
                        Total time: 3885.24s
                               ETA: 822769.8s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1030 steps/s (collection: 15.724s, learning 0.170s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0318
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 15.89s
                        Total time: 3901.13s
                               ETA: 824373.4s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1059 steps/s (collection: 15.293s, learning 0.168s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 15.46s
                        Total time: 3916.60s
                               ETA: 825878.8s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1040 steps/s (collection: 15.572s, learning 0.173s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 15.74s
                        Total time: 3932.34s
                               ETA: 827437.5s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.655s, learning 0.168s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0424
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 15.82s
                        Total time: 3948.16s
                               ETA: 829005.9s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1058 steps/s (collection: 15.296s, learning 0.181s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 15.48s
                        Total time: 3963.64s
                               ETA: 830495.1s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.641s, learning 0.167s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0419
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 15.81s
                        Total time: 3979.45s
                               ETA: 832047.2s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.961s, learning 0.191s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0490
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 16.15s
                        Total time: 3995.60s
                               ETA: 833664.6s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1034 steps/s (collection: 15.671s, learning 0.159s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0482
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 15.83s
                        Total time: 4011.43s
                               ETA: 835208.1s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1044 steps/s (collection: 15.508s, learning 0.178s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0475
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 15.69s
                        Total time: 4027.11s
                               ETA: 836715.0s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1038 steps/s (collection: 15.577s, learning 0.198s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0416
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 15.78s
                        Total time: 4042.89s
                               ETA: 838234.3s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1045 steps/s (collection: 15.504s, learning 0.173s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0438
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 15.68s
                        Total time: 4058.57s
                               ETA: 839726.9s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1034 steps/s (collection: 15.663s, learning 0.177s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0445
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 15.84s
                        Total time: 4074.41s
                               ETA: 841246.6s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1043 steps/s (collection: 15.539s, learning 0.167s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0476
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 15.71s
                        Total time: 4090.11s
                               ETA: 842732.4s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.771s, learning 0.275s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0472
             Mean action noise std: 0.73
                       Mean reward: 13.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 16.05s
                        Total time: 4106.16s
                               ETA: 844282.0s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1047 steps/s (collection: 15.467s, learning 0.173s)
               Value function loss: 1.4117
                    Surrogate loss: 0.0205
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 15.64s
                        Total time: 4121.80s
                               ETA: 845742.1s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1041 steps/s (collection: 15.561s, learning 0.167s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0260
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 15.73s
                        Total time: 4137.53s
                               ETA: 847213.9s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1066 steps/s (collection: 15.188s, learning 0.171s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 15.36s
                        Total time: 4152.89s
                               ETA: 848604.2s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.625s, learning 0.185s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 15.81s
                        Total time: 4168.70s
                               ETA: 850080.7s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.647s, learning 0.163s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0422
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 15.81s
                        Total time: 4184.51s
                               ETA: 851551.2s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.629s, learning 0.196s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 15.82s
                        Total time: 4200.33s
                               ETA: 853018.5s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1048 steps/s (collection: 15.436s, learning 0.188s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 15.62s
                        Total time: 4215.95s
                               ETA: 854439.3s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1048 steps/s (collection: 15.457s, learning 0.176s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0450
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 15.63s
                        Total time: 4231.59s
                               ETA: 855855.8s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1022 steps/s (collection: 15.746s, learning 0.279s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0425
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 16.02s
                        Total time: 4247.61s
                               ETA: 857345.6s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.638s, learning 0.172s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0417
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 15.81s
                        Total time: 4263.42s
                               ETA: 858786.2s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1032 steps/s (collection: 15.687s, learning 0.182s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0439
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 15.87s
                        Total time: 4279.29s
                               ETA: 860232.8s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.945s, learning 0.162s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0446
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 16.11s
                        Total time: 4295.40s
                               ETA: 861721.2s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1048 steps/s (collection: 15.459s, learning 0.160s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0478
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 15.62s
                        Total time: 4311.02s
                               ETA: 863105.8s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1028 steps/s (collection: 15.747s, learning 0.189s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0453
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 15.94s
                        Total time: 4326.95s
                               ETA: 864548.1s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1039 steps/s (collection: 15.571s, learning 0.196s)
               Value function loss: 0.0000
                    Surrogate loss: -0.0446
             Mean action noise std: 0.73
                       Mean reward: 13.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 15.77s
                        Total time: 4342.72s
                               ETA: 865951.0s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1023 steps/s (collection: 15.840s, learning 0.161s)
               Value function loss: 2.7002
                    Surrogate loss: 0.0364
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 16.00s
                        Total time: 4358.72s
                               ETA: 867394.5s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1074 steps/s (collection: 15.082s, learning 0.173s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0272
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 15.26s
                        Total time: 4373.98s
                               ETA: 868684.2s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1058 steps/s (collection: 15.308s, learning 0.172s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0343
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 15.48s
                        Total time: 4389.46s
                               ETA: 870013.1s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1056 steps/s (collection: 15.334s, learning 0.178s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0336
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 15.51s
                        Total time: 4404.97s
                               ETA: 871343.2s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1041 steps/s (collection: 15.560s, learning 0.167s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0320
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 15.73s
                        Total time: 4420.70s
                               ETA: 872710.3s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1034 steps/s (collection: 15.585s, learning 0.249s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0352
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 15.83s
                        Total time: 4436.53s
                               ETA: 874093.1s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1035 steps/s (collection: 15.586s, learning 0.242s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 15.83s
                        Total time: 4452.36s
                               ETA: 875469.2s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1757 steps/s (collection: 9.161s, learning 0.160s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0424
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 9.32s
                        Total time: 4461.68s
                               ETA: 875562.6s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.943s, learning 0.190s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0439
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 8.13s
                        Total time: 4469.81s
                               ETA: 875423.2s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.934s, learning 0.164s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0442
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 8.10s
                        Total time: 4477.91s
                               ETA: 875277.4s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.949s, learning 0.186s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 8.13s
                        Total time: 4486.04s
                               ETA: 875139.3s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.955s, learning 0.166s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 8.12s
                        Total time: 4494.17s
                               ETA: 874999.1s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.894s, learning 0.168s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0434
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 8.06s
                        Total time: 4502.23s
                               ETA: 874847.9s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1998 steps/s (collection: 7.917s, learning 0.282s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 8.20s
                        Total time: 4510.43s
                               ETA: 874723.8s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.014s, learning 0.192s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0425
             Mean action noise std: 0.72
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 8.21s
                        Total time: 4518.63s
                               ETA: 874601.4s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.918s, learning 0.177s)
               Value function loss: 0.0003
                    Surrogate loss: -0.0442
             Mean action noise std: 0.72
                       Mean reward: 13.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 8.10s
                        Total time: 4526.73s
                               ETA: 874458.2s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.091s, learning 0.167s)
               Value function loss: 2.0613
                    Surrogate loss: 0.0322
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 8.26s
                        Total time: 4534.98s
                               ETA: 874346.8s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 2053 steps/s (collection: 7.813s, learning 0.165s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 7.98s
                        Total time: 4542.96s
                               ETA: 874182.1s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 2073 steps/s (collection: 7.737s, learning 0.166s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0350
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 7.90s
                        Total time: 4550.87s
                               ETA: 874003.5s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.945s, learning 0.192s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 8.14s
                        Total time: 4559.00s
                               ETA: 873870.6s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 2117 steps/s (collection: 7.572s, learning 0.165s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0396
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 7.74s
                        Total time: 4566.74s
                               ETA: 873661.4s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 2064 steps/s (collection: 7.735s, learning 0.201s)
               Value function loss: 0.0008
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 7.94s
                        Total time: 4574.68s
                               ETA: 873491.2s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 2094 steps/s (collection: 7.659s, learning 0.163s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 7.82s
                        Total time: 4582.50s
                               ETA: 873299.6s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.841s, learning 0.189s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0449
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 8.03s
                        Total time: 4590.53s
                               ETA: 873148.4s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.959s, learning 0.194s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 8.15s
                        Total time: 4598.68s
                               ETA: 873021.1s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.835s, learning 0.167s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0455
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 8.00s
                        Total time: 4606.68s
                               ETA: 872865.6s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.066s, learning 0.160s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 8.23s
                        Total time: 4614.91s
                               ETA: 872753.1s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 2074 steps/s (collection: 7.734s, learning 0.164s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0471
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 7.90s
                        Total time: 4622.81s
                               ETA: 872578.9s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 2004 steps/s (collection: 7.963s, learning 0.212s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0430
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 8.18s
                        Total time: 4630.98s
                               ETA: 872457.7s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.087s, learning 0.170s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 8.26s
                        Total time: 4639.24s
                               ETA: 872352.2s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.864s, learning 0.161s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 8.02s
                        Total time: 4647.26s
                               ETA: 872203.5s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.969s, learning 0.170s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0456
             Mean action noise std: 0.72
                       Mean reward: 13.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 8.14s
                        Total time: 4655.40s
                               ETA: 872076.9s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.927s, learning 0.197s)
               Value function loss: 0.9047
                    Surrogate loss: 0.0126
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 8.12s
                        Total time: 4663.53s
                               ETA: 871947.9s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 2091 steps/s (collection: 7.642s, learning 0.190s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0295
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 7.83s
                        Total time: 4671.36s
                               ETA: 871764.9s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.257s, learning 0.170s)
               Value function loss: 0.0005
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 8.43s
                        Total time: 4679.79s
                               ETA: 871693.3s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.979s, learning 0.179s)
               Value function loss: 0.0001
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 8.16s
                        Total time: 4687.94s
                               ETA: 871572.0s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.916s, learning 0.198s)
               Value function loss: 0.0004
                    Surrogate loss: -0.0400
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 8.11s
                        Total time: 4696.06s
                               ETA: 871443.0s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.049s, learning 0.173s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0402
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 8.22s
                        Total time: 4704.28s
                               ETA: 871334.3s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.044s, learning 0.189s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0456
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 8.23s
                        Total time: 4712.51s
                               ETA: 871228.0s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.130s, learning 0.168s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 8.30s
                        Total time: 4720.81s
                               ETA: 871134.3s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 2001 steps/s (collection: 7.994s, learning 0.193s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 8.19s
                        Total time: 4729.00s
                               ETA: 871020.3s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.932s, learning 0.187s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 8.12s
                        Total time: 4737.12s
                               ETA: 870894.1s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.949s, learning 0.168s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0422
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 8.12s
                        Total time: 4745.23s
                               ETA: 870768.1s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 2030 steps/s (collection: 7.893s, learning 0.175s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0419
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 8.07s
                        Total time: 4753.30s
                               ETA: 870633.6s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.030s, learning 0.265s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 8.29s
                        Total time: 4761.60s
                               ETA: 870540.9s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.932s, learning 0.166s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 8.10s
                        Total time: 4769.70s
                               ETA: 870412.7s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 2001 steps/s (collection: 7.979s, learning 0.207s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0459
             Mean action noise std: 0.72
                       Mean reward: 14.21
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 8.19s
                        Total time: 4777.88s
                               ETA: 870301.0s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.261s, learning 0.172s)
               Value function loss: 4.2248
                    Surrogate loss: 0.0286
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 8.43s
                        Total time: 4786.32s
                               ETA: 870234.4s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.019s, learning 0.161s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0290
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 8.18s
                        Total time: 4794.50s
                               ETA: 870122.1s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 2050 steps/s (collection: 7.799s, learning 0.192s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0280
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 7.99s
                        Total time: 4802.49s
                               ETA: 869976.0s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.148s, learning 0.164s)
               Value function loss: 0.0002
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 8.31s
                        Total time: 4810.80s
                               ETA: 869888.5s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.863s, learning 0.216s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 8.08s
                        Total time: 4818.88s
                               ETA: 869759.2s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.082s, learning 0.165s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 8.25s
                        Total time: 4827.12s
                               ETA: 869660.6s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.905s, learning 0.224s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 8.13s
                        Total time: 4835.25s
                               ETA: 869541.0s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.181s, learning 0.184s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0290
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 8.36s
                        Total time: 4843.62s
                               ETA: 869464.2s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 2005 steps/s (collection: 8.010s, learning 0.159s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0319
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 8.17s
                        Total time: 4851.79s
                               ETA: 869352.6s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.809s, learning 0.194s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0353
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 8.00s
                        Total time: 4859.79s
                               ETA: 869211.8s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.984s, learning 0.166s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0305
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 8.15s
                        Total time: 4867.94s
                               ETA: 869097.6s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.001s, learning 0.215s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0240
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 8.22s
                        Total time: 4876.16s
                               ETA: 868995.6s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.218s, learning 0.282s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0195
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 8.50s
                        Total time: 4884.66s
                               ETA: 868944.4s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.067s, learning 0.197s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0325
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 8.26s
                        Total time: 4892.92s
                               ETA: 868851.5s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.962s, learning 0.176s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0278
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 8.14s
                        Total time: 4901.06s
                               ETA: 868736.7s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.977s, learning 0.189s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0236
             Mean action noise std: 0.72
                       Mean reward: 16.91
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 8.17s
                        Total time: 4909.23s
                               ETA: 868627.1s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.233s, learning 0.177s)
               Value function loss: 3.7833
                    Surrogate loss: 0.0238
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 8.41s
                        Total time: 4917.64s
                               ETA: 868560.9s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1999 steps/s (collection: 7.951s, learning 0.243s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0227
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 8.19s
                        Total time: 4925.83s
                               ETA: 868456.9s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.097s, learning 0.170s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 8.27s
                        Total time: 4934.10s
                               ETA: 868365.9s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.890s, learning 0.183s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0333
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 8.07s
                        Total time: 4942.17s
                               ETA: 868241.2s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.836s, learning 0.225s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0491
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 8.06s
                        Total time: 4950.23s
                               ETA: 868114.9s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.865s, learning 0.273s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 8.14s
                        Total time: 4958.37s
                               ETA: 868002.4s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 2002 steps/s (collection: 7.995s, learning 0.186s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 8.18s
                        Total time: 4966.55s
                               ETA: 867897.8s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.147s, learning 0.165s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 8.31s
                        Total time: 4974.86s
                               ETA: 867816.4s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.856s, learning 0.197s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 8.05s
                        Total time: 4982.91s
                               ETA: 867690.1s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.900s, learning 0.187s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0319
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 8.09s
                        Total time: 4991.00s
                               ETA: 867570.1s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1996 steps/s (collection: 7.982s, learning 0.224s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0261
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 8.21s
                        Total time: 4999.21s
                               ETA: 867471.2s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.794s, learning 0.176s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 7.97s
                        Total time: 5007.18s
                               ETA: 867331.7s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.904s, learning 0.207s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 8.11s
                        Total time: 5015.29s
                               ETA: 867217.3s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.166s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0230
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 8.29s
                        Total time: 5023.58s
                               ETA: 867133.7s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.162s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 19.32
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 8.10s
                        Total time: 5031.68s
                               ETA: 867018.5s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.992s, learning 0.167s)
               Value function loss: 7.2573
                    Surrogate loss: 0.0612
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 8.16s
                        Total time: 5039.84s
                               ETA: 866913.2s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.120s, learning 0.190s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0266
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 8.31s
                        Total time: 5048.15s
                               ETA: 866834.3s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.887s, learning 0.160s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 8.05s
                        Total time: 5056.20s
                               ETA: 866710.3s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.944s, learning 0.183s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0244
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 8.13s
                        Total time: 5064.32s
                               ETA: 866600.7s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.210s, learning 0.191s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 8.40s
                        Total time: 5072.72s
                               ETA: 866538.0s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.196s, learning 0.181s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 8.38s
                        Total time: 5081.10s
                               ETA: 866471.4s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.168s, learning 0.160s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 8.33s
                        Total time: 5089.43s
                               ETA: 866396.7s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.834s, learning 0.171s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0423
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 8.01s
                        Total time: 5097.43s
                               ETA: 866267.4s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.070s, learning 0.164s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0427
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 8.23s
                        Total time: 5105.67s
                               ETA: 866177.2s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.071s, learning 0.176s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0494
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 8.25s
                        Total time: 5113.91s
                               ETA: 866089.6s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.017s, learning 0.166s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0487
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 8.18s
                        Total time: 5122.10s
                               ETA: 865991.4s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.131s, learning 0.185s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0514
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 8.32s
                        Total time: 5130.41s
                               ETA: 865916.1s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.038s, learning 0.206s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0519
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 8.24s
                        Total time: 5138.66s
                               ETA: 865828.9s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.305s, learning 0.168s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0510
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 8.47s
                        Total time: 5147.13s
                               ETA: 865780.3s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.116s, learning 0.163s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0480
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 8.28s
                        Total time: 5155.41s
                               ETA: 865699.4s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.093s, learning 0.168s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 8.26s
                        Total time: 5163.67s
                               ETA: 865615.6s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.976s, learning 0.174s)
               Value function loss: 5.4227
                    Surrogate loss: 0.0216
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 8.15s
                        Total time: 5171.82s
                               ETA: 865513.7s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.024s, learning 0.284s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0221
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 8.31s
                        Total time: 5180.13s
                               ETA: 865438.3s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.159s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 8.24s
                        Total time: 5188.37s
                               ETA: 865352.1s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.836s, learning 0.164s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0283
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 8.00s
                        Total time: 5196.37s
                               ETA: 865226.0s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.046s, learning 0.208s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 8.25s
                        Total time: 5204.62s
                               ETA: 865142.6s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.886s, learning 0.166s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 8.05s
                        Total time: 5212.68s
                               ETA: 865025.8s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.156s, learning 0.199s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 8.36s
                        Total time: 5221.03s
                               ETA: 864959.7s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.161s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0455
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 8.44s
                        Total time: 5229.47s
                               ETA: 864907.5s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 2085 steps/s (collection: 7.674s, learning 0.180s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0461
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 7.85s
                        Total time: 5237.33s
                               ETA: 864758.9s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.013s, learning 0.219s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0476
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 8.23s
                        Total time: 5245.56s
                               ETA: 864673.1s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.440s, learning 0.165s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0485
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 8.60s
                        Total time: 5254.16s
                               ETA: 864648.8s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.469s, learning 0.174s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0486
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 8.64s
                        Total time: 5262.80s
                               ETA: 864631.0s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.042s, learning 0.193s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0464
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 8.23s
                        Total time: 5271.04s
                               ETA: 864546.2s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.266s, learning 0.162s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 8.43s
                        Total time: 5279.47s
                               ETA: 864493.2s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.535s, learning 0.175s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0471
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 8.71s
                        Total time: 5288.18s
                               ETA: 864486.6s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.049s, learning 0.197s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0406
             Mean action noise std: 0.72
                       Mean reward: 18.67
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 8.25s
                        Total time: 5296.42s
                               ETA: 864404.2s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.115s, learning 0.165s)
               Value function loss: 2.6166
                    Surrogate loss: 0.0145
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 8.28s
                        Total time: 5304.70s
                               ETA: 864327.6s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.899s, learning 0.164s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0211
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 8.06s
                        Total time: 5312.77s
                               ETA: 864215.8s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.911s, learning 0.194s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 8.11s
                        Total time: 5320.87s
                               ETA: 864111.3s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.163s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 8.25s
                        Total time: 5329.12s
                               ETA: 864029.9s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.036s, learning 0.183s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 8.22s
                        Total time: 5337.34s
                               ETA: 863944.4s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.159s, learning 0.160s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0349
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 8.32s
                        Total time: 5345.66s
                               ETA: 863875.3s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.120s, learning 0.210s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0337
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 8.33s
                        Total time: 5353.99s
                               ETA: 863808.2s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.934s, learning 0.166s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0467
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 8.10s
                        Total time: 5362.09s
                               ETA: 863704.2s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 2048 steps/s (collection: 7.836s, learning 0.162s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0507
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 8.00s
                        Total time: 5370.08s
                               ETA: 863584.2s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.218s, learning 0.175s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0488
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 8.39s
                        Total time: 5378.48s
                               ETA: 863528.0s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.390s, learning 0.215s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0521
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 8.61s
                        Total time: 5387.08s
                               ETA: 863505.9s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.160s, learning 0.200s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0539
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 8.36s
                        Total time: 5395.44s
                               ETA: 863444.4s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.174s, learning 0.179s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0556
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 8.35s
                        Total time: 5403.79s
                               ETA: 863382.2s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.115s, learning 0.230s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0545
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 8.34s
                        Total time: 5412.14s
                               ETA: 863318.8s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.302s, learning 0.169s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0527
             Mean action noise std: 0.72
                       Mean reward: 18.00
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 8.47s
                        Total time: 5420.61s
                               ETA: 863275.7s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.057s, learning 0.172s)
               Value function loss: 5.7472
                    Surrogate loss: 0.0539
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 8.23s
                        Total time: 5428.84s
                               ETA: 863194.2s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.837s, learning 0.247s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0265
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 8.08s
                        Total time: 5436.92s
                               ETA: 863090.0s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 2076 steps/s (collection: 7.724s, learning 0.168s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 7.89s
                        Total time: 5444.82s
                               ETA: 862955.5s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.019s, learning 0.186s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0307
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 8.21s
                        Total time: 5453.02s
                               ETA: 862871.1s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 2003 steps/s (collection: 7.976s, learning 0.203s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0396
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 8.18s
                        Total time: 5461.20s
                               ETA: 862782.8s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.967s, learning 0.180s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 8.15s
                        Total time: 5469.35s
                               ETA: 862689.7s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.911s, learning 0.186s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0300
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 8.10s
                        Total time: 5477.44s
                               ETA: 862589.0s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.254s, learning 0.214s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 8.47s
                        Total time: 5485.91s
                               ETA: 862546.9s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.166s, learning 0.177s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 8.34s
                        Total time: 5494.26s
                               ETA: 862485.2s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.164s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 8.36s
                        Total time: 5502.61s
                               ETA: 862426.0s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.421s, learning 0.161s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0504
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 8.58s
                        Total time: 5511.20s
                               ETA: 862402.2s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.161s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0517
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 8.12s
                        Total time: 5519.32s
                               ETA: 862307.0s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 2019 steps/s (collection: 7.802s, learning 0.312s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0556
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 8.11s
                        Total time: 5527.43s
                               ETA: 862210.3s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.173s, learning 0.162s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0540
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 8.33s
                        Total time: 5535.77s
                               ETA: 862148.2s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 2075 steps/s (collection: 7.724s, learning 0.169s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0499
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 7.89s
                        Total time: 5543.66s
                               ETA: 862017.6s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.923s, learning 0.171s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0514
             Mean action noise std: 0.72
                       Mean reward: 17.81
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 8.09s
                        Total time: 5551.75s
                               ETA: 861918.6s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.164s)
               Value function loss: 4.0979
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 8.48s
                        Total time: 5560.23s
                               ETA: 861879.5s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.144s, learning 0.214s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0237
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 8.36s
                        Total time: 5568.59s
                               ETA: 861821.9s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.945s, learning 0.211s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0247
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 8.16s
                        Total time: 5576.75s
                               ETA: 861733.2s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.019s, learning 0.163s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0342
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 8.18s
                        Total time: 5584.93s
                               ETA: 861648.8s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.985s, learning 0.176s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0339
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 8.16s
                        Total time: 5593.09s
                               ETA: 861561.4s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.864s, learning 0.166s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0298
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 8.03s
                        Total time: 5601.12s
                               ETA: 861454.2s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.078s, learning 0.170s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0357
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 8.25s
                        Total time: 5609.37s
                               ETA: 861380.6s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.152s, learning 0.218s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0293
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 8.37s
                        Total time: 5617.74s
                               ETA: 861325.9s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.045s, learning 0.180s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 8.23s
                        Total time: 5625.96s
                               ETA: 861249.3s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.164s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0497
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 8.17s
                        Total time: 5634.13s
                               ETA: 861164.5s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.933s, learning 0.159s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0459
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 8.09s
                        Total time: 5642.23s
                               ETA: 861068.0s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.903s, learning 0.196s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0526
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 8.10s
                        Total time: 5650.33s
                               ETA: 860972.8s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.906s, learning 0.192s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0523
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 8.10s
                        Total time: 5658.42s
                               ETA: 860877.7s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.736s, learning 0.203s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0485
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 7.94s
                        Total time: 5666.36s
                               ETA: 860758.8s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.916s, learning 0.166s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0488
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 8.08s
                        Total time: 5674.45s
                               ETA: 860661.7s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.107s, learning 0.158s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0438
             Mean action noise std: 0.72
                       Mean reward: 17.90
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 8.27s
                        Total time: 5682.71s
                               ETA: 860592.8s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.869s, learning 0.216s)
               Value function loss: 1.4112
                    Surrogate loss: 0.0270
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 8.08s
                        Total time: 5690.79s
                               ETA: 860496.7s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 2062 steps/s (collection: 7.765s, learning 0.178s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 7.94s
                        Total time: 5698.74s
                               ETA: 860379.6s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.965s, learning 0.175s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 8.14s
                        Total time: 5706.88s
                               ETA: 860292.5s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.782s, learning 0.166s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 7.95s
                        Total time: 5714.83s
                               ETA: 860176.8s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.816s, learning 0.205s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0319
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 8.02s
                        Total time: 5722.85s
                               ETA: 860072.3s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.153s, learning 0.185s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 8.34s
                        Total time: 5731.19s
                               ETA: 860015.7s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.165s, learning 0.168s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0423
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 8.33s
                        Total time: 5739.52s
                               ETA: 859958.5s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.875s, learning 0.164s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0349
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 8.04s
                        Total time: 5747.56s
                               ETA: 859857.4s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 2030 steps/s (collection: 7.888s, learning 0.180s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0442
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 8.07s
                        Total time: 5755.63s
                               ETA: 859761.0s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.849s, learning 0.174s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0421
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 8.02s
                        Total time: 5763.65s
                               ETA: 859658.0s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 2043 steps/s (collection: 7.855s, learning 0.161s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0281
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 8.02s
                        Total time: 5771.67s
                               ETA: 859554.4s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.928s, learning 0.171s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0439
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 8.10s
                        Total time: 5779.77s
                               ETA: 859463.4s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 2081 steps/s (collection: 7.698s, learning 0.173s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0526
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 7.87s
                        Total time: 5787.64s
                               ETA: 859338.8s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.799s, learning 0.170s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0509
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 7.97s
                        Total time: 5795.61s
                               ETA: 859229.0s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.157s, learning 0.169s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0472
             Mean action noise std: 0.72
                       Mean reward: 18.35
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 8.33s
                        Total time: 5803.93s
                               ETA: 859172.4s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.032s, learning 0.174s)
               Value function loss: 6.2753
                    Surrogate loss: 0.0107
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 8.21s
                        Total time: 5812.14s
                               ETA: 859098.1s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.929s, learning 0.164s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 8.09s
                        Total time: 5820.23s
                               ETA: 859007.4s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.881s, learning 0.167s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0183
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 8.05s
                        Total time: 5828.28s
                               ETA: 858910.3s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.816s, learning 0.157s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0223
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 7.97s
                        Total time: 5836.25s
                               ETA: 858802.4s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.994s, learning 0.155s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 8.15s
                        Total time: 5844.40s
                               ETA: 858720.7s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.025s, learning 0.306s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 8.33s
                        Total time: 5852.73s
                               ETA: 858665.9s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 2030 steps/s (collection: 7.907s, learning 0.160s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 8.07s
                        Total time: 5860.80s
                               ETA: 858572.6s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.180s, learning 0.168s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 8.35s
                        Total time: 5869.15s
                               ETA: 858520.6s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 2049 steps/s (collection: 7.705s, learning 0.291s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0447
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 8.00s
                        Total time: 5877.14s
                               ETA: 858417.3s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 2077 steps/s (collection: 7.724s, learning 0.163s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0442
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 7.89s
                        Total time: 5885.03s
                               ETA: 858298.4s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.118s, learning 0.170s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 8.29s
                        Total time: 5893.32s
                               ETA: 858238.2s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.143s, learning 0.186s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0324
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 8.33s
                        Total time: 5901.65s
                               ETA: 858184.2s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.045s, learning 0.264s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0495
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 8.31s
                        Total time: 5909.96s
                               ETA: 858127.3s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.782s, learning 0.169s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0491
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 7.95s
                        Total time: 5917.91s
                               ETA: 858018.6s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.944s, learning 0.187s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0484
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 8.13s
                        Total time: 5926.04s
                               ETA: 857936.4s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.188s, learning 0.241s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0505
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 8.43s
                        Total time: 5934.47s
                               ETA: 857897.4s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.365s, learning 0.179s)
               Value function loss: 4.2888
                    Surrogate loss: 0.0134
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 8.54s
                        Total time: 5943.01s
                               ETA: 857875.3s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.072s, learning 0.171s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 8.24s
                        Total time: 5951.25s
                               ETA: 857809.7s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 2001 steps/s (collection: 7.993s, learning 0.191s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 8.18s
                        Total time: 5959.44s
                               ETA: 857735.8s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.865s, learning 0.172s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 8.04s
                        Total time: 5967.47s
                               ETA: 857641.0s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.080s, learning 0.167s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 8.25s
                        Total time: 5975.72s
                               ETA: 857576.5s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.907s, learning 0.211s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 8.12s
                        Total time: 5983.84s
                               ETA: 857493.8s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 2087 steps/s (collection: 7.685s, learning 0.162s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 7.85s
                        Total time: 5991.69s
                               ETA: 857372.5s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.290s, learning 0.158s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0450
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 8.45s
                        Total time: 6000.14s
                               ETA: 857337.3s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.155s, learning 0.171s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0438
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 8.33s
                        Total time: 6008.46s
                               ETA: 857284.8s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.033s, learning 0.160s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 8.19s
                        Total time: 6016.65s
                               ETA: 857213.5s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 2043 steps/s (collection: 7.860s, learning 0.159s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 8.02s
                        Total time: 6024.67s
                               ETA: 857117.6s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.911s, learning 0.185s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0431
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 8.10s
                        Total time: 6032.77s
                               ETA: 857033.0s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 2066 steps/s (collection: 7.769s, learning 0.161s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 7.93s
                        Total time: 6040.70s
                               ETA: 856924.9s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.085s, learning 0.201s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0311
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 8.29s
                        Total time: 6048.98s
                               ETA: 856867.6s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.349s, learning 0.163s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0436
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 8.51s
                        Total time: 6057.50s
                               ETA: 856842.4s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 2065 steps/s (collection: 7.766s, learning 0.165s)
               Value function loss: 7.8091
                    Surrogate loss: 0.0663
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 7.93s
                        Total time: 6065.43s
                               ETA: 856735.1s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.064s, learning 0.159s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0279
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 8.22s
                        Total time: 6073.65s
                               ETA: 856669.4s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.983s, learning 0.163s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 8.15s
                        Total time: 6081.80s
                               ETA: 856593.1s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1997 steps/s (collection: 7.997s, learning 0.207s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 8.20s
                        Total time: 6090.00s
                               ETA: 856525.0s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.283s, learning 0.166s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0385
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 8.45s
                        Total time: 6098.45s
                               ETA: 856491.5s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.036s, learning 0.268s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0396
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 8.30s
                        Total time: 6106.75s
                               ETA: 856437.8s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.267s, learning 0.231s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0335
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 8.50s
                        Total time: 6115.25s
                               ETA: 856411.3s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.129s, learning 0.203s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 8.33s
                        Total time: 6123.58s
                               ETA: 856361.6s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.090s, learning 0.177s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 8.27s
                        Total time: 6131.85s
                               ETA: 856303.0s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.987s, learning 0.167s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0465
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 8.15s
                        Total time: 6140.00s
                               ETA: 856228.7s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.180s, learning 0.162s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0525
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 8.34s
                        Total time: 6148.35s
                               ETA: 856181.0s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 2005 steps/s (collection: 7.980s, learning 0.188s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0488
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 8.17s
                        Total time: 6156.51s
                               ETA: 856109.0s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.069s, learning 0.160s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0527
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 8.23s
                        Total time: 6164.74s
                               ETA: 856045.7s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 2142 steps/s (collection: 7.473s, learning 0.172s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0463
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 7.65s
                        Total time: 6172.39s
                               ETA: 855901.7s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.046s, learning 0.173s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0510
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 8.22s
                        Total time: 6180.61s
                               ETA: 855837.5s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 2066 steps/s (collection: 7.737s, learning 0.192s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 20.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 7.93s
                        Total time: 6188.54s
                               ETA: 855733.4s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 2083 steps/s (collection: 7.705s, learning 0.158s)
               Value function loss: 6.3371
                    Surrogate loss: 0.0777
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 7.86s
                        Total time: 6196.40s
                               ETA: 855620.4s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 2083 steps/s (collection: 7.679s, learning 0.185s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 7.86s
                        Total time: 6204.27s
                               ETA: 855507.9s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.935s, learning 0.163s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 8.10s
                        Total time: 6212.36s
                               ETA: 855427.8s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.957s, learning 0.164s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0288
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 8.12s
                        Total time: 6220.48s
                               ETA: 855351.1s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.127s, learning 0.261s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0324
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 8.39s
                        Total time: 6228.87s
                               ETA: 855311.2s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.077s, learning 0.220s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 8.30s
                        Total time: 6237.17s
                               ETA: 855258.9s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.107s, learning 0.159s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0269
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 8.27s
                        Total time: 6245.44s
                               ETA: 855202.6s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.252s, learning 0.164s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0283
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 8.42s
                        Total time: 6253.85s
                               ETA: 855166.8s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.337s, learning 0.168s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0298
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 8.50s
                        Total time: 6262.36s
                               ETA: 855143.2s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.240s, learning 0.170s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0237
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 8.41s
                        Total time: 6270.77s
                               ETA: 855106.7s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.521s, learning 0.183s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0316
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 8.70s
                        Total time: 6279.47s
                               ETA: 855110.4s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.169s, learning 0.207s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 8.38s
                        Total time: 6287.84s
                               ETA: 855069.3s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.459s, learning 0.289s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0321
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 8.75s
                        Total time: 6296.59s
                               ETA: 855079.0s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.938s, learning 0.167s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0425
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 8.10s
                        Total time: 6304.70s
                               ETA: 855001.3s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.159s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0404
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 8.27s
                        Total time: 6312.97s
                               ETA: 854946.6s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.247s, learning 0.182s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 20.43
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 8.43s
                        Total time: 6321.40s
                               ETA: 854913.2s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.185s, learning 0.162s)
               Value function loss: 3.7556
                    Surrogate loss: 0.0103
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 8.35s
                        Total time: 6329.75s
                               ETA: 854868.7s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 2086 steps/s (collection: 7.597s, learning 0.256s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 7.85s
                        Total time: 6337.60s
                               ETA: 854757.8s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.964s, learning 0.166s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 8.13s
                        Total time: 6345.73s
                               ETA: 854684.5s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.056s, learning 0.192s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 8.25s
                        Total time: 6353.98s
                               ETA: 854627.2s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.141s, learning 0.184s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0468
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 8.33s
                        Total time: 6362.30s
                               ETA: 854580.4s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.610s, learning 0.189s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0458
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 8.80s
                        Total time: 6371.10s
                               ETA: 854597.2s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 2068 steps/s (collection: 7.748s, learning 0.174s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 7.92s
                        Total time: 6379.02s
                               ETA: 854496.4s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.311s, learning 0.182s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0439
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 8.49s
                        Total time: 6387.52s
                               ETA: 854472.3s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 2004 steps/s (collection: 7.964s, learning 0.209s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0536
             Mean action noise std: 0.72
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 8.17s
                        Total time: 6395.69s
                               ETA: 854405.4s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.966s, learning 0.163s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 20.94
               Mean episode length: 124.49
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 8.13s
                        Total time: 6403.82s
                               ETA: 854332.8s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.328s, learning 0.233s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 20.94
               Mean episode length: 124.49
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 8.56s
                        Total time: 6412.38s
                               ETA: 854318.2s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.394s, learning 0.164s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0351
             Mean action noise std: 0.72
                       Mean reward: 20.94
               Mean episode length: 124.49
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 8.56s
                        Total time: 6420.94s
                               ETA: 854303.1s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 2005 steps/s (collection: 7.993s, learning 0.177s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0366
             Mean action noise std: 0.72
                       Mean reward: 20.94
               Mean episode length: 124.49
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 8.17s
                        Total time: 6429.11s
                               ETA: 854236.4s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.066s, learning 0.177s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 20.94
               Mean episode length: 124.49
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 8.24s
                        Total time: 6437.35s
                               ETA: 854179.6s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.046s, learning 0.184s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 20.87
               Mean episode length: 124.37
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 8.23s
                        Total time: 6445.58s
                               ETA: 854121.2s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.094s, learning 0.166s)
               Value function loss: 7.5337
                    Surrogate loss: 0.0403
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 8.26s
                        Total time: 6453.84s
                               ETA: 854066.9s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 2055 steps/s (collection: 7.779s, learning 0.194s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0186
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 7.97s
                        Total time: 6461.81s
                               ETA: 853974.7s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 2043 steps/s (collection: 7.848s, learning 0.171s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 8.02s
                        Total time: 6469.83s
                               ETA: 853888.8s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 2036 steps/s (collection: 7.874s, learning 0.170s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0275
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 8.04s
                        Total time: 6477.88s
                               ETA: 853806.5s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 2005 steps/s (collection: 8.005s, learning 0.164s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0407
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 8.17s
                        Total time: 6486.05s
                               ETA: 853740.9s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.133s, learning 0.160s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0273
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 8.29s
                        Total time: 6494.34s
                               ETA: 853691.7s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 2070 steps/s (collection: 7.742s, learning 0.171s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0337
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 7.91s
                        Total time: 6502.25s
                               ETA: 853592.7s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.922s, learning 0.169s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 8.09s
                        Total time: 6510.34s
                               ETA: 853517.3s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.290s, learning 0.194s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0413
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 8.48s
                        Total time: 6518.83s
                               ETA: 853493.5s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.156s, learning 0.176s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0520
             Mean action noise std: 0.72
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 8.33s
                        Total time: 6527.16s
                               ETA: 853449.8s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.307s, learning 0.163s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0520
             Mean action noise std: 0.72
                       Mean reward: 19.33
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 8.47s
                        Total time: 6535.63s
                               ETA: 853424.3s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.273s, learning 0.184s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 19.33
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 8.46s
                        Total time: 6544.09s
                               ETA: 853397.0s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.168s, learning 0.159s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 19.33
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 8.33s
                        Total time: 6552.41s
                               ETA: 853352.9s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.319s, learning 0.184s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 19.27
               Mean episode length: 124.79
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 8.50s
                        Total time: 6560.92s
                               ETA: 853331.8s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.010s, learning 0.169s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 19.27
               Mean episode length: 124.79
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 8.18s
                        Total time: 6569.09s
                               ETA: 853268.6s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.250s, learning 0.215s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0525
             Mean action noise std: 0.72
                       Mean reward: 19.27
               Mean episode length: 124.79
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 8.47s
                        Total time: 6577.56s
                               ETA: 853242.8s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.185s, learning 0.162s)
               Value function loss: 5.2563
                    Surrogate loss: 0.0079
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 8.35s
                        Total time: 6585.91s
                               ETA: 853201.8s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.951s, learning 0.168s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 8.12s
                        Total time: 6594.03s
                               ETA: 853131.1s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 2007 steps/s (collection: 8.000s, learning 0.160s)
               Value function loss: 0.0089
                    Surrogate loss: 0.0147
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 8.16s
                        Total time: 6602.19s
                               ETA: 853066.1s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.036s, learning 0.174s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0347
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 8.21s
                        Total time: 6610.40s
                               ETA: 853007.7s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.045s, learning 0.300s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0339
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 8.35s
                        Total time: 6618.74s
                               ETA: 852966.7s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.024s, learning 0.170s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0293
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 8.19s
                        Total time: 6626.94s
                               ETA: 852906.4s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 2076 steps/s (collection: 7.721s, learning 0.168s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0335
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 7.89s
                        Total time: 6634.83s
                               ETA: 852807.1s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.850s, learning 0.170s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0508
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 8.02s
                        Total time: 6642.85s
                               ETA: 852724.8s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.167s, learning 0.184s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 8.35s
                        Total time: 6651.20s
                               ETA: 852685.2s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.077s, learning 0.166s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0348
             Mean action noise std: 0.72
                       Mean reward: 19.48
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 8.24s
                        Total time: 6659.44s
                               ETA: 852631.8s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.071s, learning 0.191s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0581
             Mean action noise std: 0.72
                       Mean reward: 19.48
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 8.26s
                        Total time: 6667.70s
                               ETA: 852581.0s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.066s, learning 0.161s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0503
             Mean action noise std: 0.72
                       Mean reward: 19.48
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 8.23s
                        Total time: 6675.93s
                               ETA: 852525.7s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.003s, learning 0.194s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0538
             Mean action noise std: 0.72
                       Mean reward: 19.48
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 8.20s
                        Total time: 6684.13s
                               ETA: 852466.8s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.125s, learning 0.161s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0437
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 8.29s
                        Total time: 6692.41s
                               ETA: 852419.2s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.395s, learning 0.200s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0397
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 8.60s
                        Total time: 6701.01s
                               ETA: 852411.2s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.045s, learning 0.164s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0551
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 8.21s
                        Total time: 6709.22s
                               ETA: 852354.0s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.932s, learning 0.194s)
               Value function loss: 1.6853
                    Surrogate loss: 0.0094
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 8.13s
                        Total time: 6717.34s
                               ETA: 852286.5s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.121s, learning 0.203s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 8.32s
                        Total time: 6725.67s
                               ETA: 852244.2s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.488s, learning 0.171s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 8.66s
                        Total time: 6734.33s
                               ETA: 852244.5s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.926s, learning 0.198s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0332
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 8.12s
                        Total time: 6742.45s
                               ETA: 852176.9s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.960s, learning 0.164s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 8.12s
                        Total time: 6750.57s
                               ETA: 852109.6s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.111s, learning 0.189s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0390
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 8.30s
                        Total time: 6758.87s
                               ETA: 852064.7s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.169s, learning 0.220s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0456
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 8.39s
                        Total time: 6767.26s
                               ETA: 852031.1s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.128s, learning 0.162s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0382
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 8.29s
                        Total time: 6775.55s
                               ETA: 851985.0s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.076s, learning 0.166s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0405
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 8.24s
                        Total time: 6783.79s
                               ETA: 851933.0s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.964s, learning 0.167s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0448
             Mean action noise std: 0.72
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 8.13s
                        Total time: 6791.93s
                               ETA: 851867.1s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.205s, learning 0.167s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0425
             Mean action noise std: 0.72
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 8.37s
                        Total time: 6800.30s
                               ETA: 851831.7s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.034s, learning 0.185s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0371
             Mean action noise std: 0.72
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 8.22s
                        Total time: 6808.52s
                               ETA: 851777.2s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.246s, learning 0.158s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 8.40s
                        Total time: 6816.92s
                               ETA: 851745.9s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.160s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 19.60
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 8.43s
                        Total time: 6825.35s
                               ETA: 851717.7s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.011s, learning 0.192s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0498
             Mean action noise std: 0.72
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 8.20s
                        Total time: 6833.55s
                               ETA: 851661.4s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.123s, learning 0.193s)
               Value function loss: 6.4566
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 8.32s
                        Total time: 6841.87s
                               ETA: 851619.4s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.188s, learning 0.171s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 8.36s
                        Total time: 6850.23s
                               ETA: 851582.8s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.160s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 8.41s
                        Total time: 6858.64s
                               ETA: 851552.8s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.914s, learning 0.175s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0237
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 8.09s
                        Total time: 6866.73s
                               ETA: 851482.8s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.978s, learning 0.160s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0244
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 8.14s
                        Total time: 6874.87s
                               ETA: 851419.0s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.367s, learning 0.187s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0268
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 8.55s
                        Total time: 6883.42s
                               ETA: 851406.9s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.348s, learning 0.191s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 8.54s
                        Total time: 6891.96s
                               ETA: 851392.9s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.204s, learning 0.202s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0484
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 8.41s
                        Total time: 6900.36s
                               ETA: 851362.5s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.120s, learning 0.159s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0412
             Mean action noise std: 0.72
                       Mean reward: 19.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 8.28s
                        Total time: 6908.64s
                               ETA: 851316.5s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.060s, learning 0.159s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0317
             Mean action noise std: 0.72
                       Mean reward: 19.47
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 8.22s
                        Total time: 6916.86s
                               ETA: 851263.3s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.981s, learning 0.186s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0561
             Mean action noise std: 0.72
                       Mean reward: 19.47
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 8.17s
                        Total time: 6925.03s
                               ETA: 851203.8s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.337s, learning 0.179s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0551
             Mean action noise std: 0.72
                       Mean reward: 19.47
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 8.52s
                        Total time: 6933.55s
                               ETA: 851187.2s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.219s, learning 0.187s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0466
             Mean action noise std: 0.72
                       Mean reward: 19.47
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 8.41s
                        Total time: 6941.95s
                               ETA: 851157.2s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.951s, learning 0.159s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0445
             Mean action noise std: 0.72
                       Mean reward: 19.48
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 8.11s
                        Total time: 6950.06s
                               ETA: 851090.9s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.043s, learning 0.155s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 19.48
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 8.20s
                        Total time: 6958.26s
                               ETA: 851035.7s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.958s, learning 0.166s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0593
             Mean action noise std: 0.72
                       Mean reward: 19.48
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 8.12s
                        Total time: 6966.39s
                               ETA: 850971.5s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.080s, learning 0.162s)
               Value function loss: 4.5602
                    Surrogate loss: 0.0339
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 8.24s
                        Total time: 6974.63s
                               ETA: 850921.8s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.044s, learning 0.196s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 8.24s
                        Total time: 6982.87s
                               ETA: 850872.0s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.101s, learning 0.169s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 8.27s
                        Total time: 6991.14s
                               ETA: 850825.8s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.277s, learning 0.203s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0311
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 8.48s
                        Total time: 6999.62s
                               ETA: 850805.4s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 2054 steps/s (collection: 7.741s, learning 0.234s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 7.97s
                        Total time: 7007.59s
                               ETA: 850723.6s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.069s, learning 0.165s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0342
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 8.23s
                        Total time: 7015.83s
                               ETA: 850673.4s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.199s, learning 0.175s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 8.37s
                        Total time: 7024.20s
                               ETA: 850640.2s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.062s, learning 0.192s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 8.25s
                        Total time: 7032.45s
                               ETA: 850592.5s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.100s, learning 0.164s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0503
             Mean action noise std: 0.72
                       Mean reward: 20.01
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 8.26s
                        Total time: 7040.72s
                               ETA: 850546.2s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.421s, learning 0.186s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0387
             Mean action noise std: 0.72
                       Mean reward: 19.99
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 8.61s
                        Total time: 7049.33s
                               ETA: 850541.4s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.536s, learning 0.204s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0483
             Mean action noise std: 0.72
                       Mean reward: 19.99
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 8.74s
                        Total time: 7058.07s
                               ETA: 850552.7s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.185s, learning 0.213s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 19.91
               Mean episode length: 124.65
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 8.40s
                        Total time: 7066.46s
                               ETA: 850522.7s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.248s, learning 0.198s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 19.91
               Mean episode length: 124.65
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 8.45s
                        Total time: 7074.91s
                               ETA: 850498.5s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.047s, learning 0.181s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0423
             Mean action noise std: 0.72
                       Mean reward: 19.93
               Mean episode length: 124.65
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 8.23s
                        Total time: 7083.14s
                               ETA: 850448.2s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.024s, learning 0.164s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0428
             Mean action noise std: 0.72
                       Mean reward: 19.92
               Mean episode length: 124.65
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 8.19s
                        Total time: 7091.33s
                               ETA: 850393.1s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.863s, learning 0.167s)
               Value function loss: 8.2814
                    Surrogate loss: 0.0725
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 8.03s
                        Total time: 7099.35s
                               ETA: 850319.2s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 2039 steps/s (collection: 7.867s, learning 0.166s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 8.03s
                        Total time: 7107.39s
                               ETA: 850246.0s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.953s, learning 0.194s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 8.15s
                        Total time: 7115.54s
                               ETA: 850186.5s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.067s, learning 0.168s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0210
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 8.24s
                        Total time: 7123.77s
                               ETA: 850137.6s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.077s, learning 0.162s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0300
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 8.24s
                        Total time: 7132.01s
                               ETA: 850089.2s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.903s, learning 0.199s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0333
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 8.10s
                        Total time: 7140.11s
                               ETA: 850024.6s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.973s, learning 0.164s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0277
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 8.14s
                        Total time: 7148.25s
                               ETA: 849964.4s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.070s, learning 0.156s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0441
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 8.23s
                        Total time: 7156.47s
                               ETA: 849914.8s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.918s, learning 0.169s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 8.09s
                        Total time: 7164.56s
                               ETA: 849848.9s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.113s, learning 0.165s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0447
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 8.28s
                        Total time: 7172.84s
                               ETA: 849805.8s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.886s, learning 0.165s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0459
             Mean action noise std: 0.72
                       Mean reward: 19.94
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 8.05s
                        Total time: 7180.89s
                               ETA: 849735.8s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.039s, learning 0.173s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0501
             Mean action noise std: 0.72
                       Mean reward: 19.94
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 8.21s
                        Total time: 7189.10s
                               ETA: 849685.0s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.319s, learning 0.206s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0473
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 8.53s
                        Total time: 7197.63s
                               ETA: 849671.4s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.920s, learning 0.155s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0337
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 8.07s
                        Total time: 7205.70s
                               ETA: 849604.6s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.363s, learning 0.282s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0499
             Mean action noise std: 0.72
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 8.65s
                        Total time: 7214.35s
                               ETA: 849605.2s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.207s, learning 0.190s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0490
             Mean action noise std: 0.72
                       Mean reward: 19.97
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 8.40s
                        Total time: 7222.75s
                               ETA: 849576.5s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.283s, learning 0.162s)
               Value function loss: 5.7526
                    Surrogate loss: 0.0728
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 8.44s
                        Total time: 7231.19s
                               ETA: 849553.5s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.249s, learning 0.258s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0048
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 8.51s
                        Total time: 7239.70s
                               ETA: 849537.9s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 2003 steps/s (collection: 8.013s, learning 0.165s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 8.18s
                        Total time: 7247.88s
                               ETA: 849483.7s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 2042 steps/s (collection: 7.856s, learning 0.164s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0234
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 8.02s
                        Total time: 7255.90s
                               ETA: 849411.0s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.123s, learning 0.304s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0249
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 8.43s
                        Total time: 7264.32s
                               ETA: 849386.1s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.931s, learning 0.158s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0305
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 8.09s
                        Total time: 7272.41s
                               ETA: 849321.9s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.945s, learning 0.160s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0326
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 8.11s
                        Total time: 7280.52s
                               ETA: 849259.6s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.897s, learning 0.180s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 8.08s
                        Total time: 7288.59s
                               ETA: 849194.1s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.396s, learning 0.201s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 8.60s
                        Total time: 7297.19s
                               ETA: 849189.2s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.074s, learning 0.163s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0254
             Mean action noise std: 0.72
                       Mean reward: 19.54
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 8.24s
                        Total time: 7305.43s
                               ETA: 849142.6s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 2025 steps/s (collection: 7.910s, learning 0.179s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0598
             Mean action noise std: 0.72
                       Mean reward: 19.54
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 8.09s
                        Total time: 7313.52s
                               ETA: 849078.8s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.981s, learning 0.158s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0370
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 8.14s
                        Total time: 7321.66s
                               ETA: 849021.0s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.105s, learning 0.188s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0496
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 8.29s
                        Total time: 7329.95s
                               ETA: 848981.0s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.085s, learning 0.220s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0373
             Mean action noise std: 0.72
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 8.30s
                        Total time: 7338.25s
                               ETA: 848942.6s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.983s, learning 0.159s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 8.14s
                        Total time: 7346.40s
                               ETA: 848885.4s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.045s, learning 0.161s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0534
             Mean action noise std: 0.72
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 8.21s
                        Total time: 7354.60s
                               ETA: 848835.8s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.743s, learning 0.259s)
               Value function loss: 2.9471
                    Surrogate loss: 0.0262
             Mean action noise std: 0.72
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 8.00s
                        Total time: 7362.60s
                               ETA: 848762.7s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.980s, learning 0.162s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 8.14s
                        Total time: 7370.75s
                               ETA: 848705.9s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.016s, learning 0.209s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 8.23s
                        Total time: 7378.97s
                               ETA: 848658.8s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.147s, learning 0.232s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 8.38s
                        Total time: 7387.35s
                               ETA: 848629.4s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.237s, learning 0.161s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0281
             Mean action noise std: 0.72
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 8.40s
                        Total time: 7395.75s
                               ETA: 848602.2s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 2059 steps/s (collection: 7.736s, learning 0.218s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 7.95s
                        Total time: 7403.70s
                               ETA: 848524.3s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.171s, learning 0.165s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0475
             Mean action noise std: 0.71
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 8.34s
                        Total time: 7412.04s
                               ETA: 848490.1s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 2063 steps/s (collection: 7.757s, learning 0.185s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0407
             Mean action noise std: 0.71
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 7.94s
                        Total time: 7419.98s
                               ETA: 848411.0s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.167s, learning 0.180s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0413
             Mean action noise std: 0.71
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 8.35s
                        Total time: 7428.33s
                               ETA: 848378.4s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.287s, learning 0.183s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 8.47s
                        Total time: 7436.80s
                               ETA: 848359.8s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.133s, learning 0.160s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0426
             Mean action noise std: 0.71
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 8.29s
                        Total time: 7445.09s
                               ETA: 848321.0s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 2040 steps/s (collection: 7.868s, learning 0.163s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0432
             Mean action noise std: 0.71
                       Mean reward: 19.69
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 8.03s
                        Total time: 7453.12s
                               ETA: 848252.6s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.894s, learning 0.213s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0530
             Mean action noise std: 0.71
                       Mean reward: 19.69
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 8.11s
                        Total time: 7461.23s
                               ETA: 848192.9s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.416s, learning 0.212s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0495
             Mean action noise std: 0.71
                       Mean reward: 19.71
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 8.63s
                        Total time: 7469.86s
                               ETA: 848192.3s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.096s, learning 0.160s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0477
             Mean action noise std: 0.71
                       Mean reward: 19.70
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 8.26s
                        Total time: 7478.11s
                               ETA: 848149.7s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.169s, learning 0.207s)
               Value function loss: 7.6346
                    Surrogate loss: 0.0633
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 8.38s
                        Total time: 7486.49s
                               ETA: 848120.8s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.137s, learning 0.190s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 8.33s
                        Total time: 7494.82s
                               ETA: 848086.3s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 2052 steps/s (collection: 7.813s, learning 0.170s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 7.98s
                        Total time: 7502.80s
                               ETA: 848013.0s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 2060 steps/s (collection: 7.786s, learning 0.164s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0228
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 7.95s
                        Total time: 7510.75s
                               ETA: 847936.1s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.987s, learning 0.175s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0365
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 8.16s
                        Total time: 7518.91s
                               ETA: 847883.3s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.289s, learning 0.158s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0420
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 8.45s
                        Total time: 7527.36s
                               ETA: 847862.7s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.165s, learning 0.160s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0303
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 8.32s
                        Total time: 7535.68s
                               ETA: 847828.4s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.105s, learning 0.161s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0426
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 8.27s
                        Total time: 7543.95s
                               ETA: 847787.4s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.178s, learning 0.182s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0382
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 8.36s
                        Total time: 7552.31s
                               ETA: 847757.2s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.309s, learning 0.204s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 20.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 8.51s
                        Total time: 7560.82s
                               ETA: 847744.2s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 2078 steps/s (collection: 7.695s, learning 0.189s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0522
             Mean action noise std: 0.71
                       Mean reward: 20.62
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 7.88s
                        Total time: 7568.70s
                               ETA: 847660.6s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.943s, learning 0.160s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0442
             Mean action noise std: 0.71
                       Mean reward: 20.62
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 8.10s
                        Total time: 7576.81s
                               ETA: 847601.7s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.023s, learning 0.174s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0572
             Mean action noise std: 0.71
                       Mean reward: 20.64
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 8.20s
                        Total time: 7585.00s
                               ETA: 847553.5s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.058s, learning 0.237s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0429
             Mean action noise std: 0.71
                       Mean reward: 20.65
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 8.30s
                        Total time: 7593.30s
                               ETA: 847516.5s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.186s, learning 0.166s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0456
             Mean action noise std: 0.71
                       Mean reward: 20.66
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 8.35s
                        Total time: 7601.65s
                               ETA: 847485.7s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.214s, learning 0.167s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0590
             Mean action noise std: 0.71
                       Mean reward: 20.66
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 8.38s
                        Total time: 7610.03s
                               ETA: 847458.3s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.320s, learning 0.183s)
               Value function loss: 5.2277
                    Surrogate loss: 0.0134
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 8.50s
                        Total time: 7618.54s
                               ETA: 847444.5s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.110s, learning 0.169s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 8.28s
                        Total time: 7626.81s
                               ETA: 847405.7s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.834s, learning 0.171s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 8.01s
                        Total time: 7634.82s
                               ETA: 847336.7s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.874s, learning 0.207s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 8.08s
                        Total time: 7642.90s
                               ETA: 847276.1s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.093s, learning 0.155s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 8.25s
                        Total time: 7651.15s
                               ETA: 847234.3s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.156s, learning 0.235s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0384
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 8.39s
                        Total time: 7659.54s
                               ETA: 847208.2s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.185s, learning 0.168s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0449
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 8.35s
                        Total time: 7667.89s
                               ETA: 847178.1s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 2071 steps/s (collection: 7.739s, learning 0.171s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0314
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 7.91s
                        Total time: 7675.80s
                               ETA: 847099.1s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.911s, learning 0.214s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0238
             Mean action noise std: 0.71
                       Mean reward: 20.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 8.13s
                        Total time: 7683.93s
                               ETA: 847044.0s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.128s, learning 0.168s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0297
             Mean action noise std: 0.71
                       Mean reward: 20.48
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 8.30s
                        Total time: 7692.22s
                               ETA: 847007.7s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.879s, learning 0.207s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0448
             Mean action noise std: 0.71
                       Mean reward: 20.48
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 8.09s
                        Total time: 7700.31s
                               ETA: 846948.4s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.111s, learning 0.174s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0391
             Mean action noise std: 0.71
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 8.29s
                        Total time: 7708.59s
                               ETA: 846911.2s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 2050 steps/s (collection: 7.803s, learning 0.186s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0524
             Mean action noise std: 0.71
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 7.99s
                        Total time: 7716.58s
                               ETA: 846841.4s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.033s, learning 0.167s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0466
             Mean action noise std: 0.71
                       Mean reward: 20.53
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 8.20s
                        Total time: 7724.78s
                               ETA: 846795.0s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.130s, learning 0.161s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0444
             Mean action noise std: 0.71
                       Mean reward: 20.54
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 8.29s
                        Total time: 7733.07s
                               ETA: 846758.6s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.284s, learning 0.164s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0604
             Mean action noise std: 0.71
                       Mean reward: 20.54
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 8.45s
                        Total time: 7741.52s
                               ETA: 846739.5s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.985s, learning 0.154s)
               Value function loss: 1.8518
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 8.14s
                        Total time: 7749.66s
                               ETA: 846686.7s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 2047 steps/s (collection: 7.787s, learning 0.213s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 8.00s
                        Total time: 7757.66s
                               ETA: 846618.7s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.402s, learning 0.274s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 8.68s
                        Total time: 7766.34s
                               ETA: 846624.6s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.937s, learning 0.172s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 8.11s
                        Total time: 7774.45s
                               ETA: 846568.7s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 2068 steps/s (collection: 7.725s, learning 0.194s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 7.92s
                        Total time: 7782.36s
                               ETA: 846492.3s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.889s, learning 0.166s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 8.05s
                        Total time: 7790.42s
                               ETA: 846430.8s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.183s, learning 0.273s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0495
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 8.46s
                        Total time: 7798.88s
                               ETA: 846412.9s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.149s, learning 0.177s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 8.33s
                        Total time: 7807.20s
                               ETA: 846381.0s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.097s, learning 0.199s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0498
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 8.30s
                        Total time: 7815.50s
                               ETA: 846345.8s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.281s, learning 0.172s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0450
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 8.45s
                        Total time: 7823.95s
                               ETA: 846327.6s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.002s, learning 0.204s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0515
             Mean action noise std: 0.71
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 8.21s
                        Total time: 7832.16s
                               ETA: 846282.8s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.085s, learning 0.184s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0476
             Mean action noise std: 0.71
                       Mean reward: 20.92
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 8.27s
                        Total time: 7840.43s
                               ETA: 846245.0s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.289s, learning 0.191s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0488
             Mean action noise std: 0.71
                       Mean reward: 20.92
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 8.48s
                        Total time: 7848.91s
                               ETA: 846229.9s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.263s, learning 0.176s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0535
             Mean action noise std: 0.71
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 8.44s
                        Total time: 7857.34s
                               ETA: 846210.4s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.883s, learning 0.167s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0507
             Mean action noise std: 0.71
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 8.05s
                        Total time: 7865.39s
                               ETA: 846149.1s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.155s, learning 0.198s)
               Value function loss: 5.5538
                    Surrogate loss: 0.0234
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 8.35s
                        Total time: 7873.75s
                               ETA: 846120.4s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.005s, learning 0.177s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 8.18s
                        Total time: 7881.93s
                               ETA: 846073.5s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.891s, learning 0.159s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0173
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 8.05s
                        Total time: 7889.98s
                               ETA: 846012.5s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.893s, learning 0.227s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0256
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 8.12s
                        Total time: 7898.10s
                               ETA: 845959.0s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.948s, learning 0.159s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0333
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 8.11s
                        Total time: 7906.21s
                               ETA: 845904.3s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.040s, learning 0.163s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 8.20s
                        Total time: 7914.41s
                               ETA: 845859.9s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.144s, learning 0.163s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0406
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 8.31s
                        Total time: 7922.72s
                               ETA: 845826.7s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 2095 steps/s (collection: 7.657s, learning 0.164s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 7.82s
                        Total time: 7930.54s
                               ETA: 845741.7s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.061s, learning 0.192s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0446
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 8.25s
                        Total time: 7938.79s
                               ETA: 845703.0s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 2031 steps/s (collection: 7.896s, learning 0.168s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 8.06s
                        Total time: 7946.85s
                               ETA: 845644.2s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 2030 steps/s (collection: 7.910s, learning 0.159s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0468
             Mean action noise std: 0.71
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 8.07s
                        Total time: 7954.92s
                               ETA: 845586.0s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.320s, learning 0.181s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 8.50s
                        Total time: 7963.42s
                               ETA: 845573.7s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.029s, learning 0.161s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0567
             Mean action noise std: 0.71
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 8.19s
                        Total time: 7971.61s
                               ETA: 845528.6s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 2044 steps/s (collection: 7.846s, learning 0.167s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0403
             Mean action noise std: 0.71
                       Mean reward: 21.45
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 8.01s
                        Total time: 7979.63s
                               ETA: 845464.8s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.901s, learning 0.171s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0483
             Mean action noise std: 0.71
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 8.07s
                        Total time: 7987.70s
                               ETA: 845407.3s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.163s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0500
             Mean action noise std: 0.71
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 8.39s
                        Total time: 7996.09s
                               ETA: 845383.5s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 1136 steps/s (collection: 14.222s, learning 0.198s)
               Value function loss: 4.9887
                    Surrogate loss: 0.0436
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 14.42s
                        Total time: 8010.51s
                               ETA: 845996.6s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1050 steps/s (collection: 15.435s, learning 0.160s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0171
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 15.59s
                        Total time: 8026.10s
                               ETA: 846732.3s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1041 steps/s (collection: 15.551s, learning 0.176s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 15.73s
                        Total time: 8041.83s
                               ETA: 847480.3s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.950s, learning 0.172s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 16.12s
                        Total time: 8057.95s
                               ETA: 848268.4s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1049 steps/s (collection: 15.444s, learning 0.163s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0430
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 15.61s
                        Total time: 8073.56s
                               ETA: 849000.6s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1015 steps/s (collection: 15.965s, learning 0.173s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0410
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 16.14s
                        Total time: 8089.70s
                               ETA: 849786.9s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1062 steps/s (collection: 15.256s, learning 0.169s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0478
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 15.42s
                        Total time: 8105.12s
                               ETA: 850496.6s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.782s, learning 0.176s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0390
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 15.96s
                        Total time: 8121.08s
                               ETA: 851260.8s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1033 steps/s (collection: 15.692s, learning 0.164s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0413
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 15.86s
                        Total time: 8136.93s
                               ETA: 852012.7s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.795s, learning 0.173s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 15.97s
                        Total time: 8152.90s
                               ETA: 852774.7s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1058 steps/s (collection: 15.280s, learning 0.194s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0493
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 15.47s
                        Total time: 8168.38s
                               ETA: 853483.3s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.787s, learning 0.172s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0454
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 15.96s
                        Total time: 8184.34s
                               ETA: 854241.1s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.951s, learning 0.211s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0529
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 16.16s
                        Total time: 8200.50s
                               ETA: 855018.4s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1027 steps/s (collection: 15.778s, learning 0.163s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0491
             Mean action noise std: 0.71
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 15.94s
                        Total time: 8216.44s
                               ETA: 855771.0s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1050 steps/s (collection: 15.425s, learning 0.165s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0458
             Mean action noise std: 0.71
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 15.59s
                        Total time: 8232.03s
                               ETA: 856485.6s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1027 steps/s (collection: 15.750s, learning 0.193s)
               Value function loss: 8.0404
                    Surrogate loss: 0.0634
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 15.94s
                        Total time: 8247.97s
                               ETA: 857235.2s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1043 steps/s (collection: 15.509s, learning 0.188s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 15.70s
                        Total time: 8263.67s
                               ETA: 857957.6s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.995s, learning 0.175s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 16.17s
                        Total time: 8279.84s
                               ETA: 858727.5s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1034 steps/s (collection: 15.682s, learning 0.159s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 15.84s
                        Total time: 8295.68s
                               ETA: 859461.8s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1037 steps/s (collection: 15.601s, learning 0.194s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 15.80s
                        Total time: 8311.47s
                               ETA: 860189.7s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1034 steps/s (collection: 15.649s, learning 0.193s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 15.84s
                        Total time: 8327.31s
                               ETA: 860920.9s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.794s, learning 0.166s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 15.96s
                        Total time: 8343.28s
                               ETA: 861662.9s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1024 steps/s (collection: 15.824s, learning 0.163s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 15.99s
                        Total time: 8359.26s
                               ETA: 862405.9s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1048 steps/s (collection: 15.458s, learning 0.175s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 15.63s
                        Total time: 8374.90s
                               ETA: 863111.0s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.917s, learning 0.167s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 16.08s
                        Total time: 8390.98s
                               ETA: 863860.9s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1045 steps/s (collection: 15.503s, learning 0.164s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0506
             Mean action noise std: 0.71
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 15.67s
                        Total time: 8406.65s
                               ETA: 864566.3s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1031 steps/s (collection: 15.687s, learning 0.195s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0411
             Mean action noise std: 0.71
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 15.88s
                        Total time: 8422.53s
                               ETA: 865292.4s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.051s, learning 0.170s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0524
             Mean action noise std: 0.71
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 16.22s
                        Total time: 8438.75s
                               ETA: 866051.8s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1032 steps/s (collection: 15.701s, learning 0.161s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0370
             Mean action noise std: 0.71
                       Mean reward: 21.79
               Mean episode length: 124.77
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 15.86s
                        Total time: 8454.61s
                               ETA: 866772.7s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1044 steps/s (collection: 15.507s, learning 0.179s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0418
             Mean action noise std: 0.71
                       Mean reward: 21.78
               Mean episode length: 124.64
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 15.69s
                        Total time: 8470.30s
                               ETA: 867474.1s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1047 steps/s (collection: 15.462s, learning 0.174s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0536
             Mean action noise std: 0.71
                       Mean reward: 21.78
               Mean episode length: 124.64
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 15.64s
                        Total time: 8485.93s
                               ETA: 868168.8s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.872s, learning 0.165s)
               Value function loss: 6.7169
                    Surrogate loss: 0.0419
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 16.04s
                        Total time: 8501.97s
                               ETA: 868903.1s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.062s, learning 0.192s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 16.25s
                        Total time: 8518.22s
                               ETA: 869657.9s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1071 steps/s (collection: 15.123s, learning 0.167s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0217
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 15.29s
                        Total time: 8533.51s
                               ETA: 870312.9s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.007s, learning 0.176s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0282
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 16.18s
                        Total time: 8549.70s
                               ETA: 871057.5s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1056 steps/s (collection: 15.354s, learning 0.158s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0459
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 15.51s
                        Total time: 8565.21s
                               ETA: 871732.2s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1029 steps/s (collection: 15.729s, learning 0.180s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0472
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 15.91s
                        Total time: 8581.12s
                               ETA: 872446.0s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1095 steps/s (collection: 14.741s, learning 0.211s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0461
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 14.95s
                        Total time: 8596.07s
                               ETA: 873061.0s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 2089 steps/s (collection: 7.684s, learning 0.156s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0463
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 7.84s
                        Total time: 8603.91s
                               ETA: 872953.0s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.999s, learning 0.164s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0415
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 8.16s
                        Total time: 8612.07s
                               ETA: 872878.1s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.046s, learning 0.170s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0426
             Mean action noise std: 0.71
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 8.22s
                        Total time: 8620.29s
                               ETA: 872808.7s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.064s, learning 0.172s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0462
             Mean action noise std: 0.71
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 8.24s
                        Total time: 8628.53s
                               ETA: 872741.4s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.750s, learning 0.198s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 7.95s
                        Total time: 8636.47s
                               ETA: 872645.1s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.973s, learning 0.190s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0470
             Mean action noise std: 0.71
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 8.16s
                        Total time: 8644.64s
                               ETA: 872570.7s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 2034 steps/s (collection: 7.887s, learning 0.167s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0444
             Mean action noise std: 0.71
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 8.05s
                        Total time: 8652.69s
                               ETA: 872485.5s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.322s, learning 0.203s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0472
             Mean action noise std: 0.71
                       Mean reward: 21.92
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 8.52s
                        Total time: 8661.22s
                               ETA: 872447.8s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 2014 steps/s (collection: 7.970s, learning 0.164s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0588
             Mean action noise std: 0.71
                       Mean reward: 21.92
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 8.13s
                        Total time: 8669.35s
                               ETA: 872370.9s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.056s, learning 0.161s)
               Value function loss: 3.0918
                    Surrogate loss: 0.0141
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 8.22s
                        Total time: 8677.57s
                               ETA: 872302.5s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.090s, learning 0.167s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0265
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 8.26s
                        Total time: 8685.82s
                               ETA: 872238.2s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 2062 steps/s (collection: 7.780s, learning 0.162s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 7.94s
                        Total time: 8693.77s
                               ETA: 872142.4s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.203s, learning 0.159s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0395
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 8.36s
                        Total time: 8702.13s
                               ETA: 872088.8s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 2046 steps/s (collection: 7.849s, learning 0.159s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 8.01s
                        Total time: 8710.13s
                               ETA: 871999.9s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.985s, learning 0.171s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 8.16s
                        Total time: 8718.29s
                               ETA: 871926.0s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.141s, learning 0.185s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0527
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 8.33s
                        Total time: 8726.62s
                               ETA: 871869.1s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.210s, learning 0.180s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0451
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 8.39s
                        Total time: 8735.01s
                               ETA: 871818.8s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.017s, learning 0.173s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0484
             Mean action noise std: 0.71
                       Mean reward: 21.11
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 8.19s
                        Total time: 8743.20s
                               ETA: 871748.5s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 2043 steps/s (collection: 7.854s, learning 0.164s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0488
             Mean action noise std: 0.71
                       Mean reward: 21.14
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 8.02s
                        Total time: 8751.21s
                               ETA: 871661.3s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.081s, learning 0.165s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0445
             Mean action noise std: 0.71
                       Mean reward: 21.14
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 8.25s
                        Total time: 8759.46s
                               ETA: 871597.0s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.259s, learning 0.241s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0462
             Mean action noise std: 0.71
                       Mean reward: 21.13
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 8.50s
                        Total time: 8767.96s
                               ETA: 871557.9s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.324s, learning 0.176s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0443
             Mean action noise std: 0.71
                       Mean reward: 21.14
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 8.50s
                        Total time: 8776.46s
                               ETA: 871519.0s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.231s, learning 0.194s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0526
             Mean action noise std: 0.71
                       Mean reward: 21.14
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 8.43s
                        Total time: 8784.88s
                               ETA: 871472.8s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.161s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0502
             Mean action noise std: 0.71
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 8.34s
                        Total time: 8793.22s
                               ETA: 871417.8s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.116s, learning 0.163s)
               Value function loss: 7.1551
                    Surrogate loss: 0.0684
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 8.28s
                        Total time: 8801.50s
                               ETA: 871357.1s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.965s, learning 0.175s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 8.14s
                        Total time: 8809.64s
                               ETA: 871282.9s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.135s, learning 0.162s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 8.30s
                        Total time: 8817.93s
                               ETA: 871224.2s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.931s, learning 0.167s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 8.10s
                        Total time: 8826.03s
                               ETA: 871146.1s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.073s, learning 0.158s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0374
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 8.23s
                        Total time: 8834.26s
                               ETA: 871081.3s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.923s, learning 0.158s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0359
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 8.08s
                        Total time: 8842.34s
                               ETA: 871001.8s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.031s, learning 0.163s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0490
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 8.19s
                        Total time: 8850.54s
                               ETA: 870933.5s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.043s, learning 0.172s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0500
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 8.22s
                        Total time: 8858.75s
                               ETA: 870867.4s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.827s, learning 0.171s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0519
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 8.00s
                        Total time: 8866.75s
                               ETA: 870780.2s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.996s, learning 0.164s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0489
             Mean action noise std: 0.71
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 8.16s
                        Total time: 8874.91s
                               ETA: 870709.0s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.099s, learning 0.257s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0501
             Mean action noise std: 0.71
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 8.36s
                        Total time: 8883.27s
                               ETA: 870657.1s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.902s, learning 0.234s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0493
             Mean action noise std: 0.71
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 8.14s
                        Total time: 8891.40s
                               ETA: 870583.7s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.311s, learning 0.257s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0507
             Mean action noise std: 0.71
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 8.57s
                        Total time: 8899.97s
                               ETA: 870552.8s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.137s, learning 0.192s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0449
             Mean action noise std: 0.71
                       Mean reward: 21.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 8.33s
                        Total time: 8908.30s
                               ETA: 870498.5s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.853s, learning 0.226s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0450
             Mean action noise std: 0.71
                       Mean reward: 21.32
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 8.08s
                        Total time: 8916.38s
                               ETA: 870419.9s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.063s, learning 0.162s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0531
             Mean action noise std: 0.71
                       Mean reward: 21.32
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 8.23s
                        Total time: 8924.61s
                               ETA: 870355.7s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.894s, learning 0.169s)
               Value function loss: 5.4242
                    Surrogate loss: 0.0144
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 8.06s
                        Total time: 8932.67s
                               ETA: 870275.8s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 2109 steps/s (collection: 7.599s, learning 0.168s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 7.77s
                        Total time: 8940.44s
                               ETA: 870167.3s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.127s, learning 0.236s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 8.36s
                        Total time: 8948.80s
                               ETA: 870116.8s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.907s, learning 0.171s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0423
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 8.08s
                        Total time: 8956.88s
                               ETA: 870038.8s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.031s, learning 0.178s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 8.21s
                        Total time: 8965.09s
                               ETA: 869973.6s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.995s, learning 0.171s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0488
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 8.17s
                        Total time: 8973.25s
                               ETA: 869904.4s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.791s, learning 0.209s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0482
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 8.00s
                        Total time: 8981.25s
                               ETA: 869819.2s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.005s, learning 0.163s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0454
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 8.17s
                        Total time: 8989.42s
                               ETA: 869750.5s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 2097 steps/s (collection: 7.653s, learning 0.160s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0406
             Mean action noise std: 0.71
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 7.81s
                        Total time: 8997.23s
                               ETA: 869647.5s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.040s, learning 0.155s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0379
             Mean action noise std: 0.71
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 8.19s
                        Total time: 9005.43s
                               ETA: 869581.6s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.862s, learning 0.162s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0475
             Mean action noise std: 0.71
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 8.02s
                        Total time: 9013.45s
                               ETA: 869499.3s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.034s, learning 0.162s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0406
             Mean action noise std: 0.71
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 8.20s
                        Total time: 9021.65s
                               ETA: 869433.7s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.896s, learning 0.159s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0589
             Mean action noise std: 0.71
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 8.06s
                        Total time: 9029.70s
                               ETA: 869354.7s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.834s, learning 0.163s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0451
             Mean action noise std: 0.71
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 8.00s
                        Total time: 9037.70s
                               ETA: 869270.3s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.932s, learning 0.166s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0451
             Mean action noise std: 0.71
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 8.10s
                        Total time: 9045.80s
                               ETA: 869195.6s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.960s, learning 0.166s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0458
             Mean action noise std: 0.71
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 8.13s
                        Total time: 9053.92s
                               ETA: 869123.9s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.986s, learning 0.176s)
               Value function loss: 1.8934
                    Surrogate loss: 0.0355
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 8.16s
                        Total time: 9062.08s
                               ETA: 869055.6s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.005s, learning 0.169s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 8.17s
                        Total time: 9070.26s
                               ETA: 868988.7s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.020s, learning 0.161s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0263
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 8.18s
                        Total time: 9078.44s
                               ETA: 868922.5s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.928s, learning 0.163s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0342
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 8.09s
                        Total time: 9086.53s
                               ETA: 868847.8s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.453s, learning 0.167s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0301
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 8.62s
                        Total time: 9095.15s
                               ETA: 868823.8s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.046s, learning 0.162s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0388
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 8.21s
                        Total time: 9103.36s
                               ETA: 868760.5s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.107s, learning 0.163s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0500
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 8.27s
                        Total time: 9111.63s
                               ETA: 868703.3s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 2102 steps/s (collection: 7.608s, learning 0.185s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 7.79s
                        Total time: 9119.42s
                               ETA: 868600.7s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.191s, learning 0.179s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0378
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 124.45
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 8.37s
                        Total time: 9127.79s
                               ETA: 868553.2s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.169s, learning 0.162s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0550
             Mean action noise std: 0.71
                       Mean reward: 21.98
               Mean episode length: 124.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 8.33s
                        Total time: 9136.12s
                               ETA: 868502.0s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.896s, learning 0.284s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0489
             Mean action noise std: 0.71
                       Mean reward: 21.98
               Mean episode length: 124.45
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 8.18s
                        Total time: 9144.30s
                               ETA: 868436.6s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.784s, learning 0.173s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0470
             Mean action noise std: 0.71
                       Mean reward: 21.99
               Mean episode length: 124.45
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 7.96s
                        Total time: 9152.26s
                               ETA: 868350.2s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.044s, learning 0.196s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0413
             Mean action noise std: 0.71
                       Mean reward: 21.95
               Mean episode length: 124.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 8.24s
                        Total time: 9160.50s
                               ETA: 868290.7s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.208s, learning 0.165s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0555
             Mean action noise std: 0.71
                       Mean reward: 21.95
               Mean episode length: 124.45
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 8.37s
                        Total time: 9168.87s
                               ETA: 868244.0s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.291s, learning 0.185s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0451
             Mean action noise std: 0.71
                       Mean reward: 21.95
               Mean episode length: 124.45
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 8.48s
                        Total time: 9177.35s
                               ETA: 868207.0s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.228s, learning 0.301s)
               Value function loss: 5.0677
                    Surrogate loss: 0.0242
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 8.53s
                        Total time: 9185.88s
                               ETA: 868175.1s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.003s, learning 0.163s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 8.17s
                        Total time: 9194.04s
                               ETA: 868109.0s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.874s, learning 0.184s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 8.06s
                        Total time: 9202.10s
                               ETA: 868032.7s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.141s, learning 0.196s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 8.34s
                        Total time: 9210.44s
                               ETA: 867982.9s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.093s, learning 0.174s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0354
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 8.27s
                        Total time: 9218.70s
                               ETA: 867926.6s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.779s, learning 0.174s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0276
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 7.95s
                        Total time: 9226.66s
                               ETA: 867840.9s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.048s, learning 0.160s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0466
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 8.21s
                        Total time: 9234.87s
                               ETA: 867779.2s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.938s, learning 0.170s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0500
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 8.11s
                        Total time: 9242.97s
                               ETA: 867708.3s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.234s, learning 0.181s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0397
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 8.41s
                        Total time: 9251.39s
                               ETA: 867666.3s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.295s, learning 0.162s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 8.46s
                        Total time: 9259.85s
                               ETA: 867628.3s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.997s, learning 0.189s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0514
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 8.19s
                        Total time: 9268.03s
                               ETA: 867564.9s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.194s, learning 0.159s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0466
             Mean action noise std: 0.71
                       Mean reward: 22.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 8.35s
                        Total time: 9276.38s
                               ETA: 867517.3s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.090s, learning 0.163s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0554
             Mean action noise std: 0.71
                       Mean reward: 22.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 8.25s
                        Total time: 9284.64s
                               ETA: 867460.4s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.028s, learning 0.158s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0462
             Mean action noise std: 0.71
                       Mean reward: 22.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 8.19s
                        Total time: 9292.82s
                               ETA: 867397.4s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.981s, learning 0.179s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0423
             Mean action noise std: 0.71
                       Mean reward: 22.08
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 8.16s
                        Total time: 9300.98s
                               ETA: 867332.1s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.163s, learning 0.183s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0515
             Mean action noise std: 0.71
                       Mean reward: 22.08
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 8.35s
                        Total time: 9309.33s
                               ETA: 867284.2s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.427s, learning 0.213s)
               Value function loss: 4.3823
                    Surrogate loss: 0.0276
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 8.64s
                        Total time: 9317.97s
                               ETA: 867263.7s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.072s, learning 0.163s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0257
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 8.24s
                        Total time: 9326.20s
                               ETA: 867205.6s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.831s, learning 0.161s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 7.99s
                        Total time: 9334.20s
                               ETA: 867124.9s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 2139 steps/s (collection: 7.475s, learning 0.185s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0395
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 7.66s
                        Total time: 9341.86s
                               ETA: 867013.6s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.846s, learning 0.235s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 8.08s
                        Total time: 9349.94s
                               ETA: 866941.5s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.926s, learning 0.258s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0431
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 8.18s
                        Total time: 9358.12s
                               ETA: 866879.2s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.120s, learning 0.169s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0463
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 8.29s
                        Total time: 9366.41s
                               ETA: 866826.7s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.068s, learning 0.168s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0440
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 8.24s
                        Total time: 9374.65s
                               ETA: 866769.3s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.859s, learning 0.173s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0475
             Mean action noise std: 0.71
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 8.03s
                        Total time: 9382.68s
                               ETA: 866693.2s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.126s, learning 0.166s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0528
             Mean action noise std: 0.71
                       Mean reward: 22.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 8.29s
                        Total time: 9390.97s
                               ETA: 866641.1s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.180s, learning 0.167s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0476
             Mean action noise std: 0.71
                       Mean reward: 22.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 8.35s
                        Total time: 9399.32s
                               ETA: 866594.3s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.965s, learning 0.170s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0474
             Mean action noise std: 0.71
                       Mean reward: 22.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 8.13s
                        Total time: 9407.45s
                               ETA: 866527.9s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.964s, learning 0.193s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0513
             Mean action noise std: 0.71
                       Mean reward: 22.39
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 8.16s
                        Total time: 9415.61s
                               ETA: 866463.8s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.030s, learning 0.163s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0495
             Mean action noise std: 0.71
                       Mean reward: 22.46
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 8.19s
                        Total time: 9423.80s
                               ETA: 866403.0s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.988s, learning 0.161s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0408
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 8.15s
                        Total time: 9431.95s
                               ETA: 866338.3s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.041s, learning 0.178s)
               Value function loss: 3.3615
                    Surrogate loss: 0.0328
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 8.22s
                        Total time: 9440.17s
                               ETA: 866280.1s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.162s)
               Value function loss: 0.5208
                    Surrogate loss: -0.0027
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 8.43s
                        Total time: 9448.60s
                               ETA: 866241.2s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.082s, learning 0.173s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 8.25s
                        Total time: 9456.85s
                               ETA: 866186.4s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.809s, learning 0.158s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0334
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 7.97s
                        Total time: 9464.82s
                               ETA: 866105.4s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.992s, learning 0.161s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 8.15s
                        Total time: 9472.97s
                               ETA: 866041.6s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.169s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0356
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 8.43s
                        Total time: 9481.40s
                               ETA: 866003.1s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.276s, learning 0.171s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0429
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 8.45s
                        Total time: 9489.85s
                               ETA: 865966.2s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.038s, learning 0.220s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0416
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 8.26s
                        Total time: 9498.11s
                               ETA: 865912.2s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.013s, learning 0.193s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 8.21s
                        Total time: 9506.31s
                               ETA: 865853.6s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.854s, learning 0.183s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 8.04s
                        Total time: 9514.35s
                               ETA: 865779.7s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.002s, learning 0.172s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0593
             Mean action noise std: 0.71
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 8.17s
                        Total time: 9522.52s
                               ETA: 865718.3s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.709s, learning 0.163s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0360
             Mean action noise std: 0.71
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 7.87s
                        Total time: 9530.40s
                               ETA: 865629.6s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.067s, learning 0.194s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0523
             Mean action noise std: 0.71
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 8.26s
                        Total time: 9538.66s
                               ETA: 865576.3s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.342s, learning 0.184s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0396
             Mean action noise std: 0.71
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 8.53s
                        Total time: 9547.18s
                               ETA: 865547.1s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.435s, learning 0.179s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0424
             Mean action noise std: 0.71
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 8.61s
                        Total time: 9555.80s
                               ETA: 865526.0s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.268s, learning 0.166s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0512
             Mean action noise std: 0.71
                       Mean reward: 22.39
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 8.43s
                        Total time: 9564.23s
                               ETA: 865488.6s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.069s, learning 0.166s)
               Value function loss: 4.6968
                    Surrogate loss: 0.0922
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 8.24s
                        Total time: 9572.47s
                               ETA: 865433.3s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 2102 steps/s (collection: 7.604s, learning 0.188s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 7.79s
                        Total time: 9580.26s
                               ETA: 865338.1s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.045s, learning 0.167s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0255
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 8.21s
                        Total time: 9588.47s
                               ETA: 865280.9s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.094s, learning 0.166s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0371
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 8.26s
                        Total time: 9596.73s
                               ETA: 865228.0s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.855s, learning 0.334s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0424
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 8.19s
                        Total time: 9604.92s
                               ETA: 865169.0s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.138s, learning 0.177s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0426
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 8.32s
                        Total time: 9613.24s
                               ETA: 865121.3s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.184s, learning 0.233s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0453
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 8.42s
                        Total time: 9621.65s
                               ETA: 865082.9s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.097s, learning 0.175s)
               Value function loss: 0.0441
                    Surrogate loss: -0.0393
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 8.27s
                        Total time: 9629.93s
                               ETA: 865031.4s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1968 steps/s (collection: 7.993s, learning 0.330s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0428
             Mean action noise std: 0.71
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 8.32s
                        Total time: 9638.25s
                               ETA: 864984.6s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.243s, learning 0.213s)
               Value function loss: 0.0520
                    Surrogate loss: -0.0367
             Mean action noise std: 0.71
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 8.46s
                        Total time: 9646.70s
                               ETA: 864949.9s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.743s, learning 0.168s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0574
             Mean action noise std: 0.71
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 7.91s
                        Total time: 9654.62s
                               ETA: 864866.4s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.013s, learning 0.164s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0423
             Mean action noise std: 0.71
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 8.18s
                        Total time: 9662.79s
                               ETA: 864806.8s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.896s, learning 0.169s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0523
             Mean action noise std: 0.71
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 8.06s
                        Total time: 9670.86s
                               ETA: 864737.2s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.121s, learning 0.164s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0438
             Mean action noise std: 0.71
                       Mean reward: 22.06
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 8.29s
                        Total time: 9679.14s
                               ETA: 864687.5s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.069s, learning 0.165s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0454
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 8.23s
                        Total time: 9687.38s
                               ETA: 864633.3s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 2077 steps/s (collection: 7.725s, learning 0.161s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0535
             Mean action noise std: 0.71
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 7.89s
                        Total time: 9695.26s
                               ETA: 864548.1s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.077s, learning 0.166s)
               Value function loss: 2.9291
                    Surrogate loss: 0.0244
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 8.24s
                        Total time: 9703.50s
                               ETA: 864494.8s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.944s, learning 0.250s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 8.19s
                        Total time: 9711.70s
                               ETA: 864437.2s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.163s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 8.36s
                        Total time: 9720.06s
                               ETA: 864394.5s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.838s, learning 0.168s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0387
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 8.01s
                        Total time: 9728.06s
                               ETA: 864320.4s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.127s, learning 0.165s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 8.29s
                        Total time: 9736.36s
                               ETA: 864271.9s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.244s, learning 0.167s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0461
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 8.41s
                        Total time: 9744.77s
                               ETA: 864234.1s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.257s, learning 0.163s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0501
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 8.42s
                        Total time: 9753.19s
                               ETA: 864196.9s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.114s, learning 0.164s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0446
             Mean action noise std: 0.71
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 8.28s
                        Total time: 9761.46s
                               ETA: 864147.3s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.051s, learning 0.171s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0377
             Mean action noise std: 0.71
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 8.22s
                        Total time: 9769.69s
                               ETA: 864092.8s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.046s, learning 0.170s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0573
             Mean action noise std: 0.71
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 8.22s
                        Total time: 9777.90s
                               ETA: 864037.9s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.840s, learning 0.163s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0523
             Mean action noise std: 0.71
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 8.00s
                        Total time: 9785.90s
                               ETA: 863964.3s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.227s, learning 0.219s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0462
             Mean action noise std: 0.71
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 8.45s
                        Total time: 9794.35s
                               ETA: 863929.9s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.150s, learning 0.167s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0447
             Mean action noise std: 0.71
                       Mean reward: 22.50
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 8.32s
                        Total time: 9802.67s
                               ETA: 863884.1s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.954s, learning 0.167s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0477
             Mean action noise std: 0.71
                       Mean reward: 22.54
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 8.12s
                        Total time: 9810.79s
                               ETA: 863821.2s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.035s, learning 0.171s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0447
             Mean action noise std: 0.70
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 8.21s
                        Total time: 9818.99s
                               ETA: 863765.7s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.923s, learning 0.163s)
               Value function loss: 5.2507
                    Surrogate loss: 0.0421
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 8.09s
                        Total time: 9827.08s
                               ETA: 863699.8s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.039s, learning 0.160s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0221
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 8.20s
                        Total time: 9835.28s
                               ETA: 863644.1s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.045s, learning 0.189s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0298
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 8.23s
                        Total time: 9843.51s
                               ETA: 863591.4s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.053s, learning 0.189s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0288
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 8.24s
                        Total time: 9851.76s
                               ETA: 863539.6s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.211s, learning 0.179s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0347
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 8.39s
                        Total time: 9860.15s
                               ETA: 863500.7s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.903s, learning 0.165s)
               Value function loss: 0.0634
                    Surrogate loss: -0.0399
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 8.07s
                        Total time: 9868.21s
                               ETA: 863433.7s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.025s, learning 0.162s)
               Value function loss: 0.0605
                    Surrogate loss: -0.0438
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 8.19s
                        Total time: 9876.40s
                               ETA: 863377.3s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.298s, learning 0.167s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0419
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 8.47s
                        Total time: 9884.87s
                               ETA: 863345.3s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.921s, learning 0.191s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0414
             Mean action noise std: 0.70
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 8.11s
                        Total time: 9892.98s
                               ETA: 863282.4s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.051s, learning 0.161s)
               Value function loss: 0.0570
                    Surrogate loss: -0.0478
             Mean action noise std: 0.70
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 8.21s
                        Total time: 9901.19s
                               ETA: 863228.3s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.304s, learning 0.187s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0595
             Mean action noise std: 0.70
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 8.49s
                        Total time: 9909.68s
                               ETA: 863198.8s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.215s, learning 0.168s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0438
             Mean action noise std: 0.70
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 8.38s
                        Total time: 9918.06s
                               ETA: 863159.8s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.155s, learning 0.180s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0580
             Mean action noise std: 0.70
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 8.34s
                        Total time: 9926.40s
                               ETA: 863116.6s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.048s, learning 0.185s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0443
             Mean action noise std: 0.70
                       Mean reward: 21.98
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 8.23s
                        Total time: 9934.63s
                               ETA: 863064.7s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.735s, learning 0.161s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0441
             Mean action noise std: 0.70
                       Mean reward: 21.98
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 7.90s
                        Total time: 9942.53s
                               ETA: 862983.6s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.921s, learning 0.168s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0560
             Mean action noise std: 0.70
                       Mean reward: 21.98
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 8.09s
                        Total time: 9950.62s
                               ETA: 862919.3s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.356s, learning 0.225s)
               Value function loss: 4.2523
                    Surrogate loss: 0.0572
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 8.58s
                        Total time: 9959.20s
                               ETA: 862897.8s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.228s, learning 0.172s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 8.40s
                        Total time: 9967.60s
                               ETA: 862860.6s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.823s, learning 0.162s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0305
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 7.98s
                        Total time: 9975.58s
                               ETA: 862787.6s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.085s, learning 0.235s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0398
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 8.32s
                        Total time: 9983.90s
                               ETA: 862743.6s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.199s, learning 0.188s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0247
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 8.39s
                        Total time: 9992.29s
                               ETA: 862705.5s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.298s, learning 0.314s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0248
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 8.61s
                        Total time: 10000.90s
                               ETA: 862686.8s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.075s, learning 0.169s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0355
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 8.24s
                        Total time: 10009.14s
                               ETA: 862636.4s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.210s, learning 0.261s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 8.47s
                        Total time: 10017.62s
                               ETA: 862605.7s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.273s, learning 0.185s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0502
             Mean action noise std: 0.70
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 8.46s
                        Total time: 10026.07s
                               ETA: 862574.0s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.013s, learning 0.191s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0448
             Mean action noise std: 0.70
                       Mean reward: 22.30
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 8.20s
                        Total time: 10034.28s
                               ETA: 862520.4s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.924s, learning 0.177s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0561
             Mean action noise std: 0.70
                       Mean reward: 22.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 8.10s
                        Total time: 10042.38s
                               ETA: 862458.0s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.873s, learning 0.168s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0411
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 8.04s
                        Total time: 10050.42s
                               ETA: 862390.6s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.894s, learning 0.166s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0512
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 8.06s
                        Total time: 10058.48s
                               ETA: 862325.0s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.086s, learning 0.168s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0454
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 8.25s
                        Total time: 10066.73s
                               ETA: 862276.0s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.297s, learning 0.166s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0468
             Mean action noise std: 0.70
                       Mean reward: 22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 8.46s
                        Total time: 10075.20s
                               ETA: 862245.0s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.402s, learning 0.186s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0555
             Mean action noise std: 0.70
                       Mean reward: 22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 8.59s
                        Total time: 10083.79s
                               ETA: 862224.7s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.167s, learning 0.171s)
               Value function loss: 1.5950
                    Surrogate loss: 0.0469
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 8.34s
                        Total time: 10092.12s
                               ETA: 862183.1s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.977s, learning 0.210s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0271
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 8.19s
                        Total time: 10100.31s
                               ETA: 862128.7s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.216s, learning 0.182s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0352
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 8.40s
                        Total time: 10108.71s
                               ETA: 862092.3s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.240s, learning 0.187s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0283
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 8.43s
                        Total time: 10117.14s
                               ETA: 862058.5s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.742s, learning 0.168s)
               Value function loss: 0.0466
                    Surrogate loss: -0.0279
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 7.91s
                        Total time: 10125.05s
                               ETA: 861980.6s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.188s, learning 0.188s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0319
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 8.38s
                        Total time: 10133.42s
                               ETA: 861942.6s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.386s, learning 0.164s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0483
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 8.55s
                        Total time: 10141.97s
                               ETA: 861919.4s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.162s, learning 0.160s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0472
             Mean action noise std: 0.70
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 8.32s
                        Total time: 10150.29s
                               ETA: 861876.8s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.028s, learning 0.186s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0417
             Mean action noise std: 0.70
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 8.21s
                        Total time: 10158.51s
                               ETA: 861825.1s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.995s, learning 0.165s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0566
             Mean action noise std: 0.70
                       Mean reward: 22.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 8.16s
                        Total time: 10166.67s
                               ETA: 861769.0s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.933s, learning 0.188s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0501
             Mean action noise std: 0.70
                       Mean reward: 22.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 8.12s
                        Total time: 10174.79s
                               ETA: 861709.5s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.237s, learning 0.197s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0533
             Mean action noise std: 0.70
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 8.43s
                        Total time: 10183.22s
                               ETA: 861676.7s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.009s, learning 0.183s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0421
             Mean action noise std: 0.70
                       Mean reward: 22.33
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 8.19s
                        Total time: 10191.41s
                               ETA: 861623.5s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.057s, learning 0.242s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0516
             Mean action noise std: 0.70
                       Mean reward: 22.35
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 8.30s
                        Total time: 10199.71s
                               ETA: 861579.4s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.757s, learning 0.178s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0507
             Mean action noise std: 0.70
                       Mean reward: 22.40
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 7.93s
                        Total time: 10207.65s
                               ETA: 861504.6s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.271s, learning 0.166s)
               Value function loss: 5.9016
                    Surrogate loss: 0.0059
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 8.44s
                        Total time: 10216.09s
                               ETA: 861472.3s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.952s, learning 0.165s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0232
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 8.12s
                        Total time: 10224.20s
                               ETA: 861413.1s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.035s, learning 0.160s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0316
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 8.20s
                        Total time: 10232.40s
                               ETA: 861360.5s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.965s, learning 0.158s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0376
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 8.12s
                        Total time: 10240.52s
                               ETA: 861301.9s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.012s, learning 0.163s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0350
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 8.17s
                        Total time: 10248.70s
                               ETA: 861247.8s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.201s, learning 0.158s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0356
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 8.36s
                        Total time: 10257.05s
                               ETA: 861209.2s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.875s, learning 0.186s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0468
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 8.06s
                        Total time: 10265.12s
                               ETA: 861145.6s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.885s, learning 0.281s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0470
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 8.17s
                        Total time: 10273.28s
                               ETA: 861091.0s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.156s, learning 0.160s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0453
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 8.32s
                        Total time: 10281.60s
                               ETA: 861049.0s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.072s, learning 0.283s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0396
             Mean action noise std: 0.70
                       Mean reward: 23.33
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 8.35s
                        Total time: 10289.95s
                               ETA: 861010.2s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.020s, learning 0.237s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0582
             Mean action noise std: 0.70
                       Mean reward: 23.33
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 8.26s
                        Total time: 10298.21s
                               ETA: 860963.5s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.771s, learning 0.172s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0393
             Mean action noise std: 0.70
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 7.94s
                        Total time: 10306.15s
                               ETA: 860890.5s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.457s, learning 0.187s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0485
             Mean action noise std: 0.70
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 8.64s
                        Total time: 10314.80s
                               ETA: 860876.1s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.294s, learning 0.188s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0428
             Mean action noise std: 0.70
                       Mean reward: 23.31
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 8.48s
                        Total time: 10323.28s
                               ETA: 860848.2s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.314s, learning 0.167s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0457
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 8.48s
                        Total time: 10331.76s
                               ETA: 860820.3s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1994 steps/s (collection: 7.999s, learning 0.215s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0546
             Mean action noise std: 0.70
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 8.21s
                        Total time: 10339.97s
                               ETA: 860770.1s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.061s, learning 0.166s)
               Value function loss: 4.8417
                    Surrogate loss: 0.0922
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 8.23s
                        Total time: 10348.20s
                               ETA: 860721.2s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 2087 steps/s (collection: 7.674s, learning 0.175s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 7.85s
                        Total time: 10356.05s
                               ETA: 860640.9s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.320s, learning 0.186s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0205
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 8.51s
                        Total time: 10364.56s
                               ETA: 860615.3s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.928s, learning 0.203s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 8.13s
                        Total time: 10372.69s
                               ETA: 860558.6s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.063s, learning 0.161s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0297
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 8.22s
                        Total time: 10380.91s
                               ETA: 860509.6s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.736s, learning 0.183s)
               Value function loss: 0.0451
                    Surrogate loss: -0.0289
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 7.92s
                        Total time: 10388.83s
                               ETA: 860435.5s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.060s, learning 0.200s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0397
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 8.26s
                        Total time: 10397.09s
                               ETA: 860389.8s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.973s, learning 0.159s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0331
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 8.13s
                        Total time: 10405.22s
                               ETA: 860333.5s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.226s, learning 0.187s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 8.41s
                        Total time: 10413.64s
                               ETA: 860300.4s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.123s, learning 0.186s)
               Value function loss: 0.0435
                    Surrogate loss: -0.0396
             Mean action noise std: 0.70
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 8.31s
                        Total time: 10421.94s
                               ETA: 860258.8s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.959s, learning 0.184s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0452
             Mean action noise std: 0.70
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 8.14s
                        Total time: 10430.09s
                               ETA: 860203.7s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.279s, learning 0.162s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0471
             Mean action noise std: 0.70
                       Mean reward: 23.46
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 8.44s
                        Total time: 10438.53s
                               ETA: 860173.1s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.024s, learning 0.165s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0490
             Mean action noise std: 0.70
                       Mean reward: 23.46
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 8.19s
                        Total time: 10446.72s
                               ETA: 860121.8s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.164s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0508
             Mean action noise std: 0.70
                       Mean reward: 23.50
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 8.48s
                        Total time: 10455.20s
                               ETA: 860094.3s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.946s, learning 0.195s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0440
             Mean action noise std: 0.70
                       Mean reward: 23.50
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 8.14s
                        Total time: 10463.34s
                               ETA: 860039.2s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.862s, learning 0.180s)
               Value function loss: 7.2323
                    Surrogate loss: 0.0624
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 8.04s
                        Total time: 10471.38s
                               ETA: 859976.1s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.066s, learning 0.174s)
               Value function loss: 0.1233
                    Surrogate loss: -0.0211
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 8.24s
                        Total time: 10479.62s
                               ETA: 859929.3s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.044s, learning 0.197s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0312
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 8.24s
                        Total time: 10487.86s
                               ETA: 859882.6s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.056s, learning 0.163s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0334
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 8.22s
                        Total time: 10496.08s
                               ETA: 859834.2s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.160s, learning 0.261s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0409
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 8.42s
                        Total time: 10504.50s
                               ETA: 859802.5s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.966s, learning 0.168s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0340
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 8.13s
                        Total time: 10512.63s
                               ETA: 859747.3s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.919s, learning 0.184s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0421
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 8.10s
                        Total time: 10520.74s
                               ETA: 859689.7s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.066s, learning 0.202s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0555
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 8.27s
                        Total time: 10529.01s
                               ETA: 859645.5s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.250s, learning 0.238s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0495
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 8.49s
                        Total time: 10537.50s
                               ETA: 859619.5s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.092s, learning 0.163s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0515
             Mean action noise std: 0.70
                       Mean reward: 23.47
               Mean episode length: 124.40
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 8.26s
                        Total time: 10545.75s
                               ETA: 859574.4s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.960s, learning 0.169s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0459
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 124.40
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 8.13s
                        Total time: 10553.88s
                               ETA: 859519.2s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.132s, learning 0.197s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0425
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 124.40
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 8.33s
                        Total time: 10562.21s
                               ETA: 859480.2s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.204s, learning 0.170s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0510
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 124.40
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 8.37s
                        Total time: 10570.58s
                               ETA: 859445.0s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.024s, learning 0.193s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0389
             Mean action noise std: 0.70
                       Mean reward: 23.45
               Mean episode length: 124.40
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 8.22s
                        Total time: 10578.80s
                               ETA: 859397.1s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.976s, learning 0.165s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0510
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 124.40
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 8.14s
                        Total time: 10586.94s
                               ETA: 859343.0s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.894s, learning 0.170s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0487
             Mean action noise std: 0.70
                       Mean reward: 23.46
               Mean episode length: 124.40
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 8.06s
                        Total time: 10595.01s
                               ETA: 859282.8s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.046s, learning 0.163s)
               Value function loss: 6.1977
                    Surrogate loss: 0.0882
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 8.21s
                        Total time: 10603.21s
                               ETA: 859234.4s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.178s, learning 0.247s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0193
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 8.43s
                        Total time: 10611.64s
                               ETA: 859203.6s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.086s, learning 0.162s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0293
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 8.25s
                        Total time: 10619.89s
                               ETA: 859158.5s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.883s, learning 0.175s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0314
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 8.06s
                        Total time: 10627.95s
                               ETA: 859098.1s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.145s, learning 0.171s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0340
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 8.32s
                        Total time: 10636.26s
                               ETA: 859058.7s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.846s, learning 0.166s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0479
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 8.01s
                        Total time: 10644.27s
                               ETA: 858994.6s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.132s, learning 0.205s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0530
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 8.34s
                        Total time: 10652.61s
                               ETA: 858956.9s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.273s, learning 0.202s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0434
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 8.47s
                        Total time: 10661.09s
                               ETA: 858930.4s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.121s, learning 0.208s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0478
             Mean action noise std: 0.70
                       Mean reward: 23.35
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 8.33s
                        Total time: 10669.42s
                               ETA: 858892.3s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.021s, learning 0.183s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0460
             Mean action noise std: 0.70
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 8.20s
                        Total time: 10677.62s
                               ETA: 858844.0s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.140s, learning 0.162s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0536
             Mean action noise std: 0.70
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 8.30s
                        Total time: 10685.92s
                               ETA: 858803.6s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.155s, learning 0.188s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0374
             Mean action noise std: 0.70
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 8.34s
                        Total time: 10694.26s
                               ETA: 858766.7s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.906s, learning 0.177s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0514
             Mean action noise std: 0.70
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 8.08s
                        Total time: 10702.35s
                               ETA: 858708.9s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.951s, learning 0.181s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 8.13s
                        Total time: 10710.48s
                               ETA: 858655.2s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.922s, learning 0.163s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0465
             Mean action noise std: 0.70
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 8.08s
                        Total time: 10718.56s
                               ETA: 858597.7s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.201s, learning 0.193s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0507
             Mean action noise std: 0.70
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 8.39s
                        Total time: 10726.96s
                               ETA: 858565.1s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.307s, learning 0.177s)
               Value function loss: 3.9340
                    Surrogate loss: 0.0445
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 8.48s
                        Total time: 10735.44s
                               ETA: 858539.7s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.875s, learning 0.168s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0312
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 8.04s
                        Total time: 10743.48s
                               ETA: 858479.1s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.024s, learning 0.215s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0189
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 8.24s
                        Total time: 10751.72s
                               ETA: 858434.2s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.892s, learning 0.163s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0450
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 8.06s
                        Total time: 10759.78s
                               ETA: 858374.7s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.180s, learning 0.161s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 8.34s
                        Total time: 10768.12s
                               ETA: 858338.1s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.022s, learning 0.174s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0412
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 8.20s
                        Total time: 10776.31s
                               ETA: 858290.0s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.023s, learning 0.200s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0547
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 8.22s
                        Total time: 10784.54s
                               ETA: 858244.1s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.058s, learning 0.163s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0506
             Mean action noise std: 0.70
                       Mean reward: 23.34
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 8.22s
                        Total time: 10792.76s
                               ETA: 858198.1s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.028s, learning 0.190s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0350
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 8.22s
                        Total time: 10800.98s
                               ETA: 858151.9s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.035s, learning 0.256s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0490
             Mean action noise std: 0.70
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 8.29s
                        Total time: 10809.27s
                               ETA: 858111.6s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.529s, learning 0.173s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0523
             Mean action noise std: 0.70
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 8.70s
                        Total time: 10817.97s
                               ETA: 858103.9s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.885s, learning 0.163s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0497
             Mean action noise std: 0.70
                       Mean reward: 23.35
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 8.05s
                        Total time: 10826.02s
                               ETA: 858044.4s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.939s, learning 0.192s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0467
             Mean action noise std: 0.70
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 8.13s
                        Total time: 10834.15s
                               ETA: 857991.6s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.950s, learning 0.221s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0495
             Mean action noise std: 0.70
                       Mean reward: 23.31
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 8.17s
                        Total time: 10842.32s
                               ETA: 857942.0s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.257s, learning 0.215s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0485
             Mean action noise std: 0.70
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 8.47s
                        Total time: 10850.79s
                               ETA: 857916.3s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.748s, learning 0.178s)
               Value function loss: 7.7147
                    Surrogate loss: 0.0388
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 7.93s
                        Total time: 10858.72s
                               ETA: 857847.4s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.754s, learning 0.173s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0319
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 7.93s
                        Total time: 10866.64s
                               ETA: 857778.7s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.014s, learning 0.168s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 8.18s
                        Total time: 10874.83s
                               ETA: 857730.2s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.007s, learning 0.165s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0319
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 8.17s
                        Total time: 10883.00s
                               ETA: 857681.1s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.961s, learning 0.162s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0419
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 8.12s
                        Total time: 10891.12s
                               ETA: 857628.1s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.048s, learning 0.160s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0436
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 8.21s
                        Total time: 10899.33s
                               ETA: 857581.9s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.861s, learning 0.160s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0520
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 8.02s
                        Total time: 10907.35s
                               ETA: 857521.1s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.047s, learning 0.160s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0480
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 8.21s
                        Total time: 10915.56s
                               ETA: 857474.8s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.773s, learning 0.161s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0336
             Mean action noise std: 0.70
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 7.93s
                        Total time: 10923.49s
                               ETA: 857407.3s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.929s, learning 0.166s)
               Value function loss: 0.0520
                    Surrogate loss: -0.0358
             Mean action noise std: 0.70
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 8.10s
                        Total time: 10931.59s
                               ETA: 857352.5s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.262s, learning 0.192s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0534
             Mean action noise std: 0.70
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 8.45s
                        Total time: 10940.04s
                               ETA: 857325.8s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.972s, learning 0.164s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0463
             Mean action noise std: 0.70
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 8.14s
                        Total time: 10948.18s
                               ETA: 857274.4s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.895s, learning 0.165s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0428
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 8.06s
                        Total time: 10956.24s
                               ETA: 857217.0s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.712s, learning 0.262s)
               Value function loss: 0.0456
                    Surrogate loss: -0.0300
             Mean action noise std: 0.70
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 7.97s
                        Total time: 10964.21s
                               ETA: 857153.0s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.163s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0450
             Mean action noise std: 0.70
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 8.24s
                        Total time: 10972.46s
                               ETA: 857110.2s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.128s, learning 0.175s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0578
             Mean action noise std: 0.70
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 8.30s
                        Total time: 10980.76s
                               ETA: 857072.1s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.315s, learning 0.167s)
               Value function loss: 5.5610
                    Surrogate loss: 0.0871
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 8.48s
                        Total time: 10989.24s
                               ETA: 857048.0s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.111s, learning 0.173s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0263
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 8.28s
                        Total time: 10997.52s
                               ETA: 857008.4s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.063s, learning 0.170s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0258
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 8.23s
                        Total time: 11005.76s
                               ETA: 856964.9s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 2135 steps/s (collection: 7.514s, learning 0.158s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0402
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 7.67s
                        Total time: 11013.43s
                               ETA: 856877.8s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.272s, learning 0.166s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 8.44s
                        Total time: 11021.87s
                               ETA: 856850.4s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.089s, learning 0.163s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0500
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 8.25s
                        Total time: 11030.12s
                               ETA: 856808.6s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.953s, learning 0.162s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0530
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 8.12s
                        Total time: 11038.24s
                               ETA: 856756.3s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.252s, learning 0.165s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0411
             Mean action noise std: 0.70
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 8.42s
                        Total time: 11046.65s
                               ETA: 856727.4s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.130s, learning 0.159s)
               Value function loss: 0.0740
                    Surrogate loss: -0.0476
             Mean action noise std: 0.70
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 8.29s
                        Total time: 11054.94s
                               ETA: 856688.5s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 2077 steps/s (collection: 7.692s, learning 0.193s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0468
             Mean action noise std: 0.70
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 7.89s
                        Total time: 11062.83s
                               ETA: 856618.5s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.941s, learning 0.163s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0558
             Mean action noise std: 0.70
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 8.10s
                        Total time: 11070.93s
                               ETA: 856565.5s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.093s, learning 0.171s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0433
             Mean action noise std: 0.70
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 8.26s
                        Total time: 11079.19s
                               ETA: 856525.0s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.035s, learning 0.158s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0499
             Mean action noise std: 0.70
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 8.19s
                        Total time: 11087.39s
                               ETA: 856478.9s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.825s, learning 0.167s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0512
             Mean action noise std: 0.70
                       Mean reward: 22.86
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 7.99s
                        Total time: 11095.38s
                               ETA: 856417.5s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.051s, learning 0.163s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0494
             Mean action noise std: 0.70
                       Mean reward: 22.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 8.21s
                        Total time: 11103.59s
                               ETA: 856373.3s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.809s, learning 0.161s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0503
             Mean action noise std: 0.70
                       Mean reward: 22.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 7.97s
                        Total time: 11111.56s
                               ETA: 856310.2s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.370s, learning 0.188s)
               Value function loss: 2.1571
                    Surrogate loss: 0.0045
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 8.56s
                        Total time: 11120.12s
                               ETA: 856292.6s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.007s, learning 0.166s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0388
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 8.17s
                        Total time: 11128.29s
                               ETA: 856245.3s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.862s, learning 0.161s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0270
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 8.02s
                        Total time: 11136.32s
                               ETA: 856186.6s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.055s, learning 0.161s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0481
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 8.22s
                        Total time: 11144.53s
                               ETA: 856142.8s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.936s, learning 0.185s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0458
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 8.12s
                        Total time: 11152.65s
                               ETA: 856091.8s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.900s, learning 0.168s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0584
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 8.07s
                        Total time: 11160.72s
                               ETA: 856036.7s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.005s, learning 0.197s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0529
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 8.20s
                        Total time: 11168.92s
                               ETA: 855992.1s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.371s, learning 0.181s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0424
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 8.55s
                        Total time: 11177.47s
                               ETA: 855974.2s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.080s, learning 0.163s)
               Value function loss: 0.0889
                    Surrogate loss: -0.0377
             Mean action noise std: 0.70
                       Mean reward: 23.10
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 8.24s
                        Total time: 11185.72s
                               ETA: 855932.8s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.934s, learning 0.178s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0493
             Mean action noise std: 0.70
                       Mean reward: 23.09
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 8.11s
                        Total time: 11193.83s
                               ETA: 855881.4s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.155s, learning 0.164s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0536
             Mean action noise std: 0.70
                       Mean reward: 23.09
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 8.32s
                        Total time: 11202.15s
                               ETA: 855845.8s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.856s, learning 0.181s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0478
             Mean action noise std: 0.70
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 8.04s
                        Total time: 11210.18s
                               ETA: 855788.8s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.986s, learning 0.167s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0442
             Mean action noise std: 0.70
                       Mean reward: 23.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 8.15s
                        Total time: 11218.34s
                               ETA: 855740.6s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.984s, learning 0.194s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0524
             Mean action noise std: 0.70
                       Mean reward: 23.03
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 8.18s
                        Total time: 11226.51s
                               ETA: 855694.4s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.218s, learning 0.160s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0470
             Mean action noise std: 0.70
                       Mean reward: 23.03
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 8.38s
                        Total time: 11234.89s
                               ETA: 855663.6s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.995s, learning 0.172s)
               Value function loss: 8.1341
                    Surrogate loss: 0.0332
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 8.17s
                        Total time: 11243.06s
                               ETA: 855616.8s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.115s, learning 0.187s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0246
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 8.30s
                        Total time: 11251.36s
                               ETA: 855580.3s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.058s, learning 0.189s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0340
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 8.25s
                        Total time: 11259.61s
                               ETA: 855539.6s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.778s, learning 0.185s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0445
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 7.96s
                        Total time: 11267.57s
                               ETA: 855477.4s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.976s, learning 0.170s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0439
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 8.15s
                        Total time: 11275.72s
                               ETA: 855429.2s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.055s, learning 0.164s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0520
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 8.22s
                        Total time: 11283.94s
                               ETA: 855386.6s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.822s, learning 0.157s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0544
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 7.98s
                        Total time: 11291.92s
                               ETA: 855325.8s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.418s, learning 0.198s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0452
             Mean action noise std: 0.70
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 8.62s
                        Total time: 11300.53s
                               ETA: 855313.4s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.982s, learning 0.161s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0401
             Mean action noise std: 0.70
                       Mean reward: 22.68
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 8.14s
                        Total time: 11308.67s
                               ETA: 855265.1s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.315s, learning 0.168s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0392
             Mean action noise std: 0.70
                       Mean reward: 22.72
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 8.48s
                        Total time: 11317.16s
                               ETA: 855242.7s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.961s, learning 0.159s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0590
             Mean action noise std: 0.70
                       Mean reward: 22.72
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 8.12s
                        Total time: 11325.28s
                               ETA: 855192.8s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.270s, learning 0.220s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0413
             Mean action noise std: 0.70
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 8.49s
                        Total time: 11333.77s
                               ETA: 855170.9s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.139s, learning 0.183s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0566
             Mean action noise std: 0.70
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 8.32s
                        Total time: 11342.09s
                               ETA: 855136.4s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.091s, learning 0.176s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0453
             Mean action noise std: 0.70
                       Mean reward: 22.68
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 8.27s
                        Total time: 11350.36s
                               ETA: 855097.8s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.103s, learning 0.194s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0442
             Mean action noise std: 0.70
                       Mean reward: 22.68
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 8.30s
                        Total time: 11358.65s
                               ETA: 855061.4s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.193s, learning 0.170s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0576
             Mean action noise std: 0.70
                       Mean reward: 22.68
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 8.36s
                        Total time: 11367.02s
                               ETA: 855030.1s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.094s, learning 0.158s)
               Value function loss: 5.4761
                    Surrogate loss: 0.0455
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 8.25s
                        Total time: 11375.27s
                               ETA: 854990.5s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.978s, learning 0.159s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0299
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 8.14s
                        Total time: 11383.41s
                               ETA: 854942.3s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.896s, learning 0.189s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0280
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 8.09s
                        Total time: 11391.49s
                               ETA: 854890.3s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.845s, learning 0.181s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0380
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 8.03s
                        Total time: 11399.52s
                               ETA: 854834.0s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.130s, learning 0.182s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0346
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 8.31s
                        Total time: 11407.83s
                               ETA: 854799.1s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.398s, learning 0.161s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0311
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 8.56s
                        Total time: 11416.39s
                               ETA: 854782.7s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.985s, learning 0.198s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0419
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 8.18s
                        Total time: 11424.57s
                               ETA: 854738.2s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.790s, learning 0.174s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0347
             Mean action noise std: 0.70
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 7.96s
                        Total time: 11432.54s
                               ETA: 854677.4s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.145s, learning 0.192s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0422
             Mean action noise std: 0.70
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 8.34s
                        Total time: 11440.87s
                               ETA: 854644.6s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.074s, learning 0.170s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0465
             Mean action noise std: 0.70
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 8.24s
                        Total time: 11449.12s
                               ETA: 854604.8s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.168s, learning 0.193s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0561
             Mean action noise std: 0.70
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 8.36s
                        Total time: 11457.48s
                               ETA: 854573.8s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.010s, learning 0.175s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0490
             Mean action noise std: 0.70
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 8.18s
                        Total time: 11465.66s
                               ETA: 854529.7s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.758s, learning 0.158s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0587
             Mean action noise std: 0.70
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 7.92s
                        Total time: 11473.58s
                               ETA: 854465.6s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.141s, learning 0.189s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0455
             Mean action noise std: 0.70
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 8.33s
                        Total time: 11481.91s
                               ETA: 854432.5s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.212s, learning 0.168s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0475
             Mean action noise std: 0.70
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 8.38s
                        Total time: 11490.29s
                               ETA: 854403.0s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.096s, learning 0.217s)
               Value function loss: 10.0791
                    Surrogate loss: 0.0430
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 8.31s
                        Total time: 11498.60s
                               ETA: 854368.7s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.345s, learning 0.180s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0379
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 8.52s
                        Total time: 11507.13s
                               ETA: 854350.1s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.984s, learning 0.162s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0228
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 8.15s
                        Total time: 11515.27s
                               ETA: 854303.4s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.158s, learning 0.164s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0309
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 8.32s
                        Total time: 11523.60s
                               ETA: 854269.8s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.209s, learning 0.165s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0428
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 8.37s
                        Total time: 11531.97s
                               ETA: 854240.1s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.204s, learning 0.183s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0386
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 8.39s
                        Total time: 11540.36s
                               ETA: 854211.4s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.005s, learning 0.159s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0427
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 8.16s
                        Total time: 11548.52s
                               ETA: 854166.3s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.180s, learning 0.168s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0388
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 8.35s
                        Total time: 11556.87s
                               ETA: 854134.8s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.429s, learning 0.158s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0412
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 8.59s
                        Total time: 11565.45s
                               ETA: 854121.0s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.153s, learning 0.175s)
               Value function loss: 0.0491
                    Surrogate loss: -0.0345
             Mean action noise std: 0.70
                       Mean reward: 23.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 8.33s
                        Total time: 11573.78s
                               ETA: 854088.1s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.279s, learning 0.168s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0530
             Mean action noise std: 0.70
                       Mean reward: 23.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 8.45s
                        Total time: 11582.23s
                               ETA: 854063.9s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.948s, learning 0.182s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0454
             Mean action noise std: 0.70
                       Mean reward: 23.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 8.13s
                        Total time: 11590.36s
                               ETA: 854016.5s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.916s, learning 0.174s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0501
             Mean action noise std: 0.70
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 8.09s
                        Total time: 11598.45s
                               ETA: 853966.1s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.061s, learning 0.184s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0412
             Mean action noise std: 0.70
                       Mean reward: 23.65
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 8.24s
                        Total time: 11606.69s
                               ETA: 853927.2s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.090s, learning 0.161s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0411
             Mean action noise std: 0.70
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 8.25s
                        Total time: 11614.94s
                               ETA: 853888.8s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.247s, learning 0.171s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0540
             Mean action noise std: 0.70
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 8.42s
                        Total time: 11623.36s
                               ETA: 853862.8s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.396s, learning 0.212s)
               Value function loss: 5.7930
                    Surrogate loss: 0.0729
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 8.61s
                        Total time: 11631.97s
                               ETA: 853850.7s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.960s, learning 0.169s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 8.13s
                        Total time: 11640.10s
                               ETA: 853803.5s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.904s, learning 0.167s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0273
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 8.07s
                        Total time: 11648.17s
                               ETA: 853752.0s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.027s, learning 0.193s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0337
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 8.22s
                        Total time: 11656.39s
                               ETA: 853711.6s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.922s, learning 0.187s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0443
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 8.11s
                        Total time: 11664.50s
                               ETA: 853663.1s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.854s, learning 0.186s)
               Value function loss: 0.0446
                    Surrogate loss: -0.0496
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 8.04s
                        Total time: 11672.54s
                               ETA: 853609.6s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.082s, learning 0.187s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0463
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 8.27s
                        Total time: 11680.81s
                               ETA: 853573.0s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.333s, learning 0.182s)
               Value function loss: 0.0595
                    Surrogate loss: -0.0466
             Mean action noise std: 0.70
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 8.52s
                        Total time: 11689.32s
                               ETA: 853554.3s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.140s, learning 0.174s)
               Value function loss: 0.0951
                    Surrogate loss: -0.0392
             Mean action noise std: 0.70
                       Mean reward: 23.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 8.31s
                        Total time: 11697.64s
                               ETA: 853521.0s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.176s, learning 0.184s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0516
             Mean action noise std: 0.70
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 8.36s
                        Total time: 11706.00s
                               ETA: 853491.0s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.201s, learning 0.167s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0497
             Mean action noise std: 0.70
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 8.37s
                        Total time: 11714.37s
                               ETA: 853461.7s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.350s, learning 0.266s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0460
             Mean action noise std: 0.70
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 8.62s
                        Total time: 11722.98s
                               ETA: 853450.5s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.986s, learning 0.191s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0461
             Mean action noise std: 0.70
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 8.18s
                        Total time: 11731.16s
                               ETA: 853407.4s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.087s, learning 0.162s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0478
             Mean action noise std: 0.70
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 8.25s
                        Total time: 11739.41s
                               ETA: 853369.5s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.218s, learning 0.190s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0442
             Mean action noise std: 0.70
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 8.41s
                        Total time: 11747.82s
                               ETA: 853343.2s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.132s, learning 0.164s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0558
             Mean action noise std: 0.70
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 8.30s
                        Total time: 11756.11s
                               ETA: 853308.8s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.789s, learning 0.209s)
               Value function loss: 3.9734
                    Surrogate loss: 0.0111
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 8.00s
                        Total time: 11764.11s
                               ETA: 853252.8s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.265s, learning 0.293s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0312
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 8.56s
                        Total time: 11772.67s
                               ETA: 853237.5s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.954s, learning 0.165s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0292
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 8.12s
                        Total time: 11780.79s
                               ETA: 853190.4s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.951s, learning 0.191s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0311
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 8.14s
                        Total time: 11788.93s
                               ETA: 853145.0s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.104s, learning 0.175s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0381
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 8.28s
                        Total time: 11797.21s
                               ETA: 853109.5s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.999s, learning 0.189s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0394
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 8.19s
                        Total time: 11805.40s
                               ETA: 853067.5s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.871s, learning 0.162s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0428
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 8.03s
                        Total time: 11813.43s
                               ETA: 853014.4s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.991s, learning 0.165s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0542
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 8.16s
                        Total time: 11821.59s
                               ETA: 852970.2s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.006s, learning 0.214s)
               Value function loss: 0.0476
                    Surrogate loss: -0.0434
             Mean action noise std: 0.70
                       Mean reward: 23.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 8.22s
                        Total time: 11829.81s
                               ETA: 852930.8s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.464s, learning 0.188s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0469
             Mean action noise std: 0.70
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 8.65s
                        Total time: 11838.46s
                               ETA: 852922.4s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.218s, learning 0.246s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0548
             Mean action noise std: 0.70
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 8.46s
                        Total time: 11846.92s
                               ETA: 852900.5s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.135s, learning 0.173s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0437
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 8.31s
                        Total time: 11855.23s
                               ETA: 852867.5s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.165s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0435
             Mean action noise std: 0.70
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 8.38s
                        Total time: 11863.60s
                               ETA: 852839.3s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.160s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0532
             Mean action noise std: 0.70
                       Mean reward: 23.46
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 8.13s
                        Total time: 11871.74s
                               ETA: 852793.6s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.108s, learning 0.170s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0470
             Mean action noise std: 0.70
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 8.28s
                        Total time: 11880.01s
                               ETA: 852758.5s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.063s, learning 0.163s)
               Value function loss: 8.8864
                    Surrogate loss: 0.0526
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 8.23s
                        Total time: 11888.24s
                               ETA: 852719.7s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.040s, learning 0.213s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0271
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 8.25s
                        Total time: 11896.49s
                               ETA: 852682.9s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.852s, learning 0.177s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0241
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 8.03s
                        Total time: 11904.52s
                               ETA: 852630.2s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 2111 steps/s (collection: 7.600s, learning 0.159s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 7.76s
                        Total time: 11912.28s
                               ETA: 852558.1s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.919s, learning 0.188s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0409
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 8.11s
                        Total time: 11920.39s
                               ETA: 852511.0s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.343s, learning 0.223s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0381
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 8.57s
                        Total time: 11928.96s
                               ETA: 852496.8s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.986s, learning 0.211s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0499
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 8.20s
                        Total time: 11937.15s
                               ETA: 852456.2s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.163s)
               Value function loss: 0.0431
                    Surrogate loss: -0.0455
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 8.29s
                        Total time: 11945.45s
                               ETA: 852422.5s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.222s, learning 0.197s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0452
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 8.42s
                        Total time: 11953.86s
                               ETA: 852397.8s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.316s, learning 0.252s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0409
             Mean action noise std: 0.70
                       Mean reward: 23.61
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 8.57s
                        Total time: 11962.43s
                               ETA: 852383.8s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.377s, learning 0.293s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0538
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 8.67s
                        Total time: 11971.10s
                               ETA: 852377.0s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.922s, learning 0.157s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0454
             Mean action noise std: 0.70
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 8.08s
                        Total time: 11979.18s
                               ETA: 852328.2s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.452s, learning 0.186s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0487
             Mean action noise std: 0.70
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 8.64s
                        Total time: 11987.82s
                               ETA: 852319.2s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.445s, learning 0.156s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0376
             Mean action noise std: 0.70
                       Mean reward: 23.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 8.60s
                        Total time: 11996.42s
                               ETA: 852307.5s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.971s, learning 0.181s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0354
             Mean action noise std: 0.70
                       Mean reward: 23.64
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 8.15s
                        Total time: 12004.57s
                               ETA: 852264.0s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.139s, learning 0.182s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0477
             Mean action noise std: 0.70
                       Mean reward: 23.64
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 8.32s
                        Total time: 12012.89s
                               ETA: 852232.5s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.112s, learning 0.181s)
               Value function loss: 6.4530
                    Surrogate loss: 0.0497
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 8.29s
                        Total time: 12021.18s
                               ETA: 852199.1s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.909s, learning 0.159s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0272
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 8.07s
                        Total time: 12029.25s
                               ETA: 852149.8s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.239s, learning 0.183s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0284
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 8.42s
                        Total time: 12037.67s
                               ETA: 852125.6s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.057s, learning 0.194s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0388
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 8.25s
                        Total time: 12045.92s
                               ETA: 852089.3s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.080s, learning 0.161s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0411
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 8.24s
                        Total time: 12054.17s
                               ETA: 852052.4s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.917s, learning 0.161s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0463
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 8.08s
                        Total time: 12062.24s
                               ETA: 852004.0s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.163s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0489
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 8.17s
                        Total time: 12070.41s
                               ETA: 851962.1s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.264s, learning 0.164s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0492
             Mean action noise std: 0.70
                       Mean reward: 23.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 8.43s
                        Total time: 12078.84s
                               ETA: 851938.5s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.167s)
               Value function loss: 0.0582
                    Surrogate loss: -0.0438
             Mean action noise std: 0.70
                       Mean reward: 23.89
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 8.47s
                        Total time: 12087.31s
                               ETA: 851917.9s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.872s, learning 0.165s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0480
             Mean action noise std: 0.70
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 8.04s
                        Total time: 12095.35s
                               ETA: 851866.9s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.086s, learning 0.187s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0508
             Mean action noise std: 0.70
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 8.27s
                        Total time: 12103.62s
                               ETA: 851832.4s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.032s, learning 0.161s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0431
             Mean action noise std: 0.70
                       Mean reward: 23.80
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 8.19s
                        Total time: 12111.82s
                               ETA: 851792.4s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.876s, learning 0.168s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0541
             Mean action noise std: 0.70
                       Mean reward: 23.80
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 8.04s
                        Total time: 12119.86s
                               ETA: 851741.9s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.155s, learning 0.161s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0444
             Mean action noise std: 0.70
                       Mean reward: 23.82
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 8.32s
                        Total time: 12128.18s
                               ETA: 851710.7s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.218s, learning 0.211s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0432
             Mean action noise std: 0.70
                       Mean reward: 23.86
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 8.43s
                        Total time: 12136.61s
                               ETA: 851687.3s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.106s, learning 0.187s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0521
             Mean action noise std: 0.70
                       Mean reward: 23.86
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 8.29s
                        Total time: 12144.90s
                               ETA: 851654.5s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.217s, learning 0.171s)
               Value function loss: 2.5220
                    Surrogate loss: 0.0149
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 16.39s
                        Total time: 12161.29s
                               ETA: 852188.9s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.678s, learning 0.178s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0327
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 15.86s
                        Total time: 12177.14s
                               ETA: 852685.3s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.786s, learning 0.170s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0374
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 15.96s
                        Total time: 12193.10s
                               ETA: 853188.0s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.566s, learning 0.165s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0372
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 15.73s
                        Total time: 12208.83s
                               ETA: 853674.2s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.685s, learning 0.162s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0502
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 15.85s
                        Total time: 12224.68s
                               ETA: 854167.8s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.816s, learning 0.170s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0543
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 15.99s
                        Total time: 12240.66s
                               ETA: 854670.5s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.570s, learning 0.221s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0509
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 15.79s
                        Total time: 12256.45s
                               ETA: 855158.6s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.939s, learning 0.164s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0501
             Mean action noise std: 0.70
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 16.10s
                        Total time: 12272.56s
                               ETA: 855667.9s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.036s, learning 0.230s)
               Value function loss: 0.0681
                    Surrogate loss: -0.0406
             Mean action noise std: 0.70
                       Mean reward: 23.56
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 16.27s
                        Total time: 12288.82s
                               ETA: 856187.8s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.709s, learning 0.160s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0478
             Mean action noise std: 0.70
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 15.87s
                        Total time: 12304.69s
                               ETA: 856679.3s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1039 steps/s (collection: 15.598s, learning 0.164s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0419
             Mean action noise std: 0.70
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 15.76s
                        Total time: 12320.45s
                               ETA: 857162.7s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.955s, learning 0.175s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0470
             Mean action noise std: 0.70
                       Mean reward: 23.56
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 16.13s
                        Total time: 12336.58s
                               ETA: 857670.9s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.829s, learning 0.167s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0370
             Mean action noise std: 0.70
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 16.00s
                        Total time: 12352.58s
                               ETA: 858169.1s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.703s, learning 0.163s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0527
             Mean action noise std: 0.70
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 15.87s
                        Total time: 12368.45s
                               ETA: 858657.5s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.889s, learning 0.177s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0456
             Mean action noise std: 0.70
                       Mean reward: 23.54
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 16.07s
                        Total time: 12384.51s
                               ETA: 859159.1s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.646s, learning 0.167s)
               Value function loss: 6.6320
                    Surrogate loss: 0.0128
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 15.81s
                        Total time: 12400.32s
                               ETA: 859642.5s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.931s, learning 0.189s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0272
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 16.12s
                        Total time: 12416.44s
                               ETA: 860146.3s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.771s, learning 0.174s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0270
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 15.94s
                        Total time: 12432.39s
                               ETA: 860637.3s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.683s, learning 0.214s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0387
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 15.90s
                        Total time: 12448.29s
                               ETA: 861124.4s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.918s, learning 0.161s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0407
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 16.08s
                        Total time: 12464.36s
                               ETA: 861623.3s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.496s, learning 0.178s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0460
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 15.67s
                        Total time: 12480.04s
                               ETA: 862093.5s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.192s, learning 0.174s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0464
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 16.37s
                        Total time: 12496.41s
                               ETA: 862610.8s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.507s, learning 0.172s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0513
             Mean action noise std: 0.70
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 15.68s
                        Total time: 12512.08s
                               ETA: 863079.9s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.538s, learning 0.160s)
               Value function loss: 0.0769
                    Surrogate loss: -0.0465
             Mean action noise std: 0.70
                       Mean reward: 23.46
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 15.70s
                        Total time: 12527.78s
                               ETA: 863549.6s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.805s, learning 0.225s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0435
             Mean action noise std: 0.70
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 16.03s
                        Total time: 12543.81s
                               ETA: 864041.5s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.233s, learning 0.182s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0579
             Mean action noise std: 0.70
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 16.42s
                        Total time: 12560.23s
                               ETA: 864559.3s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.799s, learning 0.186s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0373
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 15.99s
                        Total time: 12576.21s
                               ETA: 865046.8s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.765s, learning 0.171s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0566
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 15.94s
                        Total time: 12592.15s
                               ETA: 865530.1s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.574s, learning 0.164s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0414
             Mean action noise std: 0.70
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 15.74s
                        Total time: 12607.89s
                               ETA: 865999.2s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.844s, learning 0.222s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0411
             Mean action noise std: 0.70
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 16.07s
                        Total time: 12623.95s
                               ETA: 866490.1s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.607s, learning 0.170s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0558
             Mean action noise std: 0.70
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 15.78s
                        Total time: 12639.73s
                               ETA: 866960.4s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.936s, learning 0.162s)
               Value function loss: 5.3660
                    Surrogate loss: 0.0374
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 16.10s
                        Total time: 12655.83s
                               ETA: 867452.2s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.141s, learning 0.166s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0320
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 16.31s
                        Total time: 12672.13s
                               ETA: 867957.4s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.763s, learning 0.259s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0333
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 16.02s
                        Total time: 12688.16s
                               ETA: 868442.5s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.890s, learning 0.161s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0414
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 16.05s
                        Total time: 12704.21s
                               ETA: 868928.9s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.876s, learning 0.165s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0384
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 16.04s
                        Total time: 12720.25s
                               ETA: 869413.9s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.725s, learning 0.201s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0469
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 15.93s
                        Total time: 12736.17s
                               ETA: 869890.3s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1264 steps/s (collection: 12.784s, learning 0.168s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0416
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 12.95s
                        Total time: 12749.13s
                               ETA: 870163.1s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.966s, learning 0.155s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0470
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 8.12s
                        Total time: 12757.25s
                               ETA: 870106.0s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.509s, learning 0.171s)
               Value function loss: 0.0719
                    Surrogate loss: -0.0478
             Mean action noise std: 0.70
                       Mean reward: 23.24
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 8.68s
                        Total time: 12765.93s
                               ETA: 870087.0s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.291s, learning 0.167s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0413
             Mean action noise std: 0.70
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 8.46s
                        Total time: 12774.38s
                               ETA: 870052.9s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.652s, learning 0.162s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0476
             Mean action noise std: 0.70
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 8.81s
                        Total time: 12783.20s
                               ETA: 870043.1s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.198s, learning 0.184s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0397
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 8.38s
                        Total time: 12791.58s
                               ETA: 870003.9s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.996s, learning 0.191s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0538
             Mean action noise std: 0.70
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 8.19s
                        Total time: 12799.77s
                               ETA: 869951.5s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.856s, learning 0.170s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0437
             Mean action noise std: 0.70
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 8.03s
                        Total time: 12807.79s
                               ETA: 869888.3s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.905s, learning 0.163s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0415
             Mean action noise std: 0.70
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 8.07s
                        Total time: 12815.86s
                               ETA: 869828.0s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.301s, learning 0.198s)
               Value function loss: 7.1553
                    Surrogate loss: 0.0478
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 8.50s
                        Total time: 12824.36s
                               ETA: 869797.0s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.104s, learning 0.164s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0363
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 8.27s
                        Total time: 12832.63s
                               ETA: 869750.3s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.968s, learning 0.196s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0271
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 8.16s
                        Total time: 12840.79s
                               ETA: 869696.6s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.971s, learning 0.164s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0312
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 8.13s
                        Total time: 12848.93s
                               ETA: 869641.1s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.284s, learning 0.164s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0470
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 8.45s
                        Total time: 12857.37s
                               ETA: 869606.8s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.019s, learning 0.167s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0458
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 8.19s
                        Total time: 12865.56s
                               ETA: 869554.8s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.139s, learning 0.156s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0466
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 8.29s
                        Total time: 12873.85s
                               ETA: 869510.1s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.033s, learning 0.164s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0499
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 8.20s
                        Total time: 12882.05s
                               ETA: 869459.1s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.139s, learning 0.156s)
               Value function loss: 0.0513
                    Surrogate loss: -0.0453
             Mean action noise std: 0.70
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 8.29s
                        Total time: 12890.35s
                               ETA: 869414.6s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.256s, learning 0.163s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0470
             Mean action noise std: 0.70
                       Mean reward: 22.97
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 8.42s
                        Total time: 12898.77s
                               ETA: 869378.5s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.909s, learning 0.214s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0546
             Mean action noise std: 0.70
                       Mean reward: 22.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 8.12s
                        Total time: 12906.89s
                               ETA: 869322.6s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.139s, learning 0.166s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0549
             Mean action noise std: 0.70
                       Mean reward: 22.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 8.30s
                        Total time: 12915.19s
                               ETA: 869279.0s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.400s, learning 0.161s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0486
             Mean action noise std: 0.70
                       Mean reward: 22.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 8.56s
                        Total time: 12923.75s
                               ETA: 869252.6s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.257s, learning 0.173s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0369
             Mean action noise std: 0.70
                       Mean reward: 23.00
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 8.43s
                        Total time: 12932.19s
                               ETA: 869217.5s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.042s, learning 0.164s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0417
             Mean action noise std: 0.70
                       Mean reward: 22.99
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 8.21s
                        Total time: 12940.39s
                               ETA: 869167.3s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.945s, learning 0.161s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0546
             Mean action noise std: 0.70
                       Mean reward: 22.99
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 8.11s
                        Total time: 12948.50s
                               ETA: 869110.6s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1985 steps/s (collection: 7.879s, learning 0.372s)
               Value function loss: 6.9382
                    Surrogate loss: 0.0330
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 8.25s
                        Total time: 12956.75s
                               ETA: 869063.5s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.919s, learning 0.190s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0204
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 8.11s
                        Total time: 12964.86s
                               ETA: 869007.1s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.013s, learning 0.161s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0265
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 8.17s
                        Total time: 12973.03s
                               ETA: 868954.9s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.986s, learning 0.166s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0393
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 8.15s
                        Total time: 12981.18s
                               ETA: 868901.5s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.013s, learning 0.184s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0460
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 8.20s
                        Total time: 12989.38s
                               ETA: 868851.0s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.997s, learning 0.183s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0442
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 8.18s
                        Total time: 12997.56s
                               ETA: 868799.6s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.856s, learning 0.211s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0440
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 8.07s
                        Total time: 13005.63s
                               ETA: 868740.6s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.064s, learning 0.162s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0502
             Mean action noise std: 0.70
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 8.23s
                        Total time: 13013.85s
                               ETA: 868692.3s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.104s, learning 0.165s)
               Value function loss: 0.0733
                    Surrogate loss: -0.0472
             Mean action noise std: 0.70
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 8.27s
                        Total time: 13022.12s
                               ETA: 868647.0s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.932s, learning 0.171s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0517
             Mean action noise std: 0.70
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 8.10s
                        Total time: 13030.23s
                               ETA: 868590.6s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.171s, learning 0.164s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0566
             Mean action noise std: 0.70
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 8.34s
                        Total time: 13038.56s
                               ETA: 868549.7s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.877s, learning 0.158s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0462
             Mean action noise std: 0.70
                       Mean reward: 23.24
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 8.04s
                        Total time: 13046.60s
                               ETA: 868489.0s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.099s, learning 0.176s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0490
             Mean action noise std: 0.70
                       Mean reward: 23.24
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 8.28s
                        Total time: 13054.87s
                               ETA: 868444.2s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.500s, learning 0.215s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0420
             Mean action noise std: 0.70
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 8.72s
                        Total time: 13063.59s
                               ETA: 868428.8s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.204s, learning 0.186s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0395
             Mean action noise std: 0.70
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 8.39s
                        Total time: 13071.98s
                               ETA: 868391.8s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.017s, learning 0.203s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0561
             Mean action noise std: 0.70
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 8.22s
                        Total time: 13080.20s
                               ETA: 868343.5s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.402s, learning 0.195s)
               Value function loss: 3.5821
                    Surrogate loss: 0.0380
             Mean action noise std: 0.70
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 8.60s
                        Total time: 13088.79s
                               ETA: 868320.2s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.095s, learning 0.170s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0300
             Mean action noise std: 0.70
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 8.26s
                        Total time: 13097.06s
                               ETA: 868275.0s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.022s, learning 0.212s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0359
             Mean action noise std: 0.70
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 8.23s
                        Total time: 13105.29s
                               ETA: 868227.7s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.041s, learning 0.163s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0401
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 8.20s
                        Total time: 13113.49s
                               ETA: 868178.5s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.958s, learning 0.169s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0395
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 8.13s
                        Total time: 13121.62s
                               ETA: 868124.4s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.191s, learning 0.162s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0472
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 8.35s
                        Total time: 13129.97s
                               ETA: 868085.2s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.033s, learning 0.167s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0396
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 8.20s
                        Total time: 13138.17s
                               ETA: 868035.9s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.076s, learning 0.171s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0451
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 8.25s
                        Total time: 13146.42s
                               ETA: 867989.8s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.116s, learning 0.218s)
               Value function loss: 0.0934
                    Surrogate loss: -0.0406
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 8.33s
                        Total time: 13154.76s
                               ETA: 867949.5s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.500s, learning 0.160s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0535
             Mean action noise std: 0.69
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 8.66s
                        Total time: 13163.41s
                               ETA: 867930.7s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.198s, learning 0.166s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0497
             Mean action noise std: 0.69
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 8.36s
                        Total time: 13171.78s
                               ETA: 867892.5s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.016s, learning 0.175s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0485
             Mean action noise std: 0.69
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 8.19s
                        Total time: 13179.97s
                               ETA: 867842.8s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.106s, learning 0.160s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0433
             Mean action noise std: 0.69
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 8.27s
                        Total time: 13188.24s
                               ETA: 867798.2s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.135s, learning 0.179s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0529
             Mean action noise std: 0.69
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 8.31s
                        Total time: 13196.55s
                               ETA: 867756.8s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.942s, learning 0.159s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0477
             Mean action noise std: 0.69
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 8.10s
                        Total time: 13204.65s
                               ETA: 867701.4s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.303s, learning 0.193s)
               Value function loss: 6.4530
                    Surrogate loss: 0.0527
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 8.50s
                        Total time: 13213.15s
                               ETA: 867672.1s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.796s, learning 0.163s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0260
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 7.96s
                        Total time: 13221.10s
                               ETA: 867607.5s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.065s, learning 0.173s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0208
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 8.24s
                        Total time: 13229.34s
                               ETA: 867561.3s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.030s, learning 0.173s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0277
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 8.20s
                        Total time: 13237.54s
                               ETA: 867512.8s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.949s, learning 0.216s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0409
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 8.16s
                        Total time: 13245.71s
                               ETA: 867461.9s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.858s, learning 0.163s)
               Value function loss: 0.0528
                    Surrogate loss: -0.0503
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 8.02s
                        Total time: 13253.73s
                               ETA: 867401.7s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.237s, learning 0.179s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0458
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 8.42s
                        Total time: 13262.15s
                               ETA: 867367.3s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.022s, learning 0.205s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0431
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 8.23s
                        Total time: 13270.37s
                               ETA: 867320.6s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.833s, learning 0.162s)
               Value function loss: 0.0976
                    Surrogate loss: -0.0355
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 7.99s
                        Total time: 13278.37s
                               ETA: 867258.8s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.385s, learning 0.246s)
               Value function loss: 0.1072
                    Surrogate loss: -0.0428
             Mean action noise std: 0.69
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 8.63s
                        Total time: 13287.00s
                               ETA: 867238.7s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.404s, learning 0.188s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0503
             Mean action noise std: 0.69
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 8.59s
                        Total time: 13295.59s
                               ETA: 867215.9s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.188s, learning 0.195s)
               Value function loss: 0.0520
                    Surrogate loss: -0.0444
             Mean action noise std: 0.69
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 8.38s
                        Total time: 13303.97s
                               ETA: 867179.6s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.817s, learning 0.168s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0416
             Mean action noise std: 0.69
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 7.98s
                        Total time: 13311.96s
                               ETA: 867117.4s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.278s, learning 0.164s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0504
             Mean action noise std: 0.69
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 8.44s
                        Total time: 13320.40s
                               ETA: 867085.0s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.451s, learning 0.179s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0421
             Mean action noise std: 0.69
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 8.63s
                        Total time: 13329.03s
                               ETA: 867064.9s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.131s, learning 0.159s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0576
             Mean action noise std: 0.69
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 8.29s
                        Total time: 13337.32s
                               ETA: 867022.7s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.275s, learning 0.245s)
               Value function loss: 4.4303
                    Surrogate loss: 0.0060
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 8.52s
                        Total time: 13345.84s
                               ETA: 866995.4s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.971s, learning 0.175s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0277
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 8.15s
                        Total time: 13353.99s
                               ETA: 866944.0s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 2160 steps/s (collection: 7.400s, learning 0.182s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0285
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 7.58s
                        Total time: 13361.57s
                               ETA: 866856.0s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.830s, learning 0.161s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0393
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 7.99s
                        Total time: 13369.56s
                               ETA: 866794.7s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.077s, learning 0.162s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0388
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 8.24s
                        Total time: 13377.80s
                               ETA: 866749.4s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.963s, learning 0.168s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0517
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 8.13s
                        Total time: 13385.93s
                               ETA: 866697.2s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.319s, learning 0.188s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0370
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 8.51s
                        Total time: 13394.44s
                               ETA: 866669.4s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.114s, learning 0.162s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0471
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 8.28s
                        Total time: 13402.71s
                               ETA: 866626.7s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.079s, learning 0.166s)
               Value function loss: 0.1027
                    Surrogate loss: -0.0444
             Mean action noise std: 0.69
                       Mean reward: 22.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 8.24s
                        Total time: 13410.96s
                               ETA: 866582.0s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.595s, learning 0.158s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0526
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 8.75s
                        Total time: 13419.71s
                               ETA: 866570.1s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.178s, learning 0.166s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0507
             Mean action noise std: 0.69
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 8.34s
                        Total time: 13428.06s
                               ETA: 866531.9s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.912s, learning 0.220s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0460
             Mean action noise std: 0.69
                       Mean reward: 22.61
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 8.13s
                        Total time: 13436.19s
                               ETA: 866480.1s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.360s, learning 0.331s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0484
             Mean action noise std: 0.69
                       Mean reward: 22.61
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 8.69s
                        Total time: 13444.88s
                               ETA: 866464.3s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.782s, learning 0.197s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0462
             Mean action noise std: 0.69
                       Mean reward: 22.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 7.98s
                        Total time: 13452.86s
                               ETA: 866402.7s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.254s, learning 0.189s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0409
             Mean action noise std: 0.69
                       Mean reward: 22.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 8.44s
                        Total time: 13461.30s
                               ETA: 866371.1s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.002s, learning 0.187s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0573
             Mean action noise std: 0.69
                       Mean reward: 22.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 8.19s
                        Total time: 13469.49s
                               ETA: 866323.0s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.139s, learning 0.182s)
               Value function loss: 2.1241
                    Surrogate loss: 0.0157
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 8.32s
                        Total time: 13477.81s
                               ETA: 866283.6s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.067s, learning 0.163s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0344
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 8.23s
                        Total time: 13486.04s
                               ETA: 866238.3s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.836s, learning 0.288s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0379
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 8.12s
                        Total time: 13494.16s
                               ETA: 866186.3s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.970s, learning 0.213s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0420
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 8.18s
                        Total time: 13502.35s
                               ETA: 866138.1s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.267s, learning 0.186s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0537
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 8.45s
                        Total time: 13510.80s
                               ETA: 866107.3s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1983 steps/s (collection: 7.994s, learning 0.267s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0518
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 8.26s
                        Total time: 13519.06s
                               ETA: 866064.2s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.919s, learning 0.173s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0461
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 8.09s
                        Total time: 13527.15s
                               ETA: 866010.4s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.977s, learning 0.170s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0400
             Mean action noise std: 0.69
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 8.15s
                        Total time: 13535.30s
                               ETA: 865960.1s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.912s, learning 0.252s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0403
             Mean action noise std: 0.69
                       Mean reward: 22.52
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 8.16s
                        Total time: 13543.46s
                               ETA: 865911.0s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.911s, learning 0.160s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0541
             Mean action noise std: 0.69
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 8.07s
                        Total time: 13551.53s
                               ETA: 865855.9s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.247s, learning 0.178s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0518
             Mean action noise std: 0.69
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 8.42s
                        Total time: 13559.96s
                               ETA: 865823.6s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 2082 steps/s (collection: 7.600s, learning 0.269s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 7.87s
                        Total time: 13567.83s
                               ETA: 865755.8s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.022s, learning 0.196s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0496
             Mean action noise std: 0.69
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 8.22s
                        Total time: 13576.04s
                               ETA: 865710.3s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.913s, learning 0.175s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0532
             Mean action noise std: 0.69
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 8.09s
                        Total time: 13584.13s
                               ETA: 865656.5s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.264s, learning 0.249s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0497
             Mean action noise std: 0.69
                       Mean reward: 22.50
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 8.51s
                        Total time: 13592.65s
                               ETA: 865630.0s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.990s, learning 0.165s)
               Value function loss: 6.8962
                    Surrogate loss: 0.0103
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 8.16s
                        Total time: 13600.80s
                               ETA: 865580.7s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.768s, learning 0.214s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0259
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 7.98s
                        Total time: 13608.78s
                               ETA: 865520.4s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.821s, learning 0.166s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0224
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 7.99s
                        Total time: 13616.77s
                               ETA: 865460.5s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.163s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0387
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 8.13s
                        Total time: 13624.90s
                               ETA: 865409.5s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.223s, learning 0.181s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0476
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 8.40s
                        Total time: 13633.30s
                               ETA: 865376.2s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.351s, learning 0.198s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0406
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 8.55s
                        Total time: 13641.85s
                               ETA: 865352.1s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.898s, learning 0.162s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0461
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 8.06s
                        Total time: 13649.91s
                               ETA: 865297.1s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 2115 steps/s (collection: 7.582s, learning 0.161s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0423
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 7.74s
                        Total time: 13657.65s
                               ETA: 865222.0s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.897s, learning 0.166s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0381
             Mean action noise std: 0.69
                       Mean reward: 22.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 8.06s
                        Total time: 13665.72s
                               ETA: 865167.3s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.085s, learning 0.165s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0460
             Mean action noise std: 0.69
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 8.25s
                        Total time: 13673.97s
                               ETA: 865124.4s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.987s, learning 0.191s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0511
             Mean action noise std: 0.69
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 8.18s
                        Total time: 13682.14s
                               ETA: 865077.1s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.949s, learning 0.164s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0436
             Mean action noise std: 0.69
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 8.11s
                        Total time: 13690.26s
                               ETA: 865025.6s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.985s, learning 0.166s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0431
             Mean action noise std: 0.69
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 8.15s
                        Total time: 13698.41s
                               ETA: 864976.6s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.250s, learning 0.180s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0438
             Mean action noise std: 0.69
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 8.43s
                        Total time: 13706.84s
                               ETA: 864945.4s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.489s, learning 0.253s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0417
             Mean action noise std: 0.69
                       Mean reward: 22.36
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 8.74s
                        Total time: 13715.58s
                               ETA: 864933.7s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.334s, learning 0.171s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0543
             Mean action noise std: 0.69
                       Mean reward: 22.36
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 8.51s
                        Total time: 13724.08s
                               ETA: 864907.2s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.194s, learning 0.192s)
               Value function loss: 4.6123
                    Surrogate loss: 0.0368
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 8.39s
                        Total time: 13732.47s
                               ETA: 864873.3s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.199s, learning 0.157s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0229
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 8.36s
                        Total time: 13740.83s
                               ETA: 864837.5s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.802s, learning 0.173s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0307
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 7.98s
                        Total time: 13748.80s
                               ETA: 864777.7s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.156s, learning 0.160s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0406
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 8.32s
                        Total time: 13757.12s
                               ETA: 864739.4s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.824s, learning 0.195s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0401
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 8.02s
                        Total time: 13765.14s
                               ETA: 864682.5s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.153s, learning 0.192s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0469
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 8.35s
                        Total time: 13773.48s
                               ETA: 864646.2s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.057s, learning 0.188s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0408
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 8.24s
                        Total time: 13781.73s
                               ETA: 864603.6s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.180s, learning 0.180s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0341
             Mean action noise std: 0.69
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 8.36s
                        Total time: 13790.09s
                               ETA: 864568.2s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.265s, learning 0.165s)
               Value function loss: 0.0692
                    Surrogate loss: -0.0466
             Mean action noise std: 0.69
                       Mean reward: 22.49
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 8.43s
                        Total time: 13798.52s
                               ETA: 864537.3s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.159s, learning 0.157s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0501
             Mean action noise std: 0.69
                       Mean reward: 22.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 8.32s
                        Total time: 13806.83s
                               ETA: 864499.2s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.963s, learning 0.255s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0500
             Mean action noise std: 0.69
                       Mean reward: 22.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 8.22s
                        Total time: 13815.05s
                               ETA: 864455.1s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.102s, learning 0.241s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0485
             Mean action noise std: 0.69
                       Mean reward: 22.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 8.34s
                        Total time: 13823.39s
                               ETA: 864418.8s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.293s, learning 0.183s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0532
             Mean action noise std: 0.69
                       Mean reward: 22.49
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 8.48s
                        Total time: 13831.87s
                               ETA: 864390.9s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.021s, learning 0.196s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0459
             Mean action noise std: 0.69
                       Mean reward: 22.50
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 8.22s
                        Total time: 13840.09s
                               ETA: 864346.8s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.977s, learning 0.190s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0420
             Mean action noise std: 0.69
                       Mean reward: 22.49
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 8.17s
                        Total time: 13848.25s
                               ETA: 864299.7s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.449s, learning 0.215s)
               Value function loss: 7.5101
                    Surrogate loss: 0.0809
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 8.66s
                        Total time: 13856.92s
                               ETA: 864283.5s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.243s, learning 0.173s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0253
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 8.42s
                        Total time: 13865.33s
                               ETA: 864252.0s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.027s, learning 0.201s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0251
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 8.23s
                        Total time: 13873.56s
                               ETA: 864208.8s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.776s, learning 0.171s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0358
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 7.95s
                        Total time: 13881.51s
                               ETA: 864148.1s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.346s, learning 0.213s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0416
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 8.56s
                        Total time: 13890.07s
                               ETA: 864125.5s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.353s, learning 0.179s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0462
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 8.53s
                        Total time: 13898.60s
                               ETA: 864101.3s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.194s, learning 0.188s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0339
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 8.38s
                        Total time: 13906.98s
                               ETA: 864067.8s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.188s, learning 0.159s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0234
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 8.35s
                        Total time: 13915.33s
                               ETA: 864032.1s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.102s, learning 0.188s)
               Value function loss: 0.0503
                    Surrogate loss: -0.0364
             Mean action noise std: 0.69
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 8.29s
                        Total time: 13923.62s
                               ETA: 863993.0s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.887s, learning 0.175s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0393
             Mean action noise std: 0.69
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 8.06s
                        Total time: 13931.68s
                               ETA: 863939.7s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.919s, learning 0.210s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0418
             Mean action noise std: 0.69
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 8.13s
                        Total time: 13939.81s
                               ETA: 863890.7s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.858s, learning 0.173s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0509
             Mean action noise std: 0.69
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 8.03s
                        Total time: 13947.84s
                               ETA: 863835.6s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.946s, learning 0.180s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0504
             Mean action noise std: 0.69
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 8.13s
                        Total time: 13955.97s
                               ETA: 863786.5s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.291s, learning 0.169s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0405
             Mean action noise std: 0.69
                       Mean reward: 22.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 8.46s
                        Total time: 13964.43s
                               ETA: 863758.1s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.826s, learning 0.220s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0451
             Mean action noise std: 0.69
                       Mean reward: 22.33
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 8.05s
                        Total time: 13972.47s
                               ETA: 863704.2s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.176s, learning 0.167s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0538
             Mean action noise std: 0.69
                       Mean reward: 22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 8.34s
                        Total time: 13980.82s
                               ETA: 863668.6s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.145s, learning 0.167s)
               Value function loss: 6.5257
                    Surrogate loss: 0.0726
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 8.31s
                        Total time: 13989.13s
                               ETA: 863631.2s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.084s, learning 0.169s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0237
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 8.25s
                        Total time: 13997.38s
                               ETA: 863590.1s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.978s, learning 0.162s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0195
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 8.14s
                        Total time: 14005.52s
                               ETA: 863542.1s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.715s, learning 0.175s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0372
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 7.89s
                        Total time: 14013.41s
                               ETA: 863478.7s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.984s, learning 0.161s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0486
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 8.14s
                        Total time: 14021.55s
                               ETA: 863431.1s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.104s, learning 0.192s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0528
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 8.30s
                        Total time: 14029.85s
                               ETA: 863392.9s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.261s, learning 0.166s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0510
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 8.43s
                        Total time: 14038.28s
                               ETA: 863362.8s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.757s, learning 0.179s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 7.94s
                        Total time: 14046.21s
                               ETA: 863302.5s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.302s, learning 0.186s)
               Value function loss: 0.1012
                    Surrogate loss: -0.0431
             Mean action noise std: 0.69
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 8.49s
                        Total time: 14054.70s
                               ETA: 863276.2s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.339s, learning 0.181s)
               Value function loss: 0.0499
                    Surrogate loss: -0.0504
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 8.52s
                        Total time: 14063.22s
                               ETA: 863251.9s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.188s, learning 0.169s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0420
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 8.36s
                        Total time: 14071.58s
                               ETA: 863217.6s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.873s, learning 0.190s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0320
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 8.06s
                        Total time: 14079.64s
                               ETA: 863165.3s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.014s, learning 0.171s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0548
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 8.18s
                        Total time: 14087.83s
                               ETA: 863120.5s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.298s, learning 0.157s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0456
             Mean action noise std: 0.69
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 8.45s
                        Total time: 14096.28s
                               ETA: 863092.3s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.359s, learning 0.205s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0463
             Mean action noise std: 0.69
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 8.56s
                        Total time: 14104.84s
                               ETA: 863070.8s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.559s, learning 0.210s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0538
             Mean action noise std: 0.69
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 8.77s
                        Total time: 14113.61s
                               ETA: 863061.8s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.169s)
               Value function loss: 3.2542
                    Surrogate loss: 0.0237
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 8.45s
                        Total time: 14122.06s
                               ETA: 863033.6s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.860s, learning 0.183s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0285
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 8.04s
                        Total time: 14130.11s
                               ETA: 862980.3s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.983s, learning 0.213s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0282
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 8.20s
                        Total time: 14138.30s
                               ETA: 862936.4s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.979s, learning 0.171s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0451
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 8.15s
                        Total time: 14146.45s
                               ETA: 862889.8s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.008s, learning 0.196s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0495
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 8.20s
                        Total time: 14154.66s
                               ETA: 862846.5s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.144s, learning 0.160s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0550
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 8.30s
                        Total time: 14162.96s
                               ETA: 862809.4s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.861s, learning 0.169s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0503
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 8.03s
                        Total time: 14170.99s
                               ETA: 862755.6s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.972s, learning 0.158s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0412
             Mean action noise std: 0.69
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 8.13s
                        Total time: 14179.12s
                               ETA: 862708.0s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.152s, learning 0.165s)
               Value function loss: 0.0673
                    Surrogate loss: -0.0405
             Mean action noise std: 0.69
                       Mean reward: 22.23
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 8.32s
                        Total time: 14187.44s
                               ETA: 862671.7s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.014s, learning 0.235s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 8.25s
                        Total time: 14195.69s
                               ETA: 862631.4s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.981s, learning 0.170s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0500
             Mean action noise std: 0.69
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 8.15s
                        Total time: 14203.84s
                               ETA: 862585.2s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.056s, learning 0.194s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0417
             Mean action noise std: 0.69
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 8.25s
                        Total time: 14212.09s
                               ETA: 862545.0s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.130s, learning 0.171s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0404
             Mean action noise std: 0.69
                       Mean reward: 22.33
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 8.30s
                        Total time: 14220.39s
                               ETA: 862507.9s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.209s, learning 0.165s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0477
             Mean action noise std: 0.69
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 8.37s
                        Total time: 14228.77s
                               ETA: 862475.3s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.926s, learning 0.168s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0428
             Mean action noise std: 0.69
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 8.09s
                        Total time: 14236.86s
                               ETA: 862425.8s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.127s, learning 0.170s)
               Value function loss: 7.4189
                    Surrogate loss: 0.0499
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 8.30s
                        Total time: 14245.16s
                               ETA: 862388.6s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.063s, learning 0.169s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0276
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 8.23s
                        Total time: 14253.39s
                               ETA: 862347.5s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.881s, learning 0.188s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0278
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 8.07s
                        Total time: 14261.46s
                               ETA: 862296.6s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.173s, learning 0.169s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0304
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 8.34s
                        Total time: 14269.80s
                               ETA: 862262.3s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.972s, learning 0.165s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0426
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 8.14s
                        Total time: 14277.94s
                               ETA: 862215.6s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.084s, learning 0.160s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0504
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 8.24s
                        Total time: 14286.18s
                               ETA: 862175.4s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.002s, learning 0.159s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0513
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 8.16s
                        Total time: 14294.34s
                               ETA: 862130.2s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.177s, learning 0.186s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0325
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 8.36s
                        Total time: 14302.70s
                               ETA: 862097.3s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.981s, learning 0.185s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 8.17s
                        Total time: 14310.87s
                               ETA: 862052.5s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.220s, learning 0.163s)
               Value function loss: 0.0448
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 8.38s
                        Total time: 14319.25s
                               ETA: 862020.8s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.119s, learning 0.202s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0504
             Mean action noise std: 0.69
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 8.32s
                        Total time: 14327.57s
                               ETA: 861985.4s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.258s, learning 0.191s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0486
             Mean action noise std: 0.69
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 8.45s
                        Total time: 14336.02s
                               ETA: 861957.8s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.849s, learning 0.203s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0526
             Mean action noise std: 0.69
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 8.05s
                        Total time: 14344.08s
                               ETA: 861906.3s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.098s, learning 0.182s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0367
             Mean action noise std: 0.69
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 8.28s
                        Total time: 14352.36s
                               ETA: 861868.6s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.279s, learning 0.157s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0349
             Mean action noise std: 0.69
                       Mean reward: 22.14
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 8.44s
                        Total time: 14360.79s
                               ETA: 861840.3s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 2095 steps/s (collection: 7.650s, learning 0.169s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0563
             Mean action noise std: 0.69
                       Mean reward: 22.14
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 7.82s
                        Total time: 14368.61s
                               ETA: 861775.0s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.014s, learning 0.217s)
               Value function loss: 5.6270
                    Surrogate loss: 0.0156
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 8.23s
                        Total time: 14376.84s
                               ETA: 861734.4s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.297s, learning 0.168s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0203
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 8.46s
                        Total time: 14385.31s
                               ETA: 861707.9s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.934s, learning 0.165s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0250
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 8.10s
                        Total time: 14393.41s
                               ETA: 861659.5s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.954s, learning 0.194s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0414
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 8.15s
                        Total time: 14401.55s
                               ETA: 861614.1s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.845s, learning 0.158s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0481
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 8.00s
                        Total time: 14409.56s
                               ETA: 861560.1s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.011s, learning 0.208s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0426
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 8.22s
                        Total time: 14417.78s
                               ETA: 861519.0s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.117s, learning 0.211s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0362
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 8.33s
                        Total time: 14426.10s
                               ETA: 861484.5s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.999s, learning 0.218s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0338
             Mean action noise std: 0.69
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 8.22s
                        Total time: 14434.32s
                               ETA: 861443.4s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.066s, learning 0.181s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0368
             Mean action noise std: 0.69
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 8.25s
                        Total time: 14442.57s
                               ETA: 861404.1s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.883s, learning 0.183s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0432
             Mean action noise std: 0.69
                       Mean reward: 22.23
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 8.07s
                        Total time: 14450.63s
                               ETA: 861354.1s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.858s, learning 0.228s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0512
             Mean action noise std: 0.69
                       Mean reward: 22.23
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 8.09s
                        Total time: 14458.72s
                               ETA: 861305.3s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.429s, learning 0.178s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0432
             Mean action noise std: 0.69
                       Mean reward: 22.26
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 8.61s
                        Total time: 14467.33s
                               ETA: 861287.6s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.163s, learning 0.163s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0588
             Mean action noise std: 0.69
                       Mean reward: 22.26
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 8.33s
                        Total time: 14475.65s
                               ETA: 861253.2s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.979s, learning 0.170s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0439
             Mean action noise std: 0.69
                       Mean reward: 22.30
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 8.15s
                        Total time: 14483.80s
                               ETA: 861208.3s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.069s, learning 0.195s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0428
             Mean action noise std: 0.69
                       Mean reward: 22.31
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 8.26s
                        Total time: 14492.07s
                               ETA: 861170.2s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.237s, learning 0.167s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0556
             Mean action noise std: 0.69
                       Mean reward: 22.31
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 8.40s
                        Total time: 14500.47s
                               ETA: 861140.6s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.757s, learning 0.212s)
               Value function loss: 1.6200
                    Surrogate loss: 0.0332
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 7.97s
                        Total time: 14508.44s
                               ETA: 861085.1s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.751s, learning 0.170s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0303
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 7.92s
                        Total time: 14516.36s
                               ETA: 861026.9s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.918s, learning 0.171s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0308
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 8.09s
                        Total time: 14524.45s
                               ETA: 860978.6s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.060s, learning 0.167s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0416
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 8.23s
                        Total time: 14532.68s
                               ETA: 860938.6s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.238s, learning 0.166s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0528
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 8.40s
                        Total time: 14541.08s
                               ETA: 860909.0s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.786s, learning 0.161s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0470
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 7.95s
                        Total time: 14549.03s
                               ETA: 860852.5s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.216s, learning 0.163s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0378
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 8.38s
                        Total time: 14557.41s
                               ETA: 860821.6s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.952s, learning 0.186s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0373
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 8.14s
                        Total time: 14565.55s
                               ETA: 860776.5s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.294s, learning 0.173s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0453
             Mean action noise std: 0.69
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 8.47s
                        Total time: 14574.01s
                               ETA: 860750.8s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.056s, learning 0.181s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0509
             Mean action noise std: 0.69
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 8.24s
                        Total time: 14582.25s
                               ETA: 860711.5s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.143s, learning 0.195s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0458
             Mean action noise std: 0.69
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 8.34s
                        Total time: 14590.59s
                               ETA: 860678.3s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.746s, learning 0.163s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0483
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 7.91s
                        Total time: 14598.50s
                               ETA: 860619.8s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.294s, learning 0.160s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0448
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 8.45s
                        Total time: 14606.95s
                               ETA: 860593.5s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.920s, learning 0.167s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0569
             Mean action noise std: 0.69
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 8.09s
                        Total time: 14615.04s
                               ETA: 860545.6s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.349s, learning 0.281s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0531
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 8.63s
                        Total time: 14623.67s
                               ETA: 860529.7s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.880s, learning 0.182s)
               Value function loss: 4.7456
                    Surrogate loss: 0.0344
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 8.06s
                        Total time: 14631.73s
                               ETA: 860480.3s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.937s, learning 0.179s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0285
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 8.12s
                        Total time: 14639.84s
                               ETA: 860434.3s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.767s, learning 0.156s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0217
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 7.92s
                        Total time: 14647.77s
                               ETA: 860376.9s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.969s, learning 0.172s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0385
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 8.14s
                        Total time: 14655.91s
                               ETA: 860332.4s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.939s, learning 0.161s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0481
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 8.10s
                        Total time: 14664.01s
                               ETA: 860285.5s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.157s, learning 0.161s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0504
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 8.32s
                        Total time: 14672.33s
                               ETA: 860251.5s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.933s, learning 0.169s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0507
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 8.10s
                        Total time: 14680.43s
                               ETA: 860204.8s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.080s, learning 0.164s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0444
             Mean action noise std: 0.69
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 8.24s
                        Total time: 14688.67s
                               ETA: 860166.6s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.172s, learning 0.156s)
               Value function loss: 0.0878
                    Surrogate loss: -0.0430
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 8.33s
                        Total time: 14697.00s
                               ETA: 860133.2s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.165s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0452
             Mean action noise std: 0.69
                       Mean reward: 21.56
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 8.34s
                        Total time: 14705.34s
                               ETA: 860100.8s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.209s, learning 0.216s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0495
             Mean action noise std: 0.69
                       Mean reward: 21.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 8.42s
                        Total time: 14713.77s
                               ETA: 860073.2s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.149s, learning 0.170s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0482
             Mean action noise std: 0.69
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 8.32s
                        Total time: 14722.09s
                               ETA: 860039.4s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.877s, learning 0.185s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0495
             Mean action noise std: 0.69
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 8.06s
                        Total time: 14730.15s
                               ETA: 859990.6s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.185s, learning 0.165s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0473
             Mean action noise std: 0.69
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 8.35s
                        Total time: 14738.50s
                               ETA: 859958.7s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.745s, learning 0.166s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0420
             Mean action noise std: 0.69
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 7.91s
                        Total time: 14746.41s
                               ETA: 859901.2s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.098s, learning 0.167s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0553
             Mean action noise std: 0.69
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 8.27s
                        Total time: 14754.68s
                               ETA: 859864.4s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.122s, learning 0.245s)
               Value function loss: 2.6991
                    Surrogate loss: 0.0306
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 8.37s
                        Total time: 14763.04s
                               ETA: 859833.6s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.217s, learning 0.187s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0309
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 8.40s
                        Total time: 14771.45s
                               ETA: 859804.9s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.105s, learning 0.333s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0337
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 8.44s
                        Total time: 14779.88s
                               ETA: 859778.2s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.983s, learning 0.191s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0396
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 8.17s
                        Total time: 14788.06s
                               ETA: 859736.3s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.949s, learning 0.203s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0465
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 8.15s
                        Total time: 14796.21s
                               ETA: 859693.1s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.991s, learning 0.175s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0524
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 8.17s
                        Total time: 14804.38s
                               ETA: 859650.7s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.277s, learning 0.363s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0529
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 8.64s
                        Total time: 14813.02s
                               ETA: 859635.9s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.098s, learning 0.166s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0490
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 8.26s
                        Total time: 14821.28s
                               ETA: 859599.3s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.111s, learning 0.164s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0490
             Mean action noise std: 0.69
                       Mean reward: 21.51
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 8.28s
                        Total time: 14829.56s
                               ETA: 859563.4s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.914s, learning 0.189s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 8.10s
                        Total time: 14837.66s
                               ETA: 859517.5s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.096s, learning 0.163s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 8.26s
                        Total time: 14845.92s
                               ETA: 859480.8s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.475s, learning 0.163s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0495
             Mean action noise std: 0.69
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 8.64s
                        Total time: 14854.56s
                               ETA: 859465.9s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.853s, learning 0.164s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0570
             Mean action noise std: 0.69
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 8.02s
                        Total time: 14862.57s
                               ETA: 859415.2s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.243s, learning 0.175s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0531
             Mean action noise std: 0.69
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 8.42s
                        Total time: 14870.99s
                               ETA: 859387.7s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.803s, learning 0.161s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0460
             Mean action noise std: 0.69
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 7.96s
                        Total time: 14878.96s
                               ETA: 859334.0s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.187s, learning 0.162s)
               Value function loss: 4.3033
                    Surrogate loss: 0.0538
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 8.35s
                        Total time: 14887.31s
                               ETA: 859302.7s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.975s, learning 0.190s)
               Value function loss: 0.4867
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 8.17s
                        Total time: 14895.47s
                               ETA: 859260.7s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.066s, learning 0.171s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0327
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 8.24s
                        Total time: 14903.71s
                               ETA: 859222.9s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.994s, learning 0.159s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0323
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 8.15s
                        Total time: 14911.86s
                               ETA: 859180.3s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.001s, learning 0.176s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0404
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 8.18s
                        Total time: 14920.04s
                               ETA: 859139.0s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.888s, learning 0.166s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0429
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 8.05s
                        Total time: 14928.09s
                               ETA: 859090.7s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.160s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0489
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 8.27s
                        Total time: 14936.36s
                               ETA: 859055.1s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.058s, learning 0.161s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0473
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 8.22s
                        Total time: 14944.58s
                               ETA: 859016.4s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.790s, learning 0.200s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0306
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 7.99s
                        Total time: 14952.57s
                               ETA: 858964.6s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.965s, learning 0.166s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0444
             Mean action noise std: 0.69
                       Mean reward: 21.76
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 8.13s
                        Total time: 14960.70s
                               ETA: 858920.9s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.180s, learning 0.170s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0471
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 8.35s
                        Total time: 14969.05s
                               ETA: 858889.9s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.801s, learning 0.160s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0489
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 7.96s
                        Total time: 14977.01s
                               ETA: 858836.5s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.005s, learning 0.174s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0415
             Mean action noise std: 0.69
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 8.18s
                        Total time: 14985.19s
                               ETA: 858795.7s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.813s, learning 0.193s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0349
             Mean action noise std: 0.69
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 8.01s
                        Total time: 14993.20s
                               ETA: 858745.0s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.072s, learning 0.164s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0400
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 8.24s
                        Total time: 15001.43s
                               ETA: 858707.6s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.061s, learning 0.165s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 8.23s
                        Total time: 15009.66s
                               ETA: 858669.6s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.281s, learning 0.175s)
               Value function loss: 6.4448
                    Surrogate loss: 0.0475
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 8.46s
                        Total time: 15018.12s
                               ETA: 858644.8s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.731s, learning 0.248s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0211
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 7.98s
                        Total time: 15026.10s
                               ETA: 858592.8s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.084s, learning 0.165s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0347
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 8.25s
                        Total time: 15034.34s
                               ETA: 858556.3s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.824s, learning 0.165s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0379
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 7.99s
                        Total time: 15042.33s
                               ETA: 858504.9s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.404s, learning 0.166s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0462
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 8.57s
                        Total time: 15050.90s
                               ETA: 858486.8s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.914s, learning 0.173s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0501
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 8.09s
                        Total time: 15058.99s
                               ETA: 858441.0s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.998s, learning 0.163s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0452
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 8.16s
                        Total time: 15067.15s
                               ETA: 858399.7s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.116s, learning 0.213s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0514
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 8.33s
                        Total time: 15075.48s
                               ETA: 858367.9s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.334s, learning 0.173s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0475
             Mean action noise std: 0.69
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 8.51s
                        Total time: 15083.99s
                               ETA: 858346.2s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.860s, learning 0.174s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0475
             Mean action noise std: 0.69
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 8.03s
                        Total time: 15092.02s
                               ETA: 858297.7s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.700s, learning 0.247s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0466
             Mean action noise std: 0.69
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 7.95s
                        Total time: 15099.97s
                               ETA: 858244.2s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.844s, learning 0.163s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0378
             Mean action noise std: 0.69
                       Mean reward: 21.76
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 8.01s
                        Total time: 15107.98s
                               ETA: 858194.2s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.009s, learning 0.160s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0466
             Mean action noise std: 0.69
                       Mean reward: 21.76
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 8.17s
                        Total time: 15116.15s
                               ETA: 858153.4s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.809s, learning 0.164s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0398
             Mean action noise std: 0.69
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 7.97s
                        Total time: 15124.12s
                               ETA: 858101.6s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.722s, learning 0.161s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0370
             Mean action noise std: 0.69
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 7.88s
                        Total time: 15132.00s
                               ETA: 858044.7s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.895s, learning 0.185s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0519
             Mean action noise std: 0.69
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 8.08s
                        Total time: 15140.08s
                               ETA: 857999.1s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.864s, learning 0.189s)
               Value function loss: 3.3399
                    Surrogate loss: 0.0337
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 8.05s
                        Total time: 15148.13s
                               ETA: 857951.9s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.920s, learning 0.245s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0262
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 8.16s
                        Total time: 15156.30s
                               ETA: 857911.1s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.833s, learning 0.171s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0248
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 8.00s
                        Total time: 15164.30s
                               ETA: 857861.3s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.157s, learning 0.180s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0411
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 8.34s
                        Total time: 15172.64s
                               ETA: 857830.3s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.033s, learning 0.177s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0460
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 8.21s
                        Total time: 15180.85s
                               ETA: 857792.2s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.958s, learning 0.167s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0519
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 8.12s
                        Total time: 15188.97s
                               ETA: 857749.3s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.747s, learning 0.170s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0506
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 7.92s
                        Total time: 15196.89s
                               ETA: 857694.7s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.162s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0511
             Mean action noise std: 0.69
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 8.13s
                        Total time: 15205.03s
                               ETA: 857652.5s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.054s, learning 0.195s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0370
             Mean action noise std: 0.69
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 8.25s
                        Total time: 15213.28s
                               ETA: 857616.7s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.160s, learning 0.172s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0491
             Mean action noise std: 0.69
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 8.33s
                        Total time: 15221.61s
                               ETA: 857585.7s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.945s, learning 0.171s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0507
             Mean action noise std: 0.69
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 8.12s
                        Total time: 15229.72s
                               ETA: 857542.5s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.803s, learning 0.159s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0456
             Mean action noise std: 0.69
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 7.96s
                        Total time: 15237.68s
                               ETA: 857490.7s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.193s, learning 0.163s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0398
             Mean action noise std: 0.69
                       Mean reward: 21.99
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 8.36s
                        Total time: 15246.04s
                               ETA: 857461.1s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.159s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 8.33s
                        Total time: 15254.37s
                               ETA: 857430.3s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.998s, learning 0.172s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0484
             Mean action noise std: 0.69
                       Mean reward: 21.99
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 8.17s
                        Total time: 15262.54s
                               ETA: 857390.2s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.202s, learning 0.172s)
               Value function loss: 5.0613
                    Surrogate loss: 0.0272
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 8.37s
                        Total time: 15270.92s
                               ETA: 857361.8s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.759s, learning 0.194s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0283
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 7.95s
                        Total time: 15278.87s
                               ETA: 857309.7s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.910s, learning 0.187s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0210
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 8.10s
                        Total time: 15286.97s
                               ETA: 857265.7s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.954s, learning 0.180s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0329
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 8.13s
                        Total time: 15295.10s
                               ETA: 857223.8s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.130s, learning 0.163s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0409
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 8.29s
                        Total time: 15303.40s
                               ETA: 857190.8s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.942s, learning 0.170s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0468
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 8.11s
                        Total time: 15311.51s
                               ETA: 857147.8s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.945s, learning 0.190s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0494
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 8.13s
                        Total time: 15319.64s
                               ETA: 857106.1s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.792s, learning 0.167s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0458
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 7.96s
                        Total time: 15327.60s
                               ETA: 857054.5s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.045s, learning 0.170s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0444
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 8.21s
                        Total time: 15335.82s
                               ETA: 857017.4s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.999s, learning 0.181s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0436
             Mean action noise std: 0.69
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 8.18s
                        Total time: 15344.00s
                               ETA: 856978.3s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.119s, learning 0.169s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0338
             Mean action noise std: 0.69
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 8.29s
                        Total time: 15352.28s
                               ETA: 856945.3s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.039s, learning 0.252s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0368
             Mean action noise std: 0.69
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 8.29s
                        Total time: 15360.57s
                               ETA: 856912.4s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.885s, learning 0.162s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0384
             Mean action noise std: 0.69
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 8.05s
                        Total time: 15368.62s
                               ETA: 856866.0s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.871s, learning 0.162s)
               Value function loss: 0.0534
                    Surrogate loss: -0.0327
             Mean action noise std: 0.69
                       Mean reward: 22.08
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 8.03s
                        Total time: 15376.65s
                               ETA: 856818.9s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.320s, learning 0.163s)
               Value function loss: 0.0462
                    Surrogate loss: -0.0373
             Mean action noise std: 0.69
                       Mean reward: 22.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 8.48s
                        Total time: 15385.14s
                               ETA: 856796.9s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.165s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0576
             Mean action noise std: 0.69
                       Mean reward: 22.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 8.45s
                        Total time: 15393.59s
                               ETA: 856773.3s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.140s, learning 0.164s)
               Value function loss: 3.5192
                    Surrogate loss: 0.0350
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 8.30s
                        Total time: 15401.90s
                               ETA: 856741.4s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.840s, learning 0.167s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0222
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 8.01s
                        Total time: 15409.90s
                               ETA: 856693.0s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.854s, learning 0.171s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0279
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 8.03s
                        Total time: 15417.93s
                               ETA: 856645.6s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.878s, learning 0.170s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0397
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 8.05s
                        Total time: 15425.98s
                               ETA: 856599.5s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.760s, learning 0.202s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0439
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 7.96s
                        Total time: 15433.94s
                               ETA: 856548.8s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.883s, learning 0.161s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0465
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 8.04s
                        Total time: 15441.98s
                               ETA: 856502.6s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.242s, learning 0.162s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0517
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 8.40s
                        Total time: 15450.39s
                               ETA: 856476.4s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.036s, learning 0.158s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0511
             Mean action noise std: 0.69
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 8.19s
                        Total time: 15458.58s
                               ETA: 856438.5s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.123s, learning 0.160s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0391
             Mean action noise std: 0.69
                       Mean reward: 22.05
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 8.28s
                        Total time: 15466.86s
                               ETA: 856405.7s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.149s, learning 0.171s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0502
             Mean action noise std: 0.69
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 8.32s
                        Total time: 15475.19s
                               ETA: 856375.0s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.303s, learning 0.187s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0471
             Mean action noise std: 0.69
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 8.49s
                        Total time: 15483.68s
                               ETA: 856353.6s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.289s, learning 0.164s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0481
             Mean action noise std: 0.68
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 8.45s
                        Total time: 15492.13s
                               ETA: 856330.2s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.042s, learning 0.169s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0409
             Mean action noise std: 0.68
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 8.21s
                        Total time: 15500.34s
                               ETA: 856293.5s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.819s, learning 0.164s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0502
             Mean action noise std: 0.68
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 7.98s
                        Total time: 15508.32s
                               ETA: 856244.2s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.890s, learning 0.169s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0487
             Mean action noise std: 0.68
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 8.06s
                        Total time: 15516.38s
                               ETA: 856199.2s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.065s, learning 0.172s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0508
             Mean action noise std: 0.68
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 8.24s
                        Total time: 15524.62s
                               ETA: 856164.0s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.352s, learning 0.170s)
               Value function loss: 1.3049
                    Surrogate loss: 0.0295
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 8.52s
                        Total time: 15533.14s
                               ETA: 856144.5s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.155s, learning 0.174s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0296
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 8.33s
                        Total time: 15541.47s
                               ETA: 856114.5s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.812s, learning 0.191s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0331
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 8.00s
                        Total time: 15549.47s
                               ETA: 856066.5s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.033s, learning 0.162s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0456
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 8.19s
                        Total time: 15557.67s
                               ETA: 856029.0s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.287s, learning 0.233s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0464
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 8.52s
                        Total time: 15566.19s
                               ETA: 856009.6s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.450s, learning 0.162s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0500
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 8.61s
                        Total time: 15574.80s
                               ETA: 855995.2s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.925s, learning 0.168s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0469
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 8.09s
                        Total time: 15582.89s
                               ETA: 855952.3s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.106s, learning 0.203s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0520
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 8.31s
                        Total time: 15591.20s
                               ETA: 855921.3s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.064s, learning 0.184s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0509
             Mean action noise std: 0.68
                       Mean reward: 21.54
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 8.25s
                        Total time: 15599.45s
                               ETA: 855887.0s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.959s, learning 0.175s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0562
             Mean action noise std: 0.68
                       Mean reward: 21.55
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 8.13s
                        Total time: 15607.59s
                               ETA: 855846.4s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.986s, learning 0.157s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0492
             Mean action noise std: 0.68
                       Mean reward: 21.55
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 8.14s
                        Total time: 15615.73s
                               ETA: 855806.4s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.177s, learning 0.184s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0474
             Mean action noise std: 0.68
                       Mean reward: 21.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 8.36s
                        Total time: 15624.09s
                               ETA: 855778.4s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.769s, learning 0.180s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0466
             Mean action noise std: 0.68
                       Mean reward: 21.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 7.95s
                        Total time: 15632.04s
                               ETA: 855727.8s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.167s, learning 0.186s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0559
             Mean action noise std: 0.68
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 8.35s
                        Total time: 15640.39s
                               ETA: 855699.3s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.868s, learning 0.169s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0483
             Mean action noise std: 0.68
                       Mean reward: 21.59
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 8.04s
                        Total time: 15648.43s
                               ETA: 855653.7s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.007s, learning 0.182s)
               Value function loss: 3.4389
                    Surrogate loss: 0.0039
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 8.19s
                        Total time: 15656.62s
                               ETA: 855616.3s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.143s, learning 0.288s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0045
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 8.43s
                        Total time: 15665.05s
                               ETA: 855592.2s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.988s, learning 0.161s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0250
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 8.15s
                        Total time: 15673.20s
                               ETA: 855552.8s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.035s, learning 0.191s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0365
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 8.23s
                        Total time: 15681.42s
                               ETA: 855517.6s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.172s, learning 0.266s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0457
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 8.44s
                        Total time: 15689.86s
                               ETA: 855493.9s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.023s, learning 0.167s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0526
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 8.19s
                        Total time: 15698.05s
                               ETA: 855456.8s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.904s, learning 0.171s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0504
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 8.08s
                        Total time: 15706.13s
                               ETA: 855413.4s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.061s, learning 0.315s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0284
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 8.38s
                        Total time: 15714.50s
                               ETA: 855386.5s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.168s, learning 0.166s)
               Value function loss: 0.0938
                    Surrogate loss: -0.0410
             Mean action noise std: 0.68
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 8.33s
                        Total time: 15722.84s
                               ETA: 855357.3s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.281s, learning 0.169s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0340
             Mean action noise std: 0.68
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 8.45s
                        Total time: 15731.29s
                               ETA: 855334.4s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.161s, learning 0.190s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0439
             Mean action noise std: 0.68
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 8.35s
                        Total time: 15739.64s
                               ETA: 855306.2s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.917s, learning 0.196s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0403
             Mean action noise std: 0.68
                       Mean reward: 21.66
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 8.11s
                        Total time: 15747.75s
                               ETA: 855265.0s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.153s, learning 0.187s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0477
             Mean action noise std: 0.68
                       Mean reward: 21.66
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 8.34s
                        Total time: 15756.09s
                               ETA: 855236.2s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.004s, learning 0.157s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0415
             Mean action noise std: 0.68
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 8.16s
                        Total time: 15764.25s
                               ETA: 855197.7s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.114s, learning 0.168s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0438
             Mean action noise std: 0.68
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 8.28s
                        Total time: 15772.53s
                               ETA: 855165.8s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.691s, learning 0.165s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0496
             Mean action noise std: 0.68
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 7.86s
                        Total time: 15780.39s
                               ETA: 855110.8s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.244s, learning 0.165s)
               Value function loss: 3.6159
                    Surrogate loss: 0.0466
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 8.41s
                        Total time: 15788.80s
                               ETA: 855085.9s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.063s, learning 0.169s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0264
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 8.23s
                        Total time: 15797.03s
                               ETA: 855051.4s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.901s, learning 0.190s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0327
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 8.09s
                        Total time: 15805.12s
                               ETA: 855009.3s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.872s, learning 0.158s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0448
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 8.03s
                        Total time: 15813.16s
                               ETA: 854964.0s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.992s, learning 0.166s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0490
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 8.16s
                        Total time: 15821.31s
                               ETA: 854925.6s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 2102 steps/s (collection: 7.605s, learning 0.187s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0423
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 7.79s
                        Total time: 15829.10s
                               ETA: 854867.4s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.125s, learning 0.193s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0547
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 8.32s
                        Total time: 15837.42s
                               ETA: 854837.7s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.265s, learning 0.171s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0500
             Mean action noise std: 0.68
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 8.44s
                        Total time: 15845.86s
                               ETA: 854814.5s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.974s, learning 0.207s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0536
             Mean action noise std: 0.68
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 8.18s
                        Total time: 15854.04s
                               ETA: 854777.4s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.304s, learning 0.161s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0476
             Mean action noise std: 0.68
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 8.47s
                        Total time: 15862.51s
                               ETA: 854755.7s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.928s, learning 0.174s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0471
             Mean action noise std: 0.68
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 8.10s
                        Total time: 15870.61s
                               ETA: 854714.5s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.103s, learning 0.200s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0414
             Mean action noise std: 0.68
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 8.30s
                        Total time: 15878.91s
                               ETA: 854684.1s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.988s, learning 0.168s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0467
             Mean action noise std: 0.68
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 8.16s
                        Total time: 15887.07s
                               ETA: 854645.8s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.962s, learning 0.165s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0480
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 8.13s
                        Total time: 15895.19s
                               ETA: 854606.0s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.011s, learning 0.164s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0523
             Mean action noise std: 0.68
                       Mean reward: 21.32
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 8.17s
                        Total time: 15903.37s
                               ETA: 854568.8s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.086s, learning 0.170s)
               Value function loss: 2.0420
                    Surrogate loss: 0.0337
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 8.26s
                        Total time: 15911.62s
                               ETA: 854536.0s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.087s, learning 0.163s)
               Value function loss: 0.4506
                    Surrogate loss: 0.0102
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 8.25s
                        Total time: 15919.87s
                               ETA: 854502.8s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.746s, learning 0.163s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0264
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 7.91s
                        Total time: 15927.78s
                               ETA: 854451.5s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.019s, learning 0.223s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0318
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 8.24s
                        Total time: 15936.02s
                               ETA: 854418.1s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.971s, learning 0.187s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0375
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 8.16s
                        Total time: 15944.18s
                               ETA: 854380.2s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.143s, learning 0.201s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0459
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 8.34s
                        Total time: 15952.53s
                               ETA: 854352.2s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.041s, learning 0.196s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0502
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 8.24s
                        Total time: 15960.76s
                               ETA: 854318.6s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.028s, learning 0.182s)
               Value function loss: 0.0500
                    Surrogate loss: -0.0406
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 8.21s
                        Total time: 15968.97s
                               ETA: 854283.5s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.991s, learning 0.163s)
               Value function loss: 0.1049
                    Surrogate loss: -0.0409
             Mean action noise std: 0.68
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 8.15s
                        Total time: 15977.13s
                               ETA: 854245.5s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.848s, learning 0.189s)
               Value function loss: 0.0471
                    Surrogate loss: -0.0515
             Mean action noise std: 0.68
                       Mean reward: 21.45
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 8.04s
                        Total time: 15985.16s
                               ETA: 854201.3s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.149s, learning 0.159s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0499
             Mean action noise std: 0.68
                       Mean reward: 21.50
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 8.31s
                        Total time: 15993.47s
                               ETA: 854171.5s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.058s, learning 0.193s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0470
             Mean action noise std: 0.68
                       Mean reward: 21.50
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 8.25s
                        Total time: 16001.72s
                               ETA: 854138.8s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.274s, learning 0.198s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0484
             Mean action noise std: 0.68
                       Mean reward: 21.50
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 8.47s
                        Total time: 16010.20s
                               ETA: 854117.8s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.348s, learning 0.161s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0363
             Mean action noise std: 0.68
                       Mean reward: 21.50
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 8.51s
                        Total time: 16018.70s
                               ETA: 854098.8s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.438s, learning 0.226s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0451
             Mean action noise std: 0.68
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 8.66s
                        Total time: 16027.37s
                               ETA: 854088.2s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.261s, learning 0.158s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0555
             Mean action noise std: 0.68
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 8.42s
                        Total time: 16035.79s
                               ETA: 854064.4s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.150s, learning 0.165s)
               Value function loss: 5.0390
                    Surrogate loss: 0.0889
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 8.31s
                        Total time: 16044.10s
                               ETA: 854035.2s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.162s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0107
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 8.13s
                        Total time: 16052.23s
                               ETA: 853995.9s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.955s, learning 0.168s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0311
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 8.12s
                        Total time: 16060.35s
                               ETA: 853956.5s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.813s, learning 0.188s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0384
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 8.00s
                        Total time: 16068.35s
                               ETA: 853910.7s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.126s, learning 0.169s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0477
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 8.30s
                        Total time: 16076.65s
                               ETA: 853880.5s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.930s, learning 0.181s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0471
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 8.11s
                        Total time: 16084.76s
                               ETA: 853840.6s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.943s, learning 0.178s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0487
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 8.12s
                        Total time: 16092.88s
                               ETA: 853801.2s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.154s, learning 0.176s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0519
             Mean action noise std: 0.68
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 8.33s
                        Total time: 16101.21s
                               ETA: 853772.9s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.987s, learning 0.157s)
               Value function loss: 0.0902
                    Surrogate loss: -0.0471
             Mean action noise std: 0.68
                       Mean reward: 21.79
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 8.14s
                        Total time: 16109.35s
                               ETA: 853734.8s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.189s, learning 0.165s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0289
             Mean action noise std: 0.68
                       Mean reward: 21.80
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 8.35s
                        Total time: 16117.71s
                               ETA: 853707.9s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.177s, learning 0.165s)
               Value function loss: 0.0593
                    Surrogate loss: -0.0288
             Mean action noise std: 0.68
                       Mean reward: 21.80
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 8.34s
                        Total time: 16126.05s
                               ETA: 853680.3s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.771s, learning 0.160s)
               Value function loss: 0.0680
                    Surrogate loss: -0.0199
             Mean action noise std: 0.68
                       Mean reward: 21.78
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 7.93s
                        Total time: 16133.98s
                               ETA: 853631.0s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.004s, learning 0.172s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0317
             Mean action noise std: 0.68
                       Mean reward: 21.78
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 8.18s
                        Total time: 16142.16s
                               ETA: 853594.8s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.127s, learning 0.226s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0449
             Mean action noise std: 0.68
                       Mean reward: 21.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 8.35s
                        Total time: 16150.51s
                               ETA: 853567.9s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.848s, learning 0.164s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0451
             Mean action noise std: 0.68
                       Mean reward: 21.84
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 8.01s
                        Total time: 16158.52s
                               ETA: 853523.0s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.075s, learning 0.169s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0483
             Mean action noise std: 0.68
                       Mean reward: 21.84
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 8.24s
                        Total time: 16166.76s
                               ETA: 853490.4s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.148s, learning 0.169s)
               Value function loss: 2.9004
                    Surrogate loss: 0.0523
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 8.32s
                        Total time: 16175.08s
                               ETA: 853461.7s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.802s, learning 0.160s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0290
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 7.96s
                        Total time: 16183.04s
                               ETA: 853414.2s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.868s, learning 0.163s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0308
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 8.03s
                        Total time: 16191.07s
                               ETA: 853370.5s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.806s, learning 0.185s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0475
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 7.99s
                        Total time: 16199.07s
                               ETA: 853324.7s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.879s, learning 0.163s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0480
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 8.04s
                        Total time: 16207.11s
                               ETA: 853281.6s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 2127 steps/s (collection: 7.493s, learning 0.209s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0517
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 7.70s
                        Total time: 16214.81s
                               ETA: 853220.7s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.280s, learning 0.164s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0587
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 8.44s
                        Total time: 16223.25s
                               ETA: 853198.8s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.920s, learning 0.161s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0495
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 8.08s
                        Total time: 16231.33s
                               ETA: 853157.9s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.838s, learning 0.164s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0401
             Mean action noise std: 0.68
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 8.00s
                        Total time: 16239.34s
                               ETA: 853112.8s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.036s, learning 0.179s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0481
             Mean action noise std: 0.68
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 8.22s
                        Total time: 16247.55s
                               ETA: 853079.0s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.032s, learning 0.168s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0498
             Mean action noise std: 0.68
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 8.20s
                        Total time: 16255.75s
                               ETA: 853044.5s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.947s, learning 0.165s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0489
             Mean action noise std: 0.68
                       Mean reward: 22.22
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 8.11s
                        Total time: 16263.86s
                               ETA: 853005.3s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.897s, learning 0.160s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0485
             Mean action noise std: 0.68
                       Mean reward: 22.21
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 8.06s
                        Total time: 16271.92s
                               ETA: 852963.3s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.954s, learning 0.166s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0543
             Mean action noise std: 0.68
                       Mean reward: 22.20
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 8.12s
                        Total time: 16280.04s
                               ETA: 852924.6s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.126s, learning 0.198s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0482
             Mean action noise std: 0.68
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 8.32s
                        Total time: 16288.37s
                               ETA: 852896.7s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1582 steps/s (collection: 10.182s, learning 0.172s)
               Value function loss: 3.2245
                    Surrogate loss: 0.0271
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 10.35s
                        Total time: 16298.72s
                               ETA: 852975.0s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 1066 steps/s (collection: 15.194s, learning 0.171s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0242
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 15.37s
                        Total time: 16314.08s
                               ETA: 853315.3s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.808s, learning 0.171s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0226
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 15.98s
                        Total time: 16330.06s
                               ETA: 853687.3s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.566s, learning 0.164s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0314
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 15.73s
                        Total time: 16345.79s
                               ETA: 854045.9s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1058 steps/s (collection: 15.255s, learning 0.218s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0440
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 15.47s
                        Total time: 16361.27s
                               ETA: 854390.7s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.685s, learning 0.161s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0513
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 15.85s
                        Total time: 16377.11s
                               ETA: 854754.6s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1065 steps/s (collection: 15.202s, learning 0.169s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0511
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 15.37s
                        Total time: 16392.48s
                               ETA: 855093.2s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.485s, learning 0.167s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0534
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 15.65s
                        Total time: 16408.13s
                               ETA: 855446.2s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.724s, learning 0.166s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0488
             Mean action noise std: 0.68
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 15.89s
                        Total time: 16424.02s
                               ETA: 855811.2s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.729s, learning 0.169s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0450
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 15.90s
                        Total time: 16439.92s
                               ETA: 856176.1s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.541s, learning 0.277s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0518
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 15.82s
                        Total time: 16455.74s
                               ETA: 856536.5s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.742s, learning 0.162s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0452
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 15.90s
                        Total time: 16471.64s
                               ETA: 856901.0s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1066 steps/s (collection: 15.200s, learning 0.165s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0502
             Mean action noise std: 0.68
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 15.36s
                        Total time: 16487.01s
                               ETA: 857237.0s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.426s, learning 0.165s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0453
             Mean action noise std: 0.68
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 15.59s
                        Total time: 16502.60s
                               ETA: 857584.4s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.507s, learning 0.203s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0336
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 15.71s
                        Total time: 16518.31s
                               ETA: 857937.7s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.626s, learning 0.161s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0312
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 15.79s
                        Total time: 16534.10s
                               ETA: 858294.6s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.723s, learning 0.173s)
               Value function loss: 3.7568
                    Surrogate loss: 0.0328
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 15.90s
                        Total time: 16549.99s
                               ETA: 858656.7s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1078 steps/s (collection: 15.033s, learning 0.155s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0176
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 15.19s
                        Total time: 16565.18s
                               ETA: 858981.6s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.910s, learning 0.170s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0370
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 16.08s
                        Total time: 16581.26s
                               ETA: 859352.5s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.703s, learning 0.191s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0438
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 15.89s
                        Total time: 16597.15s
                               ETA: 859713.3s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.446s, learning 0.236s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0519
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 15.68s
                        Total time: 16612.84s
                               ETA: 860062.7s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.756s, learning 0.252s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0471
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 16.01s
                        Total time: 16628.84s
                               ETA: 860428.7s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1052 steps/s (collection: 15.394s, learning 0.178s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0486
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 15.57s
                        Total time: 16644.42s
                               ETA: 860771.6s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.500s, learning 0.169s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0448
             Mean action noise std: 0.68
                       Mean reward: 21.91
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 15.67s
                        Total time: 16660.08s
                               ETA: 861119.2s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1048 steps/s (collection: 15.450s, learning 0.181s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0355
             Mean action noise std: 0.68
                       Mean reward: 21.93
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 15.63s
                        Total time: 16675.72s
                               ETA: 861464.5s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.429s, learning 0.167s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0581
             Mean action noise std: 0.68
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 15.60s
                        Total time: 16691.31s
                               ETA: 861807.6s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.660s, learning 0.187s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0508
             Mean action noise std: 0.68
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 15.85s
                        Total time: 16707.16s
                               ETA: 862163.2s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.338s, learning 0.190s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0459
             Mean action noise std: 0.68
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 15.53s
                        Total time: 16722.69s
                               ETA: 862502.0s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.821s, learning 0.168s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0435
             Mean action noise std: 0.68
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 15.99s
                        Total time: 16738.67s
                               ETA: 862864.2s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.524s, learning 0.168s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0498
             Mean action noise std: 0.68
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 15.69s
                        Total time: 16754.37s
                               ETA: 863210.7s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.645s, learning 0.164s)
               Value function loss: 0.0480
                    Surrogate loss: -0.0499
             Mean action noise std: 0.68
                       Mean reward: 21.92
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 15.81s
                        Total time: 16770.18s
                               ETA: 863562.8s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1039 steps/s (collection: 15.570s, learning 0.198s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0556
             Mean action noise std: 0.68
                       Mean reward: 21.92
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 15.77s
                        Total time: 16785.94s
                               ETA: 863912.5s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1081 steps/s (collection: 14.970s, learning 0.172s)
               Value function loss: 1.3137
                    Surrogate loss: 0.0216
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 15.14s
                        Total time: 16801.09s
                               ETA: 864229.6s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1048 steps/s (collection: 15.429s, learning 0.190s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0286
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 15.62s
                        Total time: 16816.71s
                               ETA: 864570.8s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.645s, learning 0.193s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0390
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 15.84s
                        Total time: 16832.54s
                               ETA: 864922.9s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.917s, learning 0.165s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0461
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 16.08s
                        Total time: 16848.62s
                               ETA: 865287.2s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.563s, learning 0.168s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0508
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 15.73s
                        Total time: 16864.36s
                               ETA: 865633.0s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.678s, learning 0.249s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0479
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 15.93s
                        Total time: 16880.28s
                               ETA: 865988.5s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.930s, learning 0.168s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0451
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 11.10s
                        Total time: 16891.38s
                               ETA: 866096.1s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.949s, learning 0.182s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0455
             Mean action noise std: 0.68
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 8.13s
                        Total time: 16899.51s
                               ETA: 866051.4s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.839s, learning 0.162s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0520
             Mean action noise std: 0.68
                       Mean reward: 21.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 8.00s
                        Total time: 16907.51s
                               ETA: 866000.2s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.865s, learning 0.189s)
               Value function loss: 0.0478
                    Surrogate loss: -0.0595
             Mean action noise std: 0.68
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 8.05s
                        Total time: 16915.57s
                               ETA: 865951.7s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.106s, learning 0.157s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0488
             Mean action noise std: 0.68
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 8.26s
                        Total time: 16923.83s
                               ETA: 865913.9s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.829s, learning 0.162s)
               Value function loss: 0.0650
                    Surrogate loss: -0.0510
             Mean action noise std: 0.68
                       Mean reward: 21.61
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 7.99s
                        Total time: 16931.82s
                               ETA: 865862.2s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.035s, learning 0.219s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0492
             Mean action noise std: 0.68
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 8.25s
                        Total time: 16940.07s
                               ETA: 865824.0s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.135s, learning 0.162s)
               Value function loss: 0.0496
                    Surrogate loss: -0.0474
             Mean action noise std: 0.68
                       Mean reward: 21.61
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 8.30s
                        Total time: 16948.37s
                               ETA: 865788.0s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.815s, learning 0.156s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0473
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 7.97s
                        Total time: 16956.34s
                               ETA: 865735.5s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.016s, learning 0.187s)
               Value function loss: 2.3072
                    Surrogate loss: 0.0357
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 8.20s
                        Total time: 16964.54s
                               ETA: 865694.9s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.833s, learning 0.166s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0187
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 8.00s
                        Total time: 16972.54s
                               ETA: 865643.8s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.927s, learning 0.162s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0294
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 8.09s
                        Total time: 16980.63s
                               ETA: 865597.4s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.313s, learning 0.268s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0353
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 8.58s
                        Total time: 16989.21s
                               ETA: 865576.1s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.207s, learning 0.187s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0417
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 8.39s
                        Total time: 16997.61s
                               ETA: 865545.4s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.829s, learning 0.165s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0503
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 7.99s
                        Total time: 17005.60s
                               ETA: 865494.2s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.925s, learning 0.165s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0511
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 8.09s
                        Total time: 17013.69s
                               ETA: 865448.0s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.193s, learning 0.212s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0510
             Mean action noise std: 0.68
                       Mean reward: 20.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 8.40s
                        Total time: 17022.10s
                               ETA: 865417.9s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.176s, learning 0.164s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0487
             Mean action noise std: 0.68
                       Mean reward: 20.56
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 8.34s
                        Total time: 17030.44s
                               ETA: 865384.5s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.236s, learning 0.170s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0503
             Mean action noise std: 0.68
                       Mean reward: 20.54
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 8.41s
                        Total time: 17038.84s
                               ETA: 865354.4s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.109s, learning 0.204s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0552
             Mean action noise std: 0.68
                       Mean reward: 20.54
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 8.31s
                        Total time: 17047.16s
                               ETA: 865319.7s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.067s, learning 0.169s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0497
             Mean action noise std: 0.68
                       Mean reward: 20.53
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 8.24s
                        Total time: 17055.39s
                               ETA: 865281.0s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.366s, learning 0.217s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0491
             Mean action noise std: 0.68
                       Mean reward: 20.53
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 8.58s
                        Total time: 17063.98s
                               ETA: 865260.0s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.176s, learning 0.183s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0478
             Mean action noise std: 0.68
                       Mean reward: 20.47
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 8.36s
                        Total time: 17072.33s
                               ETA: 865227.7s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.968s, learning 0.163s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0511
             Mean action noise std: 0.68
                       Mean reward: 20.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 8.13s
                        Total time: 17080.47s
                               ETA: 865183.8s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.855s, learning 0.219s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0602
             Mean action noise std: 0.68
                       Mean reward: 20.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 8.07s
                        Total time: 17088.54s
                               ETA: 865137.1s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.013s, learning 0.168s)
               Value function loss: 2.8567
                    Surrogate loss: 0.0199
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 8.18s
                        Total time: 17096.72s
                               ETA: 865095.8s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.023s, learning 0.190s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0227
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 8.21s
                        Total time: 17104.93s
                               ETA: 865056.2s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.886s, learning 0.176s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0327
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 8.06s
                        Total time: 17113.00s
                               ETA: 865009.0s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.199s, learning 0.167s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0448
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 8.37s
                        Total time: 17121.36s
                               ETA: 864977.2s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.969s, learning 0.215s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0439
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 8.18s
                        Total time: 17129.55s
                               ETA: 864936.2s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.871s, learning 0.193s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0504
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 8.06s
                        Total time: 17137.61s
                               ETA: 864889.2s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.006s, learning 0.173s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0527
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 8.18s
                        Total time: 17145.79s
                               ETA: 864848.0s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.052s, learning 0.215s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0528
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 8.27s
                        Total time: 17154.05s
                               ETA: 864811.3s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.974s, learning 0.223s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0489
             Mean action noise std: 0.68
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 8.20s
                        Total time: 17162.25s
                               ETA: 864771.1s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.131s, learning 0.228s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0509
             Mean action noise std: 0.68
                       Mean reward: 20.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 8.36s
                        Total time: 17170.61s
                               ETA: 864739.1s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.286s, learning 0.177s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0543
             Mean action noise std: 0.68
                       Mean reward: 20.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 8.46s
                        Total time: 17179.07s
                               ETA: 864712.3s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.090s, learning 0.207s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0566
             Mean action noise std: 0.68
                       Mean reward: 20.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 8.30s
                        Total time: 17187.37s
                               ETA: 864677.3s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.332s, learning 0.163s)
               Value function loss: 0.0396
                    Surrogate loss: -0.0532
             Mean action noise std: 0.68
                       Mean reward: 20.22
               Mean episode length: 124.71
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 8.50s
                        Total time: 17195.87s
                               ETA: 864652.2s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.248s, learning 0.208s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0524
             Mean action noise std: 0.68
                       Mean reward: 20.18
               Mean episode length: 124.71
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 8.46s
                        Total time: 17204.32s
                               ETA: 864625.2s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.365s, learning 0.180s)
               Value function loss: 0.0385
                    Surrogate loss: -0.0527
             Mean action noise std: 0.68
                       Mean reward: 20.15
               Mean episode length: 124.71
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 8.55s
                        Total time: 17212.87s
                               ETA: 864602.6s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.365s, learning 0.177s)
               Value function loss: 2.0844
                    Surrogate loss: 0.0417
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 8.54s
                        Total time: 17221.41s
                               ETA: 864579.9s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.243s, learning 0.242s)
               Value function loss: 0.6232
                    Surrogate loss: 0.0061
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 8.49s
                        Total time: 17229.89s
                               ETA: 864554.4s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.861s, learning 0.204s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0228
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 8.07s
                        Total time: 17237.96s
                               ETA: 864507.9s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.984s, learning 0.160s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0284
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 8.14s
                        Total time: 17246.10s
                               ETA: 864465.4s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.152s, learning 0.198s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0371
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 8.35s
                        Total time: 17254.45s
                               ETA: 864433.2s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.968s, learning 0.199s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0446
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 8.17s
                        Total time: 17262.62s
                               ETA: 864391.8s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.844s, learning 0.257s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0505
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 8.10s
                        Total time: 17270.72s
                               ETA: 864347.2s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.017s, learning 0.162s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0488
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 8.18s
                        Total time: 17278.90s
                               ETA: 864306.5s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.086s, learning 0.169s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0560
             Mean action noise std: 0.68
                       Mean reward: 19.36
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 8.25s
                        Total time: 17287.16s
                               ETA: 864269.6s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.882s, learning 0.163s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0483
             Mean action noise std: 0.68
                       Mean reward: 19.38
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 8.04s
                        Total time: 17295.20s
                               ETA: 864222.3s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.991s, learning 0.193s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0522
             Mean action noise std: 0.68
                       Mean reward: 19.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 8.18s
                        Total time: 17303.38s
                               ETA: 864182.0s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.005s, learning 0.175s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0447
             Mean action noise std: 0.68
                       Mean reward: 19.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 8.18s
                        Total time: 17311.56s
                               ETA: 864141.5s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 2092 steps/s (collection: 7.666s, learning 0.162s)
               Value function loss: 0.0420
                    Surrogate loss: -0.0499
             Mean action noise std: 0.68
                       Mean reward: 19.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 7.83s
                        Total time: 17319.39s
                               ETA: 864083.5s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.059s, learning 0.179s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0482
             Mean action noise std: 0.68
                       Mean reward: 19.37
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 8.24s
                        Total time: 17327.63s
                               ETA: 864045.9s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.037s, learning 0.160s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0538
             Mean action noise std: 0.68
                       Mean reward: 19.39
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 8.20s
                        Total time: 17335.83s
                               ETA: 864006.3s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.939s, learning 0.164s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0524
             Mean action noise std: 0.68
                       Mean reward: 19.40
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 8.10s
                        Total time: 17343.93s
                               ETA: 863962.1s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.947s, learning 0.203s)
               Value function loss: 2.6542
                    Surrogate loss: 0.0868
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 8.15s
                        Total time: 17352.08s
                               ETA: 863920.2s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.923s, learning 0.168s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0124
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 8.09s
                        Total time: 17360.17s
                               ETA: 863875.5s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.918s, learning 0.161s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0212
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 8.08s
                        Total time: 17368.25s
                               ETA: 863830.2s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.024s, learning 0.161s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0235
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 8.19s
                        Total time: 17376.43s
                               ETA: 863790.2s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.963s, learning 0.186s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0335
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 8.15s
                        Total time: 17384.58s
                               ETA: 863748.5s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.955s, learning 0.185s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0440
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 8.14s
                        Total time: 17392.72s
                               ETA: 863706.4s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.177s, learning 0.167s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0455
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 8.34s
                        Total time: 17401.07s
                               ETA: 863674.4s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.654s, learning 0.242s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0531
             Mean action noise std: 0.68
                       Mean reward: 19.68
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 7.90s
                        Total time: 17408.96s
                               ETA: 863620.2s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.102s, learning 0.159s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0511
             Mean action noise std: 0.68
                       Mean reward: 19.70
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 8.26s
                        Total time: 17417.22s
                               ETA: 863584.2s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.089s, learning 0.172s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0490
             Mean action noise std: 0.68
                       Mean reward: 19.73
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 8.26s
                        Total time: 17425.48s
                               ETA: 863548.2s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.416s, learning 0.184s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0495
             Mean action noise std: 0.68
                       Mean reward: 19.73
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 8.60s
                        Total time: 17434.08s
                               ETA: 863529.0s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.236s, learning 0.157s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0431
             Mean action noise std: 0.68
                       Mean reward: 19.72
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 8.39s
                        Total time: 17442.48s
                               ETA: 863499.6s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.032s, learning 0.194s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0478
             Mean action noise std: 0.68
                       Mean reward: 19.73
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 8.23s
                        Total time: 17450.70s
                               ETA: 863461.8s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 2093 steps/s (collection: 7.643s, learning 0.183s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0444
             Mean action noise std: 0.68
                       Mean reward: 19.75
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 7.83s
                        Total time: 17458.53s
                               ETA: 863404.4s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.935s, learning 0.164s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0388
             Mean action noise std: 0.68
                       Mean reward: 19.72
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 8.10s
                        Total time: 17466.63s
                               ETA: 863360.6s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.803s, learning 0.161s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0564
             Mean action noise std: 0.68
                       Mean reward: 19.72
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 7.96s
                        Total time: 17474.59s
                               ETA: 863310.1s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.990s, learning 0.169s)
               Value function loss: 3.0812
                    Surrogate loss: 0.0348
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 8.16s
                        Total time: 17482.75s
                               ETA: 863269.2s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.053s, learning 0.162s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0237
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 8.21s
                        Total time: 17490.97s
                               ETA: 863231.2s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.908s, learning 0.159s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0271
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 8.07s
                        Total time: 17499.03s
                               ETA: 863185.8s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.935s, learning 0.231s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0363
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 8.17s
                        Total time: 17507.20s
                               ETA: 863145.5s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.279s, learning 0.165s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0483
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 8.44s
                        Total time: 17515.64s
                               ETA: 863118.8s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.848s, learning 0.156s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0493
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 8.00s
                        Total time: 17523.65s
                               ETA: 863070.5s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.132s, learning 0.158s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0457
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 8.29s
                        Total time: 17531.94s
                               ETA: 863036.2s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.330s, learning 0.164s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0530
             Mean action noise std: 0.68
                       Mean reward: 19.89
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 8.49s
                        Total time: 17540.43s
                               ETA: 863012.1s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.030s, learning 0.155s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0359
             Mean action noise std: 0.68
                       Mean reward: 19.86
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 8.19s
                        Total time: 17548.62s
                               ETA: 862972.8s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.904s, learning 0.161s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0512
             Mean action noise std: 0.68
                       Mean reward: 19.87
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 8.06s
                        Total time: 17556.68s
                               ETA: 862927.6s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.939s, learning 0.161s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0361
             Mean action noise std: 0.68
                       Mean reward: 19.87
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 8.10s
                        Total time: 17564.78s
                               ETA: 862884.2s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.060s, learning 0.247s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0465
             Mean action noise std: 0.68
                       Mean reward: 19.87
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 8.31s
                        Total time: 17573.09s
                               ETA: 862850.9s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.911s, learning 0.172s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0415
             Mean action noise std: 0.68
                       Mean reward: 19.90
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 8.08s
                        Total time: 17581.17s
                               ETA: 862806.7s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.187s, learning 0.177s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0465
             Mean action noise std: 0.68
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 8.36s
                        Total time: 17589.54s
                               ETA: 862776.4s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.941s, learning 0.162s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0398
             Mean action noise std: 0.68
                       Mean reward: 19.90
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 8.10s
                        Total time: 17597.64s
                               ETA: 862733.3s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 2077 steps/s (collection: 7.694s, learning 0.192s)
               Value function loss: 7.4620
                    Surrogate loss: 0.0584
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 7.89s
                        Total time: 17605.52s
                               ETA: 862679.5s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.014s, learning 0.225s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0204
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 8.24s
                        Total time: 17613.76s
                               ETA: 862643.1s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.078s, learning 0.168s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0270
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 8.25s
                        Total time: 17622.01s
                               ETA: 862607.1s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.915s, learning 0.158s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0348
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 8.07s
                        Total time: 17630.08s
                               ETA: 862562.6s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.786s, learning 0.293s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0409
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 8.08s
                        Total time: 17638.16s
                               ETA: 862518.4s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.032s, learning 0.159s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0450
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 8.19s
                        Total time: 17646.35s
                               ETA: 862479.8s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 2087 steps/s (collection: 7.682s, learning 0.165s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0453
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 7.85s
                        Total time: 17654.20s
                               ETA: 862424.4s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.060s, learning 0.174s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0492
             Mean action noise std: 0.68
                       Mean reward: 20.05
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 8.23s
                        Total time: 17662.43s
                               ETA: 862387.9s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.238s, learning 0.166s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0503
             Mean action noise std: 0.68
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 8.40s
                        Total time: 17670.84s
                               ETA: 862359.8s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.270s, learning 0.180s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0446
             Mean action noise std: 0.68
                       Mean reward: 20.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 8.45s
                        Total time: 17679.29s
                               ETA: 862333.9s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.950s, learning 0.165s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0451
             Mean action noise std: 0.68
                       Mean reward: 20.11
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 8.12s
                        Total time: 17687.40s
                               ETA: 862291.7s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.819s, learning 0.159s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0308
             Mean action noise std: 0.68
                       Mean reward: 20.11
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 7.98s
                        Total time: 17695.38s
                               ETA: 862242.9s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.109s, learning 0.156s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0306
             Mean action noise std: 0.68
                       Mean reward: 20.09
               Mean episode length: 124.71
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 8.27s
                        Total time: 17703.65s
                               ETA: 862208.1s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.068s, learning 0.156s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0400
             Mean action noise std: 0.68
                       Mean reward: 20.12
               Mean episode length: 124.71
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 8.22s
                        Total time: 17711.87s
                               ETA: 862171.3s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.273s, learning 0.165s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0343
             Mean action noise std: 0.68
                       Mean reward: 20.12
               Mean episode length: 124.71
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 8.44s
                        Total time: 17720.31s
                               ETA: 862144.9s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.001s, learning 0.166s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0440
             Mean action noise std: 0.68
                       Mean reward: 20.12
               Mean episode length: 124.71
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 8.17s
                        Total time: 17728.48s
                               ETA: 862105.5s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.153s, learning 0.161s)
               Value function loss: 3.5894
                    Surrogate loss: 0.0882
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.31s
                        Total time: 17736.79s
                               ETA: 862073.1s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.776s, learning 0.188s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0126
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 7.96s
                        Total time: 17744.75s
                               ETA: 862023.8s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.029s, learning 0.195s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0243
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 8.22s
                        Total time: 17752.98s
                               ETA: 861987.1s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.139s, learning 0.161s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0329
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 8.30s
                        Total time: 17761.28s
                               ETA: 861954.2s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.993s, learning 0.179s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0414
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 8.17s
                        Total time: 17769.45s
                               ETA: 861915.1s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 2083 steps/s (collection: 7.707s, learning 0.158s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0444
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 7.86s
                        Total time: 17777.31s
                               ETA: 861861.1s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.988s, learning 0.160s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0452
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 8.15s
                        Total time: 17785.46s
                               ETA: 861820.9s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.728s, learning 0.160s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0460
             Mean action noise std: 0.68
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 7.89s
                        Total time: 17793.35s
                               ETA: 861768.1s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.071s, learning 0.189s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0408
             Mean action noise std: 0.68
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 8.26s
                        Total time: 17801.61s
                               ETA: 861733.4s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.067s, learning 0.249s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0431
             Mean action noise std: 0.68
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 8.32s
                        Total time: 17809.93s
                               ETA: 861701.4s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 2083 steps/s (collection: 7.705s, learning 0.160s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0502
             Mean action noise std: 0.68
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 7.87s
                        Total time: 17817.79s
                               ETA: 861647.6s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.906s, learning 0.155s)
               Value function loss: 0.0478
                    Surrogate loss: -0.0441
             Mean action noise std: 0.68
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 8.06s
                        Total time: 17825.85s
                               ETA: 861603.4s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.024s, learning 0.170s)
               Value function loss: 0.0459
                    Surrogate loss: -0.0459
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 8.19s
                        Total time: 17834.05s
                               ETA: 861565.6s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.057s, learning 0.166s)
               Value function loss: 0.0555
                    Surrogate loss: -0.0440
             Mean action noise std: 0.68
                       Mean reward: 21.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 8.22s
                        Total time: 17842.27s
                               ETA: 861529.2s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 2091 steps/s (collection: 7.671s, learning 0.163s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0424
             Mean action noise std: 0.68
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 7.83s
                        Total time: 17850.10s
                               ETA: 861474.1s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.119s, learning 0.162s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0446
             Mean action noise std: 0.68
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 8.28s
                        Total time: 17858.38s
                               ETA: 861440.6s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.853s, learning 0.157s)
               Value function loss: 2.0983
                    Surrogate loss: 0.0337
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 8.01s
                        Total time: 17866.39s
                               ETA: 861394.1s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.937s, learning 0.159s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0281
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 8.10s
                        Total time: 17874.49s
                               ETA: 861351.8s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.861s, learning 0.191s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0316
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 8.05s
                        Total time: 17882.54s
                               ETA: 861307.3s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 2073 steps/s (collection: 7.741s, learning 0.160s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0410
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 7.90s
                        Total time: 17890.44s
                               ETA: 861255.7s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.711s, learning 0.196s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0449
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 7.91s
                        Total time: 17898.35s
                               ETA: 861204.3s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.257s, learning 0.158s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0466
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 8.42s
                        Total time: 17906.77s
                               ETA: 861177.5s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.190s, learning 0.176s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0479
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 8.37s
                        Total time: 17915.13s
                               ETA: 861148.3s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.294s, learning 0.229s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0523
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 8.52s
                        Total time: 17923.66s
                               ETA: 861126.6s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.993s, learning 0.194s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0365
             Mean action noise std: 0.68
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 8.19s
                        Total time: 17931.84s
                               ETA: 861088.8s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1997 steps/s (collection: 7.990s, learning 0.213s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0378
             Mean action noise std: 0.68
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 8.20s
                        Total time: 17940.05s
                               ETA: 861051.9s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.942s, learning 0.214s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0436
             Mean action noise std: 0.68
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 8.16s
                        Total time: 17948.20s
                               ETA: 861012.7s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.040s, learning 0.161s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0421
             Mean action noise std: 0.68
                       Mean reward: 21.48
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 8.20s
                        Total time: 17956.40s
                               ETA: 860975.7s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.008s, learning 0.260s)
               Value function loss: 0.0577
                    Surrogate loss: -0.0367
             Mean action noise std: 0.68
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 8.27s
                        Total time: 17964.67s
                               ETA: 860941.9s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.994s, learning 0.160s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0422
             Mean action noise std: 0.68
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 8.15s
                        Total time: 17972.82s
                               ETA: 860902.7s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.134s, learning 0.159s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0473
             Mean action noise std: 0.68
                       Mean reward: 21.49
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 8.29s
                        Total time: 17981.12s
                               ETA: 860870.2s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.754s, learning 0.159s)
               Value function loss: 2.9745
                    Surrogate loss: 0.0469
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 7.91s
                        Total time: 17989.03s
                               ETA: 860819.5s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.180s, learning 0.205s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0075
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 8.38s
                        Total time: 17997.42s
                               ETA: 860791.4s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.900s, learning 0.162s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0274
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 8.06s
                        Total time: 18005.48s
                               ETA: 860747.9s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.071s, learning 0.174s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0362
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 8.24s
                        Total time: 18013.72s
                               ETA: 860713.2s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.682s, learning 0.190s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0341
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 7.87s
                        Total time: 18021.59s
                               ETA: 860660.7s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.756s, learning 0.159s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0442
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.91s
                        Total time: 18029.51s
                               ETA: 860610.3s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.961s, learning 0.180s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0479
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 8.14s
                        Total time: 18037.65s
                               ETA: 860570.7s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.047s, learning 0.230s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0488
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 8.28s
                        Total time: 18045.93s
                               ETA: 860537.7s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.951s, learning 0.160s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0416
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 8.11s
                        Total time: 18054.04s
                               ETA: 860496.7s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 2114 steps/s (collection: 7.587s, learning 0.160s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0433
             Mean action noise std: 0.68
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 7.75s
                        Total time: 18061.79s
                               ETA: 860438.5s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.025s, learning 0.165s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0469
             Mean action noise std: 0.68
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 8.19s
                        Total time: 18069.98s
                               ETA: 860401.4s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.927s, learning 0.159s)
               Value function loss: 0.0451
                    Surrogate loss: -0.0303
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 8.09s
                        Total time: 18078.06s
                               ETA: 860359.4s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.252s, learning 0.158s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0322
             Mean action noise std: 0.68
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 8.41s
                        Total time: 18086.47s
                               ETA: 860332.8s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.241s, learning 0.160s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0389
             Mean action noise std: 0.68
                       Mean reward: 21.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 8.40s
                        Total time: 18094.87s
                               ETA: 860305.8s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.020s, learning 0.159s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0340
             Mean action noise std: 0.68
                       Mean reward: 21.99
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 8.18s
                        Total time: 18103.05s
                               ETA: 860268.3s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.110s, learning 0.156s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0466
             Mean action noise std: 0.68
                       Mean reward: 21.99
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 8.27s
                        Total time: 18111.32s
                               ETA: 860234.9s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 2124 steps/s (collection: 7.551s, learning 0.159s)
               Value function loss: 4.6270
                    Surrogate loss: 0.0714
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 7.71s
                        Total time: 18119.03s
                               ETA: 860175.2s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.801s, learning 0.155s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0230
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 7.96s
                        Total time: 18126.98s
                               ETA: 860127.2s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 2082 steps/s (collection: 7.606s, learning 0.261s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0262
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 7.87s
                        Total time: 18134.85s
                               ETA: 860075.0s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.837s, learning 0.162s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0385
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 8.00s
                        Total time: 18142.85s
                               ETA: 860029.1s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.851s, learning 0.199s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0484
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 8.05s
                        Total time: 18150.90s
                               ETA: 859985.6s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.918s, learning 0.175s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0495
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 8.09s
                        Total time: 18158.99s
                               ETA: 859944.2s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.868s, learning 0.165s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0546
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 8.03s
                        Total time: 18167.03s
                               ETA: 859900.1s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.828s, learning 0.174s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0492
             Mean action noise std: 0.68
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 8.00s
                        Total time: 18175.03s
                               ETA: 859854.4s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.878s, learning 0.190s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0523
             Mean action noise std: 0.68
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 8.07s
                        Total time: 18183.10s
                               ETA: 859811.9s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.847s, learning 0.184s)
               Value function loss: 0.0372
                    Surrogate loss: -0.0525
             Mean action noise std: 0.68
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 8.03s
                        Total time: 18191.13s
                               ETA: 859767.7s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.877s, learning 0.191s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0462
             Mean action noise std: 0.68
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 8.07s
                        Total time: 18199.19s
                               ETA: 859725.3s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.882s, learning 0.219s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0422
             Mean action noise std: 0.68
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 8.10s
                        Total time: 18207.29s
                               ETA: 859684.5s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.803s, learning 0.208s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0438
             Mean action noise std: 0.67
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 8.01s
                        Total time: 18215.31s
                               ETA: 859639.5s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.776s, learning 0.167s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0463
             Mean action noise std: 0.67
                       Mean reward: 21.69
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 7.94s
                        Total time: 18223.25s
                               ETA: 859591.3s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.015s, learning 0.168s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0410
             Mean action noise std: 0.67
                       Mean reward: 21.71
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 8.18s
                        Total time: 18231.43s
                               ETA: 859554.5s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 2101 steps/s (collection: 7.609s, learning 0.187s)
               Value function loss: 5.9363
                    Surrogate loss: 0.0569
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 7.80s
                        Total time: 18239.23s
                               ETA: 859499.5s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.807s, learning 0.161s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0176
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 7.97s
                        Total time: 18247.20s
                               ETA: 859452.5s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.012s, learning 0.258s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0252
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 8.27s
                        Total time: 18255.47s
                               ETA: 859419.9s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.108s, learning 0.167s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0294
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 8.28s
                        Total time: 18263.74s
                               ETA: 859387.6s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.964s, learning 0.173s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0437
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 8.14s
                        Total time: 18271.88s
                               ETA: 859348.7s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.042s, learning 0.181s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0406
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 8.22s
                        Total time: 18280.10s
                               ETA: 859313.9s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.968s, learning 0.175s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0453
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 8.14s
                        Total time: 18288.24s
                               ETA: 859275.5s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.779s, learning 0.165s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0572
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 7.94s
                        Total time: 18296.19s
                               ETA: 859227.6s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.121s, learning 0.213s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0553
             Mean action noise std: 0.67
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 8.33s
                        Total time: 18304.52s
                               ETA: 859198.1s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.165s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0462
             Mean action noise std: 0.67
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 8.37s
                        Total time: 18312.89s
                               ETA: 859170.4s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.005s, learning 0.170s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0436
             Mean action noise std: 0.67
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 8.17s
                        Total time: 18321.07s
                               ETA: 859133.5s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.968s, learning 0.186s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0371
             Mean action noise std: 0.67
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 8.15s
                        Total time: 18329.22s
                               ETA: 859095.7s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 2093 steps/s (collection: 7.651s, learning 0.175s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0305
             Mean action noise std: 0.67
                       Mean reward: 21.51
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 7.83s
                        Total time: 18337.05s
                               ETA: 859042.5s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.008s, learning 0.165s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0313
             Mean action noise std: 0.67
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 8.17s
                        Total time: 18345.22s
                               ETA: 859005.6s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.107s, learning 0.197s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0326
             Mean action noise std: 0.67
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 8.30s
                        Total time: 18353.53s
                               ETA: 858974.8s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.086s, learning 0.179s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0424
             Mean action noise std: 0.67
                       Mean reward: 21.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 8.26s
                        Total time: 18361.79s
                               ETA: 858942.3s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 2080 steps/s (collection: 7.716s, learning 0.159s)
               Value function loss: 6.6976
                    Surrogate loss: 0.0738
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 7.87s
                        Total time: 18369.66s
                               ETA: 858891.5s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.008s, learning 0.158s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0129
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 8.17s
                        Total time: 18377.83s
                               ETA: 858854.3s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.769s, learning 0.256s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0212
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 8.02s
                        Total time: 18385.85s
                               ETA: 858810.6s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.871s, learning 0.157s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0361
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 8.03s
                        Total time: 18393.88s
                               ETA: 858767.1s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.835s, learning 0.268s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0469
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 8.10s
                        Total time: 18401.98s
                               ETA: 858727.1s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.257s, learning 0.197s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0501
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 8.45s
                        Total time: 18410.44s
                               ETA: 858703.6s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.099s, learning 0.220s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0494
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 8.32s
                        Total time: 18418.76s
                               ETA: 858673.7s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.090s, learning 0.160s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0536
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 8.25s
                        Total time: 18427.01s
                               ETA: 858640.7s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.731s, learning 0.184s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0454
             Mean action noise std: 0.67
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 7.91s
                        Total time: 18434.92s
                               ETA: 858592.1s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.872s, learning 0.161s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0380
             Mean action noise std: 0.67
                       Mean reward: 21.69
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 8.03s
                        Total time: 18442.96s
                               ETA: 858549.0s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.719s, learning 0.174s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0325
             Mean action noise std: 0.67
                       Mean reward: 21.69
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 7.89s
                        Total time: 18450.85s
                               ETA: 858499.4s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.229s, learning 0.161s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0402
             Mean action noise std: 0.67
                       Mean reward: 21.66
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 8.39s
                        Total time: 18459.24s
                               ETA: 858473.0s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.016s, learning 0.160s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0390
             Mean action noise std: 0.67
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 8.18s
                        Total time: 18467.41s
                               ETA: 858436.6s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.901s, learning 0.158s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0271
             Mean action noise std: 0.67
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 8.06s
                        Total time: 18475.47s
                               ETA: 858394.9s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.861s, learning 0.161s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0350
             Mean action noise std: 0.67
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 8.02s
                        Total time: 18483.50s
                               ETA: 858351.4s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.888s, learning 0.177s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0480
             Mean action noise std: 0.67
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 8.07s
                        Total time: 18491.56s
                               ETA: 858310.0s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.038s, learning 0.163s)
               Value function loss: 4.4621
                    Surrogate loss: 0.0600
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 8.20s
                        Total time: 18499.76s
                               ETA: 858275.0s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.003s, learning 0.162s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0281
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 8.16s
                        Total time: 18507.93s
                               ETA: 858238.2s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.121s, learning 0.197s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0364
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 8.32s
                        Total time: 18516.24s
                               ETA: 858208.7s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.846s, learning 0.196s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0415
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 8.04s
                        Total time: 18524.29s
                               ETA: 858166.3s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.178s, learning 0.280s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0472
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 8.46s
                        Total time: 18532.74s
                               ETA: 858143.2s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.824s, learning 0.172s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0453
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 8.00s
                        Total time: 18540.74s
                               ETA: 858098.8s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.311s, learning 0.166s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0552
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 8.48s
                        Total time: 18549.22s
                               ETA: 858076.7s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.799s, learning 0.163s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0556
             Mean action noise std: 0.67
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 7.96s
                        Total time: 18557.18s
                               ETA: 858030.7s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.020s, learning 0.162s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0365
             Mean action noise std: 0.67
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 8.18s
                        Total time: 18565.36s
                               ETA: 857995.0s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.115s, learning 0.216s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0457
             Mean action noise std: 0.67
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 8.33s
                        Total time: 18573.69s
                               ETA: 857966.1s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.845s, learning 0.225s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0326
             Mean action noise std: 0.67
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 8.07s
                        Total time: 18581.76s
                               ETA: 857925.3s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.705s, learning 0.164s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0395
             Mean action noise std: 0.67
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 7.87s
                        Total time: 18589.63s
                               ETA: 857875.2s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.971s, learning 0.158s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0373
             Mean action noise std: 0.67
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 8.13s
                        Total time: 18597.76s
                               ETA: 857837.1s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.994s, learning 0.191s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0420
             Mean action noise std: 0.67
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 8.18s
                        Total time: 18605.95s
                               ETA: 857801.6s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.172s, learning 0.166s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0415
             Mean action noise std: 0.67
                       Mean reward: 21.99
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 8.34s
                        Total time: 18614.28s
                               ETA: 857773.2s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.205s, learning 0.315s)
               Value function loss: 8.4195
                    Surrogate loss: 0.0624
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 8.52s
                        Total time: 18622.80s
                               ETA: 857753.2s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.909s, learning 0.173s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0192
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 8.08s
                        Total time: 18630.89s
                               ETA: 857713.1s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.082s, learning 0.174s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0328
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 8.26s
                        Total time: 18639.14s
                               ETA: 857680.9s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.814s, learning 0.258s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0333
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 8.07s
                        Total time: 18647.21s
                               ETA: 857640.4s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.771s, learning 0.161s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0303
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 7.93s
                        Total time: 18655.15s
                               ETA: 857593.4s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.850s, learning 0.163s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0389
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 8.01s
                        Total time: 18663.16s
                               ETA: 857550.2s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.692s, learning 0.160s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0487
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 7.85s
                        Total time: 18671.01s
                               ETA: 857499.7s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.804s, learning 0.224s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0509
             Mean action noise std: 0.67
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 8.03s
                        Total time: 18679.04s
                               ETA: 857457.2s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.941s, learning 0.163s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0308
             Mean action noise std: 0.67
                       Mean reward: 22.61
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 8.10s
                        Total time: 18687.14s
                               ETA: 857418.3s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 2089 steps/s (collection: 7.621s, learning 0.220s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0397
             Mean action noise std: 0.67
                       Mean reward: 22.61
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 7.84s
                        Total time: 18694.98s
                               ETA: 857367.3s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.875s, learning 0.182s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0396
             Mean action noise std: 0.67
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 8.06s
                        Total time: 18703.04s
                               ETA: 857326.3s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.849s, learning 0.178s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0383
             Mean action noise std: 0.67
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 8.03s
                        Total time: 18711.07s
                               ETA: 857284.0s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.069s, learning 0.161s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0320
             Mean action noise std: 0.67
                       Mean reward: 22.61
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 8.23s
                        Total time: 18719.30s
                               ETA: 857250.9s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.917s, learning 0.156s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0402
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 8.07s
                        Total time: 18727.37s
                               ETA: 857210.7s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.069s, learning 0.226s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0326
             Mean action noise std: 0.67
                       Mean reward: 22.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 8.30s
                        Total time: 18735.67s
                               ETA: 857180.8s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.839s, learning 0.192s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0524
             Mean action noise std: 0.67
                       Mean reward: 22.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 8.03s
                        Total time: 18743.70s
                               ETA: 857138.7s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 2079 steps/s (collection: 7.704s, learning 0.173s)
               Value function loss: 6.0811
                    Surrogate loss: 0.0380
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 7.88s
                        Total time: 18751.57s
                               ETA: 857089.7s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.027s, learning 0.165s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0237
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 8.19s
                        Total time: 18759.77s
                               ETA: 857055.1s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.923s, learning 0.168s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0300
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 8.09s
                        Total time: 18767.86s
                               ETA: 857015.9s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.968s, learning 0.184s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0348
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 8.15s
                        Total time: 18776.01s
                               ETA: 856979.5s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.078s, learning 0.159s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0346
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 8.24s
                        Total time: 18784.25s
                               ETA: 856946.9s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.803s, learning 0.165s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0415
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 7.97s
                        Total time: 18792.21s
                               ETA: 856902.2s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.918s, learning 0.166s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0495
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 8.08s
                        Total time: 18800.30s
                               ETA: 856862.8s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.016s, learning 0.175s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0524
             Mean action noise std: 0.67
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 8.19s
                        Total time: 18808.49s
                               ETA: 856828.2s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.141s, learning 0.162s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0468
             Mean action noise std: 0.67
                       Mean reward: 22.31
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 8.30s
                        Total time: 18816.79s
                               ETA: 856798.8s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.934s, learning 0.163s)
               Value function loss: 0.0317
                    Surrogate loss: -0.0448
             Mean action noise std: 0.67
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 8.10s
                        Total time: 18824.89s
                               ETA: 856760.1s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.982s, learning 0.184s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0431
             Mean action noise std: 0.67
                       Mean reward: 22.37
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 8.17s
                        Total time: 18833.05s
                               ETA: 856724.5s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.386s, learning 0.218s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0400
             Mean action noise std: 0.67
                       Mean reward: 22.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 8.60s
                        Total time: 18841.66s
                               ETA: 856708.9s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.754s, learning 0.165s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0381
             Mean action noise std: 0.67
                       Mean reward: 22.34
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 7.92s
                        Total time: 18849.58s
                               ETA: 856662.1s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.916s, learning 0.167s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0427
             Mean action noise std: 0.67
                       Mean reward: 22.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 8.08s
                        Total time: 18857.66s
                               ETA: 856622.9s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.891s, learning 0.176s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0341
             Mean action noise std: 0.67
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 8.07s
                        Total time: 18865.73s
                               ETA: 856582.9s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.817s, learning 0.225s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0444
             Mean action noise std: 0.67
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 8.04s
                        Total time: 18873.77s
                               ETA: 856541.8s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.853s, learning 0.187s)
               Value function loss: 2.4539
                    Surrogate loss: 0.0497
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 8.04s
                        Total time: 18881.81s
                               ETA: 856500.7s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.942s, learning 0.165s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0274
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 8.11s
                        Total time: 18889.92s
                               ETA: 856462.6s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.978s, learning 0.192s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0376
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 8.17s
                        Total time: 18898.09s
                               ETA: 856427.4s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.976s, learning 0.193s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0377
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 8.17s
                        Total time: 18906.26s
                               ETA: 856392.2s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.849s, learning 0.257s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0394
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 8.11s
                        Total time: 18914.36s
                               ETA: 856354.2s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.864s, learning 0.196s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0419
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 8.06s
                        Total time: 18922.42s
                               ETA: 856314.0s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.875s, learning 0.189s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0462
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 8.06s
                        Total time: 18930.49s
                               ETA: 856274.1s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1994 steps/s (collection: 7.988s, learning 0.226s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0509
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 8.21s
                        Total time: 18938.70s
                               ETA: 856241.0s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.943s, learning 0.217s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0447
             Mean action noise std: 0.67
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 8.16s
                        Total time: 18946.86s
                               ETA: 856205.6s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.150s, learning 0.162s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0508
             Mean action noise std: 0.67
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 8.31s
                        Total time: 18955.17s
                               ETA: 856177.0s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.876s, learning 0.196s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0460
             Mean action noise std: 0.67
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 8.07s
                        Total time: 18963.25s
                               ETA: 856137.6s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.838s, learning 0.165s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0439
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 8.00s
                        Total time: 18971.25s
                               ETA: 856095.1s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.000s, learning 0.199s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0396
             Mean action noise std: 0.67
                       Mean reward: 21.99
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 8.20s
                        Total time: 18979.45s
                               ETA: 856061.5s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.908s, learning 0.195s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0470
             Mean action noise std: 0.67
                       Mean reward: 22.01
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 8.10s
                        Total time: 18987.55s
                               ETA: 856023.6s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.167s, learning 0.190s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0467
             Mean action noise std: 0.67
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 8.36s
                        Total time: 18995.91s
                               ETA: 855997.1s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1997 steps/s (collection: 7.955s, learning 0.245s)
               Value function loss: 4.1969
                    Surrogate loss: 0.0345
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 8.20s
                        Total time: 19004.11s
                               ETA: 855963.6s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.820s, learning 0.181s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0237
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 8.00s
                        Total time: 19012.11s
                               ETA: 855921.2s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.135s, learning 0.162s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0379
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 8.30s
                        Total time: 19020.41s
                               ETA: 855892.1s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.760s, learning 0.161s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0328
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 7.92s
                        Total time: 19028.33s
                               ETA: 855846.1s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.070s, learning 0.188s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0364
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 8.26s
                        Total time: 19036.59s
                               ETA: 855815.3s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.163s, learning 0.162s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0385
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 8.33s
                        Total time: 19044.91s
                               ETA: 855787.6s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.878s, learning 0.195s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0449
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 8.07s
                        Total time: 19052.99s
                               ETA: 855748.5s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.994s, learning 0.161s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0535
             Mean action noise std: 0.67
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 8.15s
                        Total time: 19061.14s
                               ETA: 855713.1s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.918s, learning 0.181s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0482
             Mean action noise std: 0.67
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 8.10s
                        Total time: 19069.24s
                               ETA: 855675.2s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.891s, learning 0.160s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0472
             Mean action noise std: 0.67
                       Mean reward: 22.26
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 8.05s
                        Total time: 19077.29s
                               ETA: 855635.3s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.780s, learning 0.165s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0492
             Mean action noise std: 0.67
                       Mean reward: 22.26
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 7.94s
                        Total time: 19085.23s
                               ETA: 855590.5s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.880s, learning 0.248s)
               Value function loss: 0.0431
                    Surrogate loss: -0.0375
             Mean action noise std: 0.67
                       Mean reward: 22.26
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 8.13s
                        Total time: 19093.36s
                               ETA: 855554.1s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.011s, learning 0.168s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0343
             Mean action noise std: 0.67
                       Mean reward: 22.26
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 8.18s
                        Total time: 19101.54s
                               ETA: 855519.9s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.274s, learning 0.173s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0387
             Mean action noise std: 0.67
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 8.45s
                        Total time: 19109.99s
                               ETA: 855497.8s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.795s, learning 0.163s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0325
             Mean action noise std: 0.67
                       Mean reward: 22.31
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 7.96s
                        Total time: 19117.95s
                               ETA: 855453.7s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.044s, learning 0.159s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0445
             Mean action noise std: 0.67
                       Mean reward: 22.31
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 8.20s
                        Total time: 19126.15s
                               ETA: 855420.7s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.881s, learning 0.166s)
               Value function loss: 4.1948
                    Surrogate loss: 0.0594
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 8.05s
                        Total time: 19134.20s
                               ETA: 855380.7s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.917s, learning 0.206s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0327
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 8.12s
                        Total time: 19142.32s
                               ETA: 855344.2s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.978s, learning 0.163s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0418
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 8.14s
                        Total time: 19150.46s
                               ETA: 855308.5s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.759s, learning 0.164s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0336
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 7.92s
                        Total time: 19158.38s
                               ETA: 855263.1s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.808s, learning 0.158s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0337
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 7.97s
                        Total time: 19166.35s
                               ETA: 855219.7s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.861s, learning 0.165s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0376
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 8.03s
                        Total time: 19174.38s
                               ETA: 855178.9s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.853s, learning 0.161s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0445
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 8.01s
                        Total time: 19182.39s
                               ETA: 855137.6s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.846s, learning 0.164s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0539
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 8.01s
                        Total time: 19190.40s
                               ETA: 855096.2s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.044s, learning 0.163s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0558
             Mean action noise std: 0.67
                       Mean reward: 22.53
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 8.21s
                        Total time: 19198.61s
                               ETA: 855063.7s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.923s, learning 0.215s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0554
             Mean action noise std: 0.67
                       Mean reward: 22.54
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 8.14s
                        Total time: 19206.75s
                               ETA: 855028.0s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.166s, learning 0.159s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0512
             Mean action noise std: 0.67
                       Mean reward: 22.54
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 8.33s
                        Total time: 19215.07s
                               ETA: 855000.7s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.890s, learning 0.183s)
               Value function loss: 0.0523
                    Surrogate loss: -0.0430
             Mean action noise std: 0.67
                       Mean reward: 22.44
               Mean episode length: 124.65
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 8.07s
                        Total time: 19223.15s
                               ETA: 854962.3s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.992s, learning 0.159s)
               Value function loss: 0.0513
                    Surrogate loss: -0.0385
             Mean action noise std: 0.67
                       Mean reward: 22.43
               Mean episode length: 124.65
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 8.15s
                        Total time: 19231.30s
                               ETA: 854927.3s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.968s, learning 0.155s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0490
             Mean action noise std: 0.67
                       Mean reward: 22.36
               Mean episode length: 124.65
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 8.12s
                        Total time: 19239.42s
                               ETA: 854891.0s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 2083 steps/s (collection: 7.699s, learning 0.166s)
               Value function loss: 0.0455
                    Surrogate loss: -0.0529
             Mean action noise std: 0.67
                       Mean reward: 22.29
               Mean episode length: 124.65
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 7.87s
                        Total time: 19247.28s
                               ETA: 854843.4s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.983s, learning 0.171s)
               Value function loss: 1.8126
                    Surrogate loss: 0.0539
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 8.15s
                        Total time: 19255.44s
                               ETA: 854808.6s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.288s, learning 0.180s)
               Value function loss: 0.5450
                    Surrogate loss: -0.0032
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 8.47s
                        Total time: 19263.91s
                               ETA: 854787.7s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.766s, learning 0.261s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0266
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 8.03s
                        Total time: 19271.93s
                               ETA: 854747.4s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.842s, learning 0.240s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0303
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 8.08s
                        Total time: 19280.02s
                               ETA: 854709.5s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.062s, learning 0.165s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0288
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 8.23s
                        Total time: 19288.24s
                               ETA: 854678.0s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.049s, learning 0.243s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0291
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 8.29s
                        Total time: 19296.53s
                               ETA: 854649.5s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.816s, learning 0.168s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0405
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 7.98s
                        Total time: 19304.52s
                               ETA: 854607.3s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.782s, learning 0.190s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0434
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 7.97s
                        Total time: 19312.49s
                               ETA: 854564.6s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.957s, learning 0.189s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0508
             Mean action noise std: 0.67
                       Mean reward: 19.91
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 8.15s
                        Total time: 19320.64s
                               ETA: 854529.7s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.850s, learning 0.167s)
               Value function loss: 0.0471
                    Surrogate loss: -0.0522
             Mean action noise std: 0.67
                       Mean reward: 19.95
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 8.02s
                        Total time: 19328.66s
                               ETA: 854489.1s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.988s, learning 0.169s)
               Value function loss: 0.0569
                    Surrogate loss: -0.0518
             Mean action noise std: 0.67
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 8.16s
                        Total time: 19336.81s
                               ETA: 854454.7s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.910s, learning 0.200s)
               Value function loss: 0.0653
                    Surrogate loss: -0.0477
             Mean action noise std: 0.67
                       Mean reward: 19.96
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 8.11s
                        Total time: 19344.92s
                               ETA: 854418.2s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.312s, learning 0.192s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0460
             Mean action noise std: 0.67
                       Mean reward: 19.86
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 8.50s
                        Total time: 19353.43s
                               ETA: 854399.2s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.055s, learning 0.279s)
               Value function loss: 0.0499
                    Surrogate loss: -0.0494
             Mean action noise std: 0.67
                       Mean reward: 19.88
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 8.33s
                        Total time: 19361.76s
                               ETA: 854372.6s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.197s, learning 0.164s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0509
             Mean action noise std: 0.67
                       Mean reward: 19.87
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 8.36s
                        Total time: 19370.12s
                               ETA: 854347.2s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.914s, learning 0.172s)
               Value function loss: 0.0439
                    Surrogate loss: -0.0511
             Mean action noise std: 0.67
                       Mean reward: 19.86
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 8.09s
                        Total time: 19378.21s
                               ETA: 854309.8s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.901s, learning 0.170s)
               Value function loss: 3.2866
                    Surrogate loss: 0.1405
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 8.07s
                        Total time: 19386.28s
                               ETA: 854271.7s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.853s, learning 0.211s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0163
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 8.06s
                        Total time: 19394.34s
                               ETA: 854233.4s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.932s, learning 0.160s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0306
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 8.09s
                        Total time: 19402.43s
                               ETA: 854196.3s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.832s, learning 0.202s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0263
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 8.03s
                        Total time: 19410.47s
                               ETA: 854156.7s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.919s, learning 0.197s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0317
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 8.12s
                        Total time: 19418.58s
                               ETA: 854120.6s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.212s, learning 0.158s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0325
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 8.37s
                        Total time: 19426.95s
                               ETA: 854095.9s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.938s, learning 0.162s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0384
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 8.10s
                        Total time: 19435.05s
                               ETA: 854059.2s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.783s, learning 0.158s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0417
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 7.94s
                        Total time: 19442.99s
                               ETA: 854015.6s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.813s, learning 0.185s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0503
             Mean action noise std: 0.67
                       Mean reward: 19.25
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 8.00s
                        Total time: 19450.99s
                               ETA: 853974.6s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.921s, learning 0.174s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0549
             Mean action noise std: 0.67
                       Mean reward: 19.30
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 8.09s
                        Total time: 19459.09s
                               ETA: 853937.8s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.997s, learning 0.165s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0552
             Mean action noise std: 0.67
                       Mean reward: 19.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 8.16s
                        Total time: 19467.25s
                               ETA: 853903.9s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.034s, learning 0.170s)
               Value function loss: 0.0782
                    Surrogate loss: -0.0462
             Mean action noise std: 0.67
                       Mean reward: 19.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 8.20s
                        Total time: 19475.45s
                               ETA: 853872.0s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.327s, learning 0.168s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0506
             Mean action noise std: 0.67
                       Mean reward: 19.24
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 8.50s
                        Total time: 19483.95s
                               ETA: 853852.8s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.147s, learning 0.165s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0530
             Mean action noise std: 0.67
                       Mean reward: 19.21
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 8.31s
                        Total time: 19492.26s
                               ETA: 853825.6s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.716s, learning 0.165s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0516
             Mean action noise std: 0.67
                       Mean reward: 19.20
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 7.88s
                        Total time: 19500.14s
                               ETA: 853779.6s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.878s, learning 0.259s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0616
             Mean action noise std: 0.67
                       Mean reward: 19.20
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 8.14s
                        Total time: 19508.28s
                               ETA: 853744.8s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.085s, learning 0.168s)
               Value function loss: 2.3192
                    Surrogate loss: 0.0088
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 8.25s
                        Total time: 19516.53s
                               ETA: 853715.1s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.079s, learning 0.186s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0335
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 8.26s
                        Total time: 19524.80s
                               ETA: 853686.0s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.712s, learning 0.170s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0380
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 7.88s
                        Total time: 19532.68s
                               ETA: 853640.1s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.814s, learning 0.161s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0302
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 7.98s
                        Total time: 19540.65s
                               ETA: 853598.3s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 2108 steps/s (collection: 7.612s, learning 0.160s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0351
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 7.77s
                        Total time: 19548.43s
                               ETA: 853547.7s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 2108 steps/s (collection: 7.610s, learning 0.160s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0355
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 7.77s
                        Total time: 19556.20s
                               ETA: 853497.0s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 2106 steps/s (collection: 7.615s, learning 0.161s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0393
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 7.78s
                        Total time: 19563.97s
                               ETA: 853446.6s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.939s, learning 0.162s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0470
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 8.10s
                        Total time: 19572.07s
                               ETA: 853410.5s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.823s, learning 0.158s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0463
             Mean action noise std: 0.67
                       Mean reward: 19.10
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 7.98s
                        Total time: 19580.05s
                               ETA: 853369.1s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.970s, learning 0.159s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0558
             Mean action noise std: 0.67
                       Mean reward: 19.05
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 8.13s
                        Total time: 19588.18s
                               ETA: 853334.2s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.021s, learning 0.160s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0572
             Mean action noise std: 0.67
                       Mean reward: 19.05
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 8.18s
                        Total time: 19596.36s
                               ETA: 853301.6s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.948s, learning 0.196s)
               Value function loss: 0.0480
                    Surrogate loss: -0.0516
             Mean action noise std: 0.67
                       Mean reward: 19.06
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 8.14s
                        Total time: 19604.51s
                               ETA: 853267.4s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.979s, learning 0.161s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0553
             Mean action noise std: 0.67
                       Mean reward: 19.01
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 8.14s
                        Total time: 19612.65s
                               ETA: 853233.0s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.938s, learning 0.195s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0525
             Mean action noise std: 0.67
                       Mean reward: 18.96
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 8.13s
                        Total time: 19620.78s
                               ETA: 853198.4s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.935s, learning 0.205s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0504
             Mean action noise std: 0.67
                       Mean reward: 18.89
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 8.14s
                        Total time: 19628.92s
                               ETA: 853164.1s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.184s, learning 0.186s)
               Value function loss: 2.3198
                    Surrogate loss: 0.0279
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 8.37s
                        Total time: 19637.29s
                               ETA: 853139.8s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.036s, learning 0.213s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0273
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 8.25s
                        Total time: 19645.54s
                               ETA: 853110.3s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.186s, learning 0.157s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0261
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 8.34s
                        Total time: 19653.88s
                               ETA: 853084.9s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.766s, learning 0.158s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0290
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 7.92s
                        Total time: 19661.81s
                               ETA: 853041.4s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.007s, learning 0.159s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0358
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 8.17s
                        Total time: 19669.97s
                               ETA: 853008.3s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.045s, learning 0.193s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0389
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 8.24s
                        Total time: 19678.21s
                               ETA: 852978.4s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.752s, learning 0.239s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0486
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 7.99s
                        Total time: 19686.20s
                               ETA: 852937.8s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.810s, learning 0.179s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0458
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 7.99s
                        Total time: 19694.19s
                               ETA: 852897.2s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.799s, learning 0.157s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0454
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 7.96s
                        Total time: 19702.15s
                               ETA: 852855.1s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.918s, learning 0.156s)
               Value function loss: 0.0419
                    Surrogate loss: -0.0507
             Mean action noise std: 0.67
                       Mean reward: 18.45
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 8.07s
                        Total time: 19710.22s
                               ETA: 852818.2s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.077s, learning 0.159s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0480
             Mean action noise std: 0.67
                       Mean reward: 18.43
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 8.24s
                        Total time: 19718.46s
                               ETA: 852788.3s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.991s, learning 0.159s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0440
             Mean action noise std: 0.67
                       Mean reward: 18.43
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 8.15s
                        Total time: 19726.61s
                               ETA: 852754.7s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.975s, learning 0.191s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0504
             Mean action noise std: 0.67
                       Mean reward: 18.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 8.17s
                        Total time: 19734.77s
                               ETA: 852721.9s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.115s, learning 0.163s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0346
             Mean action noise std: 0.67
                       Mean reward: 18.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 8.28s
                        Total time: 19743.05s
                               ETA: 852693.9s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.965s, learning 0.160s)
               Value function loss: 0.0425
                    Surrogate loss: -0.0437
             Mean action noise std: 0.67
                       Mean reward: 18.48
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 8.13s
                        Total time: 19751.17s
                               ETA: 852659.3s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.731s, learning 0.159s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0528
             Mean action noise std: 0.67
                       Mean reward: 18.48
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 7.89s
                        Total time: 19759.07s
                               ETA: 852614.6s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.898s, learning 0.157s)
               Value function loss: 3.0551
                    Surrogate loss: 0.0762
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 8.06s
                        Total time: 19767.12s
                               ETA: 852577.0s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.962s, learning 0.158s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0299
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 8.12s
                        Total time: 19775.24s
                               ETA: 852542.3s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.778s, learning 0.188s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0315
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 7.97s
                        Total time: 19783.21s
                               ETA: 852500.9s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.233s, learning 0.282s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0324
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 8.51s
                        Total time: 19791.72s
                               ETA: 852483.2s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 2108 steps/s (collection: 7.590s, learning 0.182s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0352
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 7.77s
                        Total time: 19799.49s
                               ETA: 852433.6s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.678s, learning 0.180s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0436
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 7.86s
                        Total time: 19807.35s
                               ETA: 852387.7s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.199s, learning 0.213s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0437
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 8.41s
                        Total time: 19815.76s
                               ETA: 852365.6s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.981s, learning 0.188s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0509
             Mean action noise std: 0.67
                       Mean reward: 19.18
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 8.17s
                        Total time: 19823.93s
                               ETA: 852333.1s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.872s, learning 0.200s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0474
             Mean action noise std: 0.67
                       Mean reward: 19.13
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 8.07s
                        Total time: 19832.00s
                               ETA: 852296.5s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.919s, learning 0.160s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0439
             Mean action noise std: 0.67
                       Mean reward: 19.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 8.08s
                        Total time: 19840.08s
                               ETA: 852260.2s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 2128 steps/s (collection: 7.535s, learning 0.161s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0583
             Mean action noise std: 0.67
                       Mean reward: 19.11
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 7.70s
                        Total time: 19847.78s
                               ETA: 852207.5s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.858s, learning 0.285s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0400
             Mean action noise std: 0.67
                       Mean reward: 19.11
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 8.14s
                        Total time: 19855.92s
                               ETA: 852174.0s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.947s, learning 0.158s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0518
             Mean action noise std: 0.67
                       Mean reward: 19.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 8.11s
                        Total time: 19864.03s
                               ETA: 852138.9s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.978s, learning 0.160s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0497
             Mean action noise std: 0.67
                       Mean reward: 19.06
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 8.14s
                        Total time: 19872.17s
                               ETA: 852105.2s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.782s, learning 0.233s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0525
             Mean action noise std: 0.67
                       Mean reward: 19.05
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 8.01s
                        Total time: 19880.18s
                               ETA: 852066.3s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.274s, learning 0.158s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0540
             Mean action noise std: 0.67
                       Mean reward: 19.05
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 8.43s
                        Total time: 19888.61s
                               ETA: 852045.3s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.125s, learning 0.157s)
               Value function loss: 1.7389
                    Surrogate loss: 0.0470
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 8.28s
                        Total time: 19896.89s
                               ETA: 852017.8s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.000s, learning 0.163s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0311
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 8.16s
                        Total time: 19905.06s
                               ETA: 851985.3s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.969s, learning 0.250s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0298
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 8.22s
                        Total time: 19913.28s
                               ETA: 851955.2s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.981s, learning 0.161s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0320
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 8.14s
                        Total time: 19921.42s
                               ETA: 851921.8s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.938s, learning 0.171s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0397
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 8.11s
                        Total time: 19929.53s
                               ETA: 851887.1s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.838s, learning 0.158s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0478
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 8.00s
                        Total time: 19937.52s
                               ETA: 851847.5s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.073s, learning 0.247s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0474
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 8.32s
                        Total time: 19945.84s
                               ETA: 851821.8s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.767s, learning 0.258s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0534
             Mean action noise std: 0.67
                       Mean reward: 19.55
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 8.03s
                        Total time: 19953.87s
                               ETA: 851783.5s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.819s, learning 0.160s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0460
             Mean action noise std: 0.67
                       Mean reward: 19.57
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 7.98s
                        Total time: 19961.85s
                               ETA: 851743.3s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.094s, learning 0.179s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0479
             Mean action noise std: 0.67
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 8.27s
                        Total time: 19970.12s
                               ETA: 851715.6s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.665s, learning 0.232s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0385
             Mean action noise std: 0.67
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 7.90s
                        Total time: 19978.02s
                               ETA: 851672.0s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.955s, learning 0.160s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0414
             Mean action noise std: 0.67
                       Mean reward: 19.56
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 8.12s
                        Total time: 19986.13s
                               ETA: 851637.7s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.868s, learning 0.176s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0401
             Mean action noise std: 0.67
                       Mean reward: 19.61
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 8.04s
                        Total time: 19994.18s
                               ETA: 851600.3s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.093s, learning 0.239s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0543
             Mean action noise std: 0.67
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 8.33s
                        Total time: 20002.51s
                               ETA: 851575.2s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.002s, learning 0.171s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0472
             Mean action noise std: 0.67
                       Mean reward: 19.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 8.17s
                        Total time: 20010.68s
                               ETA: 851543.4s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.085s, learning 0.157s)
               Value function loss: 6.1107
                    Surrogate loss: 0.0417
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 8.24s
                        Total time: 20018.92s
                               ETA: 851514.5s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.796s, learning 0.264s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0291
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 8.06s
                        Total time: 20026.98s
                               ETA: 851478.0s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.125s, learning 0.269s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0318
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 8.39s
                        Total time: 20035.38s
                               ETA: 851455.6s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.891s, learning 0.159s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0333
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 8.05s
                        Total time: 20043.43s
                               ETA: 851418.7s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.003s, learning 0.271s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0331
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 8.27s
                        Total time: 20051.70s
                               ETA: 851391.3s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.821s, learning 0.160s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0310
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 7.98s
                        Total time: 20059.68s
                               ETA: 851351.4s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.912s, learning 0.161s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0484
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 8.07s
                        Total time: 20067.76s
                               ETA: 851315.5s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.931s, learning 0.163s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0524
             Mean action noise std: 0.67
                       Mean reward: 20.80
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 8.09s
                        Total time: 20075.85s
                               ETA: 851280.5s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.801s, learning 0.191s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0478
             Mean action noise std: 0.67
                       Mean reward: 20.82
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 7.99s
                        Total time: 20083.84s
                               ETA: 851241.2s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.935s, learning 0.155s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0485
             Mean action noise std: 0.67
                       Mean reward: 20.84
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 8.09s
                        Total time: 20091.93s
                               ETA: 851206.1s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.991s, learning 0.165s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0515
             Mean action noise std: 0.67
                       Mean reward: 20.84
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 8.16s
                        Total time: 20100.09s
                               ETA: 851173.8s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 2136 steps/s (collection: 7.508s, learning 0.162s)
               Value function loss: 0.0474
                    Surrogate loss: -0.0364
             Mean action noise std: 0.67
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 7.67s
                        Total time: 20107.76s
                               ETA: 851120.9s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.924s, learning 0.161s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0425
             Mean action noise std: 0.67
                       Mean reward: 20.90
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 8.08s
                        Total time: 20115.84s
                               ETA: 851085.7s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.749s, learning 0.161s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0360
             Mean action noise std: 0.67
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 7.91s
                        Total time: 20123.75s
                               ETA: 851043.0s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.854s, learning 0.157s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0419
             Mean action noise std: 0.67
                       Mean reward: 20.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 8.01s
                        Total time: 20131.76s
                               ETA: 851004.7s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.917s, learning 0.210s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0487
             Mean action noise std: 0.67
                       Mean reward: 20.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 8.13s
                        Total time: 20139.89s
                               ETA: 850971.2s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.334s, learning 0.278s)
               Value function loss: 4.2130
                    Surrogate loss: 0.1009
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 8.61s
                        Total time: 20148.50s
                               ETA: 850958.3s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.093s, learning 0.161s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0339
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 8.25s
                        Total time: 20156.75s
                               ETA: 850930.3s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.812s, learning 0.165s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0356
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 7.98s
                        Total time: 20164.73s
                               ETA: 850890.7s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.016s, learning 0.209s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0335
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 8.22s
                        Total time: 20172.96s
                               ETA: 850861.5s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.737s, learning 0.220s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0248
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 7.96s
                        Total time: 20180.91s
                               ETA: 850821.0s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.876s, learning 0.178s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0386
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 8.05s
                        Total time: 20188.97s
                               ETA: 850784.7s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.672s, learning 0.185s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0476
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 7.86s
                        Total time: 20196.82s
                               ETA: 850740.0s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.938s, learning 0.157s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0513
             Mean action noise std: 0.67
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 8.10s
                        Total time: 20204.92s
                               ETA: 850705.5s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 2123 steps/s (collection: 7.511s, learning 0.206s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0488
             Mean action noise std: 0.67
                       Mean reward: 21.19
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 7.72s
                        Total time: 20212.64s
                               ETA: 850655.0s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.856s, learning 0.182s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0570
             Mean action noise std: 0.67
                       Mean reward: 21.16
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 8.04s
                        Total time: 20220.67s
                               ETA: 850618.1s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.946s, learning 0.160s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0475
             Mean action noise std: 0.66
                       Mean reward: 21.16
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 8.11s
                        Total time: 20228.78s
                               ETA: 850584.1s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.728s, learning 0.254s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0455
             Mean action noise std: 0.66
                       Mean reward: 21.19
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 7.98s
                        Total time: 20236.76s
                               ETA: 850544.9s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.071s, learning 0.158s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0473
             Mean action noise std: 0.66
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 8.23s
                        Total time: 20244.99s
                               ETA: 850516.0s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.720s, learning 0.174s)
               Value function loss: 0.0317
                    Surrogate loss: -0.0465
             Mean action noise std: 0.66
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 7.89s
                        Total time: 20252.89s
                               ETA: 850473.2s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.066s, learning 0.157s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0427
             Mean action noise std: 0.66
                       Mean reward: 21.16
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 8.22s
                        Total time: 20261.11s
                               ETA: 850444.1s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.826s, learning 0.262s)
               Value function loss: 5.0381
                    Surrogate loss: 0.0620
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 8.09s
                        Total time: 20269.20s
                               ETA: 850409.4s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.240s, learning 0.195s)
               Value function loss: 0.2881
                    Surrogate loss: -0.0116
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 8.43s
                        Total time: 20277.63s
                               ETA: 850389.3s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.854s, learning 0.190s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0327
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 8.04s
                        Total time: 20285.67s
                               ETA: 850352.8s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.109s, learning 0.217s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0325
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 8.33s
                        Total time: 20294.00s
                               ETA: 850328.2s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.782s, learning 0.161s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0248
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 7.94s
                        Total time: 20301.94s
                               ETA: 850287.5s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 2105 steps/s (collection: 7.562s, learning 0.218s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0272
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 7.78s
                        Total time: 20309.72s
                               ETA: 850240.1s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 2133 steps/s (collection: 7.518s, learning 0.160s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0425
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 7.68s
                        Total time: 20317.40s
                               ETA: 850188.4s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.244s, learning 0.184s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0493
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 8.43s
                        Total time: 20325.83s
                               ETA: 850168.1s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.781s, learning 0.159s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0510
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 7.94s
                        Total time: 20333.77s
                               ETA: 850127.4s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.903s, learning 0.158s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0501
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 8.06s
                        Total time: 20341.83s
                               ETA: 850091.8s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.934s, learning 0.194s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0491
             Mean action noise std: 0.66
                       Mean reward: 21.32
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 8.13s
                        Total time: 20349.96s
                               ETA: 850059.0s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.161s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0444
             Mean action noise std: 0.66
                       Mean reward: 21.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 8.04s
                        Total time: 20358.00s
                               ETA: 850022.6s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.911s, learning 0.157s)
               Value function loss: 0.0473
                    Surrogate loss: -0.0184
             Mean action noise std: 0.66
                       Mean reward: 21.32
               Mean episode length: 124.70
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 8.07s
                        Total time: 20366.06s
                               ETA: 849987.3s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.721s, learning 0.178s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0351
             Mean action noise std: 0.66
                       Mean reward: 21.35
               Mean episode length: 124.70
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 7.90s
                        Total time: 20373.96s
                               ETA: 849945.0s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.875s, learning 0.156s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0413
             Mean action noise std: 0.66
                       Mean reward: 21.36
               Mean episode length: 124.70
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 8.03s
                        Total time: 20381.99s
                               ETA: 849908.3s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.950s, learning 0.216s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0534
             Mean action noise std: 0.66
                       Mean reward: 21.36
               Mean episode length: 124.70
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 8.17s
                        Total time: 20390.16s
                               ETA: 849877.2s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.990s, learning 0.213s)
               Value function loss: 6.8484
                    Surrogate loss: 0.0838
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 12.20s
                        Total time: 20402.36s
                               ETA: 850014.4s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1063 steps/s (collection: 15.184s, learning 0.219s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0296
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 15.40s
                        Total time: 20417.77s
                               ETA: 850284.6s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.691s, learning 0.162s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0295
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 15.85s
                        Total time: 20433.62s
                               ETA: 850573.4s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1058 steps/s (collection: 15.303s, learning 0.182s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0318
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 15.48s
                        Total time: 20449.10s
                               ETA: 850846.6s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.395s, learning 0.198s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0285
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 15.59s
                        Total time: 20464.70s
                               ETA: 851124.0s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1039 steps/s (collection: 15.574s, learning 0.187s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0367
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 15.76s
                        Total time: 20480.46s
                               ETA: 851408.2s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1056 steps/s (collection: 15.341s, learning 0.168s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0394
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 15.51s
                        Total time: 20495.97s
                               ETA: 851681.6s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1056 steps/s (collection: 15.356s, learning 0.157s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0499
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 15.51s
                        Total time: 20511.48s
                               ETA: 851955.0s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1040 steps/s (collection: 15.582s, learning 0.160s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0422
             Mean action noise std: 0.66
                       Mean reward: 21.53
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 15.74s
                        Total time: 20527.22s
                               ETA: 852237.6s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.343s, learning 0.180s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0473
             Mean action noise std: 0.66
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 15.52s
                        Total time: 20542.75s
                               ETA: 852510.9s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1058 steps/s (collection: 15.324s, learning 0.161s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0475
             Mean action noise std: 0.66
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 15.49s
                        Total time: 20558.23s
                               ETA: 852782.3s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1052 steps/s (collection: 15.407s, learning 0.161s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0412
             Mean action noise std: 0.66
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 15.57s
                        Total time: 20573.80s
                               ETA: 853057.0s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.391s, learning 0.331s)
               Value function loss: 0.0352
                    Surrogate loss: -0.0392
             Mean action noise std: 0.66
                       Mean reward: 21.52
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 15.72s
                        Total time: 20589.52s
                               ETA: 853337.8s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.463s, learning 0.237s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0425
             Mean action noise std: 0.66
                       Mean reward: 21.55
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 15.70s
                        Total time: 20605.22s
                               ETA: 853617.4s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.388s, learning 0.163s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0414
             Mean action noise std: 0.66
                       Mean reward: 21.54
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 15.55s
                        Total time: 20620.77s
                               ETA: 853890.6s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1063 steps/s (collection: 15.245s, learning 0.160s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0542
             Mean action noise std: 0.66
                       Mean reward: 21.54
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 15.40s
                        Total time: 20636.18s
                               ETA: 854157.5s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.620s, learning 0.156s)
               Value function loss: 3.3132
                    Surrogate loss: 0.0350
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 15.78s
                        Total time: 20651.95s
                               ETA: 854439.5s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.407s, learning 0.185s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0345
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 15.59s
                        Total time: 20667.54s
                               ETA: 854713.7s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.509s, learning 0.187s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0276
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 15.70s
                        Total time: 20683.24s
                               ETA: 854991.9s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1086 steps/s (collection: 14.921s, learning 0.160s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0279
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 15.08s
                        Total time: 20698.32s
                               ETA: 855244.5s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.417s, learning 0.186s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0303
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 15.60s
                        Total time: 20713.92s
                               ETA: 855518.4s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1070 steps/s (collection: 15.147s, learning 0.160s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0371
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 15.31s
                        Total time: 20729.23s
                               ETA: 855779.8s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1056 steps/s (collection: 15.294s, learning 0.214s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0470
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 15.51s
                        Total time: 20744.74s
                               ETA: 856049.3s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1040 steps/s (collection: 15.584s, learning 0.163s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0482
             Mean action noise std: 0.66
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 15.75s
                        Total time: 20760.49s
                               ETA: 856328.4s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.481s, learning 0.218s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0408
             Mean action noise std: 0.66
                       Mean reward: 21.40
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 15.70s
                        Total time: 20776.19s
                               ETA: 856605.3s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1065 steps/s (collection: 15.211s, learning 0.161s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0530
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 15.37s
                        Total time: 20791.56s
                               ETA: 856868.5s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1071 steps/s (collection: 15.138s, learning 0.155s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0462
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 15.29s
                        Total time: 20806.85s
                               ETA: 857128.1s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1056 steps/s (collection: 15.301s, learning 0.212s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0387
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 15.51s
                        Total time: 20822.36s
                               ETA: 857396.6s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1071 steps/s (collection: 15.105s, learning 0.192s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0366
             Mean action noise std: 0.66
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 15.30s
                        Total time: 20837.66s
                               ETA: 857656.0s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.329s, learning 0.194s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0454
             Mean action noise std: 0.66
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 15.52s
                        Total time: 20853.18s
                               ETA: 857924.4s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1056 steps/s (collection: 15.336s, learning 0.165s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0388
             Mean action noise std: 0.66
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 15.50s
                        Total time: 20868.69s
                               ETA: 858191.7s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.979s, learning 0.211s)
               Value function loss: 6.2119
                    Surrogate loss: 0.0594
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 16.19s
                        Total time: 20884.88s
                               ETA: 858487.1s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.526s, learning 0.163s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0219
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 15.69s
                        Total time: 20900.56s
                               ETA: 858761.6s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.355s, learning 0.304s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0283
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 15.66s
                        Total time: 20916.22s
                               ETA: 859034.7s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.505s, learning 0.169s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0245
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 15.67s
                        Total time: 20931.90s
                               ETA: 859308.1s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1051 steps/s (collection: 15.421s, learning 0.160s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0157
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 15.58s
                        Total time: 20947.48s
                               ETA: 859577.4s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1070 steps/s (collection: 15.143s, learning 0.163s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0280
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 15.31s
                        Total time: 20962.78s
                               ETA: 859835.3s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.752s, learning 0.156s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0479
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 15.91s
                        Total time: 20978.69s
                               ETA: 860117.6s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.940s, learning 0.178s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0481
             Mean action noise std: 0.66
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 9.12s
                        Total time: 20987.81s
                               ETA: 860121.4s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.024s, learning 0.155s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0438
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 8.18s
                        Total time: 20995.99s
                               ETA: 860086.7s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.236s, learning 0.157s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0461
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 8.39s
                        Total time: 21004.38s
                               ETA: 860060.8s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.855s, learning 0.197s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0479
             Mean action noise std: 0.66
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 8.05s
                        Total time: 21012.43s
                               ETA: 860020.9s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.809s, learning 0.163s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0335
             Mean action noise std: 0.66
                       Mean reward: 21.57
               Mean episode length: 124.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 7.97s
                        Total time: 21020.41s
                               ETA: 859977.8s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.013s, learning 0.233s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0348
             Mean action noise std: 0.66
                       Mean reward: 21.50
               Mean episode length: 124.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 8.25s
                        Total time: 21028.65s
                               ETA: 859945.9s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.191s, learning 0.161s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0428
             Mean action noise std: 0.66
                       Mean reward: 21.55
               Mean episode length: 124.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 8.35s
                        Total time: 21037.00s
                               ETA: 859918.4s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.053s, learning 0.161s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0300
             Mean action noise std: 0.66
                       Mean reward: 21.52
               Mean episode length: 124.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 8.21s
                        Total time: 21045.22s
                               ETA: 859885.3s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 2112 steps/s (collection: 7.590s, learning 0.166s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0502
             Mean action noise std: 0.66
                       Mean reward: 21.52
               Mean episode length: 124.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 7.76s
                        Total time: 21052.97s
                               ETA: 859833.4s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.901s, learning 0.192s)
               Value function loss: 6.9372
                    Surrogate loss: 0.0481
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 8.09s
                        Total time: 21061.07s
                               ETA: 859795.4s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.945s, learning 0.191s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0291
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 8.14s
                        Total time: 21069.20s
                               ETA: 859759.1s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.718s, learning 0.165s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0318
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 7.88s
                        Total time: 21077.09s
                               ETA: 859712.6s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.808s, learning 0.158s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0283
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 7.97s
                        Total time: 21085.05s
                               ETA: 859669.4s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.823s, learning 0.262s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0232
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 8.09s
                        Total time: 21093.14s
                               ETA: 859631.2s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.751s, learning 0.185s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0368
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 7.94s
                        Total time: 21101.07s
                               ETA: 859586.9s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.251s, learning 0.182s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0433
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 8.43s
                        Total time: 21109.51s
                               ETA: 859562.9s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 2104 steps/s (collection: 7.628s, learning 0.158s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0478
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 7.79s
                        Total time: 21117.29s
                               ETA: 859512.5s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.815s, learning 0.161s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0326
             Mean action noise std: 0.66
                       Mean reward: 21.63
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 7.98s
                        Total time: 21125.27s
                               ETA: 859469.9s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.058s, learning 0.160s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0391
             Mean action noise std: 0.66
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 8.22s
                        Total time: 21133.49s
                               ETA: 859437.2s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 2100 steps/s (collection: 7.629s, learning 0.169s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0458
             Mean action noise std: 0.66
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 7.80s
                        Total time: 21141.28s
                               ETA: 859387.4s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.074s, learning 0.157s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0254
             Mean action noise std: 0.66
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 8.23s
                        Total time: 21149.51s
                               ETA: 859355.3s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.004s, learning 0.169s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0372
             Mean action noise std: 0.66
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 8.17s
                        Total time: 21157.69s
                               ETA: 859320.8s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.931s, learning 0.189s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0348
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 8.12s
                        Total time: 21165.81s
                               ETA: 859284.2s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.996s, learning 0.163s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0342
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 8.16s
                        Total time: 21173.97s
                               ETA: 859249.2s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.700s, learning 0.247s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0479
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 7.95s
                        Total time: 21181.91s
                               ETA: 859205.6s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.017s, learning 0.204s)
               Value function loss: 2.4247
                    Surrogate loss: 0.0737
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 8.22s
                        Total time: 21190.13s
                               ETA: 859173.2s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.870s, learning 0.198s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0333
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 8.07s
                        Total time: 21198.20s
                               ETA: 859134.6s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 2104 steps/s (collection: 7.620s, learning 0.164s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0322
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 7.78s
                        Total time: 21205.99s
                               ETA: 859084.5s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 2094 steps/s (collection: 7.656s, learning 0.166s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0229
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 7.82s
                        Total time: 21213.81s
                               ETA: 859036.0s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.125s, learning 0.168s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0326
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 8.29s
                        Total time: 21222.10s
                               ETA: 859006.6s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.973s, learning 0.163s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0429
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 8.14s
                        Total time: 21230.24s
                               ETA: 858970.8s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.759s, learning 0.169s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0453
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 7.93s
                        Total time: 21238.17s
                               ETA: 858926.7s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.090s, learning 0.194s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0468
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 8.28s
                        Total time: 21246.45s
                               ETA: 858897.0s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.783s, learning 0.171s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0335
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 7.95s
                        Total time: 21254.41s
                               ETA: 858854.0s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.801s, learning 0.164s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0485
             Mean action noise std: 0.66
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 7.97s
                        Total time: 21262.37s
                               ETA: 858811.4s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.765s, learning 0.178s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0500
             Mean action noise std: 0.66
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 7.94s
                        Total time: 21270.31s
                               ETA: 858768.0s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.147s, learning 0.162s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0400
             Mean action noise std: 0.66
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 8.31s
                        Total time: 21278.62s
                               ETA: 858739.4s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.904s, learning 0.195s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0449
             Mean action noise std: 0.66
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 8.10s
                        Total time: 21286.72s
                               ETA: 858702.3s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.861s, learning 0.161s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0516
             Mean action noise std: 0.66
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 8.02s
                        Total time: 21294.74s
                               ETA: 858662.2s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.172s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0487
             Mean action noise std: 0.66
                       Mean reward: 21.40
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 8.58s
                        Total time: 21303.33s
                               ETA: 858644.7s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.488s, learning 0.159s)
               Value function loss: 5.0453
                    Surrogate loss: 0.0293
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 8.65s
                        Total time: 21311.98s
                               ETA: 858629.8s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.704s, learning 0.270s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0254
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 7.97s
                        Total time: 21319.95s
                               ETA: 858587.7s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.984s, learning 0.197s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0296
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 8.18s
                        Total time: 21328.13s
                               ETA: 858554.1s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.837s, learning 0.195s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0278
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 8.03s
                        Total time: 21336.16s
                               ETA: 858514.4s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.694s, learning 0.158s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0197
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 7.85s
                        Total time: 21344.02s
                               ETA: 858467.6s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.163s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0307
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 8.04s
                        Total time: 21352.06s
                               ETA: 858428.3s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.188s, learning 0.171s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0453
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 8.36s
                        Total time: 21360.42s
                               ETA: 858401.9s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.850s, learning 0.164s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0452
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 8.01s
                        Total time: 21368.43s
                               ETA: 858361.6s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.024s, learning 0.161s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0438
             Mean action noise std: 0.66
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 8.18s
                        Total time: 21376.61s
                               ETA: 858328.2s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.073s, learning 0.220s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0456
             Mean action noise std: 0.66
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 8.29s
                        Total time: 21384.91s
                               ETA: 858299.2s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.100s, learning 0.168s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0529
             Mean action noise std: 0.66
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 8.27s
                        Total time: 21393.17s
                               ETA: 858269.2s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.878s, learning 0.175s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0406
             Mean action noise std: 0.66
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 8.05s
                        Total time: 21401.23s
                               ETA: 858230.6s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.117s, learning 0.168s)
               Value function loss: 0.0429
                    Surrogate loss: -0.0399
             Mean action noise std: 0.66
                       Mean reward: 21.69
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 8.28s
                        Total time: 21409.51s
                               ETA: 858201.3s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.781s, learning 0.174s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0394
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 7.95s
                        Total time: 21417.47s
                               ETA: 858158.8s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.333s, learning 0.293s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0384
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 8.63s
                        Total time: 21426.09s
                               ETA: 858143.2s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.095s, learning 0.197s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0521
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 8.29s
                        Total time: 21434.39s
                               ETA: 858114.3s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.162s)
               Value function loss: 5.3133
                    Surrogate loss: 0.0649
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 8.13s
                        Total time: 21442.52s
                               ETA: 858079.0s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.022s, learning 0.167s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0313
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 8.19s
                        Total time: 21450.71s
                               ETA: 858045.9s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.901s, learning 0.168s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0320
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 8.07s
                        Total time: 21458.78s
                               ETA: 858008.1s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.800s, learning 0.323s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0206
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 8.12s
                        Total time: 21466.90s
                               ETA: 857972.5s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.077s, learning 0.203s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0224
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 8.28s
                        Total time: 21475.18s
                               ETA: 857943.1s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.161s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0355
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 8.27s
                        Total time: 21483.45s
                               ETA: 857913.6s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.825s, learning 0.194s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0438
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 8.02s
                        Total time: 21491.47s
                               ETA: 857873.8s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.188s, learning 0.190s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0511
             Mean action noise std: 0.66
                       Mean reward: 21.83
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 8.38s
                        Total time: 21499.85s
                               ETA: 857848.5s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.260s, learning 0.190s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0389
             Mean action noise std: 0.66
                       Mean reward: 21.80
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 8.45s
                        Total time: 21508.30s
                               ETA: 857826.0s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.067s, learning 0.178s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0408
             Mean action noise std: 0.66
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 8.25s
                        Total time: 21516.55s
                               ETA: 857795.3s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.085s, learning 0.199s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0401
             Mean action noise std: 0.66
                       Mean reward: 21.77
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 8.28s
                        Total time: 21524.83s
                               ETA: 857766.3s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.093s, learning 0.169s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0285
             Mean action noise std: 0.66
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 8.26s
                        Total time: 21533.09s
                               ETA: 857736.3s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 2080 steps/s (collection: 7.710s, learning 0.166s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0388
             Mean action noise std: 0.66
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 7.88s
                        Total time: 21540.97s
                               ETA: 857691.1s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.997s, learning 0.162s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0436
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 8.16s
                        Total time: 21549.13s
                               ETA: 857657.0s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.856s, learning 0.162s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0383
             Mean action noise std: 0.66
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 8.02s
                        Total time: 21557.15s
                               ETA: 857617.5s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.092s, learning 0.184s)
               Value function loss: 7.0730
                    Surrogate loss: 0.0518
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 8.28s
                        Total time: 21565.42s
                               ETA: 857588.1s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.011s, learning 0.193s)
               Value function loss: 0.3861
                    Surrogate loss: -0.0105
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 8.20s
                        Total time: 21573.63s
                               ETA: 857556.0s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.739s, learning 0.178s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0261
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 7.92s
                        Total time: 21581.54s
                               ETA: 857512.5s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.063s, learning 0.167s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0305
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 8.23s
                        Total time: 21589.77s
                               ETA: 857481.4s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.850s, learning 0.160s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0188
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 8.01s
                        Total time: 21597.78s
                               ETA: 857441.7s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.789s, learning 0.283s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0230
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 8.07s
                        Total time: 21605.86s
                               ETA: 857404.4s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.864s, learning 0.200s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0447
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 8.06s
                        Total time: 21613.92s
                               ETA: 857366.8s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.924s, learning 0.158s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0486
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 8.08s
                        Total time: 21622.00s
                               ETA: 857330.0s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.694s, learning 0.165s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0432
             Mean action noise std: 0.66
                       Mean reward: 22.03
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 7.86s
                        Total time: 21629.86s
                               ETA: 857284.3s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.935s, learning 0.159s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0424
             Mean action noise std: 0.66
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 8.09s
                        Total time: 21637.95s
                               ETA: 857247.9s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.024s, learning 0.161s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0530
             Mean action noise std: 0.66
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 8.18s
                        Total time: 21646.14s
                               ETA: 857215.2s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.684s, learning 0.172s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0370
             Mean action noise std: 0.66
                       Mean reward: 21.90
               Mean episode length: 124.60
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 7.86s
                        Total time: 21654.00s
                               ETA: 857169.5s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.806s, learning 0.177s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0294
             Mean action noise std: 0.66
                       Mean reward: 21.88
               Mean episode length: 124.60
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 7.98s
                        Total time: 21661.98s
                               ETA: 857128.9s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.158s, learning 0.194s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0263
             Mean action noise std: 0.66
                       Mean reward: 21.84
               Mean episode length: 124.60
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 8.35s
                        Total time: 21670.33s
                               ETA: 857102.8s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.814s, learning 0.169s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0380
             Mean action noise std: 0.66
                       Mean reward: 21.84
               Mean episode length: 124.60
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 7.98s
                        Total time: 21678.31s
                               ETA: 857062.3s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.762s, learning 0.174s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0444
             Mean action noise std: 0.66
                       Mean reward: 21.84
               Mean episode length: 124.60
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 7.94s
                        Total time: 21686.25s
                               ETA: 857019.8s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.061s, learning 0.160s)
               Value function loss: 7.9597
                    Surrogate loss: 0.1398
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 8.22s
                        Total time: 21694.47s
                               ETA: 856988.7s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.945s, learning 0.158s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0211
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 8.10s
                        Total time: 21702.57s
                               ETA: 856952.9s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.883s, learning 0.197s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0309
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 8.08s
                        Total time: 21710.65s
                               ETA: 856916.2s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.793s, learning 0.169s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0258
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 7.96s
                        Total time: 21718.62s
                               ETA: 856874.9s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.702s, learning 0.171s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0276
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 7.87s
                        Total time: 21726.49s
                               ETA: 856830.1s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.424s, learning 0.162s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0387
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 8.59s
                        Total time: 21735.07s
                               ETA: 856813.5s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.092s, learning 0.166s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0445
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 8.26s
                        Total time: 21743.33s
                               ETA: 856783.9s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.843s, learning 0.218s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0453
             Mean action noise std: 0.66
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 8.06s
                        Total time: 21751.39s
                               ETA: 856746.6s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.088s, learning 0.268s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0416
             Mean action noise std: 0.66
                       Mean reward: 22.61
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 8.36s
                        Total time: 21759.75s
                               ETA: 856721.0s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.880s, learning 0.200s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0422
             Mean action noise std: 0.66
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 8.08s
                        Total time: 21767.83s
                               ETA: 856684.4s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 2077 steps/s (collection: 7.723s, learning 0.163s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0510
             Mean action noise std: 0.66
                       Mean reward: 22.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 7.89s
                        Total time: 21775.72s
                               ETA: 856640.3s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.795s, learning 0.167s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0317
             Mean action noise std: 0.66
                       Mean reward: 22.67
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 7.96s
                        Total time: 21783.68s
                               ETA: 856599.2s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.381s, learning 0.238s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0333
             Mean action noise std: 0.66
                       Mean reward: 22.69
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 8.62s
                        Total time: 21792.30s
                               ETA: 856584.0s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.020s, learning 0.189s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0348
             Mean action noise std: 0.66
                       Mean reward: 22.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 8.21s
                        Total time: 21800.51s
                               ETA: 856552.6s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.102s, learning 0.173s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0314
             Mean action noise std: 0.66
                       Mean reward: 22.73
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 8.28s
                        Total time: 21808.78s
                               ETA: 856523.9s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.940s, learning 0.171s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0455
             Mean action noise std: 0.66
                       Mean reward: 22.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 8.11s
                        Total time: 21816.89s
                               ETA: 856488.7s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.129s, learning 0.166s)
               Value function loss: 3.7226
                    Surrogate loss: 0.0481
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 8.29s
                        Total time: 21825.19s
                               ETA: 856460.8s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 2107 steps/s (collection: 7.617s, learning 0.157s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0322
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 7.77s
                        Total time: 21832.96s
                               ETA: 856412.4s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.850s, learning 0.190s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0255
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 8.04s
                        Total time: 21841.00s
                               ETA: 856374.5s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 2105 steps/s (collection: 7.612s, learning 0.169s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0264
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 7.78s
                        Total time: 21848.78s
                               ETA: 856326.5s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.158s, learning 0.210s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0296
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 8.37s
                        Total time: 21857.15s
                               ETA: 856301.5s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 2073 steps/s (collection: 7.739s, learning 0.162s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0382
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 7.90s
                        Total time: 21865.05s
                               ETA: 856258.3s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.859s, learning 0.186s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0394
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 8.04s
                        Total time: 21873.10s
                               ETA: 856220.6s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.938s, learning 0.154s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0489
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 8.09s
                        Total time: 21881.19s
                               ETA: 856184.9s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.701s, learning 0.161s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0300
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 7.86s
                        Total time: 21889.05s
                               ETA: 856140.2s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.874s, learning 0.170s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0432
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 8.04s
                        Total time: 21897.09s
                               ETA: 856102.6s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.864s, learning 0.159s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0444
             Mean action noise std: 0.66
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 8.02s
                        Total time: 21905.12s
                               ETA: 856064.3s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.900s, learning 0.183s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0461
             Mean action noise std: 0.66
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 8.08s
                        Total time: 21913.20s
                               ETA: 856028.3s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.165s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0349
             Mean action noise std: 0.66
                       Mean reward: 22.68
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 8.04s
                        Total time: 21921.24s
                               ETA: 855990.7s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.173s, learning 0.155s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0455
             Mean action noise std: 0.66
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 8.33s
                        Total time: 21929.57s
                               ETA: 855964.3s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.873s, learning 0.221s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0360
             Mean action noise std: 0.66
                       Mean reward: 22.64
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 8.09s
                        Total time: 21937.66s
                               ETA: 855928.8s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.954s, learning 0.175s)
               Value function loss: 7.9267
                    Surrogate loss: 0.0854
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 8.13s
                        Total time: 21945.79s
                               ETA: 855894.7s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.977s, learning 0.176s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0224
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 8.15s
                        Total time: 21953.95s
                               ETA: 855861.6s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.845s, learning 0.167s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0257
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 8.01s
                        Total time: 21961.96s
                               ETA: 855822.9s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.935s, learning 0.162s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0256
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 8.10s
                        Total time: 21970.05s
                               ETA: 855787.6s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.783s, learning 0.185s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0202
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 7.97s
                        Total time: 21978.02s
                               ETA: 855747.3s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.978s, learning 0.220s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0274
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 8.20s
                        Total time: 21986.22s
                               ETA: 855716.0s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.797s, learning 0.173s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0408
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 7.97s
                        Total time: 21994.19s
                               ETA: 855675.8s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.762s, learning 0.162s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0348
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 7.92s
                        Total time: 22002.11s
                               ETA: 855633.9s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.859s, learning 0.173s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0362
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 8.03s
                        Total time: 22010.15s
                               ETA: 855596.2s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.867s, learning 0.159s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0404
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 8.03s
                        Total time: 22018.17s
                               ETA: 855558.2s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.757s, learning 0.170s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0407
             Mean action noise std: 0.66
                       Mean reward: 22.22
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 7.93s
                        Total time: 22026.10s
                               ETA: 855516.5s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.053s, learning 0.193s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0328
             Mean action noise std: 0.66
                       Mean reward: 22.23
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 8.25s
                        Total time: 22034.35s
                               ETA: 855487.2s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.065s, learning 0.228s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0310
             Mean action noise std: 0.66
                       Mean reward: 22.21
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 8.29s
                        Total time: 22042.64s
                               ETA: 855459.7s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.876s, learning 0.183s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0368
             Mean action noise std: 0.66
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 8.06s
                        Total time: 22050.70s
                               ETA: 855423.2s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.731s, learning 0.163s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0354
             Mean action noise std: 0.66
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 7.89s
                        Total time: 22058.59s
                               ETA: 855380.2s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.965s, learning 0.191s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0510
             Mean action noise std: 0.66
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 8.16s
                        Total time: 22066.75s
                               ETA: 855347.5s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.988s, learning 0.200s)
               Value function loss: 6.6907
                    Surrogate loss: 0.0572
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 8.19s
                        Total time: 22074.94s
                               ETA: 855316.0s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.951s, learning 0.165s)
               Value function loss: 0.0011
                    Surrogate loss: -0.0300
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 8.12s
                        Total time: 22083.05s
                               ETA: 855281.8s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.784s, learning 0.170s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0281
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 7.95s
                        Total time: 22091.01s
                               ETA: 855241.3s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 2094 steps/s (collection: 7.654s, learning 0.168s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0223
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 7.82s
                        Total time: 22098.83s
                               ETA: 855195.7s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.864s, learning 0.191s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0276
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 8.06s
                        Total time: 22106.88s
                               ETA: 855159.1s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.962s, learning 0.205s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0318
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 8.17s
                        Total time: 22115.05s
                               ETA: 855126.9s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.762s, learning 0.176s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0397
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 7.94s
                        Total time: 22122.99s
                               ETA: 855085.9s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 2087 steps/s (collection: 7.689s, learning 0.158s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0383
             Mean action noise std: 0.66
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 7.85s
                        Total time: 22130.84s
                               ETA: 855041.4s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.792s, learning 0.184s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0244
             Mean action noise std: 0.66
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 7.98s
                        Total time: 22138.81s
                               ETA: 855001.9s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.784s, learning 0.160s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0323
             Mean action noise std: 0.66
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 7.94s
                        Total time: 22146.76s
                               ETA: 854961.2s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.123s, learning 0.169s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0430
             Mean action noise std: 0.66
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 8.29s
                        Total time: 22155.05s
                               ETA: 854934.0s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.921s, learning 0.182s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0355
             Mean action noise std: 0.66
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 8.10s
                        Total time: 22163.15s
                               ETA: 854899.5s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.935s, learning 0.160s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0439
             Mean action noise std: 0.66
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 8.10s
                        Total time: 22171.25s
                               ETA: 854864.6s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.018s, learning 0.175s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0460
             Mean action noise std: 0.66
                       Mean reward: 21.69
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 8.19s
                        Total time: 22179.44s
                               ETA: 854833.6s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.064s, learning 0.171s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0388
             Mean action noise std: 0.66
                       Mean reward: 21.64
               Mean episode length: 124.86
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 8.24s
                        Total time: 22187.67s
                               ETA: 854804.3s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.899s, learning 0.162s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0509
             Mean action noise std: 0.66
                       Mean reward: 21.64
               Mean episode length: 124.86
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 8.06s
                        Total time: 22195.74s
                               ETA: 854768.2s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.176s, learning 0.246s)
               Value function loss: 2.5109
                    Surrogate loss: 0.0468
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 8.42s
                        Total time: 22204.16s
                               ETA: 854746.1s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.047s, learning 0.193s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0244
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 8.24s
                        Total time: 22212.40s
                               ETA: 854716.9s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.021s, learning 0.188s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0235
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 8.21s
                        Total time: 22220.61s
                               ETA: 854686.6s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.059s, learning 0.238s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0201
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 8.30s
                        Total time: 22228.90s
                               ETA: 854659.7s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.884s, learning 0.184s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0309
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 8.07s
                        Total time: 22236.97s
                               ETA: 854624.0s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 2094 steps/s (collection: 7.635s, learning 0.186s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0374
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 7.82s
                        Total time: 22244.79s
                               ETA: 854578.9s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.746s, learning 0.204s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0371
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 7.95s
                        Total time: 22252.74s
                               ETA: 854538.7s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.120s, learning 0.206s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0410
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 8.33s
                        Total time: 22261.07s
                               ETA: 854512.9s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.163s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0253
             Mean action noise std: 0.66
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 8.04s
                        Total time: 22269.11s
                               ETA: 854476.3s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.837s, learning 0.165s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0392
             Mean action noise std: 0.66
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 8.00s
                        Total time: 22277.11s
                               ETA: 854438.2s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.143s, learning 0.198s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0359
             Mean action noise std: 0.66
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 8.34s
                        Total time: 22285.45s
                               ETA: 854413.0s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.116s, learning 0.162s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0328
             Mean action noise std: 0.66
                       Mean reward: 21.79
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 8.28s
                        Total time: 22293.73s
                               ETA: 854385.5s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.756s, learning 0.184s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0360
             Mean action noise std: 0.66
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 7.94s
                        Total time: 22301.67s
                               ETA: 854345.1s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.851s, learning 0.159s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0319
             Mean action noise std: 0.66
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 8.01s
                        Total time: 22309.68s
                               ETA: 854307.4s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.285s, learning 0.161s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0409
             Mean action noise std: 0.66
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 8.45s
                        Total time: 22318.13s
                               ETA: 854286.3s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.161s, learning 0.257s)
               Value function loss: 9.6402
                    Surrogate loss: 0.0283
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 8.42s
                        Total time: 22326.54s
                               ETA: 854264.3s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.826s, learning 0.167s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0273
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 7.99s
                        Total time: 22334.54s
                               ETA: 854225.9s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.817s, learning 0.225s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0244
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 8.04s
                        Total time: 22342.58s
                               ETA: 854189.5s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.910s, learning 0.194s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0234
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 8.10s
                        Total time: 22350.68s
                               ETA: 854155.5s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.076s, learning 0.180s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0165
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 8.26s
                        Total time: 22358.94s
                               ETA: 854127.3s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.106s, learning 0.187s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0245
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 8.29s
                        Total time: 22367.23s
                               ETA: 854100.5s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.935s, learning 0.162s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0369
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 8.10s
                        Total time: 22375.33s
                               ETA: 854066.3s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.010s, learning 0.180s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0403
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 8.19s
                        Total time: 22383.52s
                               ETA: 854035.6s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.010s, learning 0.173s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0327
             Mean action noise std: 0.66
                       Mean reward: 22.11
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 8.18s
                        Total time: 22391.70s
                               ETA: 854004.7s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.843s, learning 0.203s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0386
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 8.05s
                        Total time: 22399.75s
                               ETA: 853968.5s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.902s, learning 0.168s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0429
             Mean action noise std: 0.66
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 8.07s
                        Total time: 22407.82s
                               ETA: 853933.3s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.190s, learning 0.156s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0254
             Mean action noise std: 0.66
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 8.35s
                        Total time: 22416.16s
                               ETA: 853908.7s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.111s, learning 0.166s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0307
             Mean action noise std: 0.66
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 8.28s
                        Total time: 22424.44s
                               ETA: 853881.4s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.125s, learning 0.162s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0301
             Mean action noise std: 0.66
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 8.29s
                        Total time: 22432.73s
                               ETA: 853854.5s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.880s, learning 0.161s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0214
             Mean action noise std: 0.66
                       Mean reward: 21.92
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 8.04s
                        Total time: 22440.77s
                               ETA: 853818.2s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.107s, learning 0.195s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0468
             Mean action noise std: 0.66
                       Mean reward: 21.92
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 8.30s
                        Total time: 22449.07s
                               ETA: 853791.9s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.856s, learning 0.172s)
               Value function loss: 6.4727
                    Surrogate loss: 0.0509
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 8.03s
                        Total time: 22457.10s
                               ETA: 853755.3s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.874s, learning 0.202s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0300
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 8.08s
                        Total time: 22465.17s
                               ETA: 853720.5s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.952s, learning 0.181s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0210
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 8.13s
                        Total time: 22473.31s
                               ETA: 853687.8s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.154s, learning 0.165s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0178
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 8.32s
                        Total time: 22481.63s
                               ETA: 853662.2s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.950s, learning 0.181s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0278
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 8.13s
                        Total time: 22489.76s
                               ETA: 853629.5s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.821s, learning 0.169s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0351
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 7.99s
                        Total time: 22497.75s
                               ETA: 853591.5s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.005s, learning 0.162s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0359
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 8.17s
                        Total time: 22505.91s
                               ETA: 853560.3s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.165s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0391
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 8.04s
                        Total time: 22513.96s
                               ETA: 853524.3s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.116s, learning 0.236s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0359
             Mean action noise std: 0.66
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 8.35s
                        Total time: 22522.31s
                               ETA: 853500.0s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.698s, learning 0.160s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0398
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 7.86s
                        Total time: 22530.17s
                               ETA: 853457.1s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1997 steps/s (collection: 7.944s, learning 0.259s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0396
             Mean action noise std: 0.66
                       Mean reward: 22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 8.20s
                        Total time: 22538.37s
                               ETA: 853427.3s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.873s, learning 0.194s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0306
             Mean action noise std: 0.66
                       Mean reward: 22.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 8.07s
                        Total time: 22546.44s
                               ETA: 853392.3s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.814s, learning 0.200s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0364
             Mean action noise std: 0.66
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 8.01s
                        Total time: 22554.45s
                               ETA: 853355.4s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.923s, learning 0.161s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0407
             Mean action noise std: 0.66
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 8.08s
                        Total time: 22562.54s
                               ETA: 853321.1s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.992s, learning 0.199s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0357
             Mean action noise std: 0.66
                       Mean reward: 22.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 8.19s
                        Total time: 22570.73s
                               ETA: 853290.9s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.751s, learning 0.176s)
               Value function loss: 7.4557
                    Surrogate loss: 0.0753
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 7.93s
                        Total time: 22578.65s
                               ETA: 853250.7s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.904s, learning 0.161s)
               Value function loss: 0.0926
                    Surrogate loss: -0.0217
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 8.07s
                        Total time: 22586.72s
                               ETA: 853215.7s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.978s, learning 0.175s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0249
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 8.15s
                        Total time: 22594.87s
                               ETA: 853184.1s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.908s, learning 0.159s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0267
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 8.07s
                        Total time: 22602.94s
                               ETA: 853149.3s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.951s, learning 0.172s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0221
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 8.12s
                        Total time: 22611.06s
                               ETA: 853116.6s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.153s, learning 0.198s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0216
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 8.35s
                        Total time: 22619.41s
                               ETA: 853092.5s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.148s, learning 0.160s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0306
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 8.31s
                        Total time: 22627.72s
                               ETA: 853066.8s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.854s, learning 0.164s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0398
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 8.02s
                        Total time: 22635.74s
                               ETA: 853030.2s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.894s, learning 0.179s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0427
             Mean action noise std: 0.66
                       Mean reward: 22.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 8.07s
                        Total time: 22643.81s
                               ETA: 852995.7s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.189s, learning 0.168s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0420
             Mean action noise std: 0.66
                       Mean reward: 22.46
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 8.36s
                        Total time: 22652.17s
                               ETA: 852971.9s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.189s, learning 0.175s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0441
             Mean action noise std: 0.66
                       Mean reward: 22.48
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 8.36s
                        Total time: 22660.53s
                               ETA: 852948.3s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.116s, learning 0.196s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0294
             Mean action noise std: 0.66
                       Mean reward: 22.54
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 8.31s
                        Total time: 22668.84s
                               ETA: 852922.9s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.782s, learning 0.166s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0163
             Mean action noise std: 0.66
                       Mean reward: 22.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 7.95s
                        Total time: 22676.79s
                               ETA: 852883.7s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.050s, learning 0.164s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0303
             Mean action noise std: 0.66
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 8.21s
                        Total time: 22685.00s
                               ETA: 852854.6s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.119s, learning 0.171s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0201
             Mean action noise std: 0.66
                       Mean reward: 22.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 8.29s
                        Total time: 22693.29s
                               ETA: 852828.4s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.981s, learning 0.163s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0421
             Mean action noise std: 0.66
                       Mean reward: 22.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 8.14s
                        Total time: 22701.44s
                               ETA: 852796.7s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.992s, learning 0.162s)
               Value function loss: 8.4090
                    Surrogate loss: 0.1561
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 8.15s
                        Total time: 22709.59s
                               ETA: 852765.4s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.993s, learning 0.161s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0239
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 8.15s
                        Total time: 22717.75s
                               ETA: 852734.1s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.988s, learning 0.191s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0251
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 8.18s
                        Total time: 22725.93s
                               ETA: 852703.7s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.982s, learning 0.161s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0148
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 8.14s
                        Total time: 22734.07s
                               ETA: 852672.0s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.184s, learning 0.171s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0245
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 8.36s
                        Total time: 22742.42s
                               ETA: 852648.3s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.862s, learning 0.201s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0328
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 8.06s
                        Total time: 22750.49s
                               ETA: 852613.7s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.120s, learning 0.163s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0350
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 8.28s
                        Total time: 22758.77s
                               ETA: 852587.3s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.009s, learning 0.177s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0424
             Mean action noise std: 0.66
                       Mean reward: 22.92
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 8.19s
                        Total time: 22766.96s
                               ETA: 852557.3s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.801s, learning 0.186s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0350
             Mean action noise std: 0.66
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 7.99s
                        Total time: 22774.94s
                               ETA: 852519.9s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.030s, learning 0.162s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0361
             Mean action noise std: 0.66
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 8.19s
                        Total time: 22783.14s
                               ETA: 852490.1s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.970s, learning 0.159s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0487
             Mean action noise std: 0.66
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 8.13s
                        Total time: 22791.26s
                               ETA: 852458.1s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.974s, learning 0.164s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0198
             Mean action noise std: 0.66
                       Mean reward: 22.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 8.14s
                        Total time: 22799.40s
                               ETA: 852426.3s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.926s, learning 0.194s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0256
             Mean action noise std: 0.66
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 8.12s
                        Total time: 22807.52s
                               ETA: 852393.9s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.877s, learning 0.178s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0239
             Mean action noise std: 0.66
                       Mean reward: 22.74
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 8.05s
                        Total time: 22815.58s
                               ETA: 852359.1s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.972s, learning 0.174s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0182
             Mean action noise std: 0.66
                       Mean reward: 22.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 8.15s
                        Total time: 22823.72s
                               ETA: 852327.8s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.191s, learning 0.169s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0485
             Mean action noise std: 0.66
                       Mean reward: 22.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 8.36s
                        Total time: 22832.08s
                               ETA: 852304.4s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.907s, learning 0.164s)
               Value function loss: 4.8489
                    Surrogate loss: 0.0853
             Mean action noise std: 0.66
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 8.07s
                        Total time: 22840.15s
                               ETA: 852270.2s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.936s, learning 0.203s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0290
             Mean action noise std: 0.66
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 8.14s
                        Total time: 22848.29s
                               ETA: 852238.7s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.752s, learning 0.171s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0178
             Mean action noise std: 0.66
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 7.92s
                        Total time: 22856.21s
                               ETA: 852199.0s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 2077 steps/s (collection: 7.726s, learning 0.161s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0143
             Mean action noise std: 0.66
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 7.89s
                        Total time: 22864.10s
                               ETA: 852158.1s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.135s, learning 0.165s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0248
             Mean action noise std: 0.66
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 8.30s
                        Total time: 22872.40s
                               ETA: 852132.6s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.962s, learning 0.172s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0352
             Mean action noise std: 0.66
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 8.13s
                        Total time: 22880.54s
                               ETA: 852100.9s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.827s, learning 0.168s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0340
             Mean action noise std: 0.66
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 7.99s
                        Total time: 22888.53s
                               ETA: 852064.1s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.994s, learning 0.175s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0454
             Mean action noise std: 0.65
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 8.17s
                        Total time: 22896.70s
                               ETA: 852033.7s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.072s, learning 0.161s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0350
             Mean action noise std: 0.65
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 8.23s
                        Total time: 22904.93s
                               ETA: 852005.8s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.811s, learning 0.163s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0450
             Mean action noise std: 0.65
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 7.97s
                        Total time: 22912.91s
                               ETA: 851968.2s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.032s, learning 0.160s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0335
             Mean action noise std: 0.65
                       Mean reward: 22.99
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 8.19s
                        Total time: 22921.10s
                               ETA: 851938.8s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.172s, learning 0.169s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0375
             Mean action noise std: 0.65
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 8.34s
                        Total time: 22929.44s
                               ETA: 851914.9s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.025s, learning 0.186s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0258
             Mean action noise std: 0.65
                       Mean reward: 22.96
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 8.21s
                        Total time: 22937.65s
                               ETA: 851886.2s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.953s, learning 0.161s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0392
             Mean action noise std: 0.65
                       Mean reward: 22.93
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 8.11s
                        Total time: 22945.77s
                               ETA: 851853.9s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.807s, learning 0.167s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0367
             Mean action noise std: 0.65
                       Mean reward: 22.88
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 7.97s
                        Total time: 22953.74s
                               ETA: 851816.4s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.093s, learning 0.159s)
               Value function loss: 8.5496
                    Surrogate loss: 0.0510
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 8.25s
                        Total time: 22961.99s
                               ETA: 851789.3s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.867s, learning 0.166s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0253
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 8.03s
                        Total time: 22970.03s
                               ETA: 851754.1s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.986s, learning 0.207s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0189
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 8.19s
                        Total time: 22978.22s
                               ETA: 851724.8s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.927s, learning 0.170s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0156
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 8.10s
                        Total time: 22986.31s
                               ETA: 851692.0s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.745s, learning 0.172s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0147
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 7.92s
                        Total time: 22994.23s
                               ETA: 851652.5s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.887s, learning 0.189s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0290
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 8.08s
                        Total time: 23002.31s
                               ETA: 851618.9s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.161s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0329
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 8.13s
                        Total time: 23010.44s
                               ETA: 851587.5s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.910s, learning 0.166s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0350
             Mean action noise std: 0.65
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 8.08s
                        Total time: 23018.52s
                               ETA: 851553.9s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.920s, learning 0.168s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0326
             Mean action noise std: 0.65
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 8.09s
                        Total time: 23026.60s
                               ETA: 851520.9s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.944s, learning 0.165s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0462
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 8.11s
                        Total time: 23034.71s
                               ETA: 851488.6s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.960s, learning 0.166s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0344
             Mean action noise std: 0.65
                       Mean reward: 22.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 8.13s
                        Total time: 23042.84s
                               ETA: 851457.0s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.957s, learning 0.163s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0343
             Mean action noise std: 0.65
                       Mean reward: 22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 8.12s
                        Total time: 23050.96s
                               ETA: 851425.1s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.916s, learning 0.160s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0190
             Mean action noise std: 0.65
                       Mean reward: 22.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 8.08s
                        Total time: 23059.04s
                               ETA: 851391.7s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 2112 steps/s (collection: 7.600s, learning 0.157s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0263
             Mean action noise std: 0.65
                       Mean reward: 22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 7.76s
                        Total time: 23066.79s
                               ETA: 851346.5s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.911s, learning 0.196s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0201
             Mean action noise std: 0.65
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 8.11s
                        Total time: 23074.90s
                               ETA: 851314.3s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.013s, learning 0.166s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0419
             Mean action noise std: 0.65
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 8.18s
                        Total time: 23083.08s
                               ETA: 851284.8s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.179s, learning 0.168s)
               Value function loss: 7.8996
                    Surrogate loss: 0.0955
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 8.35s
                        Total time: 23091.43s
                               ETA: 851261.4s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.731s, learning 0.160s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0215
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 7.89s
                        Total time: 23099.32s
                               ETA: 851221.3s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.796s, learning 0.163s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0155
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 7.96s
                        Total time: 23107.28s
                               ETA: 851183.6s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.167s, learning 0.181s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0182
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 8.35s
                        Total time: 23115.62s
                               ETA: 851160.3s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.809s, learning 0.167s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0205
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 7.98s
                        Total time: 23123.60s
                               ETA: 851123.3s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.867s, learning 0.195s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0300
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 8.06s
                        Total time: 23131.66s
                               ETA: 851089.5s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.109s, learning 0.161s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0322
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 8.27s
                        Total time: 23139.93s
                               ETA: 851063.4s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.846s, learning 0.160s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0315
             Mean action noise std: 0.65
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 8.01s
                        Total time: 23147.94s
                               ETA: 851027.6s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.028s, learning 0.238s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0331
             Mean action noise std: 0.65
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 8.27s
                        Total time: 23156.20s
                               ETA: 851001.4s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.328s, learning 0.159s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0333
             Mean action noise std: 0.65
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 8.49s
                        Total time: 23164.69s
                               ETA: 850983.3s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.856s, learning 0.185s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0423
             Mean action noise std: 0.65
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 8.04s
                        Total time: 23172.73s
                               ETA: 850948.8s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.119s, learning 0.210s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0213
             Mean action noise std: 0.65
                       Mean reward: 23.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 8.33s
                        Total time: 23181.06s
                               ETA: 850925.0s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.418s, learning 0.174s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0292
             Mean action noise std: 0.65
                       Mean reward: 23.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 8.59s
                        Total time: 23189.65s
                               ETA: 850910.7s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.896s, learning 0.155s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0269
             Mean action noise std: 0.65
                       Mean reward: 23.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 8.05s
                        Total time: 23197.70s
                               ETA: 850876.7s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1995 steps/s (collection: 7.923s, learning 0.288s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0245
             Mean action noise std: 0.65
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 8.21s
                        Total time: 23205.91s
                               ETA: 850848.5s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.079s, learning 0.155s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0421
             Mean action noise std: 0.65
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 8.23s
                        Total time: 23214.15s
                               ETA: 850821.3s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.090s, learning 0.167s)
               Value function loss: 2.8805
                    Surrogate loss: 0.0590
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 8.26s
                        Total time: 23222.41s
                               ETA: 850794.8s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 2156 steps/s (collection: 7.398s, learning 0.199s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0104
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 7.60s
                        Total time: 23230.00s
                               ETA: 850744.2s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.962s, learning 0.206s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0225
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 8.17s
                        Total time: 23238.17s
                               ETA: 850714.6s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.906s, learning 0.163s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0184
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 8.07s
                        Total time: 23246.24s
                               ETA: 850681.3s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.117s, learning 0.163s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0230
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 8.28s
                        Total time: 23254.52s
                               ETA: 850655.7s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.931s, learning 0.167s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0301
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 8.10s
                        Total time: 23262.62s
                               ETA: 850623.5s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.967s, learning 0.188s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0310
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 8.15s
                        Total time: 23270.77s
                               ETA: 850593.4s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.112s, learning 0.176s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0381
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 8.29s
                        Total time: 23279.06s
                               ETA: 850568.2s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.901s, learning 0.173s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0303
             Mean action noise std: 0.65
                       Mean reward: 23.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 8.07s
                        Total time: 23287.13s
                               ETA: 850535.2s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.057s, learning 0.164s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0419
             Mean action noise std: 0.65
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 8.22s
                        Total time: 23295.35s
                               ETA: 850507.6s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.912s, learning 0.161s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0369
             Mean action noise std: 0.65
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 8.07s
                        Total time: 23303.43s
                               ETA: 850474.6s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.809s, learning 0.173s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0270
             Mean action noise std: 0.65
                       Mean reward: 23.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 7.98s
                        Total time: 23311.41s
                               ETA: 850438.3s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.979s, learning 0.218s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0268
             Mean action noise std: 0.65
                       Mean reward: 23.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 8.20s
                        Total time: 23319.61s
                               ETA: 850409.8s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.133s, learning 0.164s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0306
             Mean action noise std: 0.65
                       Mean reward: 23.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 8.30s
                        Total time: 23327.90s
                               ETA: 850385.1s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.046s, learning 0.156s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0330
             Mean action noise std: 0.65
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 8.20s
                        Total time: 23336.10s
                               ETA: 850356.8s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.328s, learning 0.183s)
               Value function loss: 9.0341
                    Surrogate loss: 0.0566
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 8.51s
                        Total time: 23344.62s
                               ETA: 850339.8s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.669s, learning 0.201s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0184
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 7.87s
                        Total time: 23352.49s
                               ETA: 850299.5s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.216s, learning 0.253s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0180
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 8.47s
                        Total time: 23360.95s
                               ETA: 850281.1s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.933s, learning 0.180s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0031
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 8.11s
                        Total time: 23369.07s
                               ETA: 850249.7s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.730s, learning 0.162s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0051
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 7.89s
                        Total time: 23376.96s
                               ETA: 850210.3s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 2145 steps/s (collection: 7.443s, learning 0.192s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0251
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 7.63s
                        Total time: 23384.60s
                               ETA: 850161.5s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.897s, learning 0.173s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0252
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 8.07s
                        Total time: 23392.67s
                               ETA: 850128.6s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.081s, learning 0.182s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0306
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 8.26s
                        Total time: 23400.93s
                               ETA: 850102.7s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 2072 steps/s (collection: 7.737s, learning 0.167s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0314
             Mean action noise std: 0.65
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 7.90s
                        Total time: 23408.83s
                               ETA: 850063.8s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.976s, learning 0.158s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0356
             Mean action noise std: 0.65
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 8.13s
                        Total time: 23416.97s
                               ETA: 850033.3s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.884s, learning 0.161s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0456
             Mean action noise std: 0.65
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 8.05s
                        Total time: 23425.01s
                               ETA: 849999.5s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.952s, learning 0.161s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0182
             Mean action noise std: 0.65
                       Mean reward: 23.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 8.11s
                        Total time: 23433.13s
                               ETA: 849968.3s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.910s, learning 0.172s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0247
             Mean action noise std: 0.65
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 8.08s
                        Total time: 23441.21s
                               ETA: 849935.9s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.933s, learning 0.165s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0332
             Mean action noise std: 0.65
                       Mean reward: 23.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 8.10s
                        Total time: 23449.31s
                               ETA: 849904.1s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.175s, learning 0.198s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0213
             Mean action noise std: 0.65
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 8.37s
                        Total time: 23457.68s
                               ETA: 849882.4s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.255s, learning 0.202s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0422
             Mean action noise std: 0.65
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 8.46s
                        Total time: 23466.14s
                               ETA: 849863.6s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.355s, learning 0.192s)
               Value function loss: 6.3882
                    Surrogate loss: 0.0581
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 8.55s
                        Total time: 23474.68s
                               ETA: 849848.1s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.995s, learning 0.169s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0201
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 8.16s
                        Total time: 23482.85s
                               ETA: 849818.8s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.057s, learning 0.158s)
               Value function loss: 0.0042
                    Surrogate loss: 0.0021
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 8.21s
                        Total time: 23491.06s
                               ETA: 849791.4s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.267s, learning 0.161s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0156
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 8.43s
                        Total time: 23499.49s
                               ETA: 849771.6s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.771s, learning 0.164s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0221
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 7.93s
                        Total time: 23507.42s
                               ETA: 849734.0s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.886s, learning 0.156s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0297
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 8.04s
                        Total time: 23515.47s
                               ETA: 849700.4s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.907s, learning 0.231s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0229
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 8.14s
                        Total time: 23523.61s
                               ETA: 849670.2s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.308s, learning 0.190s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0247
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 8.50s
                        Total time: 23532.10s
                               ETA: 849653.0s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.931s, learning 0.182s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0277
             Mean action noise std: 0.65
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 8.11s
                        Total time: 23540.22s
                               ETA: 849621.9s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.978s, learning 0.167s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0371
             Mean action noise std: 0.65
                       Mean reward: 23.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 8.15s
                        Total time: 23548.36s
                               ETA: 849592.0s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.033s, learning 0.163s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0317
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 8.20s
                        Total time: 23556.56s
                               ETA: 849564.0s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.799s, learning 0.170s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0282
             Mean action noise std: 0.65
                       Mean reward: 23.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 7.97s
                        Total time: 23564.53s
                               ETA: 849527.8s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.038s, learning 0.184s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0329
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 8.22s
                        Total time: 23572.75s
                               ETA: 849500.7s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.200s, learning 0.190s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0384
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 8.39s
                        Total time: 23581.14s
                               ETA: 849479.7s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.892s, learning 0.221s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0242
             Mean action noise std: 0.65
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 8.11s
                        Total time: 23589.25s
                               ETA: 849448.8s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.154s, learning 0.214s)
               Value function loss: 9.4276
                    Surrogate loss: 0.0784
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 8.37s
                        Total time: 23597.62s
                               ETA: 849427.0s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.918s, learning 0.184s)
               Value function loss: 0.1069
                    Surrogate loss: -0.0158
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 8.10s
                        Total time: 23605.72s
                               ETA: 849395.6s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.464s, learning 0.170s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0156
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 8.63s
                        Total time: 23614.35s
                               ETA: 849383.5s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.806s, learning 0.161s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0194
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 7.97s
                        Total time: 23622.32s
                               ETA: 849347.3s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.830s, learning 0.188s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0142
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 8.02s
                        Total time: 23630.34s
                               ETA: 849313.0s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.124s, learning 0.174s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0253
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 8.30s
                        Total time: 23638.64s
                               ETA: 849288.8s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1996 steps/s (collection: 7.957s, learning 0.249s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0307
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 8.21s
                        Total time: 23646.84s
                               ETA: 849261.3s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.302s, learning 0.162s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0276
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 8.46s
                        Total time: 23655.31s
                               ETA: 849243.0s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.863s, learning 0.200s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0282
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 8.06s
                        Total time: 23663.37s
                               ETA: 849210.4s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.781s, learning 0.164s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0298
             Mean action noise std: 0.65
                       Mean reward: 23.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 7.95s
                        Total time: 23671.32s
                               ETA: 849173.6s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.896s, learning 0.189s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0385
             Mean action noise std: 0.65
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 8.09s
                        Total time: 23679.40s
                               ETA: 849141.8s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.278s, learning 0.188s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0318
             Mean action noise std: 0.65
                       Mean reward: 23.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 8.47s
                        Total time: 23687.87s
                               ETA: 849123.7s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 2072 steps/s (collection: 7.747s, learning 0.160s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0249
             Mean action noise std: 0.65
                       Mean reward: 23.46
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 7.91s
                        Total time: 23695.78s
                               ETA: 849085.5s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.117s, learning 0.166s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0312
             Mean action noise std: 0.65
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 8.28s
                        Total time: 23704.06s
                               ETA: 849060.9s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.966s, learning 0.168s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0278
             Mean action noise std: 0.65
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 8.13s
                        Total time: 23712.19s
                               ETA: 849030.9s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.129s, learning 0.166s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0432
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 8.29s
                        Total time: 23720.49s
                               ETA: 849006.7s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.115s, learning 0.185s)
               Value function loss: 6.7863
                    Surrogate loss: 0.1510
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 8.30s
                        Total time: 23728.79s
                               ETA: 848982.7s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.822s, learning 0.167s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0185
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 7.99s
                        Total time: 23736.78s
                               ETA: 848947.5s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.941s, learning 0.187s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0103
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 8.13s
                        Total time: 23744.90s
                               ETA: 848917.4s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.979s, learning 0.201s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0070
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 8.18s
                        Total time: 23753.08s
                               ETA: 848889.1s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.801s, learning 0.200s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0188
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 8.00s
                        Total time: 23761.09s
                               ETA: 848854.5s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.998s, learning 0.161s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0294
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 8.16s
                        Total time: 23769.24s
                               ETA: 848825.5s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 2087 steps/s (collection: 7.686s, learning 0.163s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0301
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 7.85s
                        Total time: 23777.09s
                               ETA: 848785.5s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.693s, learning 0.204s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0302
             Mean action noise std: 0.65
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 7.90s
                        Total time: 23784.99s
                               ETA: 848747.2s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.188s, learning 0.160s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0380
             Mean action noise std: 0.65
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 8.35s
                        Total time: 23793.34s
                               ETA: 848725.1s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.800s, learning 0.157s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0384
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 7.96s
                        Total time: 23801.30s
                               ETA: 848689.0s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.169s, learning 0.194s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0471
             Mean action noise std: 0.65
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 8.36s
                        Total time: 23809.66s
                               ETA: 848667.4s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.177s, learning 0.293s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0376
             Mean action noise std: 0.65
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 8.47s
                        Total time: 23818.13s
                               ETA: 848649.6s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.007s, learning 0.169s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0369
             Mean action noise std: 0.65
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 8.18s
                        Total time: 23826.31s
                               ETA: 848621.3s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.112s, learning 0.177s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0400
             Mean action noise std: 0.65
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 8.29s
                        Total time: 23834.60s
                               ETA: 848597.1s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.122s, learning 0.165s)
               Value function loss: 0.0348
                    Surrogate loss: -0.0336
             Mean action noise std: 0.65
                       Mean reward: 23.35
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 8.29s
                        Total time: 23842.88s
                               ETA: 848572.8s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.924s, learning 0.168s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0475
             Mean action noise std: 0.65
                       Mean reward: 23.35
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 8.09s
                        Total time: 23850.97s
                               ETA: 848541.6s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.922s, learning 0.234s)
               Value function loss: 4.3808
                    Surrogate loss: 0.0735
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 8.16s
                        Total time: 23859.13s
                               ETA: 848512.7s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.871s, learning 0.183s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0190
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 8.05s
                        Total time: 23867.18s
                               ETA: 848480.1s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.955s, learning 0.173s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0176
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 8.13s
                        Total time: 23875.31s
                               ETA: 848450.2s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.843s, learning 0.257s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0203
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 8.10s
                        Total time: 23883.41s
                               ETA: 848419.4s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.308s, learning 0.190s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0288
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 8.50s
                        Total time: 23891.91s
                               ETA: 848402.6s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.932s, learning 0.207s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0302
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 8.14s
                        Total time: 23900.05s
                               ETA: 848373.2s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.828s, learning 0.175s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0301
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 8.00s
                        Total time: 23908.05s
                               ETA: 848339.0s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.936s, learning 0.171s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0301
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 8.11s
                        Total time: 23916.16s
                               ETA: 848308.4s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.688s, learning 0.169s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0308
             Mean action noise std: 0.65
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 7.86s
                        Total time: 23924.02s
                               ETA: 848269.0s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.957s, learning 0.167s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0428
             Mean action noise std: 0.65
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 8.12s
                        Total time: 23932.14s
                               ETA: 848239.1s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.900s, learning 0.163s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0378
             Mean action noise std: 0.65
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 8.06s
                        Total time: 23940.20s
                               ETA: 848207.1s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.992s, learning 0.170s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0380
             Mean action noise std: 0.65
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 8.16s
                        Total time: 23948.37s
                               ETA: 848178.5s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.982s, learning 0.163s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0282
             Mean action noise std: 0.65
                       Mean reward: 23.05
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 8.14s
                        Total time: 23956.51s
                               ETA: 848149.4s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.811s, learning 0.168s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0432
             Mean action noise std: 0.65
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 7.98s
                        Total time: 23964.49s
                               ETA: 848114.4s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.899s, learning 0.196s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0317
             Mean action noise std: 0.65
                       Mean reward: 23.03
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 8.09s
                        Total time: 23972.58s
                               ETA: 848083.6s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.130s, learning 0.192s)
               Value function loss: 8.5313
                    Surrogate loss: 0.0652
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 8.32s
                        Total time: 23980.91s
                               ETA: 848060.7s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 2089 steps/s (collection: 7.674s, learning 0.168s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0254
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 7.84s
                        Total time: 23988.75s
                               ETA: 848021.0s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.231s, learning 0.224s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0185
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 8.46s
                        Total time: 23997.20s
                               ETA: 848002.9s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.940s, learning 0.170s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0179
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 8.11s
                        Total time: 24005.31s
                               ETA: 847972.6s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.777s, learning 0.198s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0127
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 7.97s
                        Total time: 24013.29s
                               ETA: 847937.6s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.109s, learning 0.163s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0253
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 8.27s
                        Total time: 24021.56s
                               ETA: 847913.1s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.716s, learning 0.166s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0222
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 7.88s
                        Total time: 24029.44s
                               ETA: 847874.8s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.899s, learning 0.178s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0319
             Mean action noise std: 0.65
                       Mean reward: 23.15
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 8.08s
                        Total time: 24037.52s
                               ETA: 847843.4s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.025s, learning 0.163s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0323
             Mean action noise std: 0.65
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 8.19s
                        Total time: 24045.71s
                               ETA: 847816.0s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.053s, learning 0.171s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0440
             Mean action noise std: 0.65
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 8.22s
                        Total time: 24053.93s
                               ETA: 847789.8s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.211s, learning 0.178s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0444
             Mean action noise std: 0.65
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 8.39s
                        Total time: 24062.32s
                               ETA: 847769.5s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.979s, learning 0.160s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0335
             Mean action noise std: 0.65
                       Mean reward: 23.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 8.14s
                        Total time: 24070.46s
                               ETA: 847740.5s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.032s, learning 0.168s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0378
             Mean action noise std: 0.65
                       Mean reward: 23.01
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 8.20s
                        Total time: 24078.66s
                               ETA: 847713.5s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.098s, learning 0.165s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0345
             Mean action noise std: 0.65
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 8.26s
                        Total time: 24086.92s
                               ETA: 847688.8s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.198s, learning 0.213s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0316
             Mean action noise std: 0.65
                       Mean reward: 22.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 8.41s
                        Total time: 24095.33s
                               ETA: 847669.2s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.231s, learning 0.203s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0485
             Mean action noise std: 0.65
                       Mean reward: 22.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 8.43s
                        Total time: 24103.77s
                               ETA: 847650.5s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.155s, learning 0.257s)
               Value function loss: 4.2433
                    Surrogate loss: 0.1301
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 8.41s
                        Total time: 24112.18s
                               ETA: 847631.1s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.986s, learning 0.157s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0214
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 8.14s
                        Total time: 24120.32s
                               ETA: 847602.1s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.775s, learning 0.168s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0172
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 7.94s
                        Total time: 24128.26s
                               ETA: 847566.2s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.079s, learning 0.174s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0131
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 8.25s
                        Total time: 24136.51s
                               ETA: 847541.2s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.854s, learning 0.177s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0234
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 8.03s
                        Total time: 24144.55s
                               ETA: 847508.4s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.170s, learning 0.179s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0281
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 8.35s
                        Total time: 24152.90s
                               ETA: 847486.8s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1990 steps/s (collection: 7.984s, learning 0.247s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0270
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 8.23s
                        Total time: 24161.13s
                               ETA: 847461.1s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.922s, learning 0.169s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0255
             Mean action noise std: 0.65
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 8.09s
                        Total time: 24169.22s
                               ETA: 847430.5s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.990s, learning 0.177s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0351
             Mean action noise std: 0.65
                       Mean reward: 22.40
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 8.17s
                        Total time: 24177.39s
                               ETA: 847402.5s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.806s, learning 0.160s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0441
             Mean action noise std: 0.65
                       Mean reward: 22.38
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 7.97s
                        Total time: 24185.35s
                               ETA: 847367.6s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.037s, learning 0.165s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0432
             Mean action noise std: 0.65
                       Mean reward: 22.38
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 8.20s
                        Total time: 24193.55s
                               ETA: 847340.9s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.411s, learning 0.164s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0374
             Mean action noise std: 0.65
                       Mean reward: 22.32
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 8.58s
                        Total time: 24202.13s
                               ETA: 847327.3s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.905s, learning 0.163s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0409
             Mean action noise std: 0.65
                       Mean reward: 22.28
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 8.07s
                        Total time: 24210.20s
                               ETA: 847295.9s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.104s, learning 0.174s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0424
             Mean action noise std: 0.65
                       Mean reward: 22.27
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 8.28s
                        Total time: 24218.47s
                               ETA: 847271.9s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1992 steps/s (collection: 7.992s, learning 0.229s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0363
             Mean action noise std: 0.65
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 8.22s
                        Total time: 24226.70s
                               ETA: 847245.9s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.880s, learning 0.164s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0486
             Mean action noise std: 0.65
                       Mean reward: 22.24
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 8.04s
                        Total time: 24234.74s
                               ETA: 847213.7s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.821s, learning 0.172s)
               Value function loss: 1.9559
                    Surrogate loss: 0.0512
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 7.99s
                        Total time: 24242.73s
                               ETA: 847179.8s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.802s, learning 0.215s)
               Value function loss: 0.0006
                    Surrogate loss: -0.0166
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 8.02s
                        Total time: 24250.75s
                               ETA: 847146.8s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.112s, learning 0.167s)
               Value function loss: 0.0009
                    Surrogate loss: -0.0250
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 8.28s
                        Total time: 24259.03s
                               ETA: 847122.8s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.057s, learning 0.171s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0263
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 8.23s
                        Total time: 24267.26s
                               ETA: 847097.2s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.823s, learning 0.182s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0301
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 8.01s
                        Total time: 24275.26s
                               ETA: 847063.8s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.691s, learning 0.164s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0271
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 7.86s
                        Total time: 24283.12s
                               ETA: 847025.1s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.167s, learning 0.155s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0239
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 8.32s
                        Total time: 24291.44s
                               ETA: 847002.8s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.983s, learning 0.168s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0320
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 8.15s
                        Total time: 24299.59s
                               ETA: 846974.5s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.370s, learning 0.207s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0329
             Mean action noise std: 0.65
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 8.58s
                        Total time: 24308.17s
                               ETA: 846961.0s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.127s, learning 0.198s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0460
             Mean action noise std: 0.65
                       Mean reward: 21.29
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 8.33s
                        Total time: 24316.49s
                               ETA: 846938.9s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.898s, learning 0.165s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0453
             Mean action noise std: 0.65
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 8.06s
                        Total time: 24324.56s
                               ETA: 846907.6s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.068s, learning 0.167s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0426
             Mean action noise std: 0.65
                       Mean reward: 21.30
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 8.23s
                        Total time: 24332.79s
                               ETA: 846882.2s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.101s, learning 0.166s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0399
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 8.27s
                        Total time: 24341.06s
                               ETA: 846858.1s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.109s, learning 0.166s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0385
             Mean action noise std: 0.65
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 8.28s
                        Total time: 24349.34s
                               ETA: 846834.2s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.833s, learning 0.162s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0436
             Mean action noise std: 0.65
                       Mean reward: 21.32
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 7.99s
                        Total time: 24357.33s
                               ETA: 846800.5s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.236s, learning 0.172s)
               Value function loss: 3.7263
                    Surrogate loss: 0.0469
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 8.41s
                        Total time: 24365.74s
                               ETA: 846781.3s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.037s, learning 0.163s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0167
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 8.20s
                        Total time: 24373.94s
                               ETA: 846754.8s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.918s, learning 0.163s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0204
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 8.08s
                        Total time: 24382.02s
                               ETA: 846724.2s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.072s, learning 0.198s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0140
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 8.27s
                        Total time: 24390.29s
                               ETA: 846700.2s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.137s, learning 0.163s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0196
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 8.30s
                        Total time: 24398.59s
                               ETA: 846677.2s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.318s, learning 0.162s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0313
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 8.48s
                        Total time: 24407.07s
                               ETA: 846660.5s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.889s, learning 0.256s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0269
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 8.15s
                        Total time: 24415.22s
                               ETA: 846632.2s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.067s, learning 0.171s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0200
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 8.24s
                        Total time: 24423.45s
                               ETA: 846607.2s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.924s, learning 0.159s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0190
             Mean action noise std: 0.65
                       Mean reward: 21.66
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 8.08s
                        Total time: 24431.54s
                               ETA: 846576.7s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.900s, learning 0.240s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0312
             Mean action noise std: 0.65
                       Mean reward: 21.62
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 8.14s
                        Total time: 24439.68s
                               ETA: 846548.2s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.892s, learning 0.177s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0362
             Mean action noise std: 0.65
                       Mean reward: 21.51
               Mean episode length: 124.56
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 8.07s
                        Total time: 24447.75s
                               ETA: 846517.3s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.061s, learning 0.165s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0345
             Mean action noise std: 0.65
                       Mean reward: 21.46
               Mean episode length: 124.56
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 8.23s
                        Total time: 24455.97s
                               ETA: 846491.9s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.771s, learning 0.220s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0335
             Mean action noise std: 0.65
                       Mean reward: 21.38
               Mean episode length: 124.56
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 7.99s
                        Total time: 24463.96s
                               ETA: 846458.3s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.325s, learning 0.171s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0368
             Mean action noise std: 0.65
                       Mean reward: 21.37
               Mean episode length: 124.56
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 8.50s
                        Total time: 24472.46s
                               ETA: 846442.2s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.000s, learning 0.199s)
               Value function loss: 0.0317
                    Surrogate loss: -0.0333
             Mean action noise std: 0.65
                       Mean reward: 21.35
               Mean episode length: 124.56
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 8.20s
                        Total time: 24480.66s
                               ETA: 846415.9s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.000s, learning 0.239s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0469
             Mean action noise std: 0.65
                       Mean reward: 21.35
               Mean episode length: 124.56
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 8.24s
                        Total time: 24488.90s
                               ETA: 846391.0s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1152 steps/s (collection: 14.049s, learning 0.161s)
               Value function loss: 4.3806
                    Surrogate loss: 0.0651
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 14.21s
                        Total time: 24503.11s
                               ETA: 846572.3s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.381s, learning 0.165s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0213
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 15.55s
                        Total time: 24518.65s
                               ETA: 846799.7s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1052 steps/s (collection: 15.367s, learning 0.197s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0207
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 15.56s
                        Total time: 24534.22s
                               ETA: 847027.5s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.522s, learning 0.312s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0118
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 15.83s
                        Total time: 24550.05s
                               ETA: 847264.4s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1057 steps/s (collection: 15.328s, learning 0.159s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0250
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 15.49s
                        Total time: 24565.54s
                               ETA: 847489.2s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.844s, learning 0.171s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0280
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 16.01s
                        Total time: 24581.55s
                               ETA: 847732.1s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1051 steps/s (collection: 15.430s, learning 0.157s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0223
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 15.59s
                        Total time: 24597.14s
                               ETA: 847960.0s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1049 steps/s (collection: 15.440s, learning 0.169s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0215
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 15.61s
                        Total time: 24612.75s
                               ETA: 848188.4s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.677s, learning 0.165s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0296
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 15.84s
                        Total time: 24628.59s
                               ETA: 848424.8s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.447s, learning 0.240s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0422
             Mean action noise std: 0.65
                       Mean reward: 21.65
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 15.69s
                        Total time: 24644.28s
                               ETA: 848655.6s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.602s, learning 0.193s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0385
             Mean action noise std: 0.65
                       Mean reward: 21.68
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 15.80s
                        Total time: 24660.07s
                               ETA: 848890.0s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1067 steps/s (collection: 15.139s, learning 0.211s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0402
             Mean action noise std: 0.65
                       Mean reward: 21.64
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 15.35s
                        Total time: 24675.42s
                               ETA: 849108.8s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.496s, learning 0.165s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0408
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 15.66s
                        Total time: 24691.08s
                               ETA: 849338.3s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.740s, learning 0.262s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0406
             Mean action noise std: 0.65
                       Mean reward: 21.56
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 16.00s
                        Total time: 24707.08s
                               ETA: 849579.2s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1049 steps/s (collection: 15.436s, learning 0.178s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0370
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 15.61s
                        Total time: 24722.70s
                               ETA: 849806.7s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1064 steps/s (collection: 15.219s, learning 0.169s)
               Value function loss: 5.0007
                    Surrogate loss: 0.0445
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 15.39s
                        Total time: 24738.09s
                               ETA: 850026.2s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.885s, learning 0.225s)
               Value function loss: 0.2482
                    Surrogate loss: -0.0099
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 16.11s
                        Total time: 24754.20s
                               ETA: 850270.3s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1062 steps/s (collection: 15.261s, learning 0.158s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0169
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 15.42s
                        Total time: 24769.61s
                               ETA: 850490.5s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.681s, learning 0.167s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0218
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 15.85s
                        Total time: 24785.46s
                               ETA: 850725.3s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.521s, learning 0.171s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0240
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 15.69s
                        Total time: 24801.15s
                               ETA: 850954.6s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.664s, learning 0.160s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0185
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 15.82s
                        Total time: 24816.98s
                               ETA: 851188.2s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.556s, learning 0.164s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0283
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 15.72s
                        Total time: 24832.70s
                               ETA: 851418.0s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.855s, learning 0.161s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0298
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 16.02s
                        Total time: 24848.71s
                               ETA: 851657.9s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1047 steps/s (collection: 15.478s, learning 0.163s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0244
             Mean action noise std: 0.65
                       Mean reward: 21.58
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 15.64s
                        Total time: 24864.35s
                               ETA: 851884.7s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.633s, learning 0.177s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0347
             Mean action noise std: 0.65
                       Mean reward: 21.55
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 15.81s
                        Total time: 24880.16s
                               ETA: 852117.1s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.811s, learning 0.158s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0471
             Mean action noise std: 0.65
                       Mean reward: 21.56
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 15.97s
                        Total time: 24896.13s
                               ETA: 852354.8s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.373s, learning 0.181s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0400
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 15.55s
                        Total time: 24911.69s
                               ETA: 852578.1s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.625s, learning 0.163s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0351
             Mean action noise std: 0.65
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 15.79s
                        Total time: 24927.48s
                               ETA: 852809.3s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.640s, learning 0.167s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0403
             Mean action noise std: 0.65
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 15.81s
                        Total time: 24943.28s
                               ETA: 853040.9s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.960s, learning 0.162s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0398
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 16.12s
                        Total time: 24959.40s
                               ETA: 853283.2s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.396s, learning 0.196s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0432
             Mean action noise std: 0.65
                       Mean reward: 21.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 15.59s
                        Total time: 24975.00s
                               ETA: 853507.1s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.678s, learning 0.236s)
               Value function loss: 3.8269
                    Surrogate loss: 0.1132
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 15.91s
                        Total time: 24990.91s
                               ETA: 853741.9s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.822s, learning 0.195s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0220
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 16.02s
                        Total time: 25006.93s
                               ETA: 853980.0s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.572s, learning 0.232s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0230
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 15.80s
                        Total time: 25022.73s
                               ETA: 854210.7s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.753s, learning 0.161s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0204
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 15.91s
                        Total time: 25038.65s
                               ETA: 854444.9s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.845s, learning 0.208s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0151
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 16.05s
                        Total time: 25054.70s
                               ETA: 854683.7s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.738s, learning 0.194s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0245
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 15.93s
                        Total time: 25070.63s
                               ETA: 854918.2s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1123 steps/s (collection: 14.389s, learning 0.195s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0174
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 14.58s
                        Total time: 25085.21s
                               ETA: 855106.6s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.935s, learning 0.162s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0178
             Mean action noise std: 0.65
                       Mean reward: 20.52
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 8.10s
                        Total time: 25093.31s
                               ETA: 855073.7s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.829s, learning 0.203s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0229
             Mean action noise std: 0.65
                       Mean reward: 20.50
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 8.03s
                        Total time: 25101.34s
                               ETA: 855038.7s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.959s, learning 0.163s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0314
             Mean action noise std: 0.65
                       Mean reward: 20.45
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 8.12s
                        Total time: 25109.47s
                               ETA: 855006.8s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.078s, learning 0.178s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0415
             Mean action noise std: 0.65
                       Mean reward: 20.40
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 8.26s
                        Total time: 25117.72s
                               ETA: 854979.4s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.179s, learning 0.186s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0330
             Mean action noise std: 0.65
                       Mean reward: 20.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 8.37s
                        Total time: 25126.09s
                               ETA: 854955.8s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.845s, learning 0.163s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0390
             Mean action noise std: 0.65
                       Mean reward: 20.36
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 8.01s
                        Total time: 25134.09s
                               ETA: 854920.0s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.145s, learning 0.159s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0380
             Mean action noise std: 0.65
                       Mean reward: 20.34
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 8.30s
                        Total time: 25142.40s
                               ETA: 854894.3s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.130s, learning 0.165s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0330
             Mean action noise std: 0.65
                       Mean reward: 20.32
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 8.29s
                        Total time: 25150.69s
                               ETA: 854868.4s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.216s, learning 0.162s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0455
             Mean action noise std: 0.65
                       Mean reward: 20.32
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 8.38s
                        Total time: 25159.07s
                               ETA: 854845.2s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.153s, learning 0.164s)
               Value function loss: 3.0455
                    Surrogate loss: 0.0277
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 8.32s
                        Total time: 25167.39s
                               ETA: 854820.0s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.975s, learning 0.177s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0225
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 8.15s
                        Total time: 25175.54s
                               ETA: 854789.2s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.022s, learning 0.169s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0167
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 8.19s
                        Total time: 25183.73s
                               ETA: 854759.7s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.048s, learning 0.163s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0182
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 8.21s
                        Total time: 25191.94s
                               ETA: 854731.0s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.827s, learning 0.216s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0189
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 8.04s
                        Total time: 25199.98s
                               ETA: 854696.5s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.167s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0165
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 8.41s
                        Total time: 25208.40s
                               ETA: 854674.6s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.296s, learning 0.219s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0192
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 8.52s
                        Total time: 25216.91s
                               ETA: 854656.2s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.055s, learning 0.161s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0220
             Mean action noise std: 0.65
                       Mean reward: 20.10
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 8.22s
                        Total time: 25225.13s
                               ETA: 854627.7s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.957s, learning 0.171s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0221
             Mean action noise std: 0.65
                       Mean reward: 20.22
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 8.13s
                        Total time: 25233.26s
                               ETA: 854596.2s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.097s, learning 0.170s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0414
             Mean action noise std: 0.65
                       Mean reward: 20.24
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 8.27s
                        Total time: 25241.52s
                               ETA: 854569.4s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.778s, learning 0.179s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0313
             Mean action noise std: 0.65
                       Mean reward: 20.24
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 7.96s
                        Total time: 25249.48s
                               ETA: 854532.1s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.923s, learning 0.160s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0359
             Mean action noise std: 0.65
                       Mean reward: 20.30
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 8.08s
                        Total time: 25257.56s
                               ETA: 854499.1s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.349s, learning 0.263s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0308
             Mean action noise std: 0.65
                       Mean reward: 20.33
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 8.61s
                        Total time: 25266.17s
                               ETA: 854484.0s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.832s, learning 0.172s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0437
             Mean action noise std: 0.65
                       Mean reward: 20.34
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 8.00s
                        Total time: 25274.18s
                               ETA: 854448.4s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.008s, learning 0.167s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0418
             Mean action noise std: 0.65
                       Mean reward: 20.38
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 8.18s
                        Total time: 25282.35s
                               ETA: 854418.6s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.973s, learning 0.164s)
               Value function loss: 7.4818
                    Surrogate loss: 0.0601
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 8.14s
                        Total time: 25290.49s
                               ETA: 854387.5s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.831s, learning 0.256s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0224
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 8.09s
                        Total time: 25298.58s
                               ETA: 854354.7s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.934s, learning 0.160s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0136
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 8.09s
                        Total time: 25306.67s
                               ETA: 854322.2s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1991 steps/s (collection: 7.937s, learning 0.289s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0181
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 8.23s
                        Total time: 25314.90s
                               ETA: 854294.2s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.946s, learning 0.164s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0203
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 8.11s
                        Total time: 25323.01s
                               ETA: 854262.2s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.023s, learning 0.160s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0243
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 8.18s
                        Total time: 25331.19s
                               ETA: 854232.8s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.776s, learning 0.158s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0230
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 7.93s
                        Total time: 25339.12s
                               ETA: 854195.0s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.932s, learning 0.166s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0218
             Mean action noise std: 0.65
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 8.10s
                        Total time: 25347.22s
                               ETA: 854162.7s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.752s, learning 0.252s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0269
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 8.00s
                        Total time: 25355.23s
                               ETA: 854127.2s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1980 steps/s (collection: 7.991s, learning 0.283s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0410
             Mean action noise std: 0.65
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 8.27s
                        Total time: 25363.50s
                               ETA: 854100.9s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.808s, learning 0.233s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0380
             Mean action noise std: 0.65
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 8.04s
                        Total time: 25371.54s
                               ETA: 854066.7s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.349s, learning 0.163s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0423
             Mean action noise std: 0.65
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 8.51s
                        Total time: 25380.05s
                               ETA: 854048.4s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.209s, learning 0.262s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0320
             Mean action noise std: 0.65
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 8.47s
                        Total time: 25388.52s
                               ETA: 854028.8s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.163s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0364
             Mean action noise std: 0.65
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 8.13s
                        Total time: 25396.66s
                               ETA: 853997.8s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.960s, learning 0.158s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0258
             Mean action noise std: 0.65
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 8.12s
                        Total time: 25404.78s
                               ETA: 853966.3s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.107s, learning 0.218s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0461
             Mean action noise std: 0.65
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 8.32s
                        Total time: 25413.10s
                               ETA: 853941.8s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.247s, learning 0.171s)
               Value function loss: 4.0217
                    Surrogate loss: 0.1600
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 8.42s
                        Total time: 25421.52s
                               ETA: 853920.4s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.788s, learning 0.268s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0044
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 8.06s
                        Total time: 25429.58s
                               ETA: 853886.8s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.916s, learning 0.236s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0179
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 8.15s
                        Total time: 25437.73s
                               ETA: 853856.5s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.796s, learning 0.155s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0180
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 7.95s
                        Total time: 25445.68s
                               ETA: 853819.5s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.691s, learning 0.170s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0250
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 7.86s
                        Total time: 25453.54s
                               ETA: 853779.4s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.114s, learning 0.185s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0215
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 8.30s
                        Total time: 25461.84s
                               ETA: 853754.1s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.298s, learning 0.178s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0260
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 8.48s
                        Total time: 25470.32s
                               ETA: 853734.7s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.874s, learning 0.166s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0252
             Mean action noise std: 0.65
                       Mean reward: 22.19
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 8.04s
                        Total time: 25478.36s
                               ETA: 853700.8s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.234s, learning 0.213s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0302
             Mean action noise std: 0.65
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 8.45s
                        Total time: 25486.80s
                               ETA: 853680.4s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.239s, learning 0.198s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0470
             Mean action noise std: 0.65
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 8.44s
                        Total time: 25495.24s
                               ETA: 853659.8s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.293s, learning 0.184s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0408
             Mean action noise std: 0.65
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 8.48s
                        Total time: 25503.72s
                               ETA: 853640.4s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.974s, learning 0.219s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0382
             Mean action noise std: 0.65
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 8.19s
                        Total time: 25511.91s
                               ETA: 853611.6s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.309s, learning 0.160s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0400
             Mean action noise std: 0.65
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 8.47s
                        Total time: 25520.38s
                               ETA: 853592.1s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.091s, learning 0.190s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0438
             Mean action noise std: 0.65
                       Mean reward: 22.15
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 8.28s
                        Total time: 25528.66s
                               ETA: 853566.2s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.200s, learning 0.208s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0401
             Mean action noise std: 0.65
                       Mean reward: 22.18
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 8.41s
                        Total time: 25537.07s
                               ETA: 853544.6s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.124s, learning 0.169s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0395
             Mean action noise std: 0.65
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 8.29s
                        Total time: 25545.36s
                               ETA: 853519.2s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.143s, learning 0.171s)
               Value function loss: 2.0300
                    Surrogate loss: 0.0259
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 8.31s
                        Total time: 25553.67s
                               ETA: 853494.5s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 2094 steps/s (collection: 7.650s, learning 0.171s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0194
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 7.82s
                        Total time: 25561.50s
                               ETA: 853453.3s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.909s, learning 0.181s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0150
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 8.09s
                        Total time: 25569.59s
                               ETA: 853421.2s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.831s, learning 0.159s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0282
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 7.99s
                        Total time: 25577.58s
                               ETA: 853385.7s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.641s, learning 0.232s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0272
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 7.87s
                        Total time: 25585.45s
                               ETA: 853346.3s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.117s, learning 0.165s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0236
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 8.28s
                        Total time: 25593.73s
                               ETA: 853320.6s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.778s, learning 0.158s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0246
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 7.94s
                        Total time: 25601.67s
                               ETA: 853283.4s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.980s, learning 0.156s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0303
             Mean action noise std: 0.65
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 8.14s
                        Total time: 25609.80s
                               ETA: 853252.9s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.075s, learning 0.160s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0306
             Mean action noise std: 0.65
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 8.23s
                        Total time: 25618.04s
                               ETA: 853225.6s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.002s, learning 0.184s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0446
             Mean action noise std: 0.65
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 8.19s
                        Total time: 25626.22s
                               ETA: 853196.8s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.968s, learning 0.158s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0466
             Mean action noise std: 0.65
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 8.13s
                        Total time: 25634.35s
                               ETA: 853166.0s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.685s, learning 0.213s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0410
             Mean action noise std: 0.65
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 7.90s
                        Total time: 25642.25s
                               ETA: 853127.6s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.176s, learning 0.183s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0426
             Mean action noise std: 0.65
                       Mean reward: 21.95
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 8.36s
                        Total time: 25650.61s
                               ETA: 853104.5s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.971s, learning 0.169s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0381
             Mean action noise std: 0.65
                       Mean reward: 21.96
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 8.14s
                        Total time: 25658.75s
                               ETA: 853074.2s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.193s, learning 0.180s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0443
             Mean action noise std: 0.65
                       Mean reward: 21.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 8.37s
                        Total time: 25667.12s
                               ETA: 853051.7s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.297s, learning 0.177s)
               Value function loss: 5.6022
                    Surrogate loss: 0.0228
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 8.47s
                        Total time: 25675.59s
                               ETA: 853032.5s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 2099 steps/s (collection: 7.640s, learning 0.163s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0140
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 7.80s
                        Total time: 25683.40s
                               ETA: 852991.0s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.087s, learning 0.174s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0233
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 8.26s
                        Total time: 25691.66s
                               ETA: 852964.8s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.795s, learning 0.193s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0221
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 7.99s
                        Total time: 25699.64s
                               ETA: 852929.5s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.043s, learning 0.163s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0218
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 8.21s
                        Total time: 25707.85s
                               ETA: 852901.4s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.937s, learning 0.192s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0254
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 8.13s
                        Total time: 25715.98s
                               ETA: 852870.8s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.099s, learning 0.160s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0241
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 8.26s
                        Total time: 25724.24s
                               ETA: 852844.6s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 2079 steps/s (collection: 7.684s, learning 0.193s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0253
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 7.88s
                        Total time: 25732.11s
                               ETA: 852805.7s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1996 steps/s (collection: 7.974s, learning 0.231s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0293
             Mean action noise std: 0.65
                       Mean reward: 21.73
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 8.21s
                        Total time: 25740.32s
                               ETA: 852777.7s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.325s, learning 0.245s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0438
             Mean action noise std: 0.65
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 8.57s
                        Total time: 25748.89s
                               ETA: 852761.7s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.151s, learning 0.158s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0449
             Mean action noise std: 0.65
                       Mean reward: 21.75
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 8.31s
                        Total time: 25757.20s
                               ETA: 852737.2s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.303s, learning 0.171s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0314
             Mean action noise std: 0.65
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 8.47s
                        Total time: 25765.67s
                               ETA: 852718.1s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.923s, learning 0.194s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0391
             Mean action noise std: 0.65
                       Mean reward: 21.74
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 8.12s
                        Total time: 25773.79s
                               ETA: 852687.3s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.319s, learning 0.218s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0389
             Mean action noise std: 0.65
                       Mean reward: 21.72
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 8.54s
                        Total time: 25782.33s
                               ETA: 852670.3s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.214s, learning 0.296s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0348
             Mean action noise std: 0.65
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 8.51s
                        Total time: 25790.84s
                               ETA: 852652.4s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.071s, learning 0.164s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0497
             Mean action noise std: 0.65
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 8.24s
                        Total time: 25799.07s
                               ETA: 852625.5s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.017s, learning 0.204s)
               Value function loss: 4.2606
                    Surrogate loss: 0.1006
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 8.22s
                        Total time: 25807.29s
                               ETA: 852598.1s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1988 steps/s (collection: 7.970s, learning 0.270s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0216
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 8.24s
                        Total time: 25815.53s
                               ETA: 852571.3s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 2105 steps/s (collection: 7.605s, learning 0.176s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0136
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 7.78s
                        Total time: 25823.31s
                               ETA: 852529.4s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.054s, learning 0.162s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0159
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 8.22s
                        Total time: 25831.53s
                               ETA: 852501.9s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.694s, learning 0.189s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0248
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 7.88s
                        Total time: 25839.41s
                               ETA: 852463.5s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.949s, learning 0.200s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0272
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 8.15s
                        Total time: 25847.56s
                               ETA: 852433.7s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.031s, learning 0.165s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0215
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 8.20s
                        Total time: 25855.76s
                               ETA: 852405.6s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.009s, learning 0.176s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0271
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 8.18s
                        Total time: 25863.94s
                               ETA: 852377.2s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.883s, learning 0.161s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0393
             Mean action noise std: 0.65
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 8.04s
                        Total time: 25871.99s
                               ETA: 852344.0s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.726s, learning 0.224s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0453
             Mean action noise std: 0.65
                       Mean reward: 21.47
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 7.95s
                        Total time: 25879.94s
                               ETA: 852307.9s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.046s, learning 0.182s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0380
             Mean action noise std: 0.65
                       Mean reward: 21.48
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 8.23s
                        Total time: 25888.16s
                               ETA: 852280.8s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.048s, learning 0.198s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0425
             Mean action noise std: 0.65
                       Mean reward: 21.40
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 8.25s
                        Total time: 25896.41s
                               ETA: 852254.4s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.755s, learning 0.169s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0390
             Mean action noise std: 0.65
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 7.92s
                        Total time: 25904.33s
                               ETA: 852217.4s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.126s, learning 0.161s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0433
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 8.29s
                        Total time: 25912.62s
                               ETA: 852192.4s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.102s, learning 0.162s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0365
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 8.26s
                        Total time: 25920.88s
                               ETA: 852166.6s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.877s, learning 0.182s)
               Value function loss: 4.7121
                    Surrogate loss: 0.0555
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 8.06s
                        Total time: 25928.94s
                               ETA: 852134.1s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.423s, learning 0.240s)
               Value function loss: 0.2212
                    Surrogate loss: -0.0158
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 8.66s
                        Total time: 25937.61s
                               ETA: 852121.5s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.050s, learning 0.158s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0208
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 8.21s
                        Total time: 25945.81s
                               ETA: 852093.9s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.105s, learning 0.210s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0218
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 8.31s
                        Total time: 25954.13s
                               ETA: 852069.8s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.921s, learning 0.169s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0265
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 8.09s
                        Total time: 25962.22s
                               ETA: 852038.4s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.167s, learning 0.163s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0269
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 8.33s
                        Total time: 25970.55s
                               ETA: 852014.8s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.975s, learning 0.163s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0294
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 8.14s
                        Total time: 25978.69s
                               ETA: 851985.0s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1987 steps/s (collection: 7.989s, learning 0.256s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0196
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 8.24s
                        Total time: 25986.93s
                               ETA: 851958.7s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.183s, learning 0.167s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0243
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 8.35s
                        Total time: 25995.28s
                               ETA: 851935.9s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.931s, learning 0.220s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0414
             Mean action noise std: 0.64
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 8.15s
                        Total time: 26003.43s
                               ETA: 851906.5s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.222s, learning 0.155s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0415
             Mean action noise std: 0.64
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 8.38s
                        Total time: 26011.81s
                               ETA: 851884.5s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.132s, learning 0.160s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0425
             Mean action noise std: 0.64
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 8.29s
                        Total time: 26020.10s
                               ETA: 851859.8s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.245s, learning 0.175s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0350
             Mean action noise std: 0.64
                       Mean reward: 21.40
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 8.42s
                        Total time: 26028.52s
                               ETA: 851839.3s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.031s, learning 0.194s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0345
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 8.23s
                        Total time: 26036.75s
                               ETA: 851812.4s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.115s, learning 0.167s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0368
             Mean action noise std: 0.64
                       Mean reward: 21.51
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 8.28s
                        Total time: 26045.03s
                               ETA: 851787.4s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.137s, learning 0.289s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0480
             Mean action noise std: 0.64
                       Mean reward: 21.54
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 8.43s
                        Total time: 26053.45s
                               ETA: 851767.1s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.238s, learning 0.185s)
               Value function loss: 4.8282
                    Surrogate loss: 0.1184
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 8.42s
                        Total time: 26061.88s
                               ETA: 851746.7s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.202s, learning 0.253s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0214
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 8.45s
                        Total time: 26070.33s
                               ETA: 851727.4s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.804s, learning 0.165s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0187
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 7.97s
                        Total time: 26078.30s
                               ETA: 851692.2s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 2091 steps/s (collection: 7.676s, learning 0.156s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0144
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 7.83s
                        Total time: 26086.13s
                               ETA: 851652.6s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 2190 steps/s (collection: 7.308s, learning 0.172s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0195
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 7.48s
                        Total time: 26093.61s
                               ETA: 851601.5s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.942s, learning 0.175s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0234
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 8.12s
                        Total time: 26101.73s
                               ETA: 851571.2s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.020s, learning 0.183s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0311
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 8.20s
                        Total time: 26109.94s
                               ETA: 851543.7s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.947s, learning 0.191s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0230
             Mean action noise std: 0.64
                       Mean reward: 21.34
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 8.14s
                        Total time: 26118.07s
                               ETA: 851514.1s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.251s, learning 0.172s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0336
             Mean action noise std: 0.64
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 8.42s
                        Total time: 26126.50s
                               ETA: 851493.8s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.042s, learning 0.230s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0374
             Mean action noise std: 0.64
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 8.27s
                        Total time: 26134.77s
                               ETA: 851468.6s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 2113 steps/s (collection: 7.591s, learning 0.160s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0369
             Mean action noise std: 0.64
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 7.75s
                        Total time: 26142.52s
                               ETA: 851426.5s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.731s, learning 0.161s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0330
             Mean action noise std: 0.64
                       Mean reward: 21.32
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 7.89s
                        Total time: 26150.41s
                               ETA: 851388.9s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.046s, learning 0.236s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0346
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 8.28s
                        Total time: 26158.69s
                               ETA: 851364.1s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.991s, learning 0.163s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0368
             Mean action noise std: 0.64
                       Mean reward: 21.27
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 8.15s
                        Total time: 26166.85s
                               ETA: 851335.2s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.152s, learning 0.162s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0357
             Mean action noise std: 0.64
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 8.31s
                        Total time: 26175.16s
                               ETA: 851311.4s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.783s, learning 0.157s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0456
             Mean action noise std: 0.64
                       Mean reward: 21.20
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 7.94s
                        Total time: 26183.10s
                               ETA: 851275.4s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.825s, learning 0.203s)
               Value function loss: 3.1547
                    Surrogate loss: 0.0432
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 8.03s
                        Total time: 26191.13s
                               ETA: 851242.4s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.919s, learning 0.193s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0210
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 8.11s
                        Total time: 26199.24s
                               ETA: 851212.1s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.884s, learning 0.161s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0136
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 8.05s
                        Total time: 26207.29s
                               ETA: 851179.7s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.768s, learning 0.260s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0153
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 8.03s
                        Total time: 26215.31s
                               ETA: 851146.7s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.916s, learning 0.163s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0184
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 8.08s
                        Total time: 26223.39s
                               ETA: 851115.4s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 2121 steps/s (collection: 7.562s, learning 0.162s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0220
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 7.72s
                        Total time: 26231.12s
                               ETA: 851072.5s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.889s, learning 0.175s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0233
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 8.06s
                        Total time: 26239.18s
                               ETA: 851040.8s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.917s, learning 0.160s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0121
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 8.08s
                        Total time: 26247.26s
                               ETA: 851009.5s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.023s, learning 0.156s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0216
             Mean action noise std: 0.64
                       Mean reward: 20.95
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 8.18s
                        Total time: 26255.44s
                               ETA: 850981.5s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.105s, learning 0.159s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0360
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 8.26s
                        Total time: 26263.70s
                               ETA: 850956.2s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.053s, learning 0.163s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0302
             Mean action noise std: 0.64
                       Mean reward: 20.82
               Mean episode length: 124.55
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 8.22s
                        Total time: 26271.92s
                               ETA: 850929.5s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.092s, learning 0.179s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0347
             Mean action noise std: 0.64
                       Mean reward: 20.80
               Mean episode length: 124.55
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 8.27s
                        Total time: 26280.19s
                               ETA: 850904.5s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.938s, learning 0.157s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0311
             Mean action noise std: 0.64
                       Mean reward: 20.82
               Mean episode length: 124.55
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 8.09s
                        Total time: 26288.28s
                               ETA: 850873.8s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.057s, learning 0.163s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0405
             Mean action noise std: 0.64
                       Mean reward: 20.83
               Mean episode length: 124.55
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 8.22s
                        Total time: 26296.50s
                               ETA: 850847.2s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.878s, learning 0.179s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0333
             Mean action noise std: 0.64
                       Mean reward: 20.87
               Mean episode length: 124.55
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 8.06s
                        Total time: 26304.56s
                               ETA: 850815.3s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 2113 steps/s (collection: 7.596s, learning 0.157s)
               Value function loss: 4.9941
                    Surrogate loss: 0.0433
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 7.75s
                        Total time: 26312.31s
                               ETA: 850773.6s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.758s, learning 0.201s)
               Value function loss: 0.0389
                    Surrogate loss: -0.0208
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 7.96s
                        Total time: 26320.27s
                               ETA: 850738.6s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.012s, learning 0.236s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0110
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 8.25s
                        Total time: 26328.52s
                               ETA: 850713.0s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.841s, learning 0.159s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0174
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 8.00s
                        Total time: 26336.52s
                               ETA: 850679.3s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.066s, learning 0.182s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0252
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 8.25s
                        Total time: 26344.77s
                               ETA: 850653.7s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.158s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0166
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 8.18s
                        Total time: 26352.95s
                               ETA: 850625.9s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.920s, learning 0.195s)
               Value function loss: 0.0510
                    Surrogate loss: -0.0233
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 8.12s
                        Total time: 26361.07s
                               ETA: 850596.0s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.061s, learning 0.161s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0192
             Mean action noise std: 0.64
                       Mean reward: 20.93
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 8.22s
                        Total time: 26369.29s
                               ETA: 850569.6s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.998s, learning 0.190s)
               Value function loss: 0.0531
                    Surrogate loss: -0.0155
             Mean action noise std: 0.64
                       Mean reward: 20.94
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 8.19s
                        Total time: 26377.48s
                               ETA: 850542.1s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.212s, learning 0.156s)
               Value function loss: 0.0361
                    Surrogate loss: -0.0293
             Mean action noise std: 0.64
                       Mean reward: 20.95
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 8.37s
                        Total time: 26385.85s
                               ETA: 850520.4s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.074s, learning 0.252s)
               Value function loss: 0.0431
                    Surrogate loss: -0.0327
             Mean action noise std: 0.64
                       Mean reward: 21.02
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 8.33s
                        Total time: 26394.17s
                               ETA: 850497.4s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.797s, learning 0.159s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0367
             Mean action noise std: 0.64
                       Mean reward: 21.04
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 7.96s
                        Total time: 26402.13s
                               ETA: 850462.4s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.933s, learning 0.161s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0306
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 8.09s
                        Total time: 26410.22s
                               ETA: 850431.9s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.027s, learning 0.193s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0321
             Mean action noise std: 0.64
                       Mean reward: 21.02
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 8.22s
                        Total time: 26418.44s
                               ETA: 850405.5s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.054s, learning 0.161s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0281
             Mean action noise std: 0.64
                       Mean reward: 21.02
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 8.21s
                        Total time: 26426.66s
                               ETA: 850378.9s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.709s, learning 0.161s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0461
             Mean action noise std: 0.64
                       Mean reward: 21.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 7.87s
                        Total time: 26434.53s
                               ETA: 850341.3s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.080s, learning 0.156s)
               Value function loss: 4.3896
                    Surrogate loss: 0.0659
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 8.24s
                        Total time: 26442.76s
                               ETA: 850315.4s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.962s, learning 0.181s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0244
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 8.14s
                        Total time: 26450.91s
                               ETA: 850286.6s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.940s, learning 0.159s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0153
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 8.10s
                        Total time: 26459.00s
                               ETA: 850256.4s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.854s, learning 0.174s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0206
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 8.03s
                        Total time: 26467.03s
                               ETA: 850223.9s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 2093 steps/s (collection: 7.666s, learning 0.159s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0267
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 7.82s
                        Total time: 26474.86s
                               ETA: 850184.8s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.831s, learning 0.159s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0251
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 7.99s
                        Total time: 26482.85s
                               ETA: 850151.2s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.833s, learning 0.180s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0308
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 8.01s
                        Total time: 26490.86s
                               ETA: 850118.2s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.983s, learning 0.156s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0242
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 8.14s
                        Total time: 26499.00s
                               ETA: 850089.3s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.810s, learning 0.182s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0345
             Mean action noise std: 0.64
                       Mean reward: 21.33
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 7.99s
                        Total time: 26506.99s
                               ETA: 850055.8s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.062s, learning 0.156s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0437
             Mean action noise std: 0.64
                       Mean reward: 21.22
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 8.22s
                        Total time: 26515.21s
                               ETA: 850029.5s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.959s, learning 0.158s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0421
             Mean action noise std: 0.64
                       Mean reward: 21.23
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 8.12s
                        Total time: 26523.33s
                               ETA: 849999.9s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.868s, learning 0.240s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0422
             Mean action noise std: 0.64
                       Mean reward: 21.25
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 8.11s
                        Total time: 26531.44s
                               ETA: 849970.1s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.057s, learning 0.219s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0408
             Mean action noise std: 0.64
                       Mean reward: 21.25
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 8.28s
                        Total time: 26539.71s
                               ETA: 849945.7s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.112s, learning 0.161s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0395
             Mean action noise std: 0.64
                       Mean reward: 21.24
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 8.27s
                        Total time: 26547.99s
                               ETA: 849921.2s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.795s, learning 0.162s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0343
             Mean action noise std: 0.64
                       Mean reward: 21.23
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 7.96s
                        Total time: 26555.94s
                               ETA: 849886.6s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.756s, learning 0.162s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0368
             Mean action noise std: 0.64
                       Mean reward: 21.28
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 7.92s
                        Total time: 26563.86s
                               ETA: 849850.7s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.979s, learning 0.159s)
               Value function loss: 1.8398
                    Surrogate loss: 0.0245
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 8.14s
                        Total time: 26572.00s
                               ETA: 849821.9s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.830s, learning 0.210s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0190
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 8.04s
                        Total time: 26580.04s
                               ETA: 849790.0s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.753s, learning 0.157s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0152
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 7.91s
                        Total time: 26587.95s
                               ETA: 849754.0s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.921s, learning 0.166s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0220
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 8.09s
                        Total time: 26596.04s
                               ETA: 849723.6s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 2079 steps/s (collection: 7.679s, learning 0.199s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0253
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 7.88s
                        Total time: 26603.91s
                               ETA: 849686.6s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.927s, learning 0.158s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0252
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 8.09s
                        Total time: 26612.00s
                               ETA: 849656.2s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.136s, learning 0.181s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0211
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 8.32s
                        Total time: 26620.32s
                               ETA: 849633.2s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.708s, learning 0.188s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0181
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 7.90s
                        Total time: 26628.21s
                               ETA: 849596.8s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.098s, learning 0.196s)
               Value function loss: 0.0458
                    Surrogate loss: -0.0396
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 8.29s
                        Total time: 26636.51s
                               ETA: 849573.1s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.814s, learning 0.231s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0432
             Mean action noise std: 0.64
                       Mean reward: 21.12
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 8.04s
                        Total time: 26644.55s
                               ETA: 849541.5s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.051s, learning 0.178s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0390
             Mean action noise std: 0.64
                       Mean reward: 21.14
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 8.23s
                        Total time: 26652.78s
                               ETA: 849515.8s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.020s, learning 0.167s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0344
             Mean action noise std: 0.64
                       Mean reward: 21.21
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 8.19s
                        Total time: 26660.97s
                               ETA: 849488.7s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.740s, learning 0.254s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0393
             Mean action noise std: 0.64
                       Mean reward: 21.21
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 7.99s
                        Total time: 26668.96s
                               ETA: 849455.5s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.090s, learning 0.160s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0422
             Mean action noise std: 0.64
                       Mean reward: 21.18
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 8.25s
                        Total time: 26677.21s
                               ETA: 849430.4s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.894s, learning 0.193s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0427
             Mean action noise std: 0.64
                       Mean reward: 21.19
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 8.09s
                        Total time: 26685.30s
                               ETA: 849400.2s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.808s, learning 0.184s)
               Value function loss: 5.5180
                    Surrogate loss: 0.0245
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 7.99s
                        Total time: 26693.29s
                               ETA: 849367.0s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.670s, learning 0.181s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0235
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 7.85s
                        Total time: 26701.14s
                               ETA: 849329.3s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.805s, learning 0.166s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0163
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 7.97s
                        Total time: 26709.11s
                               ETA: 849295.5s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.789s, learning 0.168s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0213
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 7.96s
                        Total time: 26717.07s
                               ETA: 849261.2s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.760s, learning 0.156s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0231
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 7.92s
                        Total time: 26724.98s
                               ETA: 849225.6s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.815s, learning 0.157s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0261
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 7.97s
                        Total time: 26732.96s
                               ETA: 849191.8s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.881s, learning 0.163s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0306
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 8.04s
                        Total time: 26741.00s
                               ETA: 849160.3s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.725s, learning 0.157s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0197
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 7.88s
                        Total time: 26748.88s
                               ETA: 849123.7s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.826s, learning 0.156s)
               Value function loss: 0.0890
                    Surrogate loss: -0.0293
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 7.98s
                        Total time: 26756.86s
                               ETA: 849090.3s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.959s, learning 0.156s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0403
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 8.11s
                        Total time: 26764.98s
                               ETA: 849061.1s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.162s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0385
             Mean action noise std: 0.64
                       Mean reward: 21.40
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 8.13s
                        Total time: 26773.11s
                               ETA: 849032.6s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.710s, learning 0.256s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0290
             Mean action noise std: 0.64
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 7.97s
                        Total time: 26781.08s
                               ETA: 848998.7s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.834s, learning 0.157s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0358
             Mean action noise std: 0.64
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 7.99s
                        Total time: 26789.07s
                               ETA: 848965.7s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.127s, learning 0.181s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0395
             Mean action noise std: 0.64
                       Mean reward: 21.46
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 8.31s
                        Total time: 26797.38s
                               ETA: 848942.7s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.599s, learning 0.271s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0335
             Mean action noise std: 0.64
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 7.87s
                        Total time: 26805.25s
                               ETA: 848905.8s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.138s, learning 0.166s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0442
             Mean action noise std: 0.64
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 8.30s
                        Total time: 26813.55s
                               ETA: 848882.7s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.066s, learning 0.169s)
               Value function loss: 4.0451
                    Surrogate loss: 0.0913
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 8.24s
                        Total time: 26821.79s
                               ETA: 848857.4s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.693s, learning 0.160s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0237
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 7.85s
                        Total time: 26829.64s
                               ETA: 848820.1s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.917s, learning 0.160s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0158
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 8.08s
                        Total time: 26837.72s
                               ETA: 848789.8s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.898s, learning 0.277s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0143
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 8.17s
                        Total time: 26845.89s
                               ETA: 848762.7s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.123s, learning 0.159s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0210
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 8.28s
                        Total time: 26854.17s
                               ETA: 848739.0s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.810s, learning 0.161s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0289
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 7.97s
                        Total time: 26862.14s
                               ETA: 848705.4s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.797s, learning 0.222s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0187
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 8.02s
                        Total time: 26870.16s
                               ETA: 848673.4s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.199s, learning 0.157s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0190
             Mean action noise std: 0.64
                       Mean reward: 21.35
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 8.36s
                        Total time: 26878.52s
                               ETA: 848652.0s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.003s, learning 0.247s)
               Value function loss: 0.0441
                    Surrogate loss: -0.0302
             Mean action noise std: 0.64
                       Mean reward: 21.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 8.25s
                        Total time: 26886.77s
                               ETA: 848627.3s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.215s, learning 0.182s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0408
             Mean action noise std: 0.64
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 8.40s
                        Total time: 26895.17s
                               ETA: 848607.3s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.161s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0313
             Mean action noise std: 0.64
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 8.12s
                        Total time: 26903.29s
                               ETA: 848578.7s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.932s, learning 0.161s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0318
             Mean action noise std: 0.64
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 8.09s
                        Total time: 26911.38s
                               ETA: 848549.0s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.284s, learning 0.230s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0320
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 8.51s
                        Total time: 26919.90s
                               ETA: 848532.7s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.151s, learning 0.162s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0331
             Mean action noise std: 0.64
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 8.31s
                        Total time: 26928.21s
                               ETA: 848510.1s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.071s, learning 0.174s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0358
             Mean action noise std: 0.64
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 8.24s
                        Total time: 26936.46s
                               ETA: 848485.3s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.970s, learning 0.161s)
               Value function loss: 4.6075
                    Surrogate loss: 0.0624
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 8.13s
                        Total time: 26944.59s
                               ETA: 848456.9s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.067s, learning 0.189s)
               Value function loss: 0.6738
                    Surrogate loss: 0.0063
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 8.26s
                        Total time: 26952.84s
                               ETA: 848432.5s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.874s, learning 0.160s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0169
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 8.03s
                        Total time: 26960.88s
                               ETA: 848401.0s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 2104 steps/s (collection: 7.625s, learning 0.161s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0137
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 7.79s
                        Total time: 26968.66s
                               ETA: 848361.8s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.105s, learning 0.176s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0197
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 8.28s
                        Total time: 26976.94s
                               ETA: 848338.2s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.047s, learning 0.158s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0288
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 8.21s
                        Total time: 26985.15s
                               ETA: 848312.2s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 2082 steps/s (collection: 7.680s, learning 0.188s)
               Value function loss: 0.0488
                    Surrogate loss: -0.0210
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 7.87s
                        Total time: 26993.02s
                               ETA: 848275.7s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.736s, learning 0.158s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0224
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 7.89s
                        Total time: 27000.91s
                               ETA: 848240.0s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.945s, learning 0.161s)
               Value function loss: 0.0620
                    Surrogate loss: -0.0110
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 8.11s
                        Total time: 27009.02s
                               ETA: 848210.9s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.931s, learning 0.159s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0295
             Mean action noise std: 0.64
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 8.09s
                        Total time: 27017.11s
                               ETA: 848181.4s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1982 steps/s (collection: 7.979s, learning 0.284s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0238
             Mean action noise std: 0.64
                       Mean reward: 21.69
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 8.26s
                        Total time: 27025.37s
                               ETA: 848157.2s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.131s, learning 0.160s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0270
             Mean action noise std: 0.64
                       Mean reward: 21.70
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 8.29s
                        Total time: 27033.66s
                               ETA: 848134.0s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.132s, learning 0.161s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0216
             Mean action noise std: 0.64
                       Mean reward: 21.66
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 8.29s
                        Total time: 27041.95s
                               ETA: 848110.9s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.123s, learning 0.193s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0254
             Mean action noise std: 0.64
                       Mean reward: 21.67
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 8.32s
                        Total time: 27050.27s
                               ETA: 848088.5s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.892s, learning 0.158s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0234
             Mean action noise std: 0.64
                       Mean reward: 21.66
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 8.05s
                        Total time: 27058.32s
                               ETA: 848057.8s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.935s, learning 0.165s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0363
             Mean action noise std: 0.64
                       Mean reward: 21.66
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 8.10s
                        Total time: 27066.42s
                               ETA: 848028.6s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.820s, learning 0.160s)
               Value function loss: 6.1156
                    Surrogate loss: 0.0731
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 7.98s
                        Total time: 27074.40s
                               ETA: 847995.7s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 2095 steps/s (collection: 7.662s, learning 0.158s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0229
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 7.82s
                        Total time: 27082.22s
                               ETA: 847957.8s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.151s, learning 0.178s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0138
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 8.33s
                        Total time: 27090.55s
                               ETA: 847935.9s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.914s, learning 0.159s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0114
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 8.07s
                        Total time: 27098.62s
                               ETA: 847906.0s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.211s, learning 0.257s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0227
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 8.47s
                        Total time: 27107.09s
                               ETA: 847888.4s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.722s, learning 0.161s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0239
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 7.88s
                        Total time: 27114.97s
                               ETA: 847852.5s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.838s, learning 0.222s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0275
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 8.06s
                        Total time: 27123.03s
                               ETA: 847822.3s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.712s, learning 0.185s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0240
             Mean action noise std: 0.64
                       Mean reward: 21.42
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 7.90s
                        Total time: 27130.93s
                               ETA: 847786.9s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.951s, learning 0.159s)
               Value function loss: 0.0659
                    Surrogate loss: -0.0234
             Mean action noise std: 0.64
                       Mean reward: 21.40
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 8.11s
                        Total time: 27139.04s
                               ETA: 847758.2s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.911s, learning 0.215s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0318
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 8.13s
                        Total time: 27147.17s
                               ETA: 847730.0s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.017s, learning 0.161s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0338
             Mean action noise std: 0.64
                       Mean reward: 21.38
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 8.18s
                        Total time: 27155.34s
                               ETA: 847703.4s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.000s, learning 0.198s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0249
             Mean action noise std: 0.64
                       Mean reward: 21.39
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 8.20s
                        Total time: 27163.54s
                               ETA: 847677.5s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.838s, learning 0.182s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0254
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 8.02s
                        Total time: 27171.56s
                               ETA: 847646.0s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.162s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0298
             Mean action noise std: 0.64
                       Mean reward: 21.40
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 8.17s
                        Total time: 27179.73s
                               ETA: 847619.2s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.168s, learning 0.161s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0247
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 8.33s
                        Total time: 27188.06s
                               ETA: 847597.4s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 2102 steps/s (collection: 7.631s, learning 0.160s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0409
             Mean action noise std: 0.64
                       Mean reward: 21.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 7.79s
                        Total time: 27195.85s
                               ETA: 847558.8s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.092s, learning 0.160s)
               Value function loss: 3.7816
                    Surrogate loss: 0.0398
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 8.25s
                        Total time: 27204.10s
                               ETA: 847534.6s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 2093 steps/s (collection: 7.670s, learning 0.156s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0223
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 7.83s
                        Total time: 27211.93s
                               ETA: 847497.2s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.873s, learning 0.189s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0211
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 8.06s
                        Total time: 27219.99s
                               ETA: 847467.1s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.916s, learning 0.181s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0182
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 8.10s
                        Total time: 27228.09s
                               ETA: 847438.2s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.080s, learning 0.157s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0259
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 8.24s
                        Total time: 27236.32s
                               ETA: 847413.6s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.955s, learning 0.166s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0293
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 8.12s
                        Total time: 27244.45s
                               ETA: 847385.4s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.697s, learning 0.160s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0253
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 7.86s
                        Total time: 27252.30s
                               ETA: 847349.0s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.753s, learning 0.160s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0187
             Mean action noise std: 0.64
                       Mean reward: 21.03
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 7.91s
                        Total time: 27260.22s
                               ETA: 847314.3s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.959s, learning 0.159s)
               Value function loss: 0.0604
                    Surrogate loss: -0.0304
             Mean action noise std: 0.64
                       Mean reward: 21.08
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 8.12s
                        Total time: 27268.33s
                               ETA: 847286.1s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.087s, learning 0.188s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0374
             Mean action noise std: 0.64
                       Mean reward: 21.10
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 8.28s
                        Total time: 27276.61s
                               ETA: 847262.7s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.108s, learning 0.164s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0311
             Mean action noise std: 0.64
                       Mean reward: 21.08
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 8.27s
                        Total time: 27284.88s
                               ETA: 847239.3s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.758s, learning 0.176s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0340
             Mean action noise std: 0.64
                       Mean reward: 21.09
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 7.93s
                        Total time: 27292.81s
                               ETA: 847205.3s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.950s, learning 0.157s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0230
             Mean action noise std: 0.64
                       Mean reward: 21.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 8.11s
                        Total time: 27300.92s
                               ETA: 847176.8s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.238s, learning 0.194s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0388
             Mean action noise std: 0.64
                       Mean reward: 21.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 8.43s
                        Total time: 27309.35s
                               ETA: 847158.4s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.160s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0271
             Mean action noise std: 0.64
                       Mean reward: 21.05
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 8.31s
                        Total time: 27317.66s
                               ETA: 847136.0s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.135s, learning 0.160s)
               Value function loss: 8.2168
                    Surrogate loss: 0.0558
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 8.29s
                        Total time: 27325.95s
                               ETA: 847113.3s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.892s, learning 0.193s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0246
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 8.09s
                        Total time: 27334.04s
                               ETA: 847084.2s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.850s, learning 0.161s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0122
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 8.01s
                        Total time: 27342.05s
                               ETA: 847052.7s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.796s, learning 0.161s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0127
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 7.96s
                        Total time: 27350.01s
                               ETA: 847019.6s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.877s, learning 0.236s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0177
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 8.11s
                        Total time: 27358.12s
                               ETA: 846991.3s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.816s, learning 0.259s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0234
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 8.07s
                        Total time: 27366.20s
                               ETA: 846961.9s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.763s, learning 0.194s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0247
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 7.96s
                        Total time: 27374.15s
                               ETA: 846928.9s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.853s, learning 0.221s)
               Value function loss: 0.0443
                    Surrogate loss: -0.0183
             Mean action noise std: 0.64
                       Mean reward: 21.31
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 8.07s
                        Total time: 27382.23s
                               ETA: 846899.4s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.116s, learning 0.248s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0241
             Mean action noise std: 0.64
                       Mean reward: 21.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 8.36s
                        Total time: 27390.59s
                               ETA: 846879.0s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.247s, learning 0.241s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0389
             Mean action noise std: 0.64
                       Mean reward: 21.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 8.49s
                        Total time: 27399.08s
                               ETA: 846862.4s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.949s, learning 0.159s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0346
             Mean action noise std: 0.64
                       Mean reward: 21.44
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 8.11s
                        Total time: 27407.19s
                               ETA: 846834.0s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.095s, learning 0.187s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0361
             Mean action noise std: 0.64
                       Mean reward: 21.54
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 8.28s
                        Total time: 27415.47s
                               ETA: 846811.1s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.299s, learning 0.179s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0332
             Mean action noise std: 0.64
                       Mean reward: 21.57
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 8.48s
                        Total time: 27423.95s
                               ETA: 846794.2s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.762s, learning 0.195s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0383
             Mean action noise std: 0.64
                       Mean reward: 21.59
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 7.96s
                        Total time: 27431.90s
                               ETA: 846761.2s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.782s, learning 0.155s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0346
             Mean action noise std: 0.64
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 7.94s
                        Total time: 27439.84s
                               ETA: 846727.6s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 2117 steps/s (collection: 7.582s, learning 0.157s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0485
             Mean action noise std: 0.64
                       Mean reward: 21.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 7.74s
                        Total time: 27447.58s
                               ETA: 846688.0s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.009s, learning 0.155s)
               Value function loss: 4.0455
                    Surrogate loss: 0.1320
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 8.16s
                        Total time: 27455.75s
                               ETA: 846661.4s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.832s, learning 0.169s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0105
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 8.00s
                        Total time: 27463.75s
                               ETA: 846629.9s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.739s, learning 0.159s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0216
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 7.90s
                        Total time: 27471.64s
                               ETA: 846595.2s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.131s, learning 0.183s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0189
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 8.31s
                        Total time: 27479.96s
                               ETA: 846573.3s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.718s, learning 0.212s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0200
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 7.93s
                        Total time: 27487.89s
                               ETA: 846539.6s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.896s, learning 0.158s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0326
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 8.05s
                        Total time: 27495.94s
                               ETA: 846509.7s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.722s, learning 0.162s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0365
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 7.88s
                        Total time: 27503.83s
                               ETA: 846474.6s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.664s, learning 0.260s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0241
             Mean action noise std: 0.64
                       Mean reward: 21.86
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 7.92s
                        Total time: 27511.75s
                               ETA: 846440.8s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.309s, learning 0.179s)
               Value function loss: 0.0463
                    Surrogate loss: -0.0357
             Mean action noise std: 0.64
                       Mean reward: 21.87
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 8.49s
                        Total time: 27520.24s
                               ETA: 846424.3s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.160s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0404
             Mean action noise std: 0.64
                       Mean reward: 21.84
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 8.13s
                        Total time: 27528.37s
                               ETA: 846396.9s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.872s, learning 0.159s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0398
             Mean action noise std: 0.64
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 8.03s
                        Total time: 27536.40s
                               ETA: 846366.4s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.836s, learning 0.163s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0323
             Mean action noise std: 0.64
                       Mean reward: 21.97
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 8.00s
                        Total time: 27544.40s
                               ETA: 846334.9s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.406s, learning 0.212s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0370
             Mean action noise std: 0.64
                       Mean reward: 21.90
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 8.62s
                        Total time: 27553.02s
                               ETA: 846322.4s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.933s, learning 0.160s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0349
             Mean action noise std: 0.64
                       Mean reward: 21.89
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 8.09s
                        Total time: 27561.11s
                               ETA: 846293.9s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.159s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0290
             Mean action noise std: 0.64
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 8.18s
                        Total time: 27569.29s
                               ETA: 846268.1s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.056s, learning 0.158s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0368
             Mean action noise std: 0.64
                       Mean reward: 22.00
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 8.21s
                        Total time: 27577.51s
                               ETA: 846243.2s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.850s, learning 0.180s)
               Value function loss: 2.4271
                    Surrogate loss: 0.0413
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 8.03s
                        Total time: 27585.54s
                               ETA: 846212.8s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.083s, learning 0.260s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0163
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 8.34s
                        Total time: 27593.88s
                               ETA: 846191.9s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.847s, learning 0.181s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0178
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 8.03s
                        Total time: 27601.91s
                               ETA: 846161.5s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.864s, learning 0.199s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0180
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 8.06s
                        Total time: 27609.97s
                               ETA: 846132.0s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.971s, learning 0.160s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0235
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 8.13s
                        Total time: 27618.10s
                               ETA: 846104.7s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.881s, learning 0.171s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0300
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 8.05s
                        Total time: 27626.16s
                               ETA: 846075.0s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.895s, learning 0.245s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0371
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 8.14s
                        Total time: 27634.30s
                               ETA: 846048.0s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.839s, learning 0.173s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0319
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 8.01s
                        Total time: 27642.31s
                               ETA: 846017.1s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.875s, learning 0.158s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0372
             Mean action noise std: 0.64
                       Mean reward: 22.17
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 8.03s
                        Total time: 27650.34s
                               ETA: 845986.8s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.998s, learning 0.161s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0385
             Mean action noise std: 0.64
                       Mean reward: 22.16
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 8.16s
                        Total time: 27658.50s
                               ETA: 845960.4s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.002s, learning 0.162s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0293
             Mean action noise std: 0.64
                       Mean reward: 22.12
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 8.16s
                        Total time: 27666.66s
                               ETA: 845934.2s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.921s, learning 0.162s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0245
             Mean action noise std: 0.64
                       Mean reward: 22.22
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 8.08s
                        Total time: 27674.75s
                               ETA: 845905.5s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.883s, learning 0.165s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0312
             Mean action noise std: 0.64
                       Mean reward: 22.13
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 8.05s
                        Total time: 27682.79s
                               ETA: 845875.8s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.159s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0297
             Mean action noise std: 0.64
                       Mean reward: 22.11
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 8.13s
                        Total time: 27690.93s
                               ETA: 845848.6s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.062s, learning 0.185s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0411
             Mean action noise std: 0.64
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 8.25s
                        Total time: 27699.17s
                               ETA: 845824.9s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.003s, learning 0.198s)
               Value function loss: 8.3406
                    Surrogate loss: 0.0346
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 8.20s
                        Total time: 27707.37s
                               ETA: 845799.9s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1984 steps/s (collection: 7.971s, learning 0.284s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0179
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 8.25s
                        Total time: 27715.63s
                               ETA: 845776.5s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.954s, learning 0.158s)
               Value function loss: 0.0317
                    Surrogate loss: -0.0147
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 8.11s
                        Total time: 27723.74s
                               ETA: 845748.7s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.798s, learning 0.158s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0134
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 7.96s
                        Total time: 27731.69s
                               ETA: 845716.2s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.017s, learning 0.281s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0120
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 8.30s
                        Total time: 27739.99s
                               ETA: 845694.2s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.876s, learning 0.228s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0257
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 8.10s
                        Total time: 27748.10s
                               ETA: 845666.3s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.841s, learning 0.156s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0258
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 8.00s
                        Total time: 27756.09s
                               ETA: 845635.1s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.871s, learning 0.157s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0267
             Mean action noise std: 0.64
                       Mean reward: 22.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 8.03s
                        Total time: 27764.12s
                               ETA: 845604.9s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.843s, learning 0.163s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0229
             Mean action noise std: 0.64
                       Mean reward: 22.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 8.01s
                        Total time: 27772.13s
                               ETA: 845574.0s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.077s, learning 0.162s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0327
             Mean action noise std: 0.64
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 8.24s
                        Total time: 27780.37s
                               ETA: 845550.2s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.793s, learning 0.255s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0332
             Mean action noise std: 0.64
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 8.05s
                        Total time: 27788.41s
                               ETA: 845520.6s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.055s, learning 0.158s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0243
             Mean action noise std: 0.64
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 8.21s
                        Total time: 27796.63s
                               ETA: 845496.0s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.027s, learning 0.197s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0242
             Mean action noise std: 0.64
                       Mean reward: 22.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 8.22s
                        Total time: 27804.85s
                               ETA: 845471.8s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.226s, learning 0.275s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0253
             Mean action noise std: 0.64
                       Mean reward: 22.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 8.50s
                        Total time: 27813.35s
                               ETA: 845456.1s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.956s, learning 0.159s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0191
             Mean action noise std: 0.64
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 8.11s
                        Total time: 27821.47s
                               ETA: 845428.5s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.088s, learning 0.162s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0434
             Mean action noise std: 0.64
                       Mean reward: 22.47
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 8.25s
                        Total time: 27829.72s
                               ETA: 845405.2s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.904s, learning 0.188s)
               Value function loss: 5.5852
                    Surrogate loss: 0.1240
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 8.09s
                        Total time: 27837.81s
                               ETA: 845377.0s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.886s, learning 0.196s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0260
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 8.08s
                        Total time: 27845.89s
                               ETA: 845348.5s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.855s, learning 0.195s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0159
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 8.05s
                        Total time: 27853.94s
                               ETA: 845319.1s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.858s, learning 0.156s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0128
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 8.01s
                        Total time: 27861.96s
                               ETA: 845288.6s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.830s, learning 0.160s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0199
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 7.99s
                        Total time: 27869.95s
                               ETA: 845257.4s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.875s, learning 0.167s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0344
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 8.04s
                        Total time: 27877.99s
                               ETA: 845227.7s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.838s, learning 0.170s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0296
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 8.01s
                        Total time: 27886.00s
                               ETA: 845197.1s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.014s, learning 0.160s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0377
             Mean action noise std: 0.64
                       Mean reward: 22.66
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 8.17s
                        Total time: 27894.17s
                               ETA: 845171.5s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.076s, learning 0.158s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0316
             Mean action noise std: 0.64
                       Mean reward: 22.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 8.23s
                        Total time: 27902.40s
                               ETA: 845147.7s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.965s, learning 0.160s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0388
             Mean action noise std: 0.64
                       Mean reward: 22.67
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 8.12s
                        Total time: 27910.53s
                               ETA: 845120.6s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.960s, learning 0.159s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0305
             Mean action noise std: 0.64
                       Mean reward: 22.71
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 8.12s
                        Total time: 27918.65s
                               ETA: 845093.4s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.133s, learning 0.168s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0309
             Mean action noise std: 0.64
                       Mean reward: 22.68
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 8.30s
                        Total time: 27926.95s
                               ETA: 845071.7s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.146s, learning 0.160s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0306
             Mean action noise std: 0.64
                       Mean reward: 22.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 8.31s
                        Total time: 27935.25s
                               ETA: 845050.1s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.968s, learning 0.159s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0360
             Mean action noise std: 0.64
                       Mean reward: 22.65
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 8.13s
                        Total time: 27943.38s
                               ETA: 845023.1s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.780s, learning 0.204s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0316
             Mean action noise std: 0.64
                       Mean reward: 22.60
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 7.98s
                        Total time: 27951.36s
                               ETA: 844991.9s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.975s, learning 0.161s)
               Value function loss: 8.3945
                    Surrogate loss: 0.0657
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 8.14s
                        Total time: 27959.50s
                               ETA: 844965.2s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.969s, learning 0.158s)
               Value function loss: 0.0584
                    Surrogate loss: -0.0267
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 8.13s
                        Total time: 27967.63s
                               ETA: 844938.3s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.103s, learning 0.244s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0216
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 8.35s
                        Total time: 27975.97s
                               ETA: 844918.0s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.054s, learning 0.198s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0168
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 8.25s
                        Total time: 27984.22s
                               ETA: 844894.9s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.096s, learning 0.182s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0116
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 8.28s
                        Total time: 27992.50s
                               ETA: 844872.5s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.116s, learning 0.216s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0249
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 8.33s
                        Total time: 28000.83s
                               ETA: 844851.9s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.013s, learning 0.174s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0348
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 8.19s
                        Total time: 28009.02s
                               ETA: 844826.8s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.058s, learning 0.180s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0383
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 8.24s
                        Total time: 28017.26s
                               ETA: 844803.3s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.881s, learning 0.160s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0288
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 8.04s
                        Total time: 28025.30s
                               ETA: 844773.8s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.909s, learning 0.178s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0325
             Mean action noise std: 0.64
                       Mean reward: 23.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 8.09s
                        Total time: 28033.39s
                               ETA: 844745.8s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.982s, learning 0.188s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0356
             Mean action noise std: 0.64
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 8.17s
                        Total time: 28041.56s
                               ETA: 844720.3s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.920s, learning 0.161s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0328
             Mean action noise std: 0.64
                       Mean reward: 23.02
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 8.08s
                        Total time: 28049.64s
                               ETA: 844692.1s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.432s, learning 0.197s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0172
             Mean action noise std: 0.64
                       Mean reward: 23.00
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 8.63s
                        Total time: 28058.27s
                               ETA: 844680.4s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.212s, learning 0.161s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0275
             Mean action noise std: 0.64
                       Mean reward: 23.00
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 8.37s
                        Total time: 28066.64s
                               ETA: 844661.0s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.018s, learning 0.195s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0224
             Mean action noise std: 0.64
                       Mean reward: 22.95
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 8.21s
                        Total time: 28074.85s
                               ETA: 844636.8s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.717s, learning 0.166s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0415
             Mean action noise std: 0.64
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 7.88s
                        Total time: 28082.74s
                               ETA: 844602.7s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.095s, learning 0.158s)
               Value function loss: 6.0285
                    Surrogate loss: 0.2195
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 8.25s
                        Total time: 28090.99s
                               ETA: 844579.8s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 2096 steps/s (collection: 7.656s, learning 0.159s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0137
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 7.81s
                        Total time: 28098.81s
                               ETA: 844543.6s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.974s, learning 0.158s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0190
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 8.13s
                        Total time: 28106.94s
                               ETA: 844517.1s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.998s, learning 0.160s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0092
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 8.16s
                        Total time: 28115.10s
                               ETA: 844491.3s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.872s, learning 0.217s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0154
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 8.09s
                        Total time: 28123.19s
                               ETA: 844463.4s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.968s, learning 0.176s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0385
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 8.14s
                        Total time: 28131.33s
                               ETA: 844437.2s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.944s, learning 0.196s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0423
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 8.14s
                        Total time: 28139.47s
                               ETA: 844410.9s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.769s, learning 0.165s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0404
             Mean action noise std: 0.64
                       Mean reward: 22.51
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 7.93s
                        Total time: 28147.40s
                               ETA: 844378.5s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.161s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0391
             Mean action noise std: 0.64
                       Mean reward: 22.52
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 8.13s
                        Total time: 28155.53s
                               ETA: 844351.7s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1991 steps/s (collection: 7.960s, learning 0.268s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0339
             Mean action noise std: 0.64
                       Mean reward: 22.55
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 8.23s
                        Total time: 28163.76s
                               ETA: 844328.1s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.130s, learning 0.159s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0326
             Mean action noise std: 0.64
                       Mean reward: 22.56
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 8.29s
                        Total time: 28172.05s
                               ETA: 844306.3s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.129s, learning 0.221s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0224
             Mean action noise std: 0.64
                       Mean reward: 22.55
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 8.35s
                        Total time: 28180.40s
                               ETA: 844286.4s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.962s, learning 0.162s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0289
             Mean action noise std: 0.64
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 8.12s
                        Total time: 28188.52s
                               ETA: 844259.7s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.927s, learning 0.160s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0314
             Mean action noise std: 0.64
                       Mean reward: 22.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 8.09s
                        Total time: 28196.61s
                               ETA: 844231.9s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.825s, learning 0.163s)
               Value function loss: 0.0291
                    Surrogate loss: -0.0213
             Mean action noise std: 0.64
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 7.99s
                        Total time: 28204.60s
                               ETA: 844201.1s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.125s, learning 0.155s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0386
             Mean action noise std: 0.64
                       Mean reward: 22.57
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 8.28s
                        Total time: 28212.88s
                               ETA: 844179.1s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.135s, learning 0.171s)
               Value function loss: 4.2632
                    Surrogate loss: 0.0641
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 8.31s
                        Total time: 28221.18s
                               ETA: 844157.9s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.109s, learning 0.160s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0213
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 8.27s
                        Total time: 28229.45s
                               ETA: 844135.6s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.002s, learning 0.162s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0083
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 8.16s
                        Total time: 28237.61s
                               ETA: 844110.1s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.135s, learning 0.169s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0141
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 8.30s
                        Total time: 28245.92s
                               ETA: 844088.9s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.927s, learning 0.228s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0209
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 8.15s
                        Total time: 28254.07s
                               ETA: 844063.2s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.986s, learning 0.160s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0392
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 8.15s
                        Total time: 28262.22s
                               ETA: 844037.2s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.958s, learning 0.160s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0440
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 8.12s
                        Total time: 28270.34s
                               ETA: 844010.4s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.058s, learning 0.161s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0345
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 8.22s
                        Total time: 28278.55s
                               ETA: 843986.6s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.060s, learning 0.272s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0301
             Mean action noise std: 0.64
                       Mean reward: 23.03
               Mean episode length: 124.38
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 8.33s
                        Total time: 28286.89s
                               ETA: 843966.2s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.932s, learning 0.188s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0331
             Mean action noise std: 0.64
                       Mean reward: 23.02
               Mean episode length: 124.38
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 8.12s
                        Total time: 28295.01s
                               ETA: 843939.6s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.812s, learning 0.160s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0295
             Mean action noise std: 0.64
                       Mean reward: 23.07
               Mean episode length: 124.38
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 7.97s
                        Total time: 28302.98s
                               ETA: 843908.5s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.029s, learning 0.220s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0329
             Mean action noise std: 0.64
                       Mean reward: 23.06
               Mean episode length: 124.38
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 8.25s
                        Total time: 28311.23s
                               ETA: 843885.7s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.035s, learning 0.166s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0232
             Mean action noise std: 0.64
                       Mean reward: 23.09
               Mean episode length: 124.38
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 8.20s
                        Total time: 28319.43s
                               ETA: 843861.4s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.851s, learning 0.252s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0323
             Mean action noise std: 0.64
                       Mean reward: 23.09
               Mean episode length: 124.38
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 8.10s
                        Total time: 28327.53s
                               ETA: 843834.3s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.156s, learning 0.283s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0294
             Mean action noise std: 0.64
                       Mean reward: 23.04
               Mean episode length: 124.38
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 8.44s
                        Total time: 28335.97s
                               ETA: 843817.1s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.173s, learning 0.160s)
               Value function loss: 9.9863
                    Surrogate loss: 0.0714
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 8.33s
                        Total time: 28344.30s
                               ETA: 843796.8s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.820s, learning 0.191s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0188
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 8.01s
                        Total time: 28352.31s
                               ETA: 843767.0s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.164s, learning 0.213s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0140
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 8.38s
                        Total time: 28360.69s
                               ETA: 843748.0s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.846s, learning 0.170s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0157
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 8.02s
                        Total time: 28368.71s
                               ETA: 843718.3s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.994s, learning 0.159s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0179
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 8.15s
                        Total time: 28376.86s
                               ETA: 843692.7s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.840s, learning 0.184s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0234
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 8.02s
                        Total time: 28384.88s
                               ETA: 843663.2s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.047s, learning 0.185s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0231
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 8.23s
                        Total time: 28393.12s
                               ETA: 843640.0s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.845s, learning 0.158s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0302
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 8.00s
                        Total time: 28401.12s
                               ETA: 843610.0s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.870s, learning 0.216s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0247
             Mean action noise std: 0.64
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 8.09s
                        Total time: 28409.20s
                               ETA: 843582.4s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.086s, learning 0.162s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0335
             Mean action noise std: 0.64
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 8.25s
                        Total time: 28417.45s
                               ETA: 843559.7s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.780s, learning 0.162s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0266
             Mean action noise std: 0.64
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 7.94s
                        Total time: 28425.39s
                               ETA: 843527.9s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.040s, learning 0.158s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0266
             Mean action noise std: 0.64
                       Mean reward: 23.22
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 8.20s
                        Total time: 28433.59s
                               ETA: 843503.7s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.794s, learning 0.185s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0251
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 7.98s
                        Total time: 28441.57s
                               ETA: 843473.0s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.951s, learning 0.160s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0268
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 8.11s
                        Total time: 28449.68s
                               ETA: 843446.3s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.903s, learning 0.189s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0223
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 8.09s
                        Total time: 28457.77s
                               ETA: 843419.0s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.824s, learning 0.189s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0432
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 8.01s
                        Total time: 28465.78s
                               ETA: 843389.3s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.380s, learning 0.184s)
               Value function loss: 5.7199
                    Surrogate loss: 0.1067
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 8.56s
                        Total time: 28474.35s
                               ETA: 843376.0s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.109s, learning 0.159s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0147
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 8.27s
                        Total time: 28482.62s
                               ETA: 843354.0s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.795s, learning 0.185s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0181
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 7.98s
                        Total time: 28490.60s
                               ETA: 843323.4s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.028s, learning 0.293s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0174
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 8.32s
                        Total time: 28498.92s
                               ETA: 843302.9s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.119s, learning 0.158s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0150
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 8.28s
                        Total time: 28507.19s
                               ETA: 843281.2s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.763s, learning 0.162s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0343
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 7.92s
                        Total time: 28515.12s
                               ETA: 843249.0s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.036s, learning 0.189s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0311
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 8.23s
                        Total time: 28523.34s
                               ETA: 843225.7s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.856s, learning 0.167s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0248
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 8.02s
                        Total time: 28531.37s
                               ETA: 843196.5s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.225s, learning 0.155s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0331
             Mean action noise std: 0.64
                       Mean reward: 23.22
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 8.38s
                        Total time: 28539.75s
                               ETA: 843177.8s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.141s, learning 0.188s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0366
             Mean action noise std: 0.64
                       Mean reward: 23.22
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 8.33s
                        Total time: 28548.08s
                               ETA: 843157.6s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.029s, learning 0.191s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0300
             Mean action noise std: 0.64
                       Mean reward: 23.22
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 8.22s
                        Total time: 28556.30s
                               ETA: 843134.2s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.854s, learning 0.194s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0284
             Mean action noise std: 0.64
                       Mean reward: 23.23
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 8.05s
                        Total time: 28564.34s
                               ETA: 843105.8s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.075s, learning 0.159s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0274
             Mean action noise std: 0.64
                       Mean reward: 23.23
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 8.23s
                        Total time: 28572.58s
                               ETA: 843082.8s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.773s, learning 0.166s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0326
             Mean action noise std: 0.64
                       Mean reward: 23.24
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 7.94s
                        Total time: 28580.52s
                               ETA: 843051.2s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.005s, learning 0.160s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0258
             Mean action noise std: 0.64
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 8.17s
                        Total time: 28588.68s
                               ETA: 843026.2s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.980s, learning 0.160s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0301
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 8.14s
                        Total time: 28596.82s
                               ETA: 843000.5s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.150s, learning 0.157s)
               Value function loss: 2.7076
                    Surrogate loss: 0.0479
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 16.31s
                        Total time: 28613.13s
                               ETA: 843215.5s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.595s, learning 0.216s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0197
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 15.81s
                        Total time: 28628.94s
                               ETA: 843415.7s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.757s, learning 0.160s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0061
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 15.92s
                        Total time: 28644.86s
                               ETA: 843619.0s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1054 steps/s (collection: 15.325s, learning 0.215s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0198
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 15.54s
                        Total time: 28660.40s
                               ETA: 843810.9s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.614s, learning 0.164s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0207
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 15.78s
                        Total time: 28676.17s
                               ETA: 844009.8s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.505s, learning 0.190s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0280
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 15.70s
                        Total time: 28691.87s
                               ETA: 844206.1s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.789s, learning 0.160s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0194
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 15.95s
                        Total time: 28707.82s
                               ETA: 844409.8s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.638s, learning 0.162s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0179
             Mean action noise std: 0.64
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 15.80s
                        Total time: 28723.62s
                               ETA: 844608.9s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.898s, learning 0.222s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0331
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 16.12s
                        Total time: 28739.74s
                               ETA: 844817.3s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.487s, learning 0.188s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0395
             Mean action noise std: 0.64
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 15.68s
                        Total time: 28755.41s
                               ETA: 845012.5s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.461s, learning 0.192s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0302
             Mean action noise std: 0.64
                       Mean reward: 23.38
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 15.65s
                        Total time: 28771.07s
                               ETA: 845206.9s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.630s, learning 0.161s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0252
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 15.79s
                        Total time: 28786.86s
                               ETA: 845405.2s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.706s, learning 0.160s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0301
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 15.87s
                        Total time: 28802.72s
                               ETA: 845605.7s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.720s, learning 0.168s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0277
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 15.89s
                        Total time: 28818.61s
                               ETA: 845806.6s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1037 steps/s (collection: 15.599s, learning 0.198s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0332
             Mean action noise std: 0.64
                       Mean reward: 23.27
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 15.80s
                        Total time: 28834.41s
                               ETA: 846004.7s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.824s, learning 0.160s)
               Value function loss: 8.2976
                    Surrogate loss: 0.0302
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 15.98s
                        Total time: 28850.39s
                               ETA: 846208.2s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1047 steps/s (collection: 15.384s, learning 0.255s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0196
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 15.64s
                        Total time: 28866.03s
                               ETA: 846401.4s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.502s, learning 0.159s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0115
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 15.66s
                        Total time: 28881.69s
                               ETA: 846595.2s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.490s, learning 0.192s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0081
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 15.68s
                        Total time: 28897.37s
                               ETA: 846789.4s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1058 steps/s (collection: 15.309s, learning 0.170s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0202
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 15.48s
                        Total time: 28912.85s
                               ETA: 846977.6s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.579s, learning 0.156s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0289
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 15.73s
                        Total time: 28928.59s
                               ETA: 847173.1s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.841s, learning 0.224s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0313
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 16.06s
                        Total time: 28944.65s
                               ETA: 847378.2s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.537s, learning 0.160s)
               Value function loss: 0.0500
                    Surrogate loss: -0.0325
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 15.70s
                        Total time: 28960.35s
                               ETA: 847572.3s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1054 steps/s (collection: 15.380s, learning 0.163s)
               Value function loss: 0.0619
                    Surrogate loss: -0.0348
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 15.54s
                        Total time: 28975.89s
                               ETA: 847761.8s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.529s, learning 0.178s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0363
             Mean action noise std: 0.64
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 15.71s
                        Total time: 28991.60s
                               ETA: 847956.0s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.748s, learning 0.161s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0364
             Mean action noise std: 0.64
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 15.91s
                        Total time: 29007.51s
                               ETA: 848156.0s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.818s, learning 0.156s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0276
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 15.97s
                        Total time: 29023.48s
                               ETA: 848357.8s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.539s, learning 0.161s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0356
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 15.70s
                        Total time: 29039.18s
                               ETA: 848551.4s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.554s, learning 0.159s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0320
             Mean action noise std: 0.64
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 15.71s
                        Total time: 29054.90s
                               ETA: 848745.3s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.515s, learning 0.182s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0329
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 15.70s
                        Total time: 29070.59s
                               ETA: 848938.6s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.512s, learning 0.158s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0442
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 15.67s
                        Total time: 29086.26s
                               ETA: 849131.0s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.680s, learning 0.159s)
               Value function loss: 5.2309
                    Surrogate loss: 0.0549
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 15.84s
                        Total time: 29102.10s
                               ETA: 849328.1s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.577s, learning 0.162s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0229
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 15.74s
                        Total time: 29117.84s
                               ETA: 849522.2s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.652s, learning 0.211s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0137
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 15.86s
                        Total time: 29133.70s
                               ETA: 849719.9s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.617s, learning 0.186s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0149
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 15.80s
                        Total time: 29149.51s
                               ETA: 849915.6s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.781s, learning 0.175s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0249
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 15.96s
                        Total time: 29165.47s
                               ETA: 850115.7s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.728s, learning 0.185s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0325
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 15.91s
                        Total time: 29181.38s
                               ETA: 850314.4s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1269 steps/s (collection: 12.732s, learning 0.171s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0257
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 12.90s
                        Total time: 29194.28s
                               ETA: 850425.3s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.219s, learning 0.184s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0251
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 8.40s
                        Total time: 29202.68s
                               ETA: 850405.0s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.075s, learning 0.157s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0319
             Mean action noise std: 0.64
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 8.23s
                        Total time: 29210.92s
                               ETA: 850379.8s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.978s, learning 0.162s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0310
             Mean action noise std: 0.64
                       Mean reward: 23.13
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 8.14s
                        Total time: 29219.06s
                               ETA: 850351.9s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.033s, learning 0.166s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0282
             Mean action noise std: 0.64
                       Mean reward: 23.03
               Mean episode length: 124.55
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 8.20s
                        Total time: 29227.25s
                               ETA: 850325.8s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.933s, learning 0.163s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0266
             Mean action noise std: 0.64
                       Mean reward: 23.03
               Mean episode length: 124.55
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 8.10s
                        Total time: 29235.35s
                               ETA: 850296.6s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.100s, learning 0.182s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0316
             Mean action noise std: 0.64
                       Mean reward: 23.02
               Mean episode length: 124.55
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 8.28s
                        Total time: 29243.63s
                               ETA: 850272.9s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.012s, learning 0.162s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0367
             Mean action noise std: 0.64
                       Mean reward: 23.02
               Mean episode length: 124.55
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 8.17s
                        Total time: 29251.81s
                               ETA: 850246.1s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.915s, learning 0.171s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0269
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 124.55
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 8.09s
                        Total time: 29259.89s
                               ETA: 850216.7s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.466s, learning 0.164s)
               Value function loss: 8.1063
                    Surrogate loss: 0.0633
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 8.63s
                        Total time: 29268.52s
                               ETA: 850203.1s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.280s, learning 0.261s)
               Value function loss: 0.1203
                    Surrogate loss: -0.0204
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 8.54s
                        Total time: 29277.06s
                               ETA: 850186.9s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.909s, learning 0.168s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0183
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 8.08s
                        Total time: 29285.14s
                               ETA: 850157.3s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.162s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0148
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 8.13s
                        Total time: 29293.27s
                               ETA: 850129.3s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.939s, learning 0.191s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0174
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 8.13s
                        Total time: 29301.40s
                               ETA: 850101.3s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.708s, learning 0.184s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0300
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 7.89s
                        Total time: 29309.30s
                               ETA: 850066.3s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.011s, learning 0.195s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0395
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 8.21s
                        Total time: 29317.50s
                               ETA: 850040.5s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.185s, learning 0.182s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0357
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 8.37s
                        Total time: 29325.87s
                               ETA: 850019.3s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.200s, learning 0.164s)
               Value function loss: 0.0552
                    Surrogate loss: -0.0277
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 8.36s
                        Total time: 29334.23s
                               ETA: 849998.1s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.936s, learning 0.187s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0411
             Mean action noise std: 0.64
                       Mean reward: 23.39
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 8.12s
                        Total time: 29342.36s
                               ETA: 849969.9s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.981s, learning 0.165s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0279
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 8.15s
                        Total time: 29350.50s
                               ETA: 849942.3s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.148s, learning 0.162s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0286
             Mean action noise std: 0.64
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 8.31s
                        Total time: 29358.81s
                               ETA: 849919.6s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.197s, learning 0.175s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0183
             Mean action noise std: 0.64
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 8.37s
                        Total time: 29367.18s
                               ETA: 849898.6s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.098s, learning 0.167s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0243
             Mean action noise std: 0.64
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.27s
                        Total time: 29375.45s
                               ETA: 849874.6s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.146s, learning 0.167s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0226
             Mean action noise std: 0.64
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 8.31s
                        Total time: 29383.76s
                               ETA: 849851.9s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.322s, learning 0.170s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0397
             Mean action noise std: 0.64
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 8.49s
                        Total time: 29392.25s
                               ETA: 849834.5s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.208s, learning 0.282s)
               Value function loss: 8.4250
                    Surrogate loss: 0.0600
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 8.49s
                        Total time: 29400.75s
                               ETA: 849816.9s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.040s, learning 0.161s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0189
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 8.20s
                        Total time: 29408.95s
                               ETA: 849791.1s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.799s, learning 0.168s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0166
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 7.97s
                        Total time: 29416.91s
                               ETA: 849758.5s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.025s, learning 0.205s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0187
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 8.23s
                        Total time: 29425.14s
                               ETA: 849733.5s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.831s, learning 0.164s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0237
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 8.00s
                        Total time: 29433.14s
                               ETA: 849701.7s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.861s, learning 0.173s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0317
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 8.03s
                        Total time: 29441.17s
                               ETA: 849671.0s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.172s, learning 0.281s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0275
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 8.45s
                        Total time: 29449.63s
                               ETA: 849652.5s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.999s, learning 0.169s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0261
             Mean action noise std: 0.64
                       Mean reward: 22.83
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 8.17s
                        Total time: 29457.79s
                               ETA: 849625.7s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.811s, learning 0.160s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0279
             Mean action noise std: 0.64
                       Mean reward: 22.85
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 7.97s
                        Total time: 29465.77s
                               ETA: 849593.3s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.961s, learning 0.172s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0344
             Mean action noise std: 0.64
                       Mean reward: 22.87
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 8.13s
                        Total time: 29473.90s
                               ETA: 849565.6s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.410s, learning 0.167s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0313
             Mean action noise std: 0.64
                       Mean reward: 22.84
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 8.58s
                        Total time: 29482.48s
                               ETA: 849550.6s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.081s, learning 0.166s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0247
             Mean action noise std: 0.64
                       Mean reward: 22.85
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 8.25s
                        Total time: 29490.72s
                               ETA: 849526.2s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.285s, learning 0.173s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0287
             Mean action noise std: 0.64
                       Mean reward: 22.86
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 8.46s
                        Total time: 29499.18s
                               ETA: 849507.8s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.143s, learning 0.199s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0355
             Mean action noise std: 0.64
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 8.34s
                        Total time: 29507.52s
                               ETA: 849486.1s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.199s, learning 0.185s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0301
             Mean action noise std: 0.64
                       Mean reward: 22.85
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 8.38s
                        Total time: 29515.91s
                               ETA: 849465.7s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.002s, learning 0.203s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0451
             Mean action noise std: 0.64
                       Mean reward: 22.85
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 8.21s
                        Total time: 29524.11s
                               ETA: 849440.1s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.199s, learning 0.162s)
               Value function loss: 4.6329
                    Surrogate loss: 0.0643
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 8.36s
                        Total time: 29532.47s
                               ETA: 849418.9s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.049s, learning 0.193s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0169
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 8.24s
                        Total time: 29540.71s
                               ETA: 849394.4s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.006s, learning 0.180s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0114
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 8.19s
                        Total time: 29548.90s
                               ETA: 849368.3s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.704s, learning 0.193s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0081
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 7.90s
                        Total time: 29556.80s
                               ETA: 849333.9s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.814s, learning 0.170s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0292
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 7.98s
                        Total time: 29564.78s
                               ETA: 849301.9s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.976s, learning 0.211s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0353
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 8.19s
                        Total time: 29572.97s
                               ETA: 849275.9s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.226s, learning 0.213s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0293
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 8.44s
                        Total time: 29581.41s
                               ETA: 849257.1s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.214s, learning 0.225s)
               Value function loss: 0.0397
                    Surrogate loss: -0.0270
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 8.44s
                        Total time: 29589.85s
                               ETA: 849238.2s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.199s, learning 0.164s)
               Value function loss: 0.0567
                    Surrogate loss: -0.0314
             Mean action noise std: 0.64
                       Mean reward: 23.35
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 8.36s
                        Total time: 29598.21s
                               ETA: 849217.2s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.173s, learning 0.213s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0378
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 8.39s
                        Total time: 29606.59s
                               ETA: 849196.9s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.084s, learning 0.185s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0290
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 8.27s
                        Total time: 29614.86s
                               ETA: 849173.3s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.036s, learning 0.199s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0335
             Mean action noise std: 0.64
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 8.23s
                        Total time: 29623.10s
                               ETA: 849148.6s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.148s, learning 0.217s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0306
             Mean action noise std: 0.64
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 8.36s
                        Total time: 29631.46s
                               ETA: 849127.7s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 2106 steps/s (collection: 7.608s, learning 0.172s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0398
             Mean action noise std: 0.64
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 7.78s
                        Total time: 29639.24s
                               ETA: 849090.0s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.034s, learning 0.161s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0303
             Mean action noise std: 0.64
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 8.19s
                        Total time: 29647.44s
                               ETA: 849064.3s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.150s, learning 0.167s)
               Value function loss: 9.4844
                    Surrogate loss: 0.0713
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 8.32s
                        Total time: 29655.75s
                               ETA: 849042.0s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.926s, learning 0.202s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0135
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 8.13s
                        Total time: 29663.88s
                               ETA: 849014.4s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 2073 steps/s (collection: 7.735s, learning 0.165s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0117
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 7.90s
                        Total time: 29671.78s
                               ETA: 848980.3s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.880s, learning 0.160s)
               Value function loss: 0.1064
                    Surrogate loss: -0.0023
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 8.04s
                        Total time: 29679.82s
                               ETA: 848950.2s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.827s, learning 0.168s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0173
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 7.99s
                        Total time: 29687.82s
                               ETA: 848918.7s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.934s, learning 0.189s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0340
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 8.12s
                        Total time: 29695.94s
                               ETA: 848891.0s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.160s, learning 0.169s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0382
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 8.33s
                        Total time: 29704.27s
                               ETA: 848869.1s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.927s, learning 0.162s)
               Value function loss: 0.0533
                    Surrogate loss: -0.0285
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 8.09s
                        Total time: 29712.36s
                               ETA: 848840.5s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.152s, learning 0.195s)
               Value function loss: 0.1231
                    Surrogate loss: -0.0254
             Mean action noise std: 0.64
                       Mean reward: 23.41
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 8.35s
                        Total time: 29720.71s
                               ETA: 848819.2s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.014s, learning 0.173s)
               Value function loss: 0.0509
                    Surrogate loss: -0.0337
             Mean action noise std: 0.64
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 8.19s
                        Total time: 29728.89s
                               ETA: 848793.3s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.066s, learning 0.187s)
               Value function loss: 0.0501
                    Surrogate loss: -0.0164
             Mean action noise std: 0.64
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 8.25s
                        Total time: 29737.15s
                               ETA: 848769.3s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.033s, learning 0.163s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0199
             Mean action noise std: 0.64
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 8.20s
                        Total time: 29745.34s
                               ETA: 848743.7s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.956s, learning 0.192s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0246
             Mean action noise std: 0.64
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 8.15s
                        Total time: 29753.49s
                               ETA: 848716.8s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.040s, learning 0.163s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0276
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 8.20s
                        Total time: 29761.69s
                               ETA: 848691.4s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.913s, learning 0.160s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0165
             Mean action noise std: 0.64
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 8.07s
                        Total time: 29769.77s
                               ETA: 848662.3s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.798s, learning 0.160s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0407
             Mean action noise std: 0.64
                       Mean reward: 23.29
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 7.96s
                        Total time: 29777.72s
                               ETA: 848630.0s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.972s, learning 0.161s)
               Value function loss: 6.9571
                    Surrogate loss: 0.1477
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 8.13s
                        Total time: 29785.86s
                               ETA: 848602.7s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.075s, learning 0.167s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0118
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 8.24s
                        Total time: 29794.10s
                               ETA: 848578.5s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.822s, learning 0.171s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0106
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 7.99s
                        Total time: 29802.09s
                               ETA: 848547.2s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.879s, learning 0.194s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0163
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 8.07s
                        Total time: 29810.17s
                               ETA: 848518.2s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.869s, learning 0.166s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0212
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 8.03s
                        Total time: 29818.20s
                               ETA: 848488.1s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.760s, learning 0.162s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0334
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 7.92s
                        Total time: 29826.12s
                               ETA: 848454.8s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.938s, learning 0.180s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0231
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 8.12s
                        Total time: 29834.24s
                               ETA: 848427.1s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.403s, learning 0.160s)
               Value function loss: 0.0368
                    Surrogate loss: -0.0261
             Mean action noise std: 0.64
                       Mean reward: 23.16
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 8.56s
                        Total time: 29842.80s
                               ETA: 848412.1s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.942s, learning 0.193s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0274
             Mean action noise std: 0.64
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 8.14s
                        Total time: 29850.94s
                               ETA: 848384.9s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.146s, learning 0.163s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0347
             Mean action noise std: 0.64
                       Mean reward: 23.19
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 8.31s
                        Total time: 29859.25s
                               ETA: 848362.7s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.971s, learning 0.167s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0283
             Mean action noise std: 0.64
                       Mean reward: 23.23
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 8.14s
                        Total time: 29867.39s
                               ETA: 848335.6s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.841s, learning 0.176s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0275
             Mean action noise std: 0.64
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 8.02s
                        Total time: 29875.40s
                               ETA: 848305.1s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.892s, learning 0.168s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0279
             Mean action noise std: 0.64
                       Mean reward: 23.30
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 8.06s
                        Total time: 29883.46s
                               ETA: 848275.9s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.006s, learning 0.166s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0314
             Mean action noise std: 0.64
                       Mean reward: 23.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 8.17s
                        Total time: 29891.64s
                               ETA: 848249.8s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.960s, learning 0.161s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0239
             Mean action noise std: 0.64
                       Mean reward: 23.31
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 8.12s
                        Total time: 29899.76s
                               ETA: 848222.3s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.885s, learning 0.162s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0267
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 8.05s
                        Total time: 29907.80s
                               ETA: 848192.7s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.058s, learning 0.172s)
               Value function loss: 3.0546
                    Surrogate loss: 0.0440
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 8.23s
                        Total time: 29916.03s
                               ETA: 848168.3s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.966s, learning 0.158s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0164
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 8.12s
                        Total time: 29924.16s
                               ETA: 848140.9s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.004s, learning 0.166s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0028
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 8.17s
                        Total time: 29932.33s
                               ETA: 848114.8s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.852s, learning 0.179s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0092
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 8.03s
                        Total time: 29940.36s
                               ETA: 848084.8s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.956s, learning 0.171s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0326
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 8.13s
                        Total time: 29948.49s
                               ETA: 848057.6s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.066s, learning 0.241s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0326
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 8.31s
                        Total time: 29956.79s
                               ETA: 848035.4s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.046s, learning 0.155s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0199
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 8.20s
                        Total time: 29964.99s
                               ETA: 848010.2s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.745s, learning 0.174s)
               Value function loss: 0.0596
                    Surrogate loss: -0.0223
             Mean action noise std: 0.64
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 7.92s
                        Total time: 29972.91s
                               ETA: 847977.1s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.112s, learning 0.190s)
               Value function loss: 0.0471
                    Surrogate loss: -0.0296
             Mean action noise std: 0.64
                       Mean reward: 23.54
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 8.30s
                        Total time: 29981.22s
                               ETA: 847954.8s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.898s, learning 0.161s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0343
             Mean action noise std: 0.64
                       Mean reward: 23.54
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 8.06s
                        Total time: 29989.28s
                               ETA: 847925.7s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.831s, learning 0.158s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0260
             Mean action noise std: 0.64
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 7.99s
                        Total time: 29997.26s
                               ETA: 847894.6s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.291s, learning 0.184s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0245
             Mean action noise std: 0.64
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 8.48s
                        Total time: 30005.74s
                               ETA: 847877.2s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.872s, learning 0.170s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0275
             Mean action noise std: 0.64
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 8.04s
                        Total time: 30013.78s
                               ETA: 847847.6s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.846s, learning 0.166s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0235
             Mean action noise std: 0.64
                       Mean reward: 23.43
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 8.01s
                        Total time: 30021.79s
                               ETA: 847817.2s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.137s, learning 0.188s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0310
             Mean action noise std: 0.64
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 8.32s
                        Total time: 30030.12s
                               ETA: 847795.6s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.112s, learning 0.185s)
               Value function loss: 11.1874
                    Surrogate loss: 0.0589
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 8.30s
                        Total time: 30038.42s
                               ETA: 847773.3s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.059s, learning 0.159s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0150
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 8.22s
                        Total time: 30046.63s
                               ETA: 847748.7s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.729s, learning 0.163s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0104
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 7.89s
                        Total time: 30054.53s
                               ETA: 847714.9s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.968s, learning 0.205s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0047
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 8.17s
                        Total time: 30062.70s
                               ETA: 847689.1s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.024s, learning 0.159s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0192
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 8.18s
                        Total time: 30070.88s
                               ETA: 847663.6s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.966s, learning 0.228s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0305
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 8.19s
                        Total time: 30079.08s
                               ETA: 847638.4s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.967s, learning 0.173s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0319
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 8.14s
                        Total time: 30087.22s
                               ETA: 847611.6s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.041s, learning 0.169s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0325
             Mean action noise std: 0.64
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 8.21s
                        Total time: 30095.43s
                               ETA: 847586.9s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.875s, learning 0.167s)
               Value function loss: 0.0644
                    Surrogate loss: -0.0308
             Mean action noise std: 0.64
                       Mean reward: 22.89
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 8.04s
                        Total time: 30103.47s
                               ETA: 847557.5s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.910s, learning 0.186s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0340
             Mean action noise std: 0.64
                       Mean reward: 22.88
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 8.10s
                        Total time: 30111.56s
                               ETA: 847529.5s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.034s, learning 0.172s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0279
             Mean action noise std: 0.64
                       Mean reward: 22.88
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 8.21s
                        Total time: 30119.77s
                               ETA: 847504.7s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.161s, learning 0.162s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0223
             Mean action noise std: 0.64
                       Mean reward: 22.87
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 8.32s
                        Total time: 30128.09s
                               ETA: 847483.2s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 2082 steps/s (collection: 7.710s, learning 0.157s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0264
             Mean action noise std: 0.64
                       Mean reward: 23.05
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 7.87s
                        Total time: 30135.96s
                               ETA: 847448.9s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.203s, learning 0.158s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0283
             Mean action noise std: 0.64
                       Mean reward: 23.01
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 8.36s
                        Total time: 30144.32s
                               ETA: 847428.4s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.156s, learning 0.168s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0215
             Mean action noise std: 0.64
                       Mean reward: 23.02
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 8.32s
                        Total time: 30152.65s
                               ETA: 847407.0s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.799s, learning 0.168s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0433
             Mean action noise std: 0.64
                       Mean reward: 23.02
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 7.97s
                        Total time: 30160.62s
                               ETA: 847375.5s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.144s, learning 0.161s)
               Value function loss: 6.7672
                    Surrogate loss: 0.1447
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 8.31s
                        Total time: 30168.92s
                               ETA: 847353.6s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.041s, learning 0.179s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0253
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 8.22s
                        Total time: 30177.14s
                               ETA: 847329.2s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.963s, learning 0.201s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0114
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 8.16s
                        Total time: 30185.31s
                               ETA: 847303.3s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.073s, learning 0.169s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0080
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 8.24s
                        Total time: 30193.55s
                               ETA: 847279.6s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.876s, learning 0.219s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0213
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 8.10s
                        Total time: 30201.64s
                               ETA: 847251.7s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.870s, learning 0.167s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0346
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 8.04s
                        Total time: 30209.68s
                               ETA: 847222.3s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 2083 steps/s (collection: 7.699s, learning 0.164s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0254
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 7.86s
                        Total time: 30217.54s
                               ETA: 847188.0s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.758s, learning 0.166s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0225
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 7.92s
                        Total time: 30225.47s
                               ETA: 847155.4s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.094s, learning 0.165s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0276
             Mean action noise std: 0.64
                       Mean reward: 22.91
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 8.26s
                        Total time: 30233.73s
                               ETA: 847132.2s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.738s, learning 0.159s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0353
             Mean action noise std: 0.64
                       Mean reward: 22.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 7.90s
                        Total time: 30241.62s
                               ETA: 847098.8s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.162s, learning 0.158s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0193
             Mean action noise std: 0.64
                       Mean reward: 23.07
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 8.32s
                        Total time: 30249.94s
                               ETA: 847077.4s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.876s, learning 0.160s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0243
             Mean action noise std: 0.64
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 8.04s
                        Total time: 30257.98s
                               ETA: 847047.9s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.985s, learning 0.159s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0275
             Mean action noise std: 0.64
                       Mean reward: 23.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 8.14s
                        Total time: 30266.12s
                               ETA: 847021.6s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.264s, learning 0.181s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0253
             Mean action noise std: 0.64
                       Mean reward: 23.05
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 8.45s
                        Total time: 30274.57s
                               ETA: 847003.6s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.115s, learning 0.169s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0182
             Mean action noise std: 0.64
                       Mean reward: 23.10
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 8.28s
                        Total time: 30282.85s
                               ETA: 846981.2s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.898s, learning 0.163s)
               Value function loss: 11.1772
                    Surrogate loss: 0.0557
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 8.06s
                        Total time: 30290.91s
                               ETA: 846952.5s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.893s, learning 0.202s)
               Value function loss: 0.2511
                    Surrogate loss: -0.0196
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 8.09s
                        Total time: 30299.01s
                               ETA: 846924.8s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.051s, learning 0.191s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0135
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 8.24s
                        Total time: 30307.25s
                               ETA: 846901.2s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.827s, learning 0.173s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0102
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 8.00s
                        Total time: 30315.25s
                               ETA: 846870.9s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.732s, learning 0.192s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0150
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 7.92s
                        Total time: 30323.17s
                               ETA: 846838.4s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.792s, learning 0.170s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0326
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 7.96s
                        Total time: 30331.14s
                               ETA: 846807.1s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1996 steps/s (collection: 7.945s, learning 0.262s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0276
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 8.21s
                        Total time: 30339.34s
                               ETA: 846782.5s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.936s, learning 0.172s)
               Value function loss: 0.0514
                    Surrogate loss: -0.0205
             Mean action noise std: 0.64
                       Mean reward: 24.05
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 8.11s
                        Total time: 30347.45s
                               ETA: 846755.3s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.800s, learning 0.166s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0202
             Mean action noise std: 0.64
                       Mean reward: 24.06
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 7.97s
                        Total time: 30355.42s
                               ETA: 846724.0s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.901s, learning 0.282s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0343
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 8.18s
                        Total time: 30363.60s
                               ETA: 846698.9s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.802s, learning 0.176s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0229
             Mean action noise std: 0.64
                       Mean reward: 24.01
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 7.98s
                        Total time: 30371.58s
                               ETA: 846668.0s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.271s, learning 0.217s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0214
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 8.49s
                        Total time: 30380.07s
                               ETA: 846651.4s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.175s, learning 0.164s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0162
             Mean action noise std: 0.64
                       Mean reward: 23.80
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 8.34s
                        Total time: 30388.40s
                               ETA: 846630.6s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 2098 steps/s (collection: 7.621s, learning 0.186s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0169
             Mean action noise std: 0.64
                       Mean reward: 23.78
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 7.81s
                        Total time: 30396.21s
                               ETA: 846595.0s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.049s, learning 0.159s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0182
             Mean action noise std: 0.64
                       Mean reward: 23.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 8.21s
                        Total time: 30404.42s
                               ETA: 846570.6s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.091s, learning 0.206s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0350
             Mean action noise std: 0.64
                       Mean reward: 23.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 8.30s
                        Total time: 30412.72s
                               ETA: 846548.7s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.905s, learning 0.165s)
               Value function loss: 8.8071
                    Surrogate loss: 0.1298
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 8.07s
                        Total time: 30420.79s
                               ETA: 846520.4s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.221s, learning 0.171s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0185
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 8.39s
                        Total time: 30429.18s
                               ETA: 846501.2s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.888s, learning 0.162s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0191
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 8.05s
                        Total time: 30437.23s
                               ETA: 846472.4s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.909s, learning 0.165s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0140
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 8.07s
                        Total time: 30445.30s
                               ETA: 846444.3s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.802s, learning 0.197s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0259
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 8.00s
                        Total time: 30453.30s
                               ETA: 846414.1s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.963s, learning 0.164s)
               Value function loss: 0.0398
                    Surrogate loss: -0.0331
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 8.13s
                        Total time: 30461.43s
                               ETA: 846387.5s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.783s, learning 0.171s)
               Value function loss: 0.0558
                    Surrogate loss: -0.0326
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 7.95s
                        Total time: 30469.38s
                               ETA: 846356.1s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.870s, learning 0.255s)
               Value function loss: 0.0936
                    Surrogate loss: -0.0243
             Mean action noise std: 0.64
                       Mean reward: 23.21
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 8.13s
                        Total time: 30477.51s
                               ETA: 846329.5s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.989s, learning 0.209s)
               Value function loss: 0.0950
                    Surrogate loss: -0.0313
             Mean action noise std: 0.64
                       Mean reward: 23.20
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 8.20s
                        Total time: 30485.71s
                               ETA: 846304.9s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.318s, learning 0.159s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0355
             Mean action noise std: 0.64
                       Mean reward: 23.26
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 8.48s
                        Total time: 30494.18s
                               ETA: 846288.1s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.989s, learning 0.169s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0250
             Mean action noise std: 0.64
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 8.16s
                        Total time: 30502.34s
                               ETA: 846262.4s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.076s, learning 0.247s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0232
             Mean action noise std: 0.64
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 8.32s
                        Total time: 30510.66s
                               ETA: 846241.3s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.034s, learning 0.162s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0193
             Mean action noise std: 0.64
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 8.20s
                        Total time: 30518.86s
                               ETA: 846216.7s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.068s, learning 0.161s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0251
             Mean action noise std: 0.64
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 8.23s
                        Total time: 30527.09s
                               ETA: 846193.0s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.098s, learning 0.252s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0226
             Mean action noise std: 0.64
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 8.35s
                        Total time: 30535.44s
                               ETA: 846172.7s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.354s, learning 0.177s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0412
             Mean action noise std: 0.64
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 8.53s
                        Total time: 30543.97s
                               ETA: 846157.4s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.314s, learning 0.268s)
               Value function loss: 4.9588
                    Surrogate loss: 0.0818
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 8.58s
                        Total time: 30552.55s
                               ETA: 846143.5s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.133s, learning 0.199s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0240
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 8.33s
                        Total time: 30560.89s
                               ETA: 846122.7s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.187s, learning 0.236s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0122
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 8.42s
                        Total time: 30569.31s
                               ETA: 846104.5s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.825s, learning 0.269s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0169
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 8.09s
                        Total time: 30577.40s
                               ETA: 846077.1s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.890s, learning 0.164s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0229
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 8.05s
                        Total time: 30585.46s
                               ETA: 846048.6s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.150s, learning 0.188s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0310
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 8.34s
                        Total time: 30593.79s
                               ETA: 846028.0s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.001s, learning 0.203s)
               Value function loss: 0.0617
                    Surrogate loss: -0.0299
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 8.20s
                        Total time: 30602.00s
                               ETA: 846003.7s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.085s, learning 0.170s)
               Value function loss: 0.0731
                    Surrogate loss: -0.0299
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 8.25s
                        Total time: 30610.25s
                               ETA: 845980.8s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.914s, learning 0.192s)
               Value function loss: 0.0603
                    Surrogate loss: -0.0340
             Mean action noise std: 0.64
                       Mean reward: 23.93
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 8.11s
                        Total time: 30618.36s
                               ETA: 845953.8s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.281s, learning 0.160s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0337
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 8.44s
                        Total time: 30626.80s
                               ETA: 845936.1s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.764s, learning 0.217s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0223
             Mean action noise std: 0.64
                       Mean reward: 23.92
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 7.98s
                        Total time: 30634.78s
                               ETA: 845905.6s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.995s, learning 0.198s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0224
             Mean action noise std: 0.64
                       Mean reward: 23.97
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 8.19s
                        Total time: 30642.97s
                               ETA: 845881.1s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.178s, learning 0.243s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0165
             Mean action noise std: 0.64
                       Mean reward: 23.95
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 8.42s
                        Total time: 30651.39s
                               ETA: 845862.8s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.029s, learning 0.164s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0309
             Mean action noise std: 0.64
                       Mean reward: 24.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 8.19s
                        Total time: 30659.59s
                               ETA: 845838.2s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.044s, learning 0.223s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0221
             Mean action noise std: 0.64
                       Mean reward: 24.29
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 8.27s
                        Total time: 30667.85s
                               ETA: 845815.7s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.296s, learning 0.186s)
               Value function loss: 11.9080
                    Surrogate loss: 0.0758
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 8.48s
                        Total time: 30676.34s
                               ETA: 845799.2s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 2113 steps/s (collection: 7.529s, learning 0.222s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0213
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 7.75s
                        Total time: 30684.09s
                               ETA: 845762.5s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.289s, learning 0.180s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0163
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 8.47s
                        Total time: 30692.56s
                               ETA: 845745.6s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.268s, learning 0.182s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0111
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 8.45s
                        Total time: 30701.01s
                               ETA: 845728.2s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.894s, learning 0.192s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0199
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 8.09s
                        Total time: 30709.09s
                               ETA: 845700.7s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.988s, learning 0.168s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0321
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 8.16s
                        Total time: 30717.25s
                               ETA: 845675.2s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.988s, learning 0.197s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0301
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 8.18s
                        Total time: 30725.43s
                               ETA: 845650.5s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.088s, learning 0.227s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0276
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 8.32s
                        Total time: 30733.75s
                               ETA: 845629.4s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.102s, learning 0.172s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0289
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 8.27s
                        Total time: 30742.02s
                               ETA: 845607.2s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.834s, learning 0.182s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0344
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 8.02s
                        Total time: 30750.04s
                               ETA: 845577.8s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.183s, learning 0.178s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0231
             Mean action noise std: 0.64
                       Mean reward: 24.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 8.36s
                        Total time: 30758.40s
                               ETA: 845558.0s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.981s, learning 0.164s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0292
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 8.14s
                        Total time: 30766.54s
                               ETA: 845532.2s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 2164 steps/s (collection: 7.406s, learning 0.163s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0237
             Mean action noise std: 0.64
                       Mean reward: 24.39
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 7.57s
                        Total time: 30774.11s
                               ETA: 845490.7s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.154s, learning 0.164s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0254
             Mean action noise std: 0.64
                       Mean reward: 24.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 8.32s
                        Total time: 30782.43s
                               ETA: 845469.7s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.013s, learning 0.186s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0199
             Mean action noise std: 0.64
                       Mean reward: 24.33
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 8.20s
                        Total time: 30790.63s
                               ETA: 845445.5s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.209s, learning 0.163s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0420
             Mean action noise std: 0.64
                       Mean reward: 24.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 8.37s
                        Total time: 30799.00s
                               ETA: 845426.0s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.026s, learning 0.254s)
               Value function loss: 8.6295
                    Surrogate loss: 0.1111
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 8.28s
                        Total time: 30807.28s
                               ETA: 845404.0s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.932s, learning 0.159s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0230
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 8.09s
                        Total time: 30815.37s
                               ETA: 845376.9s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.919s, learning 0.205s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0231
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 8.12s
                        Total time: 30823.50s
                               ETA: 845350.6s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.033s, learning 0.192s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0235
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 8.23s
                        Total time: 30831.72s
                               ETA: 845327.2s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.860s, learning 0.160s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0292
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 8.02s
                        Total time: 30839.74s
                               ETA: 845298.1s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.934s, learning 0.201s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0368
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 8.13s
                        Total time: 30847.88s
                               ETA: 845272.1s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1992 steps/s (collection: 7.947s, learning 0.275s)
               Value function loss: 0.0468
                    Surrogate loss: -0.0288
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 8.22s
                        Total time: 30856.10s
                               ETA: 845248.6s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.021s, learning 0.170s)
               Value function loss: 0.0504
                    Surrogate loss: -0.0304
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 8.19s
                        Total time: 30864.29s
                               ETA: 845224.3s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.870s, learning 0.159s)
               Value function loss: 0.0643
                    Surrogate loss: -0.0337
             Mean action noise std: 0.64
                       Mean reward: 23.69
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 8.03s
                        Total time: 30872.32s
                               ETA: 845195.5s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.146s, learning 0.171s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0390
             Mean action noise std: 0.64
                       Mean reward: 23.75
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 8.32s
                        Total time: 30880.64s
                               ETA: 845174.6s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.231s, learning 0.191s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0296
             Mean action noise std: 0.64
                       Mean reward: 23.80
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 8.42s
                        Total time: 30889.06s
                               ETA: 845156.5s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.004s, learning 0.168s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0281
             Mean action noise std: 0.64
                       Mean reward: 23.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 8.17s
                        Total time: 30897.23s
                               ETA: 845131.7s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.808s, learning 0.166s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0294
             Mean action noise std: 0.64
                       Mean reward: 23.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 7.97s
                        Total time: 30905.21s
                               ETA: 845101.5s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.188s, learning 0.188s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0272
             Mean action noise std: 0.64
                       Mean reward: 23.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 8.38s
                        Total time: 30913.58s
                               ETA: 845082.2s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.054s, learning 0.174s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0205
             Mean action noise std: 0.64
                       Mean reward: 23.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 8.23s
                        Total time: 30921.81s
                               ETA: 845058.9s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.163s, learning 0.162s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0210
             Mean action noise std: 0.64
                       Mean reward: 23.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 8.33s
                        Total time: 30930.14s
                               ETA: 845038.3s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.836s, learning 0.164s)
               Value function loss: 3.0892
                    Surrogate loss: 0.0291
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 8.00s
                        Total time: 30938.14s
                               ETA: 845008.8s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.883s, learning 0.171s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0309
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 8.05s
                        Total time: 30946.19s
                               ETA: 844980.7s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.930s, learning 0.165s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0171
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 8.09s
                        Total time: 30954.28s
                               ETA: 844953.8s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.125s, learning 0.177s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0217
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 8.30s
                        Total time: 30962.59s
                               ETA: 844932.6s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.781s, learning 0.162s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0276
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 7.94s
                        Total time: 30970.53s
                               ETA: 844901.6s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.999s, learning 0.165s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0247
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 8.16s
                        Total time: 30978.69s
                               ETA: 844876.6s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.398s, learning 0.168s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0266
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 8.57s
                        Total time: 30987.26s
                               ETA: 844862.6s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.475s, learning 0.226s)
               Value function loss: 0.0483
                    Surrogate loss: -0.0252
             Mean action noise std: 0.64
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 8.70s
                        Total time: 30995.96s
                               ETA: 844852.3s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.193s, learning 0.167s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0299
             Mean action noise std: 0.64
                       Mean reward: 24.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 8.36s
                        Total time: 31004.32s
                               ETA: 844832.7s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.070s, learning 0.160s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0412
             Mean action noise std: 0.64
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 8.23s
                        Total time: 31012.55s
                               ETA: 844809.5s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.853s, learning 0.167s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0280
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 8.02s
                        Total time: 31020.57s
                               ETA: 844780.7s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.138s, learning 0.159s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0203
             Mean action noise std: 0.64
                       Mean reward: 23.99
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 8.30s
                        Total time: 31028.87s
                               ETA: 844759.4s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.969s, learning 0.192s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0288
             Mean action noise std: 0.64
                       Mean reward: 23.97
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 8.16s
                        Total time: 31037.03s
                               ETA: 844734.4s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.262s, learning 0.227s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0191
             Mean action noise std: 0.64
                       Mean reward: 23.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 8.49s
                        Total time: 31045.52s
                               ETA: 844718.3s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.056s, learning 0.163s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0331
             Mean action noise std: 0.64
                       Mean reward: 23.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 8.22s
                        Total time: 31053.74s
                               ETA: 844694.9s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.855s, learning 0.163s)
               Value function loss: 10.7500
                    Surrogate loss: 0.0399
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 8.02s
                        Total time: 31061.75s
                               ETA: 844666.0s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.995s, learning 0.169s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0186
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 8.16s
                        Total time: 31069.92s
                               ETA: 844641.1s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.063s, learning 0.178s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0238
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 8.24s
                        Total time: 31078.16s
                               ETA: 844618.4s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.232s, learning 0.160s)
               Value function loss: 0.0333
                    Surrogate loss: -0.0196
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 8.39s
                        Total time: 31086.55s
                               ETA: 844599.7s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.257s, learning 0.164s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0323
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 8.42s
                        Total time: 31094.97s
                               ETA: 844581.8s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.899s, learning 0.168s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0324
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 8.07s
                        Total time: 31103.04s
                               ETA: 844554.3s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.026s, learning 0.195s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0317
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 8.22s
                        Total time: 31111.26s
                               ETA: 844531.0s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.114s, learning 0.170s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0287
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 8.28s
                        Total time: 31119.54s
                               ETA: 844509.5s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.161s)
               Value function loss: 0.0732
                    Surrogate loss: -0.0211
             Mean action noise std: 0.64
                       Mean reward: 24.11
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 8.17s
                        Total time: 31127.71s
                               ETA: 844484.8s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.790s, learning 0.199s)
               Value function loss: 0.0440
                    Surrogate loss: -0.0363
             Mean action noise std: 0.64
                       Mean reward: 24.12
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 7.99s
                        Total time: 31135.70s
                               ETA: 844455.2s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.148s, learning 0.196s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0225
             Mean action noise std: 0.64
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 8.34s
                        Total time: 31144.05s
                               ETA: 844435.3s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.016s, learning 0.196s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0236
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 8.21s
                        Total time: 31152.26s
                               ETA: 844411.8s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.146s, learning 0.239s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0273
             Mean action noise std: 0.64
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 8.39s
                        Total time: 31160.64s
                               ETA: 844393.0s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.023s, learning 0.158s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0209
             Mean action noise std: 0.64
                       Mean reward: 23.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 8.18s
                        Total time: 31168.82s
                               ETA: 844368.7s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.200s, learning 0.197s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0187
             Mean action noise std: 0.64
                       Mean reward: 23.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 8.40s
                        Total time: 31177.22s
                               ETA: 844350.2s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.087s, learning 0.385s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0433
             Mean action noise std: 0.64
                       Mean reward: 23.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 8.47s
                        Total time: 31185.69s
                               ETA: 844333.8s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.217s, learning 0.161s)
               Value function loss: 7.0583
                    Surrogate loss: 0.0531
             Mean action noise std: 0.64
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 8.38s
                        Total time: 31194.07s
                               ETA: 844314.8s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.097s, learning 0.195s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0201
             Mean action noise std: 0.64
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 8.29s
                        Total time: 31202.36s
                               ETA: 844293.5s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.168s, learning 0.159s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0114
             Mean action noise std: 0.64
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 8.33s
                        Total time: 31210.69s
                               ETA: 844273.2s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.910s, learning 0.157s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0190
             Mean action noise std: 0.64
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 8.07s
                        Total time: 31218.76s
                               ETA: 844245.9s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.876s, learning 0.198s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0304
             Mean action noise std: 0.64
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 8.07s
                        Total time: 31226.83s
                               ETA: 844218.8s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.874s, learning 0.157s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0310
             Mean action noise std: 0.64
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 8.03s
                        Total time: 31234.86s
                               ETA: 844190.4s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 2068 steps/s (collection: 7.757s, learning 0.163s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0194
             Mean action noise std: 0.64
                       Mean reward: 23.96
               Mean episode length: 124.64
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 7.92s
                        Total time: 31242.78s
                               ETA: 844159.1s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.870s, learning 0.159s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0308
             Mean action noise std: 0.64
                       Mean reward: 23.96
               Mean episode length: 124.64
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 8.03s
                        Total time: 31250.81s
                               ETA: 844130.8s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.087s, learning 0.187s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0276
             Mean action noise std: 0.64
                       Mean reward: 23.95
               Mean episode length: 124.64
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 8.27s
                        Total time: 31259.08s
                               ETA: 844109.1s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.765s, learning 0.190s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0369
             Mean action noise std: 0.64
                       Mean reward: 23.94
               Mean episode length: 124.64
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 7.95s
                        Total time: 31267.04s
                               ETA: 844078.8s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.180s, learning 0.193s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0253
             Mean action noise std: 0.64
                       Mean reward: 23.88
               Mean episode length: 124.64
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 8.37s
                        Total time: 31275.41s
                               ETA: 844059.8s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.199s, learning 0.198s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0304
             Mean action noise std: 0.64
                       Mean reward: 23.82
               Mean episode length: 124.64
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 8.40s
                        Total time: 31283.81s
                               ETA: 844041.4s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.265s, learning 0.179s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0287
             Mean action noise std: 0.64
                       Mean reward: 23.73
               Mean episode length: 124.64
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 8.44s
                        Total time: 31292.25s
                               ETA: 844024.3s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.108s, learning 0.165s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0258
             Mean action noise std: 0.64
                       Mean reward: 23.71
               Mean episode length: 124.64
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 8.27s
                        Total time: 31300.53s
                               ETA: 844002.6s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.314s, learning 0.174s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0224
             Mean action noise std: 0.64
                       Mean reward: 23.60
               Mean episode length: 124.64
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 8.49s
                        Total time: 31309.02s
                               ETA: 843986.7s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.138s, learning 0.195s)
               Value function loss: 11.1537
                    Surrogate loss: 0.1068
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 8.33s
                        Total time: 31317.35s
                               ETA: 843966.6s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.974s, learning 0.225s)
               Value function loss: 0.1647
                    Surrogate loss: -0.0208
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 8.20s
                        Total time: 31325.55s
                               ETA: 843943.0s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.003s, learning 0.163s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0188
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 8.17s
                        Total time: 31333.71s
                               ETA: 843918.4s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.947s, learning 0.179s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0161
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 8.13s
                        Total time: 31341.84s
                               ETA: 843892.8s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.761s, learning 0.217s)
               Value function loss: 0.0104
                    Surrogate loss: -0.0252
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 7.98s
                        Total time: 31349.82s
                               ETA: 843863.2s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.970s, learning 0.168s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0316
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 8.14s
                        Total time: 31357.96s
                               ETA: 843838.0s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.150s, learning 0.228s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0370
             Mean action noise std: 0.64
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 8.38s
                        Total time: 31366.33s
                               ETA: 843819.1s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.122s, learning 0.162s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0270
             Mean action noise std: 0.64
                       Mean reward: 23.31
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 8.28s
                        Total time: 31374.62s
                               ETA: 843797.8s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.884s, learning 0.157s)
               Value function loss: 0.0477
                    Surrogate loss: -0.0207
             Mean action noise std: 0.64
                       Mean reward: 23.17
               Mean episode length: 124.35
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 8.04s
                        Total time: 31382.66s
                               ETA: 843769.9s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.045s, learning 0.275s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0310
             Mean action noise std: 0.64
                       Mean reward: 23.17
               Mean episode length: 124.35
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 8.32s
                        Total time: 31390.98s
                               ETA: 843749.6s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 2072 steps/s (collection: 7.738s, learning 0.169s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0207
             Mean action noise std: 0.64
                       Mean reward: 23.20
               Mean episode length: 124.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 7.91s
                        Total time: 31398.89s
                               ETA: 843718.2s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.907s, learning 0.174s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0297
             Mean action noise std: 0.64
                       Mean reward: 23.35
               Mean episode length: 124.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 8.08s
                        Total time: 31406.97s
                               ETA: 843691.4s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.264s, learning 0.282s)
               Value function loss: 0.0418
                    Surrogate loss: -0.0191
             Mean action noise std: 0.64
                       Mean reward: 23.42
               Mean episode length: 124.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 8.55s
                        Total time: 31415.51s
                               ETA: 843677.2s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.252s, learning 0.196s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0203
             Mean action noise std: 0.64
                       Mean reward: 23.46
               Mean episode length: 124.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 8.45s
                        Total time: 31423.96s
                               ETA: 843660.3s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.212s, learning 0.166s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0205
             Mean action noise std: 0.64
                       Mean reward: 23.46
               Mean episode length: 124.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 8.38s
                        Total time: 31432.34s
                               ETA: 843641.5s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 2092 steps/s (collection: 7.661s, learning 0.169s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0366
             Mean action noise std: 0.64
                       Mean reward: 23.47
               Mean episode length: 124.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 7.83s
                        Total time: 31440.17s
                               ETA: 843608.1s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.944s, learning 0.193s)
               Value function loss: 9.1750
                    Surrogate loss: 0.0968
             Mean action noise std: 0.64
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 8.14s
                        Total time: 31448.31s
                               ETA: 843582.9s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.904s, learning 0.158s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0231
             Mean action noise std: 0.64
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 8.06s
                        Total time: 31456.37s
                               ETA: 843555.7s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.754s, learning 0.160s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0191
             Mean action noise std: 0.64
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 7.91s
                        Total time: 31464.28s
                               ETA: 843524.5s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.127s, learning 0.167s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0179
             Mean action noise std: 0.64
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 8.29s
                        Total time: 31472.58s
                               ETA: 843503.6s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.258s, learning 0.159s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0244
             Mean action noise std: 0.64
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 8.42s
                        Total time: 31480.99s
                               ETA: 843485.9s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.795s, learning 0.162s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0383
             Mean action noise std: 0.64
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 7.96s
                        Total time: 31488.95s
                               ETA: 843455.9s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.940s, learning 0.159s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0319
             Mean action noise std: 0.64
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 8.10s
                        Total time: 31497.05s
                               ETA: 843429.8s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.956s, learning 0.163s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0332
             Mean action noise std: 0.64
                       Mean reward: 24.17
               Mean episode length: 124.34
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 8.12s
                        Total time: 31505.17s
                               ETA: 843404.1s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.198s, learning 0.173s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0293
             Mean action noise std: 0.64
                       Mean reward: 24.19
               Mean episode length: 124.34
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 8.37s
                        Total time: 31513.54s
                               ETA: 843385.3s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.041s, learning 0.167s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0264
             Mean action noise std: 0.64
                       Mean reward: 24.23
               Mean episode length: 124.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 8.21s
                        Total time: 31521.75s
                               ETA: 843362.0s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.164s, learning 0.182s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0202
             Mean action noise std: 0.64
                       Mean reward: 24.19
               Mean episode length: 124.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 8.35s
                        Total time: 31530.09s
                               ETA: 843342.5s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.065s, learning 0.168s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0234
             Mean action noise std: 0.64
                       Mean reward: 24.14
               Mean episode length: 124.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 8.23s
                        Total time: 31538.33s
                               ETA: 843320.0s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.064s, learning 0.170s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0243
             Mean action noise std: 0.64
                       Mean reward: 24.13
               Mean episode length: 124.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 8.23s
                        Total time: 31546.56s
                               ETA: 843297.5s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.929s, learning 0.165s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0219
             Mean action noise std: 0.64
                       Mean reward: 24.10
               Mean episode length: 124.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 8.09s
                        Total time: 31554.66s
                               ETA: 843271.3s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.079s, learning 0.160s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0214
             Mean action noise std: 0.64
                       Mean reward: 24.13
               Mean episode length: 124.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 8.24s
                        Total time: 31562.89s
                               ETA: 843248.9s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.062s, learning 0.195s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0402
             Mean action noise std: 0.64
                       Mean reward: 24.13
               Mean episode length: 124.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 8.26s
                        Total time: 31571.15s
                               ETA: 843227.1s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.132s, learning 0.196s)
               Value function loss: 5.7371
                    Surrogate loss: 0.0598
             Mean action noise std: 0.64
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 8.33s
                        Total time: 31579.48s
                               ETA: 843207.1s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.773s, learning 0.201s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0283
             Mean action noise std: 0.64
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 7.97s
                        Total time: 31587.45s
                               ETA: 843177.7s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.792s, learning 0.184s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0148
             Mean action noise std: 0.64
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 7.98s
                        Total time: 31595.43s
                               ETA: 843148.4s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.148s, learning 0.158s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0255
             Mean action noise std: 0.64
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 8.31s
                        Total time: 31603.74s
                               ETA: 843127.8s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.791s, learning 0.163s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0308
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 7.95s
                        Total time: 31611.69s
                               ETA: 843098.0s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.887s, learning 0.194s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0337
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 8.08s
                        Total time: 31619.77s
                               ETA: 843071.5s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.211s, learning 0.248s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0340
             Mean action noise std: 0.63
                       Mean reward: 24.53
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 8.46s
                        Total time: 31628.23s
                               ETA: 843055.0s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.360s, learning 0.177s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0281
             Mean action noise std: 0.63
                       Mean reward: 24.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 8.54s
                        Total time: 31636.77s
                               ETA: 843040.7s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.104s, learning 0.170s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0249
             Mean action noise std: 0.63
                       Mean reward: 24.57
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 8.27s
                        Total time: 31645.04s
                               ETA: 843019.4s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.023s, learning 0.175s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0324
             Mean action noise std: 0.63
                       Mean reward: 24.58
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 8.20s
                        Total time: 31653.24s
                               ETA: 842996.0s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.128s, learning 0.221s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0280
             Mean action noise std: 0.63
                       Mean reward: 24.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 8.35s
                        Total time: 31661.59s
                               ETA: 842976.6s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.114s, learning 0.165s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0319
             Mean action noise std: 0.63
                       Mean reward: 24.53
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 8.28s
                        Total time: 31669.87s
                               ETA: 842955.5s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.009s, learning 0.194s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 24.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 8.20s
                        Total time: 31678.07s
                               ETA: 842932.3s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.007s, learning 0.200s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0345
             Mean action noise std: 0.63
                       Mean reward: 24.53
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 8.21s
                        Total time: 31686.28s
                               ETA: 842909.2s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.019s, learning 0.174s)
               Value function loss: 0.0186
                    Surrogate loss: -0.0267
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 8.19s
                        Total time: 31694.47s
                               ETA: 842885.7s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.898s, learning 0.159s)
               Value function loss: 8.5507
                    Surrogate loss: 0.0791
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 8.06s
                        Total time: 31702.53s
                               ETA: 842858.7s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.224s, learning 0.187s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0189
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 8.41s
                        Total time: 31710.94s
                               ETA: 842841.1s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.979s, learning 0.164s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0214
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 8.14s
                        Total time: 31719.08s
                               ETA: 842816.3s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.897s, learning 0.162s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0126
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 8.06s
                        Total time: 31727.14s
                               ETA: 842789.3s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.744s, learning 0.180s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0258
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 7.92s
                        Total time: 31735.07s
                               ETA: 842758.8s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.945s, learning 0.215s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0320
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 8.16s
                        Total time: 31743.22s
                               ETA: 842734.5s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.280s, learning 0.174s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0376
             Mean action noise std: 0.63
                       Mean reward: 24.51
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 8.45s
                        Total time: 31751.68s
                               ETA: 842718.0s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.910s, learning 0.195s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0398
             Mean action noise std: 0.63
                       Mean reward: 24.49
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 8.10s
                        Total time: 31759.78s
                               ETA: 842692.3s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.987s, learning 0.166s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0336
             Mean action noise std: 0.63
                       Mean reward: 24.54
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 8.15s
                        Total time: 31767.94s
                               ETA: 842667.9s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.041s, learning 0.189s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0387
             Mean action noise std: 0.63
                       Mean reward: 24.55
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 8.23s
                        Total time: 31776.17s
                               ETA: 842645.5s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.061s, learning 0.190s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0206
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 8.25s
                        Total time: 31784.42s
                               ETA: 842623.7s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.111s, learning 0.156s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0370
             Mean action noise std: 0.63
                       Mean reward: 24.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 8.27s
                        Total time: 31792.68s
                               ETA: 842602.3s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.168s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0206
             Mean action noise std: 0.63
                       Mean reward: 24.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 8.46s
                        Total time: 31801.14s
                               ETA: 842586.0s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.771s, learning 0.208s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0161
             Mean action noise std: 0.63
                       Mean reward: 24.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 7.98s
                        Total time: 31809.12s
                               ETA: 842557.0s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.941s, learning 0.173s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0198
             Mean action noise std: 0.63
                       Mean reward: 24.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 8.11s
                        Total time: 31817.24s
                               ETA: 842531.6s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.943s, learning 0.161s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0420
             Mean action noise std: 0.63
                       Mean reward: 24.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 8.10s
                        Total time: 31825.34s
                               ETA: 842505.9s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.025s, learning 0.202s)
               Value function loss: 7.4378
                    Surrogate loss: 0.1043
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 8.23s
                        Total time: 31833.57s
                               ETA: 842483.5s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.951s, learning 0.248s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0192
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 8.20s
                        Total time: 31841.77s
                               ETA: 842460.4s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.126s, learning 0.164s)
               Value function loss: 0.0121
                    Surrogate loss: -0.0213
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 8.29s
                        Total time: 31850.06s
                               ETA: 842439.6s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.992s, learning 0.172s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0269
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 8.16s
                        Total time: 31858.22s
                               ETA: 842415.6s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.274s, learning 0.189s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0252
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 8.46s
                        Total time: 31866.68s
                               ETA: 842399.4s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.792s, learning 0.168s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0370
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 7.96s
                        Total time: 31874.64s
                               ETA: 842370.0s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.238s, learning 0.212s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0285
             Mean action noise std: 0.63
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 8.45s
                        Total time: 31883.09s
                               ETA: 842353.6s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 2101 steps/s (collection: 7.636s, learning 0.161s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0327
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 7.80s
                        Total time: 31890.89s
                               ETA: 842319.9s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.060s, learning 0.168s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0385
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 8.23s
                        Total time: 31899.12s
                               ETA: 842297.5s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.048s, learning 0.170s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0338
             Mean action noise std: 0.63
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 8.22s
                        Total time: 31907.34s
                               ETA: 842275.0s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.971s, learning 0.167s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0302
             Mean action noise std: 0.63
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 8.14s
                        Total time: 31915.47s
                               ETA: 842250.3s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.965s, learning 0.180s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 8.14s
                        Total time: 31923.62s
                               ETA: 842225.8s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.919s, learning 0.193s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0277
             Mean action noise std: 0.63
                       Mean reward: 23.40
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 8.11s
                        Total time: 31931.73s
                               ETA: 842200.5s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.197s, learning 0.195s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0286
             Mean action noise std: 0.63
                       Mean reward: 23.35
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 8.39s
                        Total time: 31940.12s
                               ETA: 842182.5s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.803s, learning 0.162s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0236
             Mean action noise std: 0.63
                       Mean reward: 23.36
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 7.96s
                        Total time: 31948.09s
                               ETA: 842153.3s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.917s, learning 0.162s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0262
             Mean action noise std: 0.63
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 8.08s
                        Total time: 31956.17s
                               ETA: 842127.2s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.939s, learning 0.178s)
               Value function loss: 3.0767
                    Surrogate loss: 0.0258
             Mean action noise std: 0.63
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 8.12s
                        Total time: 31964.28s
                               ETA: 842102.0s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.927s, learning 0.190s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0165
             Mean action noise std: 0.63
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 8.12s
                        Total time: 31972.40s
                               ETA: 842076.8s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.732s, learning 0.163s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0163
             Mean action noise std: 0.63
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 7.89s
                        Total time: 31980.30s
                               ETA: 842045.8s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.957s, learning 0.184s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0296
             Mean action noise std: 0.63
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 8.14s
                        Total time: 31988.44s
                               ETA: 842021.3s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.979s, learning 0.163s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0305
             Mean action noise std: 0.63
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 8.14s
                        Total time: 31996.58s
                               ETA: 841996.8s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.100s, learning 0.176s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0345
             Mean action noise std: 0.63
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 8.28s
                        Total time: 32004.86s
                               ETA: 841975.9s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.088s, learning 0.171s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0331
             Mean action noise std: 0.63
                       Mean reward: 22.79
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 8.26s
                        Total time: 32013.11s
                               ETA: 841954.5s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.940s, learning 0.169s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0262
             Mean action noise std: 0.63
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 8.11s
                        Total time: 32021.22s
                               ETA: 841929.2s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.206s, learning 0.192s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0256
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 8.40s
                        Total time: 32029.62s
                               ETA: 841911.5s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.115s, learning 0.175s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0336
             Mean action noise std: 0.63
                       Mean reward: 22.77
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 8.29s
                        Total time: 32037.91s
                               ETA: 841890.9s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.228s, learning 0.175s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0256
             Mean action noise std: 0.63
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 8.40s
                        Total time: 32046.31s
                               ETA: 841873.4s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.954s, learning 0.170s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0218
             Mean action noise std: 0.63
                       Mean reward: 22.76
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 8.12s
                        Total time: 32054.44s
                               ETA: 841848.5s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.817s, learning 0.161s)
               Value function loss: 0.0191
                    Surrogate loss: -0.0223
             Mean action noise std: 0.63
                       Mean reward: 22.77
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 7.98s
                        Total time: 32062.42s
                               ETA: 841819.8s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 1978 steps/s (collection: 7.943s, learning 0.339s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 22.78
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 8.28s
                        Total time: 32070.70s
                               ETA: 841799.0s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.948s, learning 0.178s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0282
             Mean action noise std: 0.63
                       Mean reward: 22.81
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 8.13s
                        Total time: 32078.82s
                               ETA: 841774.2s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.180s, learning 0.170s)
               Value function loss: 10.1051
                    Surrogate loss: 0.0329
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 8.35s
                        Total time: 32087.18s
                               ETA: 841755.3s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.813s, learning 0.170s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0152
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 7.98s
                        Total time: 32095.16s
                               ETA: 841726.8s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 2089 steps/s (collection: 7.677s, learning 0.165s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0149
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 7.84s
                        Total time: 32103.00s
                               ETA: 841694.5s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.042s, learning 0.172s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0122
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 8.21s
                        Total time: 32111.21s
                               ETA: 841672.1s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.839s, learning 0.225s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0263
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 8.06s
                        Total time: 32119.28s
                               ETA: 841645.7s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.173s, learning 0.173s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0231
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 8.35s
                        Total time: 32127.62s
                               ETA: 841626.7s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 2080 steps/s (collection: 7.707s, learning 0.168s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0255
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 7.87s
                        Total time: 32135.50s
                               ETA: 841595.3s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.834s, learning 0.174s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0378
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 8.01s
                        Total time: 32143.51s
                               ETA: 841567.5s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.041s, learning 0.173s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0351
             Mean action noise std: 0.63
                       Mean reward: 23.58
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 8.21s
                        Total time: 32151.72s
                               ETA: 841545.1s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.912s, learning 0.182s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0333
             Mean action noise std: 0.63
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 8.09s
                        Total time: 32159.81s
                               ETA: 841519.5s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.007s, learning 0.206s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0258
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 8.21s
                        Total time: 32168.03s
                               ETA: 841497.1s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.217s, learning 0.173s)
               Value function loss: 0.0340
                    Surrogate loss: -0.0255
             Mean action noise std: 0.63
                       Mean reward: 23.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 8.39s
                        Total time: 32176.42s
                               ETA: 841479.3s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.854s, learning 0.167s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0198
             Mean action noise std: 0.63
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 8.02s
                        Total time: 32184.44s
                               ETA: 841451.9s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.164s, learning 0.176s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0291
             Mean action noise std: 0.63
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 8.34s
                        Total time: 32192.78s
                               ETA: 841432.8s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.997s, learning 0.221s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0196
             Mean action noise std: 0.63
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 8.22s
                        Total time: 32201.00s
                               ETA: 841410.5s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.028s, learning 0.188s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0389
             Mean action noise std: 0.63
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 8.22s
                        Total time: 32209.21s
                               ETA: 841388.2s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.834s, learning 0.174s)
               Value function loss: 7.5301
                    Surrogate loss: 0.1223
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 8.01s
                        Total time: 32217.22s
                               ETA: 841360.4s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.910s, learning 0.183s)
               Value function loss: 0.0012
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 8.09s
                        Total time: 32225.32s
                               ETA: 841334.9s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.858s, learning 0.200s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0212
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 8.06s
                        Total time: 32233.37s
                               ETA: 841308.5s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.964s, learning 0.165s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0242
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 8.13s
                        Total time: 32241.50s
                               ETA: 841284.0s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.994s, learning 0.190s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0314
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 8.18s
                        Total time: 32249.69s
                               ETA: 841260.9s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.197s, learning 0.248s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0290
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 8.45s
                        Total time: 32258.13s
                               ETA: 841244.6s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.942s, learning 0.159s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0238
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 8.10s
                        Total time: 32266.23s
                               ETA: 841219.3s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 2110 steps/s (collection: 7.600s, learning 0.162s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0260
             Mean action noise std: 0.63
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 7.76s
                        Total time: 32274.00s
                               ETA: 841185.2s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.178s, learning 0.214s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0365
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 8.39s
                        Total time: 32282.39s
                               ETA: 841167.6s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.200s, learning 0.157s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0344
             Mean action noise std: 0.63
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 8.36s
                        Total time: 32290.74s
                               ETA: 841149.0s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.765s, learning 0.160s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0273
             Mean action noise std: 0.63
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 7.93s
                        Total time: 32298.67s
                               ETA: 841119.2s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.966s, learning 0.161s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0269
             Mean action noise std: 0.63
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 8.13s
                        Total time: 32306.80s
                               ETA: 841094.7s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.995s, learning 0.164s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0309
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 8.16s
                        Total time: 32314.96s
                               ETA: 841071.0s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.135s, learning 0.159s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0252
             Mean action noise std: 0.63
                       Mean reward: 24.08
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 8.29s
                        Total time: 32323.25s
                               ETA: 841050.8s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.155s, learning 0.192s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0222
             Mean action noise std: 0.63
                       Mean reward: 24.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 8.35s
                        Total time: 32331.60s
                               ETA: 841032.0s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.868s, learning 0.158s)
               Value function loss: 9.0335
                    Surrogate loss: 0.0639
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 8.03s
                        Total time: 32339.62s
                               ETA: 841004.9s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.816s, learning 0.161s)
               Value function loss: 0.6744
                    Surrogate loss: -0.0065
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 7.98s
                        Total time: 32347.60s
                               ETA: 840976.5s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.097s, learning 0.263s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0255
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 8.36s
                        Total time: 32355.96s
                               ETA: 840958.0s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.993s, learning 0.201s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0256
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 8.19s
                        Total time: 32364.15s
                               ETA: 840935.3s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.318s, learning 0.267s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0275
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 8.58s
                        Total time: 32372.74s
                               ETA: 840922.7s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.887s, learning 0.194s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0253
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 8.08s
                        Total time: 32380.82s
                               ETA: 840897.1s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.950s, learning 0.165s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0255
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 8.11s
                        Total time: 32388.93s
                               ETA: 840872.3s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.972s, learning 0.198s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0341
             Mean action noise std: 0.63
                       Mean reward: 24.44
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 8.17s
                        Total time: 32397.10s
                               ETA: 840848.9s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.986s, learning 0.180s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0313
             Mean action noise std: 0.63
                       Mean reward: 24.48
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 8.17s
                        Total time: 32405.27s
                               ETA: 840825.5s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.032s, learning 0.166s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0373
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 8.20s
                        Total time: 32413.47s
                               ETA: 840802.9s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.108s, learning 0.160s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0211
             Mean action noise std: 0.63
                       Mean reward: 24.38
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 8.27s
                        Total time: 32421.74s
                               ETA: 840782.2s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.664s, learning 0.187s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0339
             Mean action noise std: 0.63
                       Mean reward: 24.36
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 7.85s
                        Total time: 32429.59s
                               ETA: 840750.6s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.141s, learning 0.159s)
               Value function loss: 0.0401
                    Surrogate loss: -0.0133
             Mean action noise std: 0.63
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 8.30s
                        Total time: 32437.89s
                               ETA: 840730.7s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 2113 steps/s (collection: 7.561s, learning 0.192s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0315
             Mean action noise std: 0.63
                       Mean reward: 24.27
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 7.75s
                        Total time: 32445.64s
                               ETA: 840696.6s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.799s, learning 0.242s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0190
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 8.04s
                        Total time: 32453.68s
                               ETA: 840670.0s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.956s, learning 0.162s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0387
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 8.12s
                        Total time: 32461.80s
                               ETA: 840645.4s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.081s, learning 0.173s)
               Value function loss: 8.9461
                    Surrogate loss: 0.2018
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 8.25s
                        Total time: 32470.06s
                               ETA: 840624.3s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.718s, learning 0.194s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0126
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 7.91s
                        Total time: 32477.97s
                               ETA: 840594.4s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.972s, learning 0.164s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0151
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 8.14s
                        Total time: 32486.10s
                               ETA: 840570.3s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.759s, learning 0.164s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0215
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 7.92s
                        Total time: 32494.03s
                               ETA: 840540.7s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.067s, learning 0.302s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0259
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 8.37s
                        Total time: 32502.40s
                               ETA: 840522.6s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.087s, learning 0.248s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0228
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 8.34s
                        Total time: 32510.73s
                               ETA: 840503.7s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.993s, learning 0.224s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0318
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 8.22s
                        Total time: 32518.95s
                               ETA: 840481.7s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.176s, learning 0.164s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0363
             Mean action noise std: 0.63
                       Mean reward: 24.46
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 8.34s
                        Total time: 32527.29s
                               ETA: 840462.9s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.125s, learning 0.301s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0409
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 8.43s
                        Total time: 32535.71s
                               ETA: 840446.3s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.079s, learning 0.220s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0428
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 8.30s
                        Total time: 32544.01s
                               ETA: 840426.4s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.939s, learning 0.178s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0346
             Mean action noise std: 0.63
                       Mean reward: 24.39
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 8.12s
                        Total time: 32552.13s
                               ETA: 840401.9s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.910s, learning 0.163s)
               Value function loss: 0.0332
                    Surrogate loss: -0.0289
             Mean action noise std: 0.63
                       Mean reward: 24.36
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 8.07s
                        Total time: 32560.20s
                               ETA: 840376.2s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.979s, learning 0.172s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0324
             Mean action noise std: 0.63
                       Mean reward: 24.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 8.15s
                        Total time: 32568.35s
                               ETA: 840352.5s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.182s, learning 0.231s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0281
             Mean action noise std: 0.63
                       Mean reward: 24.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 8.41s
                        Total time: 32576.77s
                               ETA: 840335.7s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.788s, learning 0.157s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0239
             Mean action noise std: 0.63
                       Mean reward: 24.33
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 7.95s
                        Total time: 32584.71s
                               ETA: 840306.7s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.382s, learning 0.204s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0426
             Mean action noise std: 0.63
                       Mean reward: 24.33
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 8.59s
                        Total time: 32593.30s
                               ETA: 840294.3s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.255s, learning 0.174s)
               Value function loss: 5.4872
                    Surrogate loss: 0.0753
             Mean action noise std: 0.63
                       Mean reward: 24.14
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 8.43s
                        Total time: 32601.73s
                               ETA: 840277.9s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.680s, learning 0.174s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0188
             Mean action noise std: 0.63
                       Mean reward: 24.14
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 7.85s
                        Total time: 32609.58s
                               ETA: 840246.6s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.971s, learning 0.193s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0258
             Mean action noise std: 0.63
                       Mean reward: 24.14
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 8.16s
                        Total time: 32617.74s
                               ETA: 840223.3s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.032s, learning 0.178s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0316
             Mean action noise std: 0.63
                       Mean reward: 24.14
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 8.21s
                        Total time: 32625.95s
                               ETA: 840201.2s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.710s, learning 0.179s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0328
             Mean action noise std: 0.63
                       Mean reward: 24.14
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 7.89s
                        Total time: 32633.84s
                               ETA: 840170.9s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.146s, learning 0.231s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0271
             Mean action noise std: 0.63
                       Mean reward: 24.14
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 8.38s
                        Total time: 32642.22s
                               ETA: 840153.1s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.352s, learning 0.194s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0208
             Mean action noise std: 0.63
                       Mean reward: 24.15
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 8.55s
                        Total time: 32650.77s
                               ETA: 840139.7s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.177s, learning 0.161s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0293
             Mean action noise std: 0.63
                       Mean reward: 24.12
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 8.34s
                        Total time: 32659.10s
                               ETA: 840120.9s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.470s, learning 0.166s)
               Value function loss: 0.0170
                    Surrogate loss: -0.0290
             Mean action noise std: 0.63
                       Mean reward: 24.10
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 8.64s
                        Total time: 32667.74s
                               ETA: 840109.9s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.716s, learning 0.193s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0381
             Mean action noise std: 0.63
                       Mean reward: 24.11
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 7.91s
                        Total time: 32675.65s
                               ETA: 840080.1s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.712s, learning 0.158s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0255
             Mean action noise std: 0.63
                       Mean reward: 24.01
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 7.87s
                        Total time: 32683.52s
                               ETA: 840049.3s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.289s, learning 0.181s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0316
             Mean action noise std: 0.63
                       Mean reward: 23.95
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 8.47s
                        Total time: 32691.99s
                               ETA: 840034.0s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.861s, learning 0.192s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0285
             Mean action noise std: 0.63
                       Mean reward: 23.97
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 8.05s
                        Total time: 32700.04s
                               ETA: 840007.9s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.242s, learning 0.164s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0410
             Mean action noise std: 0.63
                       Mean reward: 23.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 8.41s
                        Total time: 32708.45s
                               ETA: 839990.9s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.801s, learning 0.158s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0314
             Mean action noise std: 0.63
                       Mean reward: 23.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 7.96s
                        Total time: 32716.41s
                               ETA: 839962.5s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 1558 steps/s (collection: 10.248s, learning 0.266s)
               Value function loss: 9.6995
                    Surrogate loss: 0.0839
             Mean action noise std: 0.63
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 10.51s
                        Total time: 32726.92s
                               ETA: 839999.6s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 1047 steps/s (collection: 15.442s, learning 0.193s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0137
             Mean action noise std: 0.63
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 15.63s
                        Total time: 32742.55s
                               ETA: 840168.1s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.799s, learning 0.186s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0202
             Mean action noise std: 0.63
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 15.99s
                        Total time: 32758.54s
                               ETA: 840345.6s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.693s, learning 0.187s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0174
             Mean action noise std: 0.63
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 15.88s
                        Total time: 32774.42s
                               ETA: 840520.2s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.637s, learning 0.219s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0235
             Mean action noise std: 0.63
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 15.86s
                        Total time: 32790.27s
                               ETA: 840694.0s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.767s, learning 0.244s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0323
             Mean action noise std: 0.63
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 16.01s
                        Total time: 32806.28s
                               ETA: 840871.8s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.852s, learning 0.220s)
               Value function loss: 0.0167
                    Surrogate loss: -0.0310
             Mean action noise std: 0.63
                       Mean reward: 23.04
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 16.07s
                        Total time: 32822.36s
                               ETA: 841051.0s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.469s, learning 0.202s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0319
             Mean action noise std: 0.63
                       Mean reward: 23.06
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 15.67s
                        Total time: 32838.03s
                               ETA: 841219.9s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.485s, learning 0.165s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0385
             Mean action noise std: 0.63
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 15.65s
                        Total time: 32853.68s
                               ETA: 841388.1s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1103 steps/s (collection: 14.685s, learning 0.160s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0289
             Mean action noise std: 0.63
                       Mean reward: 23.12
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 14.84s
                        Total time: 32868.52s
                               ETA: 841535.6s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.436s, learning 0.159s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0356
             Mean action noise std: 0.63
                       Mean reward: 23.10
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 15.59s
                        Total time: 32884.12s
                               ETA: 841702.2s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 1049 steps/s (collection: 15.444s, learning 0.165s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0428
             Mean action noise std: 0.63
                       Mean reward: 23.11
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 15.61s
                        Total time: 32899.73s
                               ETA: 841869.1s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.394s, learning 0.159s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0270
             Mean action noise std: 0.63
                       Mean reward: 23.14
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 15.55s
                        Total time: 32915.28s
                               ETA: 842034.4s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.647s, learning 0.181s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0322
             Mean action noise std: 0.63
                       Mean reward: 23.17
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 15.83s
                        Total time: 32931.11s
                               ETA: 842206.7s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1050 steps/s (collection: 15.389s, learning 0.203s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0172
             Mean action noise std: 0.63
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 15.59s
                        Total time: 32946.70s
                               ETA: 842372.9s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1067 steps/s (collection: 15.179s, learning 0.165s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0428
             Mean action noise std: 0.63
                       Mean reward: 23.18
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 15.34s
                        Total time: 32962.04s
                               ETA: 842532.6s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1048 steps/s (collection: 15.459s, learning 0.161s)
               Value function loss: 5.4961
                    Surrogate loss: 0.1867
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 15.62s
                        Total time: 32977.66s
                               ETA: 842699.2s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.391s, learning 0.164s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0051
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 15.55s
                        Total time: 32993.22s
                               ETA: 842864.2s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1059 steps/s (collection: 15.306s, learning 0.164s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0175
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 15.47s
                        Total time: 33008.69s
                               ETA: 843026.8s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.334s, learning 0.195s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0180
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 15.53s
                        Total time: 33024.22s
                               ETA: 843190.9s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.383s, learning 0.166s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0201
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 15.55s
                        Total time: 33039.77s
                               ETA: 843355.4s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.385s, learning 0.171s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0203
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 15.56s
                        Total time: 33055.32s
                               ETA: 843519.9s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.564s, learning 0.163s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0136
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 15.73s
                        Total time: 33071.05s
                               ETA: 843688.8s

################################################################################
                    [1m Learning iteration 3772/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.492s, learning 0.184s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0275
             Mean action noise std: 0.63
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61816832
                    Iteration time: 15.68s
                        Total time: 33086.73s
                               ETA: 843856.2s

################################################################################
                    [1m Learning iteration 3773/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.506s, learning 0.182s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0387
             Mean action noise std: 0.63
                       Mean reward: 23.47
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 15.69s
                        Total time: 33102.41s
                               ETA: 844023.8s

################################################################################
                    [1m Learning iteration 3774/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.752s, learning 0.169s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0387
             Mean action noise std: 0.63
                       Mean reward: 23.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61849600
                    Iteration time: 15.92s
                        Total time: 33118.34s
                               ETA: 844197.3s

################################################################################
                    [1m Learning iteration 3775/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.748s, learning 0.171s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0375
             Mean action noise std: 0.63
                       Mean reward: 23.43
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61865984
                    Iteration time: 15.92s
                        Total time: 33134.25s
                               ETA: 844370.7s

################################################################################
                    [1m Learning iteration 3776/100000 [0m                    

                       Computation: 1052 steps/s (collection: 15.394s, learning 0.170s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0283
             Mean action noise std: 0.63
                       Mean reward: 23.37
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61882368
                    Iteration time: 15.56s
                        Total time: 33149.82s
                               ETA: 844534.8s

################################################################################
                    [1m Learning iteration 3777/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.574s, learning 0.257s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0309
             Mean action noise std: 0.63
                       Mean reward: 23.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61898752
                    Iteration time: 15.83s
                        Total time: 33165.65s
                               ETA: 844705.7s

################################################################################
                    [1m Learning iteration 3778/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.726s, learning 0.167s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0364
             Mean action noise std: 0.63
                       Mean reward: 23.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61915136
                    Iteration time: 15.89s
                        Total time: 33181.54s
                               ETA: 844878.1s

################################################################################
                    [1m Learning iteration 3779/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.585s, learning 0.218s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0264
             Mean action noise std: 0.63
                       Mean reward: 23.23
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 15.80s
                        Total time: 33197.34s
                               ETA: 845048.1s

################################################################################
                    [1m Learning iteration 3780/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.781s, learning 0.159s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0281
             Mean action noise std: 0.63
                       Mean reward: 23.20
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61947904
                    Iteration time: 15.94s
                        Total time: 33213.28s
                               ETA: 845221.4s

################################################################################
                    [1m Learning iteration 3781/100000 [0m                    

                       Computation: 1053 steps/s (collection: 15.382s, learning 0.174s)
               Value function loss: 2.7183
                    Surrogate loss: 0.0303
             Mean action noise std: 0.63
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61964288
                    Iteration time: 15.56s
                        Total time: 33228.84s
                               ETA: 845384.9s

################################################################################
                    [1m Learning iteration 3782/100000 [0m                    

                       Computation: 1039 steps/s (collection: 15.598s, learning 0.164s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0169
             Mean action noise std: 0.63
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61980672
                    Iteration time: 15.76s
                        Total time: 33244.60s
                               ETA: 845553.6s

################################################################################
                    [1m Learning iteration 3783/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.359s, learning 0.163s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0159
             Mean action noise std: 0.63
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61997056
                    Iteration time: 15.52s
                        Total time: 33260.12s
                               ETA: 845716.0s

################################################################################
                    [1m Learning iteration 3784/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.713s, learning 0.201s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0224
             Mean action noise std: 0.63
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62013440
                    Iteration time: 15.91s
                        Total time: 33276.04s
                               ETA: 845888.3s

################################################################################
                    [1m Learning iteration 3785/100000 [0m                    

                       Computation: 1051 steps/s (collection: 15.416s, learning 0.163s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0310
             Mean action noise std: 0.63
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 15.58s
                        Total time: 33291.62s
                               ETA: 846052.0s

################################################################################
                    [1m Learning iteration 3786/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.610s, learning 0.171s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0256
             Mean action noise std: 0.63
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62046208
                    Iteration time: 15.78s
                        Total time: 33307.40s
                               ETA: 846220.7s

################################################################################
                    [1m Learning iteration 3787/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.846s, learning 0.172s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0144
             Mean action noise std: 0.63
                       Mean reward: 22.10
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62062592
                    Iteration time: 11.02s
                        Total time: 33318.42s
                               ETA: 846268.4s

################################################################################
                    [1m Learning iteration 3788/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.998s, learning 0.186s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0191
             Mean action noise std: 0.63
                       Mean reward: 22.09
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62078976
                    Iteration time: 8.18s
                        Total time: 33326.60s
                               ETA: 846244.1s

################################################################################
                    [1m Learning iteration 3789/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.980s, learning 0.239s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0259
             Mean action noise std: 0.63
                       Mean reward: 22.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62095360
                    Iteration time: 8.22s
                        Total time: 33334.82s
                               ETA: 846220.6s

################################################################################
                    [1m Learning iteration 3790/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.826s, learning 0.256s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0349
             Mean action noise std: 0.63
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62111744
                    Iteration time: 8.08s
                        Total time: 33342.90s
                               ETA: 846193.7s

################################################################################
                    [1m Learning iteration 3791/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.929s, learning 0.163s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 22.06
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 8.09s
                        Total time: 33350.99s
                               ETA: 846167.1s

################################################################################
                    [1m Learning iteration 3792/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.017s, learning 0.174s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0236
             Mean action noise std: 0.63
                       Mean reward: 22.06
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62144512
                    Iteration time: 8.19s
                        Total time: 33359.18s
                               ETA: 846142.9s

################################################################################
                    [1m Learning iteration 3793/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.439s, learning 0.168s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0285
             Mean action noise std: 0.63
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62160896
                    Iteration time: 8.61s
                        Total time: 33367.79s
                               ETA: 846129.4s

################################################################################
                    [1m Learning iteration 3794/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.279s, learning 0.188s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0261
             Mean action noise std: 0.63
                       Mean reward: 22.02
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62177280
                    Iteration time: 8.47s
                        Total time: 33376.26s
                               ETA: 846112.3s

################################################################################
                    [1m Learning iteration 3795/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.163s, learning 0.173s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0303
             Mean action noise std: 0.63
                       Mean reward: 22.04
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62193664
                    Iteration time: 8.34s
                        Total time: 33384.59s
                               ETA: 846091.9s

################################################################################
                    [1m Learning iteration 3796/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.051s, learning 0.216s)
               Value function loss: 10.9857
                    Surrogate loss: 0.0413
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62210048
                    Iteration time: 8.27s
                        Total time: 33392.86s
                               ETA: 846069.7s

################################################################################
                    [1m Learning iteration 3797/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.192s, learning 0.164s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0105
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 8.36s
                        Total time: 33401.22s
                               ETA: 846049.8s

################################################################################
                    [1m Learning iteration 3798/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.054s, learning 0.189s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0180
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62242816
                    Iteration time: 8.24s
                        Total time: 33409.46s
                               ETA: 846027.1s

################################################################################
                    [1m Learning iteration 3799/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.853s, learning 0.200s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0167
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62259200
                    Iteration time: 8.05s
                        Total time: 33417.51s
                               ETA: 845999.5s

################################################################################
                    [1m Learning iteration 3800/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.918s, learning 0.177s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0246
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62275584
                    Iteration time: 8.10s
                        Total time: 33425.61s
                               ETA: 845973.0s

################################################################################
                    [1m Learning iteration 3801/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.030s, learning 0.181s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0244
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62291968
                    Iteration time: 8.21s
                        Total time: 33433.82s
                               ETA: 845949.5s

################################################################################
                    [1m Learning iteration 3802/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.901s, learning 0.162s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0229
             Mean action noise std: 0.63
                       Mean reward: 22.80
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62308352
                    Iteration time: 8.06s
                        Total time: 33441.88s
                               ETA: 845922.2s

################################################################################
                    [1m Learning iteration 3803/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.949s, learning 0.163s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0298
             Mean action noise std: 0.63
                       Mean reward: 22.82
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 8.11s
                        Total time: 33449.99s
                               ETA: 845896.2s

################################################################################
                    [1m Learning iteration 3804/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.837s, learning 0.165s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0302
             Mean action noise std: 0.63
                       Mean reward: 22.88
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62341120
                    Iteration time: 8.00s
                        Total time: 33458.00s
                               ETA: 845867.4s

################################################################################
                    [1m Learning iteration 3805/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.974s, learning 0.159s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0325
             Mean action noise std: 0.63
                       Mean reward: 22.93
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62357504
                    Iteration time: 8.13s
                        Total time: 33466.13s
                               ETA: 845841.9s

################################################################################
                    [1m Learning iteration 3806/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.754s, learning 0.187s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0289
             Mean action noise std: 0.63
                       Mean reward: 22.93
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62373888
                    Iteration time: 7.94s
                        Total time: 33474.07s
                               ETA: 845811.6s

################################################################################
                    [1m Learning iteration 3807/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.800s, learning 0.165s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0290
             Mean action noise std: 0.63
                       Mean reward: 22.95
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62390272
                    Iteration time: 7.97s
                        Total time: 33482.04s
                               ETA: 845781.9s

################################################################################
                    [1m Learning iteration 3808/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.130s, learning 0.156s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0305
             Mean action noise std: 0.63
                       Mean reward: 22.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62406656
                    Iteration time: 8.29s
                        Total time: 33490.32s
                               ETA: 845760.3s

################################################################################
                    [1m Learning iteration 3809/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.993s, learning 0.163s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0274
             Mean action noise std: 0.63
                       Mean reward: 22.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 8.16s
                        Total time: 33498.48s
                               ETA: 845735.4s

################################################################################
                    [1m Learning iteration 3810/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.094s, learning 0.158s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0226
             Mean action noise std: 0.63
                       Mean reward: 22.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62439424
                    Iteration time: 8.25s
                        Total time: 33506.73s
                               ETA: 845713.0s

################################################################################
                    [1m Learning iteration 3811/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.126s, learning 0.168s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0418
             Mean action noise std: 0.63
                       Mean reward: 22.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62455808
                    Iteration time: 8.29s
                        Total time: 33515.02s
                               ETA: 845691.7s

################################################################################
                    [1m Learning iteration 3812/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.083s, learning 0.156s)
               Value function loss: 7.4244
                    Surrogate loss: 0.0987
             Mean action noise std: 0.63
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62472192
                    Iteration time: 8.24s
                        Total time: 33523.26s
                               ETA: 845668.9s

################################################################################
                    [1m Learning iteration 3813/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.858s, learning 0.272s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0066
             Mean action noise std: 0.63
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62488576
                    Iteration time: 8.13s
                        Total time: 33531.39s
                               ETA: 845643.5s

################################################################################
                    [1m Learning iteration 3814/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.892s, learning 0.197s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0116
             Mean action noise std: 0.63
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62504960
                    Iteration time: 8.09s
                        Total time: 33539.48s
                               ETA: 845616.9s

################################################################################
                    [1m Learning iteration 3815/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.959s, learning 0.188s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0122
             Mean action noise std: 0.63
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 8.15s
                        Total time: 33547.63s
                               ETA: 845591.9s

################################################################################
                    [1m Learning iteration 3816/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.712s, learning 0.269s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0146
             Mean action noise std: 0.63
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62537728
                    Iteration time: 7.98s
                        Total time: 33555.61s
                               ETA: 845562.7s

################################################################################
                    [1m Learning iteration 3817/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.331s, learning 0.210s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0150
             Mean action noise std: 0.63
                       Mean reward: 23.57
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62554112
                    Iteration time: 8.54s
                        Total time: 33564.15s
                               ETA: 845547.6s

################################################################################
                    [1m Learning iteration 3818/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.025s, learning 0.190s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0158
             Mean action noise std: 0.63
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62570496
                    Iteration time: 8.21s
                        Total time: 33572.37s
                               ETA: 845524.3s

################################################################################
                    [1m Learning iteration 3819/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.235s, learning 0.189s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0187
             Mean action noise std: 0.63
                       Mean reward: 23.59
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62586880
                    Iteration time: 8.42s
                        Total time: 33580.79s
                               ETA: 845506.3s

################################################################################
                    [1m Learning iteration 3820/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.164s, learning 0.200s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0239
             Mean action noise std: 0.63
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62603264
                    Iteration time: 8.36s
                        Total time: 33589.15s
                               ETA: 845486.8s

################################################################################
                    [1m Learning iteration 3821/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.219s, learning 0.164s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0329
             Mean action noise std: 0.63
                       Mean reward: 23.64
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 8.38s
                        Total time: 33597.54s
                               ETA: 845467.7s

################################################################################
                    [1m Learning iteration 3822/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.866s, learning 0.166s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0298
             Mean action noise std: 0.63
                       Mean reward: 24.00
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62636032
                    Iteration time: 8.03s
                        Total time: 33605.57s
                               ETA: 845439.8s

################################################################################
                    [1m Learning iteration 3823/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.862s, learning 0.195s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0312
             Mean action noise std: 0.63
                       Mean reward: 24.00
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62652416
                    Iteration time: 8.06s
                        Total time: 33613.63s
                               ETA: 845412.6s

################################################################################
                    [1m Learning iteration 3824/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.819s, learning 0.193s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0330
             Mean action noise std: 0.63
                       Mean reward: 24.00
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62668800
                    Iteration time: 8.01s
                        Total time: 33621.64s
                               ETA: 845384.2s

################################################################################
                    [1m Learning iteration 3825/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.742s, learning 0.196s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0359
             Mean action noise std: 0.63
                       Mean reward: 23.97
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62685184
                    Iteration time: 7.94s
                        Total time: 33629.58s
                               ETA: 845354.0s

################################################################################
                    [1m Learning iteration 3826/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.085s, learning 0.157s)
               Value function loss: 0.0317
                    Surrogate loss: -0.0282
             Mean action noise std: 0.63
                       Mean reward: 24.00
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62701568
                    Iteration time: 8.24s
                        Total time: 33637.82s
                               ETA: 845331.5s

################################################################################
                    [1m Learning iteration 3827/100000 [0m                    

                       Computation: 1990 steps/s (collection: 7.991s, learning 0.239s)
               Value function loss: 6.3814
                    Surrogate loss: 0.0725
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 8.23s
                        Total time: 33646.05s
                               ETA: 845308.6s

################################################################################
                    [1m Learning iteration 3828/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.977s, learning 0.186s)
               Value function loss: 0.9957
                    Surrogate loss: 0.0016
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62734336
                    Iteration time: 8.16s
                        Total time: 33654.21s
                               ETA: 845284.1s

################################################################################
                    [1m Learning iteration 3829/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.831s, learning 0.162s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0194
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62750720
                    Iteration time: 7.99s
                        Total time: 33662.20s
                               ETA: 845255.3s

################################################################################
                    [1m Learning iteration 3830/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.057s, learning 0.308s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0130
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62767104
                    Iteration time: 8.36s
                        Total time: 33670.57s
                               ETA: 845235.9s

################################################################################
                    [1m Learning iteration 3831/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.120s, learning 0.275s)
               Value function loss: 0.0334
                    Surrogate loss: -0.0206
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62783488
                    Iteration time: 8.39s
                        Total time: 33678.96s
                               ETA: 845217.2s

################################################################################
                    [1m Learning iteration 3832/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.000s, learning 0.183s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0160
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62799872
                    Iteration time: 8.18s
                        Total time: 33687.15s
                               ETA: 845193.2s

################################################################################
                    [1m Learning iteration 3833/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.368s, learning 0.199s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0186
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 8.57s
                        Total time: 33695.71s
                               ETA: 845178.9s

################################################################################
                    [1m Learning iteration 3834/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.973s, learning 0.168s)
               Value function loss: 0.0144
                    Surrogate loss: -0.0268
             Mean action noise std: 0.63
                       Mean reward: 23.69
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62832640
                    Iteration time: 8.14s
                        Total time: 33703.86s
                               ETA: 845153.8s

################################################################################
                    [1m Learning iteration 3835/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.165s, learning 0.179s)
               Value function loss: 0.0265
                    Surrogate loss: -0.0297
             Mean action noise std: 0.63
                       Mean reward: 23.70
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62849024
                    Iteration time: 8.34s
                        Total time: 33712.20s
                               ETA: 845133.9s

################################################################################
                    [1m Learning iteration 3836/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.113s, learning 0.185s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0366
             Mean action noise std: 0.63
                       Mean reward: 23.73
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62865408
                    Iteration time: 8.30s
                        Total time: 33720.50s
                               ETA: 845112.8s

################################################################################
                    [1m Learning iteration 3837/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.100s, learning 0.226s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0280
             Mean action noise std: 0.63
                       Mean reward: 23.74
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62881792
                    Iteration time: 8.33s
                        Total time: 33728.82s
                               ETA: 845092.5s

################################################################################
                    [1m Learning iteration 3838/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.044s, learning 0.162s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0347
             Mean action noise std: 0.63
                       Mean reward: 23.73
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62898176
                    Iteration time: 8.21s
                        Total time: 33737.03s
                               ETA: 845069.1s

################################################################################
                    [1m Learning iteration 3839/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.005s, learning 0.175s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0150
             Mean action noise std: 0.63
                       Mean reward: 23.68
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 8.18s
                        Total time: 33745.21s
                               ETA: 845045.1s

################################################################################
                    [1m Learning iteration 3840/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.171s, learning 0.163s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0267
             Mean action noise std: 0.63
                       Mean reward: 23.67
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62930944
                    Iteration time: 8.33s
                        Total time: 33753.54s
                               ETA: 845024.9s

################################################################################
                    [1m Learning iteration 3841/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.055s, learning 0.165s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0253
             Mean action noise std: 0.63
                       Mean reward: 23.66
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62947328
                    Iteration time: 8.22s
                        Total time: 33761.76s
                               ETA: 845001.9s

################################################################################
                    [1m Learning iteration 3842/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.936s, learning 0.202s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0419
             Mean action noise std: 0.63
                       Mean reward: 23.68
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62963712
                    Iteration time: 8.14s
                        Total time: 33769.90s
                               ETA: 844976.9s

################################################################################
                    [1m Learning iteration 3843/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.180s, learning 0.172s)
               Value function loss: 9.3277
                    Surrogate loss: 0.1264
             Mean action noise std: 0.63
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62980096
                    Iteration time: 8.35s
                        Total time: 33778.25s
                               ETA: 844957.2s

################################################################################
                    [1m Learning iteration 3844/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.170s, learning 0.262s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0120
             Mean action noise std: 0.63
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62996480
                    Iteration time: 8.43s
                        Total time: 33786.68s
                               ETA: 844939.5s

################################################################################
                    [1m Learning iteration 3845/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.902s, learning 0.164s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0200
             Mean action noise std: 0.63
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 8.07s
                        Total time: 33794.75s
                               ETA: 844912.7s

################################################################################
                    [1m Learning iteration 3846/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.934s, learning 0.185s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0262
             Mean action noise std: 0.63
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63029248
                    Iteration time: 8.12s
                        Total time: 33802.87s
                               ETA: 844887.2s

################################################################################
                    [1m Learning iteration 3847/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.851s, learning 0.218s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0288
             Mean action noise std: 0.63
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63045632
                    Iteration time: 8.07s
                        Total time: 33810.94s
                               ETA: 844860.5s

################################################################################
                    [1m Learning iteration 3848/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.962s, learning 0.177s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0276
             Mean action noise std: 0.63
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63062016
                    Iteration time: 8.14s
                        Total time: 33819.08s
                               ETA: 844835.5s

################################################################################
                    [1m Learning iteration 3849/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.185s, learning 0.168s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0240
             Mean action noise std: 0.63
                       Mean reward: 23.63
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63078400
                    Iteration time: 8.35s
                        Total time: 33827.43s
                               ETA: 844815.9s

################################################################################
                    [1m Learning iteration 3850/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.940s, learning 0.204s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0156
             Mean action noise std: 0.63
                       Mean reward: 23.62
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63094784
                    Iteration time: 8.14s
                        Total time: 33835.57s
                               ETA: 844791.1s

################################################################################
                    [1m Learning iteration 3851/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.172s, learning 0.183s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0269
             Mean action noise std: 0.63
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 8.36s
                        Total time: 33843.93s
                               ETA: 844771.5s

################################################################################
                    [1m Learning iteration 3852/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.135s, learning 0.162s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0348
             Mean action noise std: 0.63
                       Mean reward: 23.60
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63127552
                    Iteration time: 8.30s
                        Total time: 33852.23s
                               ETA: 844750.6s

################################################################################
                    [1m Learning iteration 3853/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.983s, learning 0.167s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0331
             Mean action noise std: 0.63
                       Mean reward: 23.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63143936
                    Iteration time: 8.15s
                        Total time: 33860.38s
                               ETA: 844725.9s

################################################################################
                    [1m Learning iteration 3854/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.209s, learning 0.199s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0253
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63160320
                    Iteration time: 8.41s
                        Total time: 33868.79s
                               ETA: 844707.7s

################################################################################
                    [1m Learning iteration 3855/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.159s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0329
             Mean action noise std: 0.63
                       Mean reward: 23.51
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63176704
                    Iteration time: 8.18s
                        Total time: 33876.97s
                               ETA: 844683.8s

################################################################################
                    [1m Learning iteration 3856/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.158s, learning 0.220s)
               Value function loss: 0.0252
                    Surrogate loss: -0.0307
             Mean action noise std: 0.63
                       Mean reward: 23.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63193088
                    Iteration time: 8.38s
                        Total time: 33885.34s
                               ETA: 844664.9s

################################################################################
                    [1m Learning iteration 3857/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.099s, learning 0.170s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0260
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 8.27s
                        Total time: 33893.61s
                               ETA: 844643.3s

################################################################################
                    [1m Learning iteration 3858/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.890s, learning 0.183s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0399
             Mean action noise std: 0.63
                       Mean reward: 23.52
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63225856
                    Iteration time: 8.07s
                        Total time: 33901.69s
                               ETA: 844616.7s

################################################################################
                    [1m Learning iteration 3859/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.034s, learning 0.165s)
               Value function loss: 5.6166
                    Surrogate loss: 0.0627
             Mean action noise std: 0.63
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63242240
                    Iteration time: 8.20s
                        Total time: 33909.89s
                               ETA: 844593.4s

################################################################################
                    [1m Learning iteration 3860/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.962s, learning 0.159s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0092
             Mean action noise std: 0.63
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63258624
                    Iteration time: 8.12s
                        Total time: 33918.01s
                               ETA: 844568.1s

################################################################################
                    [1m Learning iteration 3861/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.906s, learning 0.168s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0255
             Mean action noise std: 0.63
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63275008
                    Iteration time: 8.07s
                        Total time: 33926.08s
                               ETA: 844541.6s

################################################################################
                    [1m Learning iteration 3862/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.082s, learning 0.192s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0268
             Mean action noise std: 0.63
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63291392
                    Iteration time: 8.27s
                        Total time: 33934.36s
                               ETA: 844520.1s

################################################################################
                    [1m Learning iteration 3863/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.762s, learning 0.163s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0262
             Mean action noise std: 0.63
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 7.92s
                        Total time: 33942.28s
                               ETA: 844489.9s

################################################################################
                    [1m Learning iteration 3864/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.079s, learning 0.181s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0303
             Mean action noise std: 0.63
                       Mean reward: 23.44
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63324160
                    Iteration time: 8.26s
                        Total time: 33950.54s
                               ETA: 844468.1s

################################################################################
                    [1m Learning iteration 3865/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.082s, learning 0.183s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0243
             Mean action noise std: 0.63
                       Mean reward: 23.45
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63340544
                    Iteration time: 8.27s
                        Total time: 33958.81s
                               ETA: 844446.4s

################################################################################
                    [1m Learning iteration 3866/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.838s, learning 0.173s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0294
             Mean action noise std: 0.63
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63356928
                    Iteration time: 8.01s
                        Total time: 33966.82s
                               ETA: 844418.4s

################################################################################
                    [1m Learning iteration 3867/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.021s, learning 0.167s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0331
             Mean action noise std: 0.63
                       Mean reward: 23.53
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63373312
                    Iteration time: 8.19s
                        Total time: 33975.00s
                               ETA: 844394.8s

################################################################################
                    [1m Learning iteration 3868/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.859s, learning 0.167s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0405
             Mean action noise std: 0.63
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63389696
                    Iteration time: 8.03s
                        Total time: 33983.03s
                               ETA: 844367.2s

################################################################################
                    [1m Learning iteration 3869/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.251s, learning 0.164s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0318
             Mean action noise std: 0.63
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 8.41s
                        Total time: 33991.44s
                               ETA: 844349.2s

################################################################################
                    [1m Learning iteration 3870/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.043s, learning 0.161s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0279
             Mean action noise std: 0.63
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63422464
                    Iteration time: 8.20s
                        Total time: 33999.65s
                               ETA: 844326.1s

################################################################################
                    [1m Learning iteration 3871/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.909s, learning 0.164s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63438848
                    Iteration time: 8.07s
                        Total time: 34007.72s
                               ETA: 844299.7s

################################################################################
                    [1m Learning iteration 3872/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.162s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0371
             Mean action noise std: 0.63
                       Mean reward: 23.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63455232
                    Iteration time: 8.34s
                        Total time: 34016.06s
                               ETA: 844279.8s

################################################################################
                    [1m Learning iteration 3873/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.924s, learning 0.158s)
               Value function loss: 0.0175
                    Surrogate loss: -0.0232
             Mean action noise std: 0.63
                       Mean reward: 23.49
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63471616
                    Iteration time: 8.08s
                        Total time: 34024.14s
                               ETA: 844253.6s

################################################################################
                    [1m Learning iteration 3874/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.011s, learning 0.165s)
               Value function loss: 12.4953
                    Surrogate loss: 0.0661
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63488000
                    Iteration time: 8.18s
                        Total time: 34032.32s
                               ETA: 844229.8s

################################################################################
                    [1m Learning iteration 3875/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.835s, learning 0.191s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0082
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 8.03s
                        Total time: 34040.34s
                               ETA: 844202.3s

################################################################################
                    [1m Learning iteration 3876/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.776s, learning 0.161s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0195
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63520768
                    Iteration time: 7.94s
                        Total time: 34048.28s
                               ETA: 844172.5s

################################################################################
                    [1m Learning iteration 3877/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.726s, learning 0.190s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0252
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63537152
                    Iteration time: 7.92s
                        Total time: 34056.19s
                               ETA: 844142.2s

################################################################################
                    [1m Learning iteration 3878/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.843s, learning 0.251s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0243
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63553536
                    Iteration time: 8.09s
                        Total time: 34064.29s
                               ETA: 844116.4s

################################################################################
                    [1m Learning iteration 3879/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.833s, learning 0.175s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63569920
                    Iteration time: 8.01s
                        Total time: 34072.30s
                               ETA: 844088.4s

################################################################################
                    [1m Learning iteration 3880/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.068s, learning 0.215s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0263
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63586304
                    Iteration time: 8.28s
                        Total time: 34080.58s
                               ETA: 844067.3s

################################################################################
                    [1m Learning iteration 3881/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.829s, learning 0.198s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0325
             Mean action noise std: 0.63
                       Mean reward: 24.01
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 8.03s
                        Total time: 34088.61s
                               ETA: 844039.9s

################################################################################
                    [1m Learning iteration 3882/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.826s, learning 0.186s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0312
             Mean action noise std: 0.63
                       Mean reward: 24.02
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63619072
                    Iteration time: 8.01s
                        Total time: 34096.62s
                               ETA: 844012.1s

################################################################################
                    [1m Learning iteration 3883/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.207s, learning 0.189s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0365
             Mean action noise std: 0.63
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63635456
                    Iteration time: 8.40s
                        Total time: 34105.02s
                               ETA: 843993.8s

################################################################################
                    [1m Learning iteration 3884/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.250s, learning 0.162s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0205
             Mean action noise std: 0.63
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63651840
                    Iteration time: 8.41s
                        Total time: 34113.43s
                               ETA: 843975.8s

################################################################################
                    [1m Learning iteration 3885/100000 [0m                    

                       Computation: 2082 steps/s (collection: 7.685s, learning 0.182s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0386
             Mean action noise std: 0.63
                       Mean reward: 24.01
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63668224
                    Iteration time: 7.87s
                        Total time: 34121.29s
                               ETA: 843944.5s

################################################################################
                    [1m Learning iteration 3886/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.095s, learning 0.168s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0226
             Mean action noise std: 0.63
                       Mean reward: 23.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63684608
                    Iteration time: 8.26s
                        Total time: 34129.56s
                               ETA: 843922.9s

################################################################################
                    [1m Learning iteration 3887/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.154s, learning 0.161s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0284
             Mean action noise std: 0.63
                       Mean reward: 23.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 8.31s
                        Total time: 34137.87s
                               ETA: 843902.6s

################################################################################
                    [1m Learning iteration 3888/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.170s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 23.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63717376
                    Iteration time: 8.41s
                        Total time: 34146.29s
                               ETA: 843884.8s

################################################################################
                    [1m Learning iteration 3889/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.006s, learning 0.170s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0421
             Mean action noise std: 0.63
                       Mean reward: 23.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63733760
                    Iteration time: 8.18s
                        Total time: 34154.46s
                               ETA: 843861.1s

################################################################################
                    [1m Learning iteration 3890/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.058s, learning 0.172s)
               Value function loss: 7.5685
                    Surrogate loss: 0.1310
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63750144
                    Iteration time: 8.23s
                        Total time: 34162.69s
                               ETA: 843838.7s

################################################################################
                    [1m Learning iteration 3891/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.048s, learning 0.167s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0116
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63766528
                    Iteration time: 8.22s
                        Total time: 34170.91s
                               ETA: 843816.0s

################################################################################
                    [1m Learning iteration 3892/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.812s, learning 0.237s)
               Value function loss: 0.0163
                    Surrogate loss: -0.0152
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63782912
                    Iteration time: 8.05s
                        Total time: 34178.96s
                               ETA: 843789.1s

################################################################################
                    [1m Learning iteration 3893/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.896s, learning 0.166s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0151
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 8.06s
                        Total time: 34187.02s
                               ETA: 843762.7s

################################################################################
                    [1m Learning iteration 3894/100000 [0m                    

                       Computation: 2094 steps/s (collection: 7.648s, learning 0.173s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0236
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63815680
                    Iteration time: 7.82s
                        Total time: 34194.84s
                               ETA: 843730.2s

################################################################################
                    [1m Learning iteration 3895/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.825s, learning 0.162s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0287
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63832064
                    Iteration time: 7.99s
                        Total time: 34202.83s
                               ETA: 843701.9s

################################################################################
                    [1m Learning iteration 3896/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.002s, learning 0.160s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0246
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63848448
                    Iteration time: 8.16s
                        Total time: 34210.99s
                               ETA: 843677.9s

################################################################################
                    [1m Learning iteration 3897/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.772s, learning 0.161s)
               Value function loss: 0.0320
                    Surrogate loss: -0.0300
             Mean action noise std: 0.63
                       Mean reward: 24.22
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63864832
                    Iteration time: 7.93s
                        Total time: 34218.92s
                               ETA: 843648.3s

################################################################################
                    [1m Learning iteration 3898/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.769s, learning 0.165s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0392
             Mean action noise std: 0.63
                       Mean reward: 24.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63881216
                    Iteration time: 7.93s
                        Total time: 34226.86s
                               ETA: 843618.7s

################################################################################
                    [1m Learning iteration 3899/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.069s, learning 0.183s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0327
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 8.25s
                        Total time: 34235.11s
                               ETA: 843597.0s

################################################################################
                    [1m Learning iteration 3900/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.899s, learning 0.189s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0283
             Mean action noise std: 0.63
                       Mean reward: 24.27
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63913984
                    Iteration time: 8.09s
                        Total time: 34243.20s
                               ETA: 843571.2s

################################################################################
                    [1m Learning iteration 3901/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.901s, learning 0.160s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0254
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63930368
                    Iteration time: 8.06s
                        Total time: 34251.26s
                               ETA: 843544.7s

################################################################################
                    [1m Learning iteration 3902/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.804s, learning 0.162s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0261
             Mean action noise std: 0.63
                       Mean reward: 24.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63946752
                    Iteration time: 7.97s
                        Total time: 34259.22s
                               ETA: 843516.0s

################################################################################
                    [1m Learning iteration 3903/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.000s, learning 0.213s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0302
             Mean action noise std: 0.63
                       Mean reward: 24.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63963136
                    Iteration time: 8.21s
                        Total time: 34267.44s
                               ETA: 843493.3s

################################################################################
                    [1m Learning iteration 3904/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.289s, learning 0.159s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0222
             Mean action noise std: 0.63
                       Mean reward: 24.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63979520
                    Iteration time: 8.45s
                        Total time: 34275.88s
                               ETA: 843476.4s

################################################################################
                    [1m Learning iteration 3905/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.166s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0250
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 8.46s
                        Total time: 34284.34s
                               ETA: 843459.8s

################################################################################
                    [1m Learning iteration 3906/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.984s, learning 0.161s)
               Value function loss: 3.3101
                    Surrogate loss: 0.0552
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64012288
                    Iteration time: 8.15s
                        Total time: 34292.49s
                               ETA: 843435.5s

################################################################################
                    [1m Learning iteration 3907/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.934s, learning 0.167s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0147
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64028672
                    Iteration time: 8.10s
                        Total time: 34300.59s
                               ETA: 843410.1s

################################################################################
                    [1m Learning iteration 3908/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.284s, learning 0.174s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0066
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64045056
                    Iteration time: 8.46s
                        Total time: 34309.05s
                               ETA: 843393.5s

################################################################################
                    [1m Learning iteration 3909/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.976s, learning 0.195s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64061440
                    Iteration time: 8.17s
                        Total time: 34317.22s
                               ETA: 843369.8s

################################################################################
                    [1m Learning iteration 3910/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.969s, learning 0.155s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0268
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64077824
                    Iteration time: 8.12s
                        Total time: 34325.35s
                               ETA: 843345.0s

################################################################################
                    [1m Learning iteration 3911/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.789s, learning 0.163s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0316
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 7.95s
                        Total time: 34333.30s
                               ETA: 843316.0s

################################################################################
                    [1m Learning iteration 3912/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.924s, learning 0.183s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0244
             Mean action noise std: 0.63
                       Mean reward: 24.23
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64110592
                    Iteration time: 8.11s
                        Total time: 34341.40s
                               ETA: 843290.8s

################################################################################
                    [1m Learning iteration 3913/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.910s, learning 0.171s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0269
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64126976
                    Iteration time: 8.08s
                        Total time: 34349.48s
                               ETA: 843264.9s

################################################################################
                    [1m Learning iteration 3914/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.001s, learning 0.161s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0344
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64143360
                    Iteration time: 8.16s
                        Total time: 34357.65s
                               ETA: 843241.1s

################################################################################
                    [1m Learning iteration 3915/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.959s, learning 0.163s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0423
             Mean action noise std: 0.63
                       Mean reward: 24.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64159744
                    Iteration time: 8.12s
                        Total time: 34365.77s
                               ETA: 843216.3s

################################################################################
                    [1m Learning iteration 3916/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.778s, learning 0.155s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 24.19
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64176128
                    Iteration time: 7.93s
                        Total time: 34373.70s
                               ETA: 843186.8s

################################################################################
                    [1m Learning iteration 3917/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.255s, learning 0.165s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0252
             Mean action noise std: 0.63
                       Mean reward: 24.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 8.42s
                        Total time: 34382.12s
                               ETA: 843169.3s

################################################################################
                    [1m Learning iteration 3918/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.253s, learning 0.220s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0271
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64208896
                    Iteration time: 8.47s
                        Total time: 34390.60s
                               ETA: 843153.1s

################################################################################
                    [1m Learning iteration 3919/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.878s, learning 0.163s)
               Value function loss: 0.0166
                    Surrogate loss: -0.0222
             Mean action noise std: 0.63
                       Mean reward: 24.20
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64225280
                    Iteration time: 8.04s
                        Total time: 34398.64s
                               ETA: 843126.4s

################################################################################
                    [1m Learning iteration 3920/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.924s, learning 0.163s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0303
             Mean action noise std: 0.63
                       Mean reward: 24.18
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64241664
                    Iteration time: 8.09s
                        Total time: 34406.72s
                               ETA: 843100.7s

################################################################################
                    [1m Learning iteration 3921/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.985s, learning 0.199s)
               Value function loss: 12.2571
                    Surrogate loss: 0.0407
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64258048
                    Iteration time: 8.18s
                        Total time: 34414.91s
                               ETA: 843077.5s

################################################################################
                    [1m Learning iteration 3922/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.929s, learning 0.168s)
               Value function loss: 0.0090
                    Surrogate loss: -0.0100
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64274432
                    Iteration time: 8.10s
                        Total time: 34423.00s
                               ETA: 843052.1s

################################################################################
                    [1m Learning iteration 3923/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.932s, learning 0.200s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0168
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 8.13s
                        Total time: 34431.14s
                               ETA: 843027.6s

################################################################################
                    [1m Learning iteration 3924/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.829s, learning 0.170s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0220
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64307200
                    Iteration time: 8.00s
                        Total time: 34439.13s
                               ETA: 842999.8s

################################################################################
                    [1m Learning iteration 3925/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.939s, learning 0.188s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0160
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64323584
                    Iteration time: 8.13s
                        Total time: 34447.26s
                               ETA: 842975.2s

################################################################################
                    [1m Learning iteration 3926/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.840s, learning 0.164s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0276
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64339968
                    Iteration time: 8.00s
                        Total time: 34455.27s
                               ETA: 842947.6s

################################################################################
                    [1m Learning iteration 3927/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.157s, learning 0.176s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0309
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64356352
                    Iteration time: 8.33s
                        Total time: 34463.60s
                               ETA: 842928.0s

################################################################################
                    [1m Learning iteration 3928/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.276s, learning 0.204s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0472
             Mean action noise std: 0.63
                       Mean reward: 24.27
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64372736
                    Iteration time: 8.48s
                        Total time: 34472.08s
                               ETA: 842912.1s

################################################################################
                    [1m Learning iteration 3929/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.324s, learning 0.246s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0362
             Mean action noise std: 0.63
                       Mean reward: 24.28
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 8.57s
                        Total time: 34480.65s
                               ETA: 842898.3s

################################################################################
                    [1m Learning iteration 3930/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.235s, learning 0.166s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0319
             Mean action noise std: 0.63
                       Mean reward: 24.29
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64405504
                    Iteration time: 8.40s
                        Total time: 34489.05s
                               ETA: 842880.4s

################################################################################
                    [1m Learning iteration 3931/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.922s, learning 0.160s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0215
             Mean action noise std: 0.63
                       Mean reward: 24.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64421888
                    Iteration time: 8.08s
                        Total time: 34497.13s
                               ETA: 842854.8s

################################################################################
                    [1m Learning iteration 3932/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.760s, learning 0.171s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0288
             Mean action noise std: 0.63
                       Mean reward: 24.33
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64438272
                    Iteration time: 7.93s
                        Total time: 34505.06s
                               ETA: 842825.4s

################################################################################
                    [1m Learning iteration 3933/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.282s, learning 0.181s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0206
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64454656
                    Iteration time: 8.46s
                        Total time: 34513.53s
                               ETA: 842809.1s

################################################################################
                    [1m Learning iteration 3934/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.384s, learning 0.203s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0279
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64471040
                    Iteration time: 8.59s
                        Total time: 34522.11s
                               ETA: 842795.7s

################################################################################
                    [1m Learning iteration 3935/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.393s, learning 0.171s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0173
             Mean action noise std: 0.63
                       Mean reward: 24.38
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 8.56s
                        Total time: 34530.68s
                               ETA: 842781.9s

################################################################################
                    [1m Learning iteration 3936/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.051s, learning 0.248s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0411
             Mean action noise std: 0.63
                       Mean reward: 24.38
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64503808
                    Iteration time: 8.30s
                        Total time: 34538.98s
                               ETA: 842761.5s

################################################################################
                    [1m Learning iteration 3937/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.845s, learning 0.168s)
               Value function loss: 7.1515
                    Surrogate loss: 0.1209
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64520192
                    Iteration time: 8.01s
                        Total time: 34546.99s
                               ETA: 842734.2s

################################################################################
                    [1m Learning iteration 3938/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.023s, learning 0.179s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0139
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64536576
                    Iteration time: 8.20s
                        Total time: 34555.19s
                               ETA: 842711.5s

################################################################################
                    [1m Learning iteration 3939/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.939s, learning 0.169s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0215
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64552960
                    Iteration time: 8.11s
                        Total time: 34563.30s
                               ETA: 842686.5s

################################################################################
                    [1m Learning iteration 3940/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.147s, learning 0.171s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0162
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64569344
                    Iteration time: 8.32s
                        Total time: 34571.62s
                               ETA: 842666.7s

################################################################################
                    [1m Learning iteration 3941/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.766s, learning 0.168s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0179
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 7.93s
                        Total time: 34579.55s
                               ETA: 842637.5s

################################################################################
                    [1m Learning iteration 3942/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.148s, learning 0.273s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0282
             Mean action noise std: 0.63
                       Mean reward: 24.52
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64602112
                    Iteration time: 8.42s
                        Total time: 34587.97s
                               ETA: 842620.2s

################################################################################
                    [1m Learning iteration 3943/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.152s, learning 0.184s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0294
             Mean action noise std: 0.63
                       Mean reward: 24.53
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64618496
                    Iteration time: 8.34s
                        Total time: 34596.31s
                               ETA: 842600.8s

################################################################################
                    [1m Learning iteration 3944/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.079s, learning 0.188s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0350
             Mean action noise std: 0.63
                       Mean reward: 24.56
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64634880
                    Iteration time: 8.27s
                        Total time: 34604.57s
                               ETA: 842579.7s

################################################################################
                    [1m Learning iteration 3945/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.800s, learning 0.157s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0352
             Mean action noise std: 0.63
                       Mean reward: 24.57
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64651264
                    Iteration time: 7.96s
                        Total time: 34612.53s
                               ETA: 842551.1s

################################################################################
                    [1m Learning iteration 3946/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.093s, learning 0.165s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0286
             Mean action noise std: 0.63
                       Mean reward: 24.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64667648
                    Iteration time: 8.26s
                        Total time: 34620.79s
                               ETA: 842529.9s

################################################################################
                    [1m Learning iteration 3947/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.967s, learning 0.169s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0218
             Mean action noise std: 0.63
                       Mean reward: 24.61
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 8.14s
                        Total time: 34628.93s
                               ETA: 842505.6s

################################################################################
                    [1m Learning iteration 3948/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.754s, learning 0.161s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 24.62
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64700416
                    Iteration time: 7.92s
                        Total time: 34636.84s
                               ETA: 842476.1s

################################################################################
                    [1m Learning iteration 3949/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.216s, learning 0.214s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0232
             Mean action noise std: 0.63
                       Mean reward: 24.63
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64716800
                    Iteration time: 8.43s
                        Total time: 34645.27s
                               ETA: 842459.0s

################################################################################
                    [1m Learning iteration 3950/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.300s, learning 0.161s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0244
             Mean action noise std: 0.63
                       Mean reward: 24.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64733184
                    Iteration time: 8.46s
                        Total time: 34653.73s
                               ETA: 842442.7s

################################################################################
                    [1m Learning iteration 3951/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.953s, learning 0.212s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 24.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64749568
                    Iteration time: 8.17s
                        Total time: 34661.90s
                               ETA: 842419.2s

################################################################################
                    [1m Learning iteration 3952/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.371s, learning 0.184s)
               Value function loss: 10.0339
                    Surrogate loss: 0.0897
             Mean action noise std: 0.63
                       Mean reward: 24.43
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64765952
                    Iteration time: 8.56s
                        Total time: 34670.45s
                               ETA: 842405.2s

################################################################################
                    [1m Learning iteration 3953/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.077s, learning 0.238s)
               Value function loss: 0.1693
                    Surrogate loss: -0.0109
             Mean action noise std: 0.63
                       Mean reward: 24.43
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 8.31s
                        Total time: 34678.77s
                               ETA: 842385.3s

################################################################################
                    [1m Learning iteration 3954/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.852s, learning 0.185s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0157
             Mean action noise std: 0.63
                       Mean reward: 24.43
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64798720
                    Iteration time: 8.04s
                        Total time: 34686.81s
                               ETA: 842358.8s

################################################################################
                    [1m Learning iteration 3955/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.842s, learning 0.189s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0108
             Mean action noise std: 0.63
                       Mean reward: 24.43
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64815104
                    Iteration time: 8.03s
                        Total time: 34694.84s
                               ETA: 842332.0s

################################################################################
                    [1m Learning iteration 3956/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.182s, learning 0.163s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0193
             Mean action noise std: 0.63
                       Mean reward: 24.43
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64831488
                    Iteration time: 8.34s
                        Total time: 34703.18s
                               ETA: 842312.9s

################################################################################
                    [1m Learning iteration 3957/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.851s, learning 0.156s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0222
             Mean action noise std: 0.63
                       Mean reward: 24.43
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64847872
                    Iteration time: 8.01s
                        Total time: 34711.19s
                               ETA: 842285.7s

################################################################################
                    [1m Learning iteration 3958/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.042s, learning 0.191s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0327
             Mean action noise std: 0.63
                       Mean reward: 24.43
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64864256
                    Iteration time: 8.23s
                        Total time: 34719.42s
                               ETA: 842263.9s

################################################################################
                    [1m Learning iteration 3959/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.850s, learning 0.166s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0345
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 8.02s
                        Total time: 34727.44s
                               ETA: 842236.8s

################################################################################
                    [1m Learning iteration 3960/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.949s, learning 0.162s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0359
             Mean action noise std: 0.63
                       Mean reward: 24.38
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64897024
                    Iteration time: 8.11s
                        Total time: 34735.55s
                               ETA: 842212.1s

################################################################################
                    [1m Learning iteration 3961/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.903s, learning 0.160s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0314
             Mean action noise std: 0.63
                       Mean reward: 24.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64913408
                    Iteration time: 8.06s
                        Total time: 34743.61s
                               ETA: 842186.2s

################################################################################
                    [1m Learning iteration 3962/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.773s, learning 0.164s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0221
             Mean action noise std: 0.63
                       Mean reward: 24.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64929792
                    Iteration time: 7.94s
                        Total time: 34751.55s
                               ETA: 842157.3s

################################################################################
                    [1m Learning iteration 3963/100000 [0m                    

                       Computation: 2073 steps/s (collection: 7.733s, learning 0.168s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0279
             Mean action noise std: 0.63
                       Mean reward: 24.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64946176
                    Iteration time: 7.90s
                        Total time: 34759.45s
                               ETA: 842127.5s

################################################################################
                    [1m Learning iteration 3964/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.887s, learning 0.169s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0142
             Mean action noise std: 0.63
                       Mean reward: 24.31
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64962560
                    Iteration time: 8.06s
                        Total time: 34767.51s
                               ETA: 842101.4s

################################################################################
                    [1m Learning iteration 3965/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.896s, learning 0.222s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0172
             Mean action noise std: 0.63
                       Mean reward: 24.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 8.12s
                        Total time: 34775.62s
                               ETA: 842076.9s

################################################################################
                    [1m Learning iteration 3966/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.702s, learning 0.156s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0194
             Mean action noise std: 0.63
                       Mean reward: 24.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64995328
                    Iteration time: 7.86s
                        Total time: 34783.48s
                               ETA: 842046.1s

################################################################################
                    [1m Learning iteration 3967/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.694s, learning 0.160s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0337
             Mean action noise std: 0.63
                       Mean reward: 24.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65011712
                    Iteration time: 7.85s
                        Total time: 34791.34s
                               ETA: 842015.2s

################################################################################
                    [1m Learning iteration 3968/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.840s, learning 0.286s)
               Value function loss: 9.6342
                    Surrogate loss: 0.0708
             Mean action noise std: 0.63
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65028096
                    Iteration time: 8.13s
                        Total time: 34799.46s
                               ETA: 841990.9s

################################################################################
                    [1m Learning iteration 3969/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.895s, learning 0.166s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0183
             Mean action noise std: 0.63
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65044480
                    Iteration time: 8.06s
                        Total time: 34807.52s
                               ETA: 841965.0s

################################################################################
                    [1m Learning iteration 3970/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.826s, learning 0.167s)
               Value function loss: 0.0010
                    Surrogate loss: -0.0171
             Mean action noise std: 0.63
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65060864
                    Iteration time: 7.99s
                        Total time: 34815.51s
                               ETA: 841937.5s

################################################################################
                    [1m Learning iteration 3971/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.815s, learning 0.187s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0077
             Mean action noise std: 0.63
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 8.00s
                        Total time: 34823.52s
                               ETA: 841910.2s

################################################################################
                    [1m Learning iteration 3972/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.953s, learning 0.168s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0122
             Mean action noise std: 0.63
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65093632
                    Iteration time: 8.12s
                        Total time: 34831.64s
                               ETA: 841885.9s

################################################################################
                    [1m Learning iteration 3973/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.099s, learning 0.197s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0303
             Mean action noise std: 0.63
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65110016
                    Iteration time: 8.30s
                        Total time: 34839.93s
                               ETA: 841865.7s

################################################################################
                    [1m Learning iteration 3974/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.928s, learning 0.179s)
               Value function loss: 0.0139
                    Surrogate loss: -0.0274
             Mean action noise std: 0.63
                       Mean reward: 23.87
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65126400
                    Iteration time: 8.11s
                        Total time: 34848.04s
                               ETA: 841841.0s

################################################################################
                    [1m Learning iteration 3975/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.059s, learning 0.175s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0275
             Mean action noise std: 0.63
                       Mean reward: 23.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65142784
                    Iteration time: 8.23s
                        Total time: 34856.28s
                               ETA: 841819.4s

################################################################################
                    [1m Learning iteration 3976/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.847s, learning 0.199s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0310
             Mean action noise std: 0.63
                       Mean reward: 23.93
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65159168
                    Iteration time: 8.05s
                        Total time: 34864.32s
                               ETA: 841793.2s

################################################################################
                    [1m Learning iteration 3977/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.988s, learning 0.164s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 23.96
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 8.15s
                        Total time: 34872.47s
                               ETA: 841769.6s

################################################################################
                    [1m Learning iteration 3978/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.024s, learning 0.199s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0201
             Mean action noise std: 0.63
                       Mean reward: 23.99
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65191936
                    Iteration time: 8.22s
                        Total time: 34880.70s
                               ETA: 841747.7s

################################################################################
                    [1m Learning iteration 3979/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.128s, learning 0.176s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0162
             Mean action noise std: 0.63
                       Mean reward: 23.98
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65208320
                    Iteration time: 8.30s
                        Total time: 34889.00s
                               ETA: 841727.8s

################################################################################
                    [1m Learning iteration 3980/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.944s, learning 0.192s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 24.01
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65224704
                    Iteration time: 8.14s
                        Total time: 34897.14s
                               ETA: 841703.9s

################################################################################
                    [1m Learning iteration 3981/100000 [0m                    

                       Computation: 2115 steps/s (collection: 7.589s, learning 0.156s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0175
             Mean action noise std: 0.63
                       Mean reward: 24.01
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65241088
                    Iteration time: 7.74s
                        Total time: 34904.88s
                               ETA: 841670.5s

################################################################################
                    [1m Learning iteration 3982/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.946s, learning 0.163s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0092
             Mean action noise std: 0.63
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65257472
                    Iteration time: 8.11s
                        Total time: 34912.99s
                               ETA: 841645.9s

################################################################################
                    [1m Learning iteration 3983/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.868s, learning 0.259s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0356
             Mean action noise std: 0.63
                       Mean reward: 24.03
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 8.13s
                        Total time: 34921.12s
                               ETA: 841621.7s

################################################################################
                    [1m Learning iteration 3984/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.973s, learning 0.175s)
               Value function loss: 6.0832
                    Surrogate loss: 0.0793
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65290240
                    Iteration time: 8.15s
                        Total time: 34929.27s
                               ETA: 841598.1s

################################################################################
                    [1m Learning iteration 3985/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.769s, learning 0.163s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0111
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65306624
                    Iteration time: 7.93s
                        Total time: 34937.20s
                               ETA: 841569.2s

################################################################################
                    [1m Learning iteration 3986/100000 [0m                    

                       Computation: 2129 steps/s (collection: 7.526s, learning 0.166s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0149
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65323008
                    Iteration time: 7.69s
                        Total time: 34944.89s
                               ETA: 841534.6s

################################################################################
                    [1m Learning iteration 3987/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.045s, learning 0.158s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0083
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65339392
                    Iteration time: 8.20s
                        Total time: 34953.09s
                               ETA: 841512.3s

################################################################################
                    [1m Learning iteration 3988/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.927s, learning 0.175s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0323
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65355776
                    Iteration time: 8.10s
                        Total time: 34961.19s
                               ETA: 841487.6s

################################################################################
                    [1m Learning iteration 3989/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.960s, learning 0.162s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0356
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 8.12s
                        Total time: 34969.32s
                               ETA: 841463.4s

################################################################################
                    [1m Learning iteration 3990/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.038s, learning 0.158s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0349
             Mean action noise std: 0.63
                       Mean reward: 25.09
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65388544
                    Iteration time: 8.20s
                        Total time: 34977.51s
                               ETA: 841441.0s

################################################################################
                    [1m Learning iteration 3991/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.780s, learning 0.168s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0388
             Mean action noise std: 0.63
                       Mean reward: 25.10
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65404928
                    Iteration time: 7.95s
                        Total time: 34985.46s
                               ETA: 841412.6s

################################################################################
                    [1m Learning iteration 3992/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.914s, learning 0.163s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0290
             Mean action noise std: 0.63
                       Mean reward: 25.12
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65421312
                    Iteration time: 8.08s
                        Total time: 34993.54s
                               ETA: 841387.3s

################################################################################
                    [1m Learning iteration 3993/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.872s, learning 0.168s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0371
             Mean action noise std: 0.63
                       Mean reward: 25.13
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65437696
                    Iteration time: 8.04s
                        Total time: 35001.58s
                               ETA: 841361.2s

################################################################################
                    [1m Learning iteration 3994/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.753s, learning 0.164s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0223
             Mean action noise std: 0.63
                       Mean reward: 25.17
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65454080
                    Iteration time: 7.92s
                        Total time: 35009.49s
                               ETA: 841332.0s

################################################################################
                    [1m Learning iteration 3995/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.994s, learning 0.167s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0227
             Mean action noise std: 0.63
                       Mean reward: 25.19
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 8.16s
                        Total time: 35017.66s
                               ETA: 841308.8s

################################################################################
                    [1m Learning iteration 3996/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.124s, learning 0.163s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0183
             Mean action noise std: 0.63
                       Mean reward: 25.20
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65486848
                    Iteration time: 8.29s
                        Total time: 35025.94s
                               ETA: 841288.6s

################################################################################
                    [1m Learning iteration 3997/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.089s, learning 0.163s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0298
             Mean action noise std: 0.63
                       Mean reward: 25.19
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65503232
                    Iteration time: 8.25s
                        Total time: 35034.19s
                               ETA: 841267.6s

################################################################################
                    [1m Learning iteration 3998/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.162s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0213
             Mean action noise std: 0.63
                       Mean reward: 25.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65519616
                    Iteration time: 8.24s
                        Total time: 35042.44s
                               ETA: 841246.3s

################################################################################
                    [1m Learning iteration 3999/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.907s, learning 0.217s)
               Value function loss: 12.3028
                    Surrogate loss: 0.0708
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536000
                    Iteration time: 8.12s
                        Total time: 35050.56s
                               ETA: 841222.3s

################################################################################
                    [1m Learning iteration 4000/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.824s, learning 0.165s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0122
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65552384
                    Iteration time: 7.99s
                        Total time: 35058.55s
                               ETA: 841194.9s

################################################################################
                    [1m Learning iteration 4001/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.165s, learning 0.196s)
               Value function loss: 0.0177
                    Surrogate loss: 0.0010
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 8.36s
                        Total time: 35066.91s
                               ETA: 841176.5s

################################################################################
                    [1m Learning iteration 4002/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.846s, learning 0.218s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0195
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65585152
                    Iteration time: 8.06s
                        Total time: 35074.98s
                               ETA: 841151.0s

################################################################################
                    [1m Learning iteration 4003/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.912s, learning 0.167s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0248
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65601536
                    Iteration time: 8.08s
                        Total time: 35083.06s
                               ETA: 841125.9s

################################################################################
                    [1m Learning iteration 4004/100000 [0m                    

                       Computation: 2093 steps/s (collection: 7.632s, learning 0.193s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0286
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65617920
                    Iteration time: 7.82s
                        Total time: 35090.88s
                               ETA: 841094.7s

################################################################################
                    [1m Learning iteration 4005/100000 [0m                    

                       Computation: 2159 steps/s (collection: 7.347s, learning 0.240s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0359
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65634304
                    Iteration time: 7.59s
                        Total time: 35098.47s
                               ETA: 841057.8s

################################################################################
                    [1m Learning iteration 4006/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.020s, learning 0.160s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0215
             Mean action noise std: 0.63
                       Mean reward: 25.85
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65650688
                    Iteration time: 8.18s
                        Total time: 35106.65s
                               ETA: 841035.1s

################################################################################
                    [1m Learning iteration 4007/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.870s, learning 0.164s)
               Value function loss: 0.0531
                    Surrogate loss: -0.0302
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 8.03s
                        Total time: 35114.68s
                               ETA: 841008.9s

################################################################################
                    [1m Learning iteration 4008/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.108s, learning 0.163s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0308
             Mean action noise std: 0.63
                       Mean reward: 25.85
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65683456
                    Iteration time: 8.27s
                        Total time: 35122.95s
                               ETA: 840988.4s

################################################################################
                    [1m Learning iteration 4009/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.162s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0183
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65699840
                    Iteration time: 8.10s
                        Total time: 35131.06s
                               ETA: 840963.9s

################################################################################
                    [1m Learning iteration 4010/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.861s, learning 0.301s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0327
             Mean action noise std: 0.63
                       Mean reward: 25.84
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65716224
                    Iteration time: 8.16s
                        Total time: 35139.22s
                               ETA: 840940.8s

################################################################################
                    [1m Learning iteration 4011/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.952s, learning 0.167s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0144
             Mean action noise std: 0.63
                       Mean reward: 25.80
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65732608
                    Iteration time: 8.12s
                        Total time: 35147.34s
                               ETA: 840916.7s

################################################################################
                    [1m Learning iteration 4012/100000 [0m                    

                       Computation: 2090 steps/s (collection: 7.647s, learning 0.190s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0198
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65748992
                    Iteration time: 7.84s
                        Total time: 35155.17s
                               ETA: 840885.9s

################################################################################
                    [1m Learning iteration 4013/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.160s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0150
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 8.27s
                        Total time: 35163.45s
                               ETA: 840865.4s

################################################################################
                    [1m Learning iteration 4014/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.918s, learning 0.181s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0381
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65781760
                    Iteration time: 8.10s
                        Total time: 35171.55s
                               ETA: 840840.9s

################################################################################
                    [1m Learning iteration 4015/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.918s, learning 0.179s)
               Value function loss: 8.7856
                    Surrogate loss: 0.1126
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65798144
                    Iteration time: 8.10s
                        Total time: 35179.64s
                               ETA: 840816.3s

################################################################################
                    [1m Learning iteration 4016/100000 [0m                    

                       Computation: 2088 steps/s (collection: 7.630s, learning 0.214s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0147
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65814528
                    Iteration time: 7.84s
                        Total time: 35187.49s
                               ETA: 840785.6s

################################################################################
                    [1m Learning iteration 4017/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.912s, learning 0.161s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0145
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65830912
                    Iteration time: 8.07s
                        Total time: 35195.56s
                               ETA: 840760.5s

################################################################################
                    [1m Learning iteration 4018/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.991s, learning 0.170s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0170
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65847296
                    Iteration time: 8.16s
                        Total time: 35203.72s
                               ETA: 840737.4s

################################################################################
                    [1m Learning iteration 4019/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.814s, learning 0.177s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0115
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.99s
                        Total time: 35211.71s
                               ETA: 840710.3s

################################################################################
                    [1m Learning iteration 4020/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.071s, learning 0.162s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0295
             Mean action noise std: 0.63
                       Mean reward: 25.07
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65880064
                    Iteration time: 8.23s
                        Total time: 35219.95s
                               ETA: 840689.0s

################################################################################
                    [1m Learning iteration 4021/100000 [0m                    

                       Computation: 2111 steps/s (collection: 7.595s, learning 0.165s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0241
             Mean action noise std: 0.63
                       Mean reward: 25.08
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65896448
                    Iteration time: 7.76s
                        Total time: 35227.71s
                               ETA: 840656.4s

################################################################################
                    [1m Learning iteration 4022/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.766s, learning 0.170s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0277
             Mean action noise std: 0.63
                       Mean reward: 25.12
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65912832
                    Iteration time: 7.94s
                        Total time: 35235.64s
                               ETA: 840628.0s

################################################################################
                    [1m Learning iteration 4023/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.204s, learning 0.187s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0307
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65929216
                    Iteration time: 8.39s
                        Total time: 35244.03s
                               ETA: 840610.5s

################################################################################
                    [1m Learning iteration 4024/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.898s, learning 0.190s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0223
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65945600
                    Iteration time: 8.09s
                        Total time: 35252.12s
                               ETA: 840585.8s

################################################################################
                    [1m Learning iteration 4025/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.802s, learning 0.158s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0163
             Mean action noise std: 0.63
                       Mean reward: 25.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.96s
                        Total time: 35260.08s
                               ETA: 840558.0s

################################################################################
                    [1m Learning iteration 4026/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.828s, learning 0.248s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0234
             Mean action noise std: 0.63
                       Mean reward: 25.24
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65978368
                    Iteration time: 8.08s
                        Total time: 35268.16s
                               ETA: 840533.0s

################################################################################
                    [1m Learning iteration 4027/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.934s, learning 0.174s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0190
             Mean action noise std: 0.63
                       Mean reward: 25.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65994752
                    Iteration time: 8.11s
                        Total time: 35276.27s
                               ETA: 840508.7s

################################################################################
                    [1m Learning iteration 4028/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.971s, learning 0.167s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0229
             Mean action noise std: 0.63
                       Mean reward: 25.27
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66011136
                    Iteration time: 8.14s
                        Total time: 35284.40s
                               ETA: 840485.2s

################################################################################
                    [1m Learning iteration 4029/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.117s, learning 0.190s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0173
             Mean action noise std: 0.63
                       Mean reward: 25.24
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66027520
                    Iteration time: 8.31s
                        Total time: 35292.71s
                               ETA: 840465.7s

################################################################################
                    [1m Learning iteration 4030/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.338s, learning 0.232s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0207
             Mean action noise std: 0.63
                       Mean reward: 25.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66043904
                    Iteration time: 8.57s
                        Total time: 35301.28s
                               ETA: 840452.5s

################################################################################
                    [1m Learning iteration 4031/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.993s, learning 0.170s)
               Value function loss: 3.4857
                    Surrogate loss: 0.0654
             Mean action noise std: 0.63
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 8.16s
                        Total time: 35309.45s
                               ETA: 840429.6s

################################################################################
                    [1m Learning iteration 4032/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.940s, learning 0.183s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0208
             Mean action noise std: 0.63
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66076672
                    Iteration time: 8.12s
                        Total time: 35317.57s
                               ETA: 840405.7s

################################################################################
                    [1m Learning iteration 4033/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.958s, learning 0.164s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0091
             Mean action noise std: 0.63
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66093056
                    Iteration time: 8.12s
                        Total time: 35325.69s
                               ETA: 840381.9s

################################################################################
                    [1m Learning iteration 4034/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.797s, learning 0.165s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0131
             Mean action noise std: 0.63
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66109440
                    Iteration time: 7.96s
                        Total time: 35333.65s
                               ETA: 840354.2s

################################################################################
                    [1m Learning iteration 4035/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.867s, learning 0.189s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0331
             Mean action noise std: 0.63
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66125824
                    Iteration time: 8.06s
                        Total time: 35341.71s
                               ETA: 840328.8s

################################################################################
                    [1m Learning iteration 4036/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.720s, learning 0.237s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0366
             Mean action noise std: 0.63
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66142208
                    Iteration time: 7.96s
                        Total time: 35349.67s
                               ETA: 840301.0s

################################################################################
                    [1m Learning iteration 4037/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.902s, learning 0.168s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0244
             Mean action noise std: 0.63
                       Mean reward: 25.55
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 8.07s
                        Total time: 35357.74s
                               ETA: 840276.0s

################################################################################
                    [1m Learning iteration 4038/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.041s, learning 0.215s)
               Value function loss: 0.0303
                    Surrogate loss: -0.0250
             Mean action noise std: 0.63
                       Mean reward: 25.57
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66174976
                    Iteration time: 8.26s
                        Total time: 35365.99s
                               ETA: 840255.3s

################################################################################
                    [1m Learning iteration 4039/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.212s, learning 0.160s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0227
             Mean action noise std: 0.63
                       Mean reward: 25.58
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66191360
                    Iteration time: 8.37s
                        Total time: 35374.36s
                               ETA: 840237.5s

################################################################################
                    [1m Learning iteration 4040/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.282s, learning 0.169s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0274
             Mean action noise std: 0.63
                       Mean reward: 25.58
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66207744
                    Iteration time: 8.45s
                        Total time: 35382.82s
                               ETA: 840221.5s

################################################################################
                    [1m Learning iteration 4041/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.026s, learning 0.182s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0180
             Mean action noise std: 0.63
                       Mean reward: 25.67
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66224128
                    Iteration time: 8.21s
                        Total time: 35391.02s
                               ETA: 840199.7s

################################################################################
                    [1m Learning iteration 4042/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.069s, learning 0.157s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0107
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66240512
                    Iteration time: 8.23s
                        Total time: 35399.25s
                               ETA: 840178.4s

################################################################################
                    [1m Learning iteration 4043/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.835s, learning 0.185s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0151
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 8.02s
                        Total time: 35407.27s
                               ETA: 840152.2s

################################################################################
                    [1m Learning iteration 4044/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.927s, learning 0.156s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0160
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66273280
                    Iteration time: 8.08s
                        Total time: 35415.35s
                               ETA: 840127.4s

################################################################################
                    [1m Learning iteration 4045/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.720s, learning 0.275s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0191
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66289664
                    Iteration time: 8.00s
                        Total time: 35423.35s
                               ETA: 840100.7s

################################################################################
                    [1m Learning iteration 4046/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.143s, learning 0.178s)
               Value function loss: 13.7235
                    Surrogate loss: 0.0572
             Mean action noise std: 0.63
                       Mean reward: 26.16
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66306048
                    Iteration time: 8.32s
                        Total time: 35431.67s
                               ETA: 840081.6s

################################################################################
                    [1m Learning iteration 4047/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.018s, learning 0.212s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0055
             Mean action noise std: 0.63
                       Mean reward: 26.16
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66322432
                    Iteration time: 8.23s
                        Total time: 35439.90s
                               ETA: 840060.5s

################################################################################
                    [1m Learning iteration 4048/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.847s, learning 0.202s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0034
             Mean action noise std: 0.63
                       Mean reward: 26.16
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66338816
                    Iteration time: 8.05s
                        Total time: 35447.95s
                               ETA: 840035.0s

################################################################################
                    [1m Learning iteration 4049/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.680s, learning 0.181s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0189
             Mean action noise std: 0.63
                       Mean reward: 26.16
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.86s
                        Total time: 35455.81s
                               ETA: 840005.0s

################################################################################
                    [1m Learning iteration 4050/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.785s, learning 0.189s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0293
             Mean action noise std: 0.63
                       Mean reward: 26.16
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66371584
                    Iteration time: 7.97s
                        Total time: 35463.78s
                               ETA: 839977.8s

################################################################################
                    [1m Learning iteration 4051/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.954s, learning 0.245s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0399
             Mean action noise std: 0.63
                       Mean reward: 26.16
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66387968
                    Iteration time: 8.20s
                        Total time: 35471.98s
                               ETA: 839955.9s

################################################################################
                    [1m Learning iteration 4052/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.072s, learning 0.170s)
               Value function loss: 0.0117
                    Surrogate loss: -0.0253
             Mean action noise std: 0.63
                       Mean reward: 26.16
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66404352
                    Iteration time: 8.24s
                        Total time: 35480.22s
                               ETA: 839935.0s

################################################################################
                    [1m Learning iteration 4053/100000 [0m                    

                       Computation: 2268 steps/s (collection: 7.062s, learning 0.161s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0362
             Mean action noise std: 0.63
                       Mean reward: 26.18
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66420736
                    Iteration time: 7.22s
                        Total time: 35487.45s
                               ETA: 839890.0s

################################################################################
                    [1m Learning iteration 4054/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.769s, learning 0.165s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0291
             Mean action noise std: 0.63
                       Mean reward: 26.20
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66437120
                    Iteration time: 7.93s
                        Total time: 35495.38s
                               ETA: 839861.9s

################################################################################
                    [1m Learning iteration 4055/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.790s, learning 0.213s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0211
             Mean action noise std: 0.63
                       Mean reward: 26.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 8.00s
                        Total time: 35503.38s
                               ETA: 839835.3s

################################################################################
                    [1m Learning iteration 4056/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.938s, learning 0.157s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0210
             Mean action noise std: 0.63
                       Mean reward: 26.20
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66469888
                    Iteration time: 8.10s
                        Total time: 35511.48s
                               ETA: 839811.0s

################################################################################
                    [1m Learning iteration 4057/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.945s, learning 0.168s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0140
             Mean action noise std: 0.63
                       Mean reward: 26.20
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66486272
                    Iteration time: 8.11s
                        Total time: 35519.59s
                               ETA: 839787.1s

################################################################################
                    [1m Learning iteration 4058/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.106s, learning 0.157s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0169
             Mean action noise std: 0.63
                       Mean reward: 26.15
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66502656
                    Iteration time: 8.26s
                        Total time: 35527.85s
                               ETA: 839766.8s

################################################################################
                    [1m Learning iteration 4059/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.908s, learning 0.195s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0220
             Mean action noise std: 0.63
                       Mean reward: 26.12
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66519040
                    Iteration time: 8.10s
                        Total time: 35535.96s
                               ETA: 839742.7s

################################################################################
                    [1m Learning iteration 4060/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.732s, learning 0.212s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0142
             Mean action noise std: 0.63
                       Mean reward: 26.10
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66535424
                    Iteration time: 7.94s
                        Total time: 35543.90s
                               ETA: 839714.8s

################################################################################
                    [1m Learning iteration 4061/100000 [0m                    

                       Computation: 2083 steps/s (collection: 7.696s, learning 0.169s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0388
             Mean action noise std: 0.63
                       Mean reward: 26.10
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 7.87s
                        Total time: 35551.77s
                               ETA: 839685.1s

################################################################################
                    [1m Learning iteration 4062/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.783s, learning 0.174s)
               Value function loss: 8.4990
                    Surrogate loss: 0.1024
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66568192
                    Iteration time: 7.96s
                        Total time: 35559.72s
                               ETA: 839657.6s

################################################################################
                    [1m Learning iteration 4063/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.028s, learning 0.160s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0106
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66584576
                    Iteration time: 8.19s
                        Total time: 35567.91s
                               ETA: 839635.5s

################################################################################
                    [1m Learning iteration 4064/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.911s, learning 0.188s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0189
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66600960
                    Iteration time: 8.10s
                        Total time: 35576.01s
                               ETA: 839611.4s

################################################################################
                    [1m Learning iteration 4065/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.833s, learning 0.244s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0083
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66617344
                    Iteration time: 8.08s
                        Total time: 35584.09s
                               ETA: 839586.7s

################################################################################
                    [1m Learning iteration 4066/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.954s, learning 0.221s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0259
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66633728
                    Iteration time: 8.18s
                        Total time: 35592.26s
                               ETA: 839564.4s

################################################################################
                    [1m Learning iteration 4067/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.971s, learning 0.222s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0347
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 8.19s
                        Total time: 35600.46s
                               ETA: 839542.4s

################################################################################
                    [1m Learning iteration 4068/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.176s, learning 0.166s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0297
             Mean action noise std: 0.63
                       Mean reward: 25.65
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66666496
                    Iteration time: 8.34s
                        Total time: 35608.80s
                               ETA: 839524.1s

################################################################################
                    [1m Learning iteration 4069/100000 [0m                    

                       Computation: 1995 steps/s (collection: 7.967s, learning 0.245s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0298
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66682880
                    Iteration time: 8.21s
                        Total time: 35617.01s
                               ETA: 839502.6s

################################################################################
                    [1m Learning iteration 4070/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.944s, learning 0.168s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0330
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66699264
                    Iteration time: 8.11s
                        Total time: 35625.12s
                               ETA: 839478.8s

################################################################################
                    [1m Learning iteration 4071/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.849s, learning 0.166s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0308
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66715648
                    Iteration time: 8.01s
                        Total time: 35633.14s
                               ETA: 839452.7s

################################################################################
                    [1m Learning iteration 4072/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.971s, learning 0.176s)
               Value function loss: 0.0314
                    Surrogate loss: -0.0213
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66732032
                    Iteration time: 8.15s
                        Total time: 35641.29s
                               ETA: 839429.8s

################################################################################
                    [1m Learning iteration 4073/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.752s, learning 0.188s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0226
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 7.94s
                        Total time: 35649.23s
                               ETA: 839401.9s

################################################################################
                    [1m Learning iteration 4074/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.984s, learning 0.157s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0266
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66764800
                    Iteration time: 8.14s
                        Total time: 35657.37s
                               ETA: 839378.8s

################################################################################
                    [1m Learning iteration 4075/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.935s, learning 0.160s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0278
             Mean action noise std: 0.63
                       Mean reward: 25.64
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66781184
                    Iteration time: 8.09s
                        Total time: 35665.46s
                               ETA: 839354.6s

################################################################################
                    [1m Learning iteration 4076/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.884s, learning 0.169s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0217
             Mean action noise std: 0.63
                       Mean reward: 25.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66797568
                    Iteration time: 8.05s
                        Total time: 35673.52s
                               ETA: 839329.5s

################################################################################
                    [1m Learning iteration 4077/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.031s, learning 0.166s)
               Value function loss: 8.0300
                    Surrogate loss: 0.1028
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66813952
                    Iteration time: 8.20s
                        Total time: 35681.71s
                               ETA: 839307.7s

################################################################################
                    [1m Learning iteration 4078/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.793s, learning 0.170s)
               Value function loss: 0.7304
                    Surrogate loss: 0.0004
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66830336
                    Iteration time: 7.96s
                        Total time: 35689.68s
                               ETA: 839280.5s

################################################################################
                    [1m Learning iteration 4079/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.786s, learning 0.189s)
               Value function loss: 0.0910
                    Surrogate loss: -0.0070
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 7.97s
                        Total time: 35697.65s
                               ETA: 839253.5s

################################################################################
                    [1m Learning iteration 4080/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.064s, learning 0.169s)
               Value function loss: 0.0744
                    Surrogate loss: -0.0059
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66863104
                    Iteration time: 8.23s
                        Total time: 35705.88s
                               ETA: 839232.6s

################################################################################
                    [1m Learning iteration 4081/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.686s, learning 0.175s)
               Value function loss: 0.0637
                    Surrogate loss: -0.0202
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66879488
                    Iteration time: 7.86s
                        Total time: 35713.74s
                               ETA: 839203.0s

################################################################################
                    [1m Learning iteration 4082/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.243s, learning 0.166s)
               Value function loss: 0.1010
                    Surrogate loss: -0.0259
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66895872
                    Iteration time: 8.41s
                        Total time: 35722.15s
                               ETA: 839186.2s

################################################################################
                    [1m Learning iteration 4083/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.881s, learning 0.178s)
               Value function loss: 0.0676
                    Surrogate loss: -0.0231
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66912256
                    Iteration time: 8.06s
                        Total time: 35730.21s
                               ETA: 839161.3s

################################################################################
                    [1m Learning iteration 4084/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.949s, learning 0.160s)
               Value function loss: 0.0493
                    Surrogate loss: -0.0237
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66928640
                    Iteration time: 8.11s
                        Total time: 35738.32s
                               ETA: 839137.5s

################################################################################
                    [1m Learning iteration 4085/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.771s, learning 0.159s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0330
             Mean action noise std: 0.63
                       Mean reward: 25.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 7.93s
                        Total time: 35746.25s
                               ETA: 839109.6s

################################################################################
                    [1m Learning iteration 4086/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.798s, learning 0.159s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0284
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66961408
                    Iteration time: 7.96s
                        Total time: 35754.21s
                               ETA: 839082.2s

################################################################################
                    [1m Learning iteration 4087/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.927s, learning 0.158s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0154
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66977792
                    Iteration time: 8.09s
                        Total time: 35762.29s
                               ETA: 839057.9s

################################################################################
                    [1m Learning iteration 4088/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.913s, learning 0.167s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0276
             Mean action noise std: 0.63
                       Mean reward: 25.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66994176
                    Iteration time: 8.08s
                        Total time: 35770.37s
                               ETA: 839033.5s

################################################################################
                    [1m Learning iteration 4089/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.862s, learning 0.155s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0121
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67010560
                    Iteration time: 8.02s
                        Total time: 35778.39s
                               ETA: 839007.6s

################################################################################
                    [1m Learning iteration 4090/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.023s, learning 0.219s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0238
             Mean action noise std: 0.63
                       Mean reward: 25.48
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67026944
                    Iteration time: 8.24s
                        Total time: 35786.63s
                               ETA: 838987.0s

################################################################################
                    [1m Learning iteration 4091/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.084s, learning 0.211s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0216
             Mean action noise std: 0.63
                       Mean reward: 25.47
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 8.30s
                        Total time: 35794.93s
                               ETA: 838967.7s

################################################################################
                    [1m Learning iteration 4092/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.133s, learning 0.195s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0325
             Mean action noise std: 0.63
                       Mean reward: 25.46
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67059712
                    Iteration time: 8.33s
                        Total time: 35803.26s
                               ETA: 838949.1s

################################################################################
                    [1m Learning iteration 4093/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.089s, learning 0.163s)
               Value function loss: 9.1926
                    Surrogate loss: 0.1233
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67076096
                    Iteration time: 8.25s
                        Total time: 35811.51s
                               ETA: 838928.7s

################################################################################
                    [1m Learning iteration 4094/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.807s, learning 0.171s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0120
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67092480
                    Iteration time: 7.98s
                        Total time: 35819.49s
                               ETA: 838902.0s

################################################################################
                    [1m Learning iteration 4095/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.025s, learning 0.174s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0166
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67108864
                    Iteration time: 8.20s
                        Total time: 35827.68s
                               ETA: 838880.4s

################################################################################
                    [1m Learning iteration 4096/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.906s, learning 0.170s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0126
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67125248
                    Iteration time: 8.08s
                        Total time: 35835.76s
                               ETA: 838855.9s

################################################################################
                    [1m Learning iteration 4097/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.938s, learning 0.174s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 8.11s
                        Total time: 35843.87s
                               ETA: 838832.3s

################################################################################
                    [1m Learning iteration 4098/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.930s, learning 0.171s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0342
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67158016
                    Iteration time: 8.10s
                        Total time: 35851.97s
                               ETA: 838808.5s

################################################################################
                    [1m Learning iteration 4099/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.907s, learning 0.226s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0276
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67174400
                    Iteration time: 8.13s
                        Total time: 35860.11s
                               ETA: 838785.4s

################################################################################
                    [1m Learning iteration 4100/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.139s, learning 0.161s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0297
             Mean action noise std: 0.63
                       Mean reward: 25.61
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67190784
                    Iteration time: 8.30s
                        Total time: 35868.41s
                               ETA: 838766.2s

################################################################################
                    [1m Learning iteration 4101/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.983s, learning 0.192s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0296
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67207168
                    Iteration time: 8.17s
                        Total time: 35876.58s
                               ETA: 838744.1s

################################################################################
                    [1m Learning iteration 4102/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.027s, learning 0.161s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0235
             Mean action noise std: 0.63
                       Mean reward: 25.64
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67223552
                    Iteration time: 8.19s
                        Total time: 35884.77s
                               ETA: 838722.3s

################################################################################
                    [1m Learning iteration 4103/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.844s, learning 0.195s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0151
             Mean action noise std: 0.63
                       Mean reward: 25.65
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 8.04s
                        Total time: 35892.81s
                               ETA: 838697.0s

################################################################################
                    [1m Learning iteration 4104/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.196s, learning 0.195s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0157
             Mean action noise std: 0.63
                       Mean reward: 25.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67256320
                    Iteration time: 8.39s
                        Total time: 35901.20s
                               ETA: 838680.0s

################################################################################
                    [1m Learning iteration 4105/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.913s, learning 0.192s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0176
             Mean action noise std: 0.63
                       Mean reward: 25.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67272704
                    Iteration time: 8.11s
                        Total time: 35909.30s
                               ETA: 838656.3s

################################################################################
                    [1m Learning iteration 4106/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.079s, learning 0.156s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0187
             Mean action noise std: 0.63
                       Mean reward: 25.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67289088
                    Iteration time: 8.24s
                        Total time: 35917.54s
                               ETA: 838635.6s

################################################################################
                    [1m Learning iteration 4107/100000 [0m                    

                       Computation: 2100 steps/s (collection: 7.633s, learning 0.168s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0138
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67305472
                    Iteration time: 7.80s
                        Total time: 35925.34s
                               ETA: 838604.8s

################################################################################
                    [1m Learning iteration 4108/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.272s, learning 0.216s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0375
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67321856
                    Iteration time: 8.49s
                        Total time: 35933.83s
                               ETA: 838590.1s

################################################################################
                    [1m Learning iteration 4109/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.942s, learning 0.170s)
               Value function loss: 4.7276
                    Surrogate loss: 0.0971
             Mean action noise std: 0.63
                       Mean reward: 25.95
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 8.11s
                        Total time: 35941.94s
                               ETA: 838566.5s

################################################################################
                    [1m Learning iteration 4110/100000 [0m                    

                       Computation: 2090 steps/s (collection: 7.679s, learning 0.160s)
               Value function loss: 0.0115
                    Surrogate loss: -0.0145
             Mean action noise std: 0.63
                       Mean reward: 25.95
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67354624
                    Iteration time: 7.84s
                        Total time: 35949.78s
                               ETA: 838536.7s

################################################################################
                    [1m Learning iteration 4111/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.827s, learning 0.197s)
               Value function loss: 0.0108
                    Surrogate loss: -0.0039
             Mean action noise std: 0.63
                       Mean reward: 25.95
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67371008
                    Iteration time: 8.02s
                        Total time: 35957.80s
                               ETA: 838511.1s

################################################################################
                    [1m Learning iteration 4112/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.986s, learning 0.163s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0181
             Mean action noise std: 0.63
                       Mean reward: 25.95
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67387392
                    Iteration time: 8.15s
                        Total time: 35965.95s
                               ETA: 838488.5s

################################################################################
                    [1m Learning iteration 4113/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.929s, learning 0.164s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0358
             Mean action noise std: 0.63
                       Mean reward: 25.95
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67403776
                    Iteration time: 8.09s
                        Total time: 35974.04s
                               ETA: 838464.5s

################################################################################
                    [1m Learning iteration 4114/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.761s, learning 0.156s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0345
             Mean action noise std: 0.63
                       Mean reward: 25.95
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67420160
                    Iteration time: 7.92s
                        Total time: 35981.96s
                               ETA: 838436.5s

################################################################################
                    [1m Learning iteration 4115/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.984s, learning 0.169s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0392
             Mean action noise std: 0.63
                       Mean reward: 25.94
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 8.15s
                        Total time: 35990.11s
                               ETA: 838414.0s

################################################################################
                    [1m Learning iteration 4116/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.034s, learning 0.161s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0392
             Mean action noise std: 0.63
                       Mean reward: 25.96
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67452928
                    Iteration time: 8.20s
                        Total time: 35998.31s
                               ETA: 838392.5s

################################################################################
                    [1m Learning iteration 4117/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.675s, learning 0.261s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0323
             Mean action noise std: 0.63
                       Mean reward: 25.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67469312
                    Iteration time: 7.94s
                        Total time: 36006.24s
                               ETA: 838364.9s

################################################################################
                    [1m Learning iteration 4118/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.049s, learning 0.251s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0354
             Mean action noise std: 0.63
                       Mean reward: 26.00
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67485696
                    Iteration time: 8.30s
                        Total time: 36014.54s
                               ETA: 838345.8s

################################################################################
                    [1m Learning iteration 4119/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.909s, learning 0.181s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0201
             Mean action noise std: 0.63
                       Mean reward: 25.98
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67502080
                    Iteration time: 8.09s
                        Total time: 36022.63s
                               ETA: 838321.9s

################################################################################
                    [1m Learning iteration 4120/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.895s, learning 0.164s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0243
             Mean action noise std: 0.63
                       Mean reward: 26.00
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67518464
                    Iteration time: 8.06s
                        Total time: 36030.69s
                               ETA: 838297.2s

################################################################################
                    [1m Learning iteration 4121/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.958s, learning 0.166s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0181
             Mean action noise std: 0.63
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 8.12s
                        Total time: 36038.82s
                               ETA: 838274.1s

################################################################################
                    [1m Learning iteration 4122/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.865s, learning 0.241s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0329
             Mean action noise std: 0.63
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67551232
                    Iteration time: 8.11s
                        Total time: 36046.92s
                               ETA: 838250.5s

################################################################################
                    [1m Learning iteration 4123/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.172s, learning 0.199s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0229
             Mean action noise std: 0.63
                       Mean reward: 25.85
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67567616
                    Iteration time: 8.37s
                        Total time: 36055.29s
                               ETA: 838233.1s

################################################################################
                    [1m Learning iteration 4124/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.165s, learning 0.215s)
               Value function loss: 10.3315
                    Surrogate loss: 0.0688
             Mean action noise std: 0.63
                       Mean reward: 24.94
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67584000
                    Iteration time: 8.38s
                        Total time: 36063.67s
                               ETA: 838216.0s

################################################################################
                    [1m Learning iteration 4125/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.190s, learning 0.188s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0021
             Mean action noise std: 0.63
                       Mean reward: 24.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67600384
                    Iteration time: 8.38s
                        Total time: 36072.05s
                               ETA: 838198.7s

################################################################################
                    [1m Learning iteration 4126/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.848s, learning 0.214s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0124
             Mean action noise std: 0.63
                       Mean reward: 24.94
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67616768
                    Iteration time: 8.06s
                        Total time: 36080.11s
                               ETA: 838174.2s

################################################################################
                    [1m Learning iteration 4127/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.714s, learning 0.168s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0176
             Mean action noise std: 0.63
                       Mean reward: 24.94
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 7.88s
                        Total time: 36088.00s
                               ETA: 838145.5s

################################################################################
                    [1m Learning iteration 4128/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.942s, learning 0.249s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0224
             Mean action noise std: 0.63
                       Mean reward: 24.94
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67649536
                    Iteration time: 8.19s
                        Total time: 36096.19s
                               ETA: 838123.9s

################################################################################
                    [1m Learning iteration 4129/100000 [0m                    

                       Computation: 1994 steps/s (collection: 7.958s, learning 0.258s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0270
             Mean action noise std: 0.63
                       Mean reward: 24.94
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67665920
                    Iteration time: 8.22s
                        Total time: 36104.40s
                               ETA: 838103.0s

################################################################################
                    [1m Learning iteration 4130/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.203s, learning 0.170s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0202
             Mean action noise std: 0.63
                       Mean reward: 24.94
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67682304
                    Iteration time: 8.37s
                        Total time: 36112.78s
                               ETA: 838085.7s

################################################################################
                    [1m Learning iteration 4131/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.141s, learning 0.159s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0295
             Mean action noise std: 0.63
                       Mean reward: 24.90
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67698688
                    Iteration time: 8.30s
                        Total time: 36121.08s
                               ETA: 838066.6s

################################################################################
                    [1m Learning iteration 4132/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.083s, learning 0.171s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 24.90
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67715072
                    Iteration time: 8.25s
                        Total time: 36129.33s
                               ETA: 838046.6s

################################################################################
                    [1m Learning iteration 4133/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.134s, learning 0.164s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0344
             Mean action noise std: 0.63
                       Mean reward: 24.89
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 8.30s
                        Total time: 36137.63s
                               ETA: 838027.6s

################################################################################
                    [1m Learning iteration 4134/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.801s, learning 0.188s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0142
             Mean action noise std: 0.63
                       Mean reward: 24.87
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67747840
                    Iteration time: 7.99s
                        Total time: 36145.62s
                               ETA: 838001.4s

################################################################################
                    [1m Learning iteration 4135/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.100s, learning 0.168s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0349
             Mean action noise std: 0.63
                       Mean reward: 24.84
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67764224
                    Iteration time: 8.27s
                        Total time: 36153.88s
                               ETA: 837981.7s

################################################################################
                    [1m Learning iteration 4136/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.176s, learning 0.167s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0112
             Mean action noise std: 0.63
                       Mean reward: 24.79
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67780608
                    Iteration time: 8.34s
                        Total time: 36162.23s
                               ETA: 837963.7s

################################################################################
                    [1m Learning iteration 4137/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.727s, learning 0.190s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0215
             Mean action noise std: 0.63
                       Mean reward: 24.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67796992
                    Iteration time: 7.92s
                        Total time: 36170.14s
                               ETA: 837935.8s

################################################################################
                    [1m Learning iteration 4138/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.184s, learning 0.226s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0095
             Mean action noise std: 0.63
                       Mean reward: 24.68
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67813376
                    Iteration time: 8.41s
                        Total time: 36178.55s
                               ETA: 837919.4s

################################################################################
                    [1m Learning iteration 4139/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.067s, learning 0.163s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0408
             Mean action noise std: 0.63
                       Mean reward: 24.68
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 8.23s
                        Total time: 36186.78s
                               ETA: 837898.9s

################################################################################
                    [1m Learning iteration 4140/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.369s, learning 0.178s)
               Value function loss: 7.8334
                    Surrogate loss: 0.2473
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67846144
                    Iteration time: 8.55s
                        Total time: 36195.33s
                               ETA: 837885.6s

################################################################################
                    [1m Learning iteration 4141/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.760s, learning 0.165s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0022
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67862528
                    Iteration time: 7.93s
                        Total time: 36203.26s
                               ETA: 837858.0s

################################################################################
                    [1m Learning iteration 4142/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.892s, learning 0.192s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0074
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67878912
                    Iteration time: 8.08s
                        Total time: 36211.34s
                               ETA: 837834.1s

################################################################################
                    [1m Learning iteration 4143/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.182s, learning 0.228s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0117
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67895296
                    Iteration time: 8.41s
                        Total time: 36219.75s
                               ETA: 837817.7s

################################################################################
                    [1m Learning iteration 4144/100000 [0m                    

                       Computation: 2104 steps/s (collection: 7.609s, learning 0.176s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0197
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67911680
                    Iteration time: 7.79s
                        Total time: 36227.54s
                               ETA: 837786.9s

################################################################################
                    [1m Learning iteration 4145/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.228s, learning 0.161s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0216
             Mean action noise std: 0.63
                       Mean reward: 24.45
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 8.39s
                        Total time: 36235.92s
                               ETA: 837770.0s

################################################################################
                    [1m Learning iteration 4146/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.130s, learning 0.161s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0184
             Mean action noise std: 0.63
                       Mean reward: 24.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67944448
                    Iteration time: 8.29s
                        Total time: 36244.21s
                               ETA: 837750.9s

################################################################################
                    [1m Learning iteration 4147/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.939s, learning 0.162s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0268
             Mean action noise std: 0.63
                       Mean reward: 24.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67960832
                    Iteration time: 8.10s
                        Total time: 36252.32s
                               ETA: 837727.4s

################################################################################
                    [1m Learning iteration 4148/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.832s, learning 0.166s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0357
             Mean action noise std: 0.63
                       Mean reward: 24.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67977216
                    Iteration time: 8.00s
                        Total time: 36260.31s
                               ETA: 837701.5s

################################################################################
                    [1m Learning iteration 4149/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.934s, learning 0.170s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0255
             Mean action noise std: 0.63
                       Mean reward: 24.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67993600
                    Iteration time: 8.10s
                        Total time: 36268.42s
                               ETA: 837678.1s

################################################################################
                    [1m Learning iteration 4150/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.059s, learning 0.270s)
               Value function loss: 0.0272
                    Surrogate loss: -0.0172
             Mean action noise std: 0.63
                       Mean reward: 24.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68009984
                    Iteration time: 8.33s
                        Total time: 36276.75s
                               ETA: 837659.9s

################################################################################
                    [1m Learning iteration 4151/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.846s, learning 0.165s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0155
             Mean action noise std: 0.63
                       Mean reward: 24.68
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 8.01s
                        Total time: 36284.76s
                               ETA: 837634.3s

################################################################################
                    [1m Learning iteration 4152/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.839s, learning 0.158s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0179
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68042752
                    Iteration time: 8.00s
                        Total time: 36292.76s
                               ETA: 837608.5s

################################################################################
                    [1m Learning iteration 4153/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.912s, learning 0.193s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0207
             Mean action noise std: 0.63
                       Mean reward: 24.76
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68059136
                    Iteration time: 8.11s
                        Total time: 36300.86s
                               ETA: 837585.1s

################################################################################
                    [1m Learning iteration 4154/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.245s, learning 0.165s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0179
             Mean action noise std: 0.63
                       Mean reward: 24.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68075520
                    Iteration time: 8.41s
                        Total time: 36309.27s
                               ETA: 837568.8s

################################################################################
                    [1m Learning iteration 4155/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.210s, learning 0.216s)
               Value function loss: 0.0147
                    Surrogate loss: -0.0167
             Mean action noise std: 0.63
                       Mean reward: 24.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68091904
                    Iteration time: 8.43s
                        Total time: 36317.70s
                               ETA: 837552.9s

################################################################################
                    [1m Learning iteration 4156/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.860s, learning 0.165s)
               Value function loss: 3.4484
                    Surrogate loss: 0.0559
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68108288
                    Iteration time: 8.03s
                        Total time: 36325.72s
                               ETA: 837527.7s

################################################################################
                    [1m Learning iteration 4157/100000 [0m                    

                       Computation: 2123 steps/s (collection: 7.557s, learning 0.159s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0110
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 7.72s
                        Total time: 36333.44s
                               ETA: 837495.4s

################################################################################
                    [1m Learning iteration 4158/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.797s, learning 0.166s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0136
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68141056
                    Iteration time: 7.96s
                        Total time: 36341.40s
                               ETA: 837468.8s

################################################################################
                    [1m Learning iteration 4159/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.955s, learning 0.174s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0098
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68157440
                    Iteration time: 8.13s
                        Total time: 36349.53s
                               ETA: 837446.0s

################################################################################
                    [1m Learning iteration 4160/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.119s, learning 0.164s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0264
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68173824
                    Iteration time: 8.28s
                        Total time: 36357.81s
                               ETA: 837426.8s

################################################################################
                    [1m Learning iteration 4161/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.856s, learning 0.189s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0265
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68190208
                    Iteration time: 8.04s
                        Total time: 36365.86s
                               ETA: 837402.1s

################################################################################
                    [1m Learning iteration 4162/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.889s, learning 0.181s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0166
             Mean action noise std: 0.63
                       Mean reward: 25.85
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68206592
                    Iteration time: 8.07s
                        Total time: 36373.93s
                               ETA: 837378.0s

################################################################################
                    [1m Learning iteration 4163/100000 [0m                    

                       Computation: 2086 steps/s (collection: 7.684s, learning 0.168s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0263
             Mean action noise std: 0.63
                       Mean reward: 25.85
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 7.85s
                        Total time: 36381.78s
                               ETA: 837348.9s

################################################################################
                    [1m Learning iteration 4164/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.890s, learning 0.204s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 25.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68239360
                    Iteration time: 8.09s
                        Total time: 36389.87s
                               ETA: 837325.3s

################################################################################
                    [1m Learning iteration 4165/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.224s, learning 0.203s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0272
             Mean action noise std: 0.63
                       Mean reward: 25.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68255744
                    Iteration time: 8.43s
                        Total time: 36398.30s
                               ETA: 837309.5s

################################################################################
                    [1m Learning iteration 4166/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.255s, learning 0.244s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0166
             Mean action noise std: 0.63
                       Mean reward: 25.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68272128
                    Iteration time: 8.50s
                        Total time: 36406.80s
                               ETA: 837295.2s

################################################################################
                    [1m Learning iteration 4167/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.888s, learning 0.169s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0124
             Mean action noise std: 0.63
                       Mean reward: 25.89
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68288512
                    Iteration time: 8.06s
                        Total time: 36414.86s
                               ETA: 837270.9s

################################################################################
                    [1m Learning iteration 4168/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.046s, learning 0.173s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0152
             Mean action noise std: 0.63
                       Mean reward: 25.85
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68304896
                    Iteration time: 8.22s
                        Total time: 36423.08s
                               ETA: 837250.2s

################################################################################
                    [1m Learning iteration 4169/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.914s, learning 0.184s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0154
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 8.10s
                        Total time: 36431.17s
                               ETA: 837226.8s

################################################################################
                    [1m Learning iteration 4170/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.958s, learning 0.201s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0237
             Mean action noise std: 0.63
                       Mean reward: 25.81
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68337664
                    Iteration time: 8.16s
                        Total time: 36439.33s
                               ETA: 837204.8s

################################################################################
                    [1m Learning iteration 4171/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.811s, learning 0.164s)
               Value function loss: 13.2780
                    Surrogate loss: 0.0544
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68354048
                    Iteration time: 7.97s
                        Total time: 36447.31s
                               ETA: 837178.6s

################################################################################
                    [1m Learning iteration 4172/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.099s, learning 0.163s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0072
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68370432
                    Iteration time: 8.26s
                        Total time: 36455.57s
                               ETA: 837158.9s

################################################################################
                    [1m Learning iteration 4173/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.973s, learning 0.189s)
               Value function loss: 0.0471
                    Surrogate loss: -0.0093
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68386816
                    Iteration time: 8.16s
                        Total time: 36463.73s
                               ETA: 837137.0s

################################################################################
                    [1m Learning iteration 4174/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.109s, learning 0.166s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0083
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68403200
                    Iteration time: 8.28s
                        Total time: 36472.00s
                               ETA: 837117.7s

################################################################################
                    [1m Learning iteration 4175/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.883s, learning 0.157s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0257
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 8.04s
                        Total time: 36480.04s
                               ETA: 837093.0s

################################################################################
                    [1m Learning iteration 4176/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.988s, learning 0.167s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0279
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68435968
                    Iteration time: 8.15s
                        Total time: 36488.20s
                               ETA: 837070.9s

################################################################################
                    [1m Learning iteration 4177/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.006s, learning 0.159s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0357
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68452352
                    Iteration time: 8.16s
                        Total time: 36496.36s
                               ETA: 837049.1s

################################################################################
                    [1m Learning iteration 4178/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.987s, learning 0.208s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0351
             Mean action noise std: 0.63
                       Mean reward: 25.61
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68468736
                    Iteration time: 8.20s
                        Total time: 36504.56s
                               ETA: 837028.0s

################################################################################
                    [1m Learning iteration 4179/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.131s, learning 0.166s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0323
             Mean action noise std: 0.63
                       Mean reward: 25.61
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68485120
                    Iteration time: 8.30s
                        Total time: 36512.86s
                               ETA: 837009.2s

################################################################################
                    [1m Learning iteration 4180/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.913s, learning 0.166s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0243
             Mean action noise std: 0.63
                       Mean reward: 25.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68501504
                    Iteration time: 8.08s
                        Total time: 36520.94s
                               ETA: 836985.4s

################################################################################
                    [1m Learning iteration 4181/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.820s, learning 0.202s)
               Value function loss: 0.0301
                    Surrogate loss: -0.0185
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 8.02s
                        Total time: 36528.96s
                               ETA: 836960.4s

################################################################################
                    [1m Learning iteration 4182/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.059s, learning 0.166s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0103
             Mean action noise std: 0.63
                       Mean reward: 25.64
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68534272
                    Iteration time: 8.22s
                        Total time: 36537.18s
                               ETA: 836939.9s

################################################################################
                    [1m Learning iteration 4183/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.217s, learning 0.165s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0130
             Mean action noise std: 0.63
                       Mean reward: 25.61
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68550656
                    Iteration time: 8.38s
                        Total time: 36545.57s
                               ETA: 836923.1s

################################################################################
                    [1m Learning iteration 4184/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.932s, learning 0.162s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0150
             Mean action noise std: 0.63
                       Mean reward: 25.58
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68567040
                    Iteration time: 8.09s
                        Total time: 36553.66s
                               ETA: 836899.7s

################################################################################
                    [1m Learning iteration 4185/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.018s, learning 0.161s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0084
             Mean action noise std: 0.63
                       Mean reward: 25.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68583424
                    Iteration time: 8.18s
                        Total time: 36561.84s
                               ETA: 836878.3s

################################################################################
                    [1m Learning iteration 4186/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.868s, learning 0.194s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0400
             Mean action noise std: 0.63
                       Mean reward: 25.60
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68599808
                    Iteration time: 8.06s
                        Total time: 36569.90s
                               ETA: 836854.2s

################################################################################
                    [1m Learning iteration 4187/100000 [0m                    

                       Computation: 2043 steps/s (collection: 7.856s, learning 0.163s)
               Value function loss: 7.7514
                    Surrogate loss: 0.1681
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 8.02s
                        Total time: 36577.92s
                               ETA: 836829.0s

################################################################################
                    [1m Learning iteration 4188/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.076s, learning 0.164s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0103
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68632576
                    Iteration time: 8.24s
                        Total time: 36586.16s
                               ETA: 836809.0s

################################################################################
                    [1m Learning iteration 4189/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.881s, learning 0.168s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0069
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68648960
                    Iteration time: 8.05s
                        Total time: 36594.21s
                               ETA: 836784.6s

################################################################################
                    [1m Learning iteration 4190/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.993s, learning 0.168s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0017
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68665344
                    Iteration time: 8.16s
                        Total time: 36602.37s
                               ETA: 836762.8s

################################################################################
                    [1m Learning iteration 4191/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.124s, learning 0.224s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0297
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68681728
                    Iteration time: 8.35s
                        Total time: 36610.72s
                               ETA: 836745.3s

################################################################################
                    [1m Learning iteration 4192/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.319s, learning 0.197s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0278
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68698112
                    Iteration time: 8.52s
                        Total time: 36619.23s
                               ETA: 836731.6s

################################################################################
                    [1m Learning iteration 4193/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.561s, learning 0.191s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0286
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 8.75s
                        Total time: 36627.98s
                               ETA: 836723.2s

################################################################################
                    [1m Learning iteration 4194/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.217s, learning 0.172s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0302
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68730880
                    Iteration time: 8.39s
                        Total time: 36636.37s
                               ETA: 836706.6s

################################################################################
                    [1m Learning iteration 4195/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.940s, learning 0.174s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0320
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68747264
                    Iteration time: 8.11s
                        Total time: 36644.49s
                               ETA: 836683.8s

################################################################################
                    [1m Learning iteration 4196/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.099s, learning 0.158s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0306
             Mean action noise std: 0.63
                       Mean reward: 25.37
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68763648
                    Iteration time: 8.26s
                        Total time: 36652.74s
                               ETA: 836664.2s

################################################################################
                    [1m Learning iteration 4197/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.316s, learning 0.167s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0106
             Mean action noise std: 0.63
                       Mean reward: 25.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68780032
                    Iteration time: 8.48s
                        Total time: 36661.23s
                               ETA: 836649.7s

################################################################################
                    [1m Learning iteration 4198/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.807s, learning 0.165s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0174
             Mean action noise std: 0.63
                       Mean reward: 25.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68796416
                    Iteration time: 7.97s
                        Total time: 36669.20s
                               ETA: 836623.6s

################################################################################
                    [1m Learning iteration 4199/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.812s, learning 0.280s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0213
             Mean action noise std: 0.63
                       Mean reward: 25.32
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 8.09s
                        Total time: 36677.29s
                               ETA: 836600.3s

################################################################################
                    [1m Learning iteration 4200/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.136s, learning 0.226s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0193
             Mean action noise std: 0.63
                       Mean reward: 25.29
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68829184
                    Iteration time: 8.36s
                        Total time: 36685.65s
                               ETA: 836583.1s

################################################################################
                    [1m Learning iteration 4201/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.893s, learning 0.166s)
               Value function loss: 0.0180
                    Surrogate loss: -0.0153
             Mean action noise std: 0.63
                       Mean reward: 25.38
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68845568
                    Iteration time: 8.06s
                        Total time: 36693.71s
                               ETA: 836559.0s

################################################################################
                    [1m Learning iteration 4202/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.825s, learning 0.163s)
               Value function loss: 13.0104
                    Surrogate loss: 0.0971
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68861952
                    Iteration time: 7.99s
                        Total time: 36701.70s
                               ETA: 836533.3s

################################################################################
                    [1m Learning iteration 4203/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.141s, learning 0.191s)
               Value function loss: 0.1369
                    Surrogate loss: -0.0099
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68878336
                    Iteration time: 8.33s
                        Total time: 36710.03s
                               ETA: 836515.4s

################################################################################
                    [1m Learning iteration 4204/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.753s, learning 0.164s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0042
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68894720
                    Iteration time: 7.92s
                        Total time: 36717.95s
                               ETA: 836488.1s

################################################################################
                    [1m Learning iteration 4205/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.007s, learning 0.171s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0150
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 8.18s
                        Total time: 36726.13s
                               ETA: 836466.8s

################################################################################
                    [1m Learning iteration 4206/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.152s, learning 0.184s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0116
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68927488
                    Iteration time: 8.34s
                        Total time: 36734.46s
                               ETA: 836449.0s

################################################################################
                    [1m Learning iteration 4207/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.721s, learning 0.176s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0300
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68943872
                    Iteration time: 7.90s
                        Total time: 36742.36s
                               ETA: 836421.3s

################################################################################
                    [1m Learning iteration 4208/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.015s, learning 0.167s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68960256
                    Iteration time: 8.18s
                        Total time: 36750.54s
                               ETA: 836400.1s

################################################################################
                    [1m Learning iteration 4209/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.090s, learning 0.199s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0315
             Mean action noise std: 0.63
                       Mean reward: 25.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68976640
                    Iteration time: 8.29s
                        Total time: 36758.83s
                               ETA: 836381.2s

################################################################################
                    [1m Learning iteration 4210/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.867s, learning 0.167s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0311
             Mean action noise std: 0.63
                       Mean reward: 25.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68993024
                    Iteration time: 8.03s
                        Total time: 36766.86s
                               ETA: 836356.7s

################################################################################
                    [1m Learning iteration 4211/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.115s, learning 0.163s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0216
             Mean action noise std: 0.63
                       Mean reward: 25.51
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 8.28s
                        Total time: 36775.14s
                               ETA: 836337.6s

################################################################################
                    [1m Learning iteration 4212/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.940s, learning 0.161s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0108
             Mean action noise std: 0.63
                       Mean reward: 25.52
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69025792
                    Iteration time: 8.10s
                        Total time: 36783.24s
                               ETA: 836314.6s

################################################################################
                    [1m Learning iteration 4213/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.246s, learning 0.185s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0268
             Mean action noise std: 0.63
                       Mean reward: 25.49
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69042176
                    Iteration time: 8.43s
                        Total time: 36791.67s
                               ETA: 836299.0s

################################################################################
                    [1m Learning iteration 4214/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.629s, learning 0.269s)
               Value function loss: 0.0347
                    Surrogate loss: -0.0091
             Mean action noise std: 0.63
                       Mean reward: 25.48
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69058560
                    Iteration time: 7.90s
                        Total time: 36799.57s
                               ETA: 836271.4s

################################################################################
                    [1m Learning iteration 4215/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.019s, learning 0.162s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0152
             Mean action noise std: 0.63
                       Mean reward: 25.46
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69074944
                    Iteration time: 8.18s
                        Total time: 36807.75s
                               ETA: 836250.2s

################################################################################
                    [1m Learning iteration 4216/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.099s, learning 0.164s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0156
             Mean action noise std: 0.63
                       Mean reward: 25.48
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69091328
                    Iteration time: 8.26s
                        Total time: 36816.02s
                               ETA: 836230.8s

################################################################################
                    [1m Learning iteration 4217/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.051s, learning 0.177s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0352
             Mean action noise std: 0.63
                       Mean reward: 25.44
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 8.23s
                        Total time: 36824.24s
                               ETA: 836210.7s

################################################################################
                    [1m Learning iteration 4218/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.919s, learning 0.200s)
               Value function loss: 10.7740
                    Surrogate loss: 0.1647
             Mean action noise std: 0.63
                       Mean reward: 25.92
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69124096
                    Iteration time: 12.12s
                        Total time: 36836.36s
                               ETA: 836278.9s

################################################################################
                    [1m Learning iteration 4219/100000 [0m                    

                       Computation: 1058 steps/s (collection: 15.254s, learning 0.229s)
               Value function loss: 0.0052
                    Surrogate loss: 0.0027
             Mean action noise std: 0.63
                       Mean reward: 25.92
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69140480
                    Iteration time: 15.48s
                        Total time: 36851.85s
                               ETA: 836423.4s

################################################################################
                    [1m Learning iteration 4220/100000 [0m                    

                       Computation: 1070 steps/s (collection: 15.121s, learning 0.178s)
               Value function loss: 0.0086
                    Surrogate loss: 0.0008
             Mean action noise std: 0.63
                       Mean reward: 25.92
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69156864
                    Iteration time: 15.30s
                        Total time: 36867.15s
                               ETA: 836563.7s

################################################################################
                    [1m Learning iteration 4221/100000 [0m                    

                       Computation: 1056 steps/s (collection: 15.335s, learning 0.171s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0114
             Mean action noise std: 0.63
                       Mean reward: 25.92
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69173248
                    Iteration time: 15.51s
                        Total time: 36882.65s
                               ETA: 836708.6s

################################################################################
                    [1m Learning iteration 4222/100000 [0m                    

                       Computation: 1061 steps/s (collection: 15.268s, learning 0.166s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0205
             Mean action noise std: 0.63
                       Mean reward: 25.92
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69189632
                    Iteration time: 15.43s
                        Total time: 36898.09s
                               ETA: 836851.8s

################################################################################
                    [1m Learning iteration 4223/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.554s, learning 0.165s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0401
             Mean action noise std: 0.63
                       Mean reward: 25.92
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 15.72s
                        Total time: 36913.81s
                               ETA: 837001.3s

################################################################################
                    [1m Learning iteration 4224/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.671s, learning 0.222s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0391
             Mean action noise std: 0.63
                       Mean reward: 25.92
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69222400
                    Iteration time: 15.89s
                        Total time: 36929.70s
                               ETA: 837154.8s

################################################################################
                    [1m Learning iteration 4225/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.528s, learning 0.163s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0270
             Mean action noise std: 0.63
                       Mean reward: 25.90
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69238784
                    Iteration time: 15.69s
                        Total time: 36945.39s
                               ETA: 837303.5s

################################################################################
                    [1m Learning iteration 4226/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.743s, learning 0.167s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0332
             Mean action noise std: 0.63
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69255168
                    Iteration time: 15.91s
                        Total time: 36961.30s
                               ETA: 837457.2s

################################################################################
                    [1m Learning iteration 4227/100000 [0m                    

                       Computation: 1069 steps/s (collection: 15.148s, learning 0.167s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0283
             Mean action noise std: 0.63
                       Mean reward: 25.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69271552
                    Iteration time: 15.31s
                        Total time: 36976.61s
                               ETA: 837597.3s

################################################################################
                    [1m Learning iteration 4228/100000 [0m                    

                       Computation: 1045 steps/s (collection: 15.472s, learning 0.199s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0221
             Mean action noise std: 0.63
                       Mean reward: 25.89
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69287936
                    Iteration time: 15.67s
                        Total time: 36992.29s
                               ETA: 837745.4s

################################################################################
                    [1m Learning iteration 4229/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.778s, learning 0.166s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 15.94s
                        Total time: 37008.23s
                               ETA: 837899.6s

################################################################################
                    [1m Learning iteration 4230/100000 [0m                    

                       Computation: 1048 steps/s (collection: 15.425s, learning 0.195s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0179
             Mean action noise std: 0.63
                       Mean reward: 25.88
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69320704
                    Iteration time: 15.62s
                        Total time: 37023.85s
                               ETA: 838046.3s

################################################################################
                    [1m Learning iteration 4231/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.760s, learning 0.183s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0200
             Mean action noise std: 0.63
                       Mean reward: 25.86
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69337088
                    Iteration time: 15.94s
                        Total time: 37039.79s
                               ETA: 838200.3s

################################################################################
                    [1m Learning iteration 4232/100000 [0m                    

                       Computation: 1042 steps/s (collection: 15.512s, learning 0.199s)
               Value function loss: 0.0262
                    Surrogate loss: -0.0139
             Mean action noise std: 0.63
                       Mean reward: 25.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69353472
                    Iteration time: 15.71s
                        Total time: 37055.50s
                               ETA: 838349.0s

################################################################################
                    [1m Learning iteration 4233/100000 [0m                    

                       Computation: 1086 steps/s (collection: 14.905s, learning 0.169s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0387
             Mean action noise std: 0.63
                       Mean reward: 25.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69369856
                    Iteration time: 15.07s
                        Total time: 37070.58s
                               ETA: 838483.2s

################################################################################
                    [1m Learning iteration 4234/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.844s, learning 0.228s)
               Value function loss: 4.9054
                    Surrogate loss: 0.0938
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69386240
                    Iteration time: 16.07s
                        Total time: 37086.65s
                               ETA: 838639.9s

################################################################################
                    [1m Learning iteration 4235/100000 [0m                    

                       Computation: 1064 steps/s (collection: 15.157s, learning 0.231s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0053
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 15.39s
                        Total time: 37102.04s
                               ETA: 838781.0s

################################################################################
                    [1m Learning iteration 4236/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.845s, learning 0.165s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0075
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69419008
                    Iteration time: 16.01s
                        Total time: 37118.05s
                               ETA: 838936.2s

################################################################################
                    [1m Learning iteration 4237/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.634s, learning 0.223s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0168
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69435392
                    Iteration time: 15.86s
                        Total time: 37133.90s
                               ETA: 839087.8s

################################################################################
                    [1m Learning iteration 4238/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.664s, learning 0.167s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0378
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69451776
                    Iteration time: 15.83s
                        Total time: 37149.73s
                               ETA: 839238.7s

################################################################################
                    [1m Learning iteration 4239/100000 [0m                    

                       Computation: 1043 steps/s (collection: 15.418s, learning 0.275s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0456
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69468160
                    Iteration time: 15.69s
                        Total time: 37165.43s
                               ETA: 839386.4s

################################################################################
                    [1m Learning iteration 4240/100000 [0m                    

                       Computation: 1066 steps/s (collection: 15.178s, learning 0.184s)
               Value function loss: 0.0349
                    Surrogate loss: -0.0403
             Mean action noise std: 0.63
                       Mean reward: 25.32
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69484544
                    Iteration time: 15.36s
                        Total time: 37180.79s
                               ETA: 839526.6s

################################################################################
                    [1m Learning iteration 4241/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.666s, learning 0.349s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0399
             Mean action noise std: 0.63
                       Mean reward: 25.34
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 16.02s
                        Total time: 37196.81s
                               ETA: 839681.5s

################################################################################
                    [1m Learning iteration 4242/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.625s, learning 0.191s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0300
             Mean action noise std: 0.63
                       Mean reward: 25.35
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69517312
                    Iteration time: 15.82s
                        Total time: 37212.62s
                               ETA: 839831.8s

################################################################################
                    [1m Learning iteration 4243/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.461s, learning 0.196s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0350
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69533696
                    Iteration time: 15.66s
                        Total time: 37228.28s
                               ETA: 839978.4s

################################################################################
                    [1m Learning iteration 4244/100000 [0m                    

                       Computation: 1049 steps/s (collection: 15.440s, learning 0.164s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0131
             Mean action noise std: 0.63
                       Mean reward: 25.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69550080
                    Iteration time: 15.60s
                        Total time: 37243.88s
                               ETA: 840123.8s

################################################################################
                    [1m Learning iteration 4245/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.866s, learning 0.171s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0222
             Mean action noise std: 0.63
                       Mean reward: 25.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69566464
                    Iteration time: 16.04s
                        Total time: 37259.92s
                               ETA: 840278.8s

################################################################################
                    [1m Learning iteration 4246/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.363s, learning 0.291s)
               Value function loss: 0.0204
                    Surrogate loss: -0.0175
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69582848
                    Iteration time: 15.65s
                        Total time: 37275.58s
                               ETA: 840425.1s

################################################################################
                    [1m Learning iteration 4247/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.871s, learning 0.165s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0352
             Mean action noise std: 0.63
                       Mean reward: 25.40
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 16.04s
                        Total time: 37291.61s
                               ETA: 840580.0s

################################################################################
                    [1m Learning iteration 4248/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.543s, learning 0.292s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0157
             Mean action noise std: 0.63
                       Mean reward: 25.37
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69615616
                    Iteration time: 15.83s
                        Total time: 37307.45s
                               ETA: 840730.2s

################################################################################
                    [1m Learning iteration 4249/100000 [0m                    

                       Computation: 1051 steps/s (collection: 15.416s, learning 0.162s)
               Value function loss: 12.2090
                    Surrogate loss: 0.0664
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69632000
                    Iteration time: 15.58s
                        Total time: 37323.02s
                               ETA: 840874.6s

################################################################################
                    [1m Learning iteration 4250/100000 [0m                    

                       Computation: 1051 steps/s (collection: 15.402s, learning 0.175s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0024
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69648384
                    Iteration time: 15.58s
                        Total time: 37338.60s
                               ETA: 841018.8s

################################################################################
                    [1m Learning iteration 4251/100000 [0m                    

                       Computation: 1034 steps/s (collection: 15.663s, learning 0.168s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0104
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69664768
                    Iteration time: 15.83s
                        Total time: 37354.43s
                               ETA: 841168.7s

################################################################################
                    [1m Learning iteration 4252/100000 [0m                    

                       Computation: 1046 steps/s (collection: 15.464s, learning 0.191s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0151
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69681152
                    Iteration time: 15.66s
                        Total time: 37370.09s
                               ETA: 841314.6s

################################################################################
                    [1m Learning iteration 4253/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.651s, learning 0.163s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0304
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 15.81s
                        Total time: 37385.90s
                               ETA: 841464.0s

################################################################################
                    [1m Learning iteration 4254/100000 [0m                    

                       Computation: 1060 steps/s (collection: 15.275s, learning 0.171s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0389
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69713920
                    Iteration time: 15.45s
                        Total time: 37401.35s
                               ETA: 841605.0s

################################################################################
                    [1m Learning iteration 4255/100000 [0m                    

                       Computation: 1070 steps/s (collection: 15.138s, learning 0.162s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0380
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69730304
                    Iteration time: 15.30s
                        Total time: 37416.65s
                               ETA: 841742.7s

################################################################################
                    [1m Learning iteration 4256/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.869s, learning 0.203s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0309
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69746688
                    Iteration time: 9.07s
                        Total time: 37425.72s
                               ETA: 841740.2s

################################################################################
                    [1m Learning iteration 4257/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.792s, learning 0.167s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0288
             Mean action noise std: 0.63
                       Mean reward: 25.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69763072
                    Iteration time: 7.96s
                        Total time: 37433.68s
                               ETA: 841712.7s

################################################################################
                    [1m Learning iteration 4258/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.908s, learning 0.176s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0294
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69779456
                    Iteration time: 8.08s
                        Total time: 37441.76s
                               ETA: 841688.0s

################################################################################
                    [1m Learning iteration 4259/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.895s, learning 0.193s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0145
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 8.09s
                        Total time: 37449.85s
                               ETA: 841663.4s

################################################################################
                    [1m Learning iteration 4260/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.070s, learning 0.216s)
               Value function loss: 0.0169
                    Surrogate loss: -0.0342
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69812224
                    Iteration time: 8.29s
                        Total time: 37458.13s
                               ETA: 841643.2s

################################################################################
                    [1m Learning iteration 4261/100000 [0m                    

                       Computation: 1991 steps/s (collection: 7.947s, learning 0.279s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0084
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69828608
                    Iteration time: 8.23s
                        Total time: 37466.36s
                               ETA: 841621.7s

################################################################################
                    [1m Learning iteration 4262/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.911s, learning 0.187s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0148
             Mean action noise std: 0.63
                       Mean reward: 25.65
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69844992
                    Iteration time: 8.10s
                        Total time: 37474.46s
                               ETA: 841597.4s

################################################################################
                    [1m Learning iteration 4263/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.027s, learning 0.162s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0073
             Mean action noise std: 0.63
                       Mean reward: 25.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69861376
                    Iteration time: 8.19s
                        Total time: 37482.65s
                               ETA: 841575.1s

################################################################################
                    [1m Learning iteration 4264/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.669s, learning 0.258s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0398
             Mean action noise std: 0.63
                       Mean reward: 25.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69877760
                    Iteration time: 7.93s
                        Total time: 37490.57s
                               ETA: 841546.9s

################################################################################
                    [1m Learning iteration 4265/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.164s)
               Value function loss: 7.9775
                    Surrogate loss: 0.2333
             Mean action noise std: 0.63
                       Mean reward: 25.90
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 8.39s
                        Total time: 37498.96s
                               ETA: 841529.2s

################################################################################
                    [1m Learning iteration 4266/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.924s, learning 0.159s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0058
             Mean action noise std: 0.63
                       Mean reward: 25.90
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69910528
                    Iteration time: 8.08s
                        Total time: 37507.05s
                               ETA: 841504.5s

################################################################################
                    [1m Learning iteration 4267/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.978s, learning 0.195s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0131
             Mean action noise std: 0.63
                       Mean reward: 25.90
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69926912
                    Iteration time: 8.17s
                        Total time: 37515.22s
                               ETA: 841481.9s

################################################################################
                    [1m Learning iteration 4268/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.705s, learning 0.195s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0140
             Mean action noise std: 0.63
                       Mean reward: 25.90
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69943296
                    Iteration time: 7.90s
                        Total time: 37523.12s
                               ETA: 841453.1s

################################################################################
                    [1m Learning iteration 4269/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.864s, learning 0.170s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0284
             Mean action noise std: 0.63
                       Mean reward: 25.90
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69959680
                    Iteration time: 8.03s
                        Total time: 37531.15s
                               ETA: 841427.4s

################################################################################
                    [1m Learning iteration 4270/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.775s, learning 0.163s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0386
             Mean action noise std: 0.63
                       Mean reward: 25.90
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69976064
                    Iteration time: 7.94s
                        Total time: 37539.09s
                               ETA: 841399.5s

################################################################################
                    [1m Learning iteration 4271/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.703s, learning 0.193s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0303
             Mean action noise std: 0.63
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 7.90s
                        Total time: 37546.99s
                               ETA: 841370.7s

################################################################################
                    [1m Learning iteration 4272/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.968s, learning 0.160s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0315
             Mean action noise std: 0.63
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70008832
                    Iteration time: 8.13s
                        Total time: 37555.12s
                               ETA: 841347.1s

################################################################################
                    [1m Learning iteration 4273/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.767s, learning 0.166s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0347
             Mean action noise std: 0.63
                       Mean reward: 25.89
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70025216
                    Iteration time: 7.93s
                        Total time: 37563.05s
                               ETA: 841319.2s

################################################################################
                    [1m Learning iteration 4274/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.156s, learning 0.157s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0306
             Mean action noise std: 0.63
                       Mean reward: 25.85
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70041600
                    Iteration time: 8.31s
                        Total time: 37571.36s
                               ETA: 841299.7s

################################################################################
                    [1m Learning iteration 4275/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.864s, learning 0.187s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0233
             Mean action noise std: 0.63
                       Mean reward: 25.82
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70057984
                    Iteration time: 8.05s
                        Total time: 37579.41s
                               ETA: 841274.4s

################################################################################
                    [1m Learning iteration 4276/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.052s, learning 0.228s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0191
             Mean action noise std: 0.63
                       Mean reward: 25.76
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70074368
                    Iteration time: 8.28s
                        Total time: 37587.69s
                               ETA: 841254.3s

################################################################################
                    [1m Learning iteration 4277/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.052s, learning 0.171s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0220
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 8.22s
                        Total time: 37595.92s
                               ETA: 841232.8s

################################################################################
                    [1m Learning iteration 4278/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.067s, learning 0.214s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0275
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70107136
                    Iteration time: 8.28s
                        Total time: 37604.20s
                               ETA: 841212.7s

################################################################################
                    [1m Learning iteration 4279/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.783s, learning 0.162s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70123520
                    Iteration time: 7.95s
                        Total time: 37612.14s
                               ETA: 841185.0s

################################################################################
                    [1m Learning iteration 4280/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.219s, learning 0.183s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0191
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70139904
                    Iteration time: 8.40s
                        Total time: 37620.54s
                               ETA: 841167.6s

################################################################################
                    [1m Learning iteration 4281/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.018s, learning 0.166s)
               Value function loss: 3.2956
                    Surrogate loss: 0.0358
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70156288
                    Iteration time: 8.18s
                        Total time: 37628.73s
                               ETA: 841145.3s

################################################################################
                    [1m Learning iteration 4282/100000 [0m                    

                       Computation: 2110 steps/s (collection: 7.592s, learning 0.170s)
               Value function loss: 0.0007
                    Surrogate loss: -0.0145
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70172672
                    Iteration time: 7.76s
                        Total time: 37636.49s
                               ETA: 841113.6s

################################################################################
                    [1m Learning iteration 4283/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.804s, learning 0.190s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 7.99s
                        Total time: 37644.48s
                               ETA: 841087.1s

################################################################################
                    [1m Learning iteration 4284/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.686s, learning 0.195s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0185
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70205440
                    Iteration time: 7.88s
                        Total time: 37652.36s
                               ETA: 841058.1s

################################################################################
                    [1m Learning iteration 4285/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.768s, learning 0.175s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0378
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70221824
                    Iteration time: 7.94s
                        Total time: 37660.31s
                               ETA: 841030.4s

################################################################################
                    [1m Learning iteration 4286/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.908s, learning 0.166s)
               Value function loss: 0.0107
                    Surrogate loss: -0.0338
             Mean action noise std: 0.63
                       Mean reward: 24.40
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70238208
                    Iteration time: 8.07s
                        Total time: 37668.38s
                               ETA: 841005.7s

################################################################################
                    [1m Learning iteration 4287/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.745s, learning 0.164s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0350
             Mean action noise std: 0.63
                       Mean reward: 24.41
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70254592
                    Iteration time: 7.91s
                        Total time: 37676.29s
                               ETA: 840977.3s

################################################################################
                    [1m Learning iteration 4288/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.016s, learning 0.158s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0285
             Mean action noise std: 0.63
                       Mean reward: 24.44
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70270976
                    Iteration time: 8.17s
                        Total time: 37684.46s
                               ETA: 840954.9s

################################################################################
                    [1m Learning iteration 4289/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.972s, learning 0.166s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0278
             Mean action noise std: 0.63
                       Mean reward: 24.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 8.14s
                        Total time: 37692.60s
                               ETA: 840931.6s

################################################################################
                    [1m Learning iteration 4290/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.077s, learning 0.185s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0319
             Mean action noise std: 0.63
                       Mean reward: 24.46
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70303744
                    Iteration time: 8.26s
                        Total time: 37700.86s
                               ETA: 840911.1s

################################################################################
                    [1m Learning iteration 4291/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.802s, learning 0.256s)
               Value function loss: 0.0190
                    Surrogate loss: -0.0281
             Mean action noise std: 0.63
                       Mean reward: 24.44
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70320128
                    Iteration time: 8.06s
                        Total time: 37708.92s
                               ETA: 840886.1s

################################################################################
                    [1m Learning iteration 4292/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.813s, learning 0.160s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0187
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70336512
                    Iteration time: 7.97s
                        Total time: 37716.89s
                               ETA: 840859.2s

################################################################################
                    [1m Learning iteration 4293/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.985s, learning 0.164s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0245
             Mean action noise std: 0.63
                       Mean reward: 24.42
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70352896
                    Iteration time: 8.15s
                        Total time: 37725.04s
                               ETA: 840836.2s

################################################################################
                    [1m Learning iteration 4294/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.900s, learning 0.216s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0192
             Mean action noise std: 0.63
                       Mean reward: 24.46
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70369280
                    Iteration time: 8.12s
                        Total time: 37733.16s
                               ETA: 840812.5s

################################################################################
                    [1m Learning iteration 4295/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.926s, learning 0.161s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0308
             Mean action noise std: 0.63
                       Mean reward: 24.46
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 8.09s
                        Total time: 37741.25s
                               ETA: 840788.2s

################################################################################
                    [1m Learning iteration 4296/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.940s, learning 0.166s)
               Value function loss: 4.4927
                    Surrogate loss: 0.0682
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70402048
                    Iteration time: 8.11s
                        Total time: 37749.35s
                               ETA: 840764.3s

################################################################################
                    [1m Learning iteration 4297/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.107s, learning 0.187s)
               Value function loss: 0.0073
                    Surrogate loss: -0.0120
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70418432
                    Iteration time: 8.29s
                        Total time: 37757.65s
                               ETA: 840744.6s

################################################################################
                    [1m Learning iteration 4298/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.818s, learning 0.161s)
               Value function loss: 0.0448
                    Surrogate loss: -0.0113
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70434816
                    Iteration time: 7.98s
                        Total time: 37765.63s
                               ETA: 840717.9s

################################################################################
                    [1m Learning iteration 4299/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.930s, learning 0.167s)
               Value function loss: 0.0538
                    Surrogate loss: -0.0074
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70451200
                    Iteration time: 8.10s
                        Total time: 37773.72s
                               ETA: 840693.7s

################################################################################
                    [1m Learning iteration 4300/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.084s, learning 0.165s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0310
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70467584
                    Iteration time: 8.25s
                        Total time: 37781.97s
                               ETA: 840673.1s

################################################################################
                    [1m Learning iteration 4301/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.083s, learning 0.196s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0246
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 8.28s
                        Total time: 37790.25s
                               ETA: 840653.0s

################################################################################
                    [1m Learning iteration 4302/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.837s, learning 0.166s)
               Value function loss: 0.0615
                    Surrogate loss: -0.0093
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70500352
                    Iteration time: 8.00s
                        Total time: 37798.25s
                               ETA: 840626.9s

################################################################################
                    [1m Learning iteration 4303/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.181s, learning 0.176s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0203
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70516736
                    Iteration time: 8.36s
                        Total time: 37806.61s
                               ETA: 840608.6s

################################################################################
                    [1m Learning iteration 4304/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.151s, learning 0.169s)
               Value function loss: 0.1114
                    Surrogate loss: -0.0289
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70533120
                    Iteration time: 8.32s
                        Total time: 37814.93s
                               ETA: 840589.5s

################################################################################
                    [1m Learning iteration 4305/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.902s, learning 0.167s)
               Value function loss: 0.0780
                    Surrogate loss: -0.0147
             Mean action noise std: 0.63
                       Mean reward: 25.75
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70549504
                    Iteration time: 8.07s
                        Total time: 37823.00s
                               ETA: 840564.8s

################################################################################
                    [1m Learning iteration 4306/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.933s, learning 0.172s)
               Value function loss: 0.0591
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70565888
                    Iteration time: 8.11s
                        Total time: 37831.11s
                               ETA: 840541.0s

################################################################################
                    [1m Learning iteration 4307/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.897s, learning 0.166s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0183
             Mean action noise std: 0.63
                       Mean reward: 25.67
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 8.06s
                        Total time: 37839.17s
                               ETA: 840516.2s

################################################################################
                    [1m Learning iteration 4308/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.699s, learning 0.193s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0270
             Mean action noise std: 0.63
                       Mean reward: 25.60
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70598656
                    Iteration time: 7.89s
                        Total time: 37847.06s
                               ETA: 840487.6s

################################################################################
                    [1m Learning iteration 4309/100000 [0m                    

                       Computation: 2108 steps/s (collection: 7.607s, learning 0.163s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0262
             Mean action noise std: 0.63
                       Mean reward: 25.57
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70615040
                    Iteration time: 7.77s
                        Total time: 37854.83s
                               ETA: 840456.3s

################################################################################
                    [1m Learning iteration 4310/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.912s, learning 0.190s)
               Value function loss: 0.0362
                    Surrogate loss: -0.0230
             Mean action noise std: 0.63
                       Mean reward: 25.56
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70631424
                    Iteration time: 8.10s
                        Total time: 37862.93s
                               ETA: 840432.4s

################################################################################
                    [1m Learning iteration 4311/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.063s, learning 0.187s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0385
             Mean action noise std: 0.63
                       Mean reward: 25.56
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70647808
                    Iteration time: 8.25s
                        Total time: 37871.18s
                               ETA: 840411.8s

################################################################################
                    [1m Learning iteration 4312/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.011s, learning 0.169s)
               Value function loss: 5.5164
                    Surrogate loss: 0.1802
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70664192
                    Iteration time: 8.18s
                        Total time: 37879.36s
                               ETA: 840389.6s

################################################################################
                    [1m Learning iteration 4313/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.796s, learning 0.164s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0093
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 7.96s
                        Total time: 37887.32s
                               ETA: 840362.6s

################################################################################
                    [1m Learning iteration 4314/100000 [0m                    

                       Computation: 2100 steps/s (collection: 7.632s, learning 0.168s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0039
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70696960
                    Iteration time: 7.80s
                        Total time: 37895.12s
                               ETA: 840332.1s

################################################################################
                    [1m Learning iteration 4315/100000 [0m                    

                       Computation: 2111 steps/s (collection: 7.604s, learning 0.156s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0061
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70713344
                    Iteration time: 7.76s
                        Total time: 37902.88s
                               ETA: 840300.6s

################################################################################
                    [1m Learning iteration 4316/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.099s, learning 0.159s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0340
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70729728
                    Iteration time: 8.26s
                        Total time: 37911.14s
                               ETA: 840280.2s

################################################################################
                    [1m Learning iteration 4317/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.885s, learning 0.161s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0437
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70746112
                    Iteration time: 8.05s
                        Total time: 37919.19s
                               ETA: 840255.1s

################################################################################
                    [1m Learning iteration 4318/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.120s, learning 0.168s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0399
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70762496
                    Iteration time: 8.29s
                        Total time: 37927.48s
                               ETA: 840235.4s

################################################################################
                    [1m Learning iteration 4319/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.052s, learning 0.274s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0188
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 8.33s
                        Total time: 37935.80s
                               ETA: 840216.5s

################################################################################
                    [1m Learning iteration 4320/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.007s, learning 0.160s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0355
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70795264
                    Iteration time: 8.17s
                        Total time: 37943.97s
                               ETA: 840194.2s

################################################################################
                    [1m Learning iteration 4321/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.016s, learning 0.163s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0304
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70811648
                    Iteration time: 8.18s
                        Total time: 37952.15s
                               ETA: 840172.0s

################################################################################
                    [1m Learning iteration 4322/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.047s, learning 0.166s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0185
             Mean action noise std: 0.63
                       Mean reward: 25.65
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70828032
                    Iteration time: 8.21s
                        Total time: 37960.36s
                               ETA: 840150.7s

################################################################################
                    [1m Learning iteration 4323/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.010s, learning 0.220s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0212
             Mean action noise std: 0.63
                       Mean reward: 25.61
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70844416
                    Iteration time: 8.23s
                        Total time: 37968.59s
                               ETA: 840129.7s

################################################################################
                    [1m Learning iteration 4324/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.961s, learning 0.163s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0232
             Mean action noise std: 0.63
                       Mean reward: 25.57
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70860800
                    Iteration time: 8.12s
                        Total time: 37976.72s
                               ETA: 840106.4s

################################################################################
                    [1m Learning iteration 4325/100000 [0m                    

                       Computation: 2081 steps/s (collection: 7.710s, learning 0.163s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0258
             Mean action noise std: 0.63
                       Mean reward: 25.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 7.87s
                        Total time: 37984.59s
                               ETA: 840077.6s

################################################################################
                    [1m Learning iteration 4326/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.109s, learning 0.160s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0163
             Mean action noise std: 0.63
                       Mean reward: 25.50
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70893568
                    Iteration time: 8.27s
                        Total time: 37992.86s
                               ETA: 840057.5s

################################################################################
                    [1m Learning iteration 4327/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.947s, learning 0.170s)
               Value function loss: 13.1556
                    Surrogate loss: 0.0826
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70909952
                    Iteration time: 8.12s
                        Total time: 38000.97s
                               ETA: 840034.0s

################################################################################
                    [1m Learning iteration 4328/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.832s, learning 0.183s)
               Value function loss: 0.1616
                    Surrogate loss: 0.0047
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70926336
                    Iteration time: 8.01s
                        Total time: 38008.99s
                               ETA: 840008.3s

################################################################################
                    [1m Learning iteration 4329/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.056s, learning 0.173s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0142
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70942720
                    Iteration time: 8.23s
                        Total time: 38017.22s
                               ETA: 839987.4s

################################################################################
                    [1m Learning iteration 4330/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.821s, learning 0.199s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0170
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70959104
                    Iteration time: 8.02s
                        Total time: 38025.24s
                               ETA: 839961.8s

################################################################################
                    [1m Learning iteration 4331/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.044s, learning 0.218s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0178
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 8.26s
                        Total time: 38033.50s
                               ETA: 839941.6s

################################################################################
                    [1m Learning iteration 4332/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.995s, learning 0.164s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0293
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70991872
                    Iteration time: 8.16s
                        Total time: 38041.66s
                               ETA: 839919.1s

################################################################################
                    [1m Learning iteration 4333/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.899s, learning 0.170s)
               Value function loss: 0.0233
                    Surrogate loss: -0.0393
             Mean action noise std: 0.63
                       Mean reward: 24.72
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71008256
                    Iteration time: 8.07s
                        Total time: 38049.73s
                               ETA: 839894.6s

################################################################################
                    [1m Learning iteration 4334/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.886s, learning 0.164s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0393
             Mean action noise std: 0.63
                       Mean reward: 24.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71024640
                    Iteration time: 8.05s
                        Total time: 38057.78s
                               ETA: 839869.8s

################################################################################
                    [1m Learning iteration 4335/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.849s, learning 0.193s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0318
             Mean action noise std: 0.63
                       Mean reward: 24.80
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71041024
                    Iteration time: 8.04s
                        Total time: 38065.82s
                               ETA: 839844.7s

################################################################################
                    [1m Learning iteration 4336/100000 [0m                    

                       Computation: 2003 steps/s (collection: 7.982s, learning 0.196s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0322
             Mean action noise std: 0.63
                       Mean reward: 24.85
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71057408
                    Iteration time: 8.18s
                        Total time: 38074.00s
                               ETA: 839822.7s

################################################################################
                    [1m Learning iteration 4337/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.007s, learning 0.170s)
               Value function loss: 0.0338
                    Surrogate loss: -0.0144
             Mean action noise std: 0.63
                       Mean reward: 24.86
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 8.18s
                        Total time: 38082.17s
                               ETA: 839800.6s

################################################################################
                    [1m Learning iteration 4338/100000 [0m                    

                       Computation: 2084 steps/s (collection: 7.695s, learning 0.164s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0216
             Mean action noise std: 0.63
                       Mean reward: 24.88
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71090176
                    Iteration time: 7.86s
                        Total time: 38090.03s
                               ETA: 839771.6s

################################################################################
                    [1m Learning iteration 4339/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.860s, learning 0.187s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0130
             Mean action noise std: 0.63
                       Mean reward: 24.86
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71106560
                    Iteration time: 8.05s
                        Total time: 38098.08s
                               ETA: 839746.7s

################################################################################
                    [1m Learning iteration 4340/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.025s, learning 0.164s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0194
             Mean action noise std: 0.63
                       Mean reward: 24.85
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71122944
                    Iteration time: 8.19s
                        Total time: 38106.27s
                               ETA: 839724.9s

################################################################################
                    [1m Learning iteration 4341/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.978s, learning 0.162s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0143
             Mean action noise std: 0.63
                       Mean reward: 24.91
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71139328
                    Iteration time: 8.14s
                        Total time: 38114.41s
                               ETA: 839702.1s

################################################################################
                    [1m Learning iteration 4342/100000 [0m                    

                       Computation: 1996 steps/s (collection: 7.980s, learning 0.225s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0342
             Mean action noise std: 0.63
                       Mean reward: 24.89
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71155712
                    Iteration time: 8.20s
                        Total time: 38122.62s
                               ETA: 839680.7s

################################################################################
                    [1m Learning iteration 4343/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.961s, learning 0.174s)
               Value function loss: 6.8332
                    Surrogate loss: 0.1988
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 8.13s
                        Total time: 38130.75s
                               ETA: 839657.7s

################################################################################
                    [1m Learning iteration 4344/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.847s, learning 0.193s)
               Value function loss: 0.0325
                    Surrogate loss: 0.0102
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71188480
                    Iteration time: 8.04s
                        Total time: 38138.79s
                               ETA: 839632.7s

################################################################################
                    [1m Learning iteration 4345/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.949s, learning 0.162s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0089
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71204864
                    Iteration time: 8.11s
                        Total time: 38146.90s
                               ETA: 839609.3s

################################################################################
                    [1m Learning iteration 4346/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.781s, learning 0.177s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0121
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71221248
                    Iteration time: 7.96s
                        Total time: 38154.86s
                               ETA: 839582.5s

################################################################################
                    [1m Learning iteration 4347/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.017s, learning 0.165s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0320
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71237632
                    Iteration time: 8.18s
                        Total time: 38163.04s
                               ETA: 839560.6s

################################################################################
                    [1m Learning iteration 4348/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.916s, learning 0.160s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0418
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71254016
                    Iteration time: 8.08s
                        Total time: 38171.12s
                               ETA: 839536.4s

################################################################################
                    [1m Learning iteration 4349/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.916s, learning 0.273s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0351
             Mean action noise std: 0.63
                       Mean reward: 25.42
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 8.19s
                        Total time: 38179.31s
                               ETA: 839514.7s

################################################################################
                    [1m Learning iteration 4350/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.068s, learning 0.206s)
               Value function loss: 0.0527
                    Surrogate loss: -0.0401
             Mean action noise std: 0.63
                       Mean reward: 25.47
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71286784
                    Iteration time: 8.27s
                        Total time: 38187.58s
                               ETA: 839494.9s

################################################################################
                    [1m Learning iteration 4351/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.166s)
               Value function loss: 0.0466
                    Surrogate loss: -0.0388
             Mean action noise std: 0.63
                       Mean reward: 25.48
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71303168
                    Iteration time: 8.37s
                        Total time: 38195.95s
                               ETA: 839477.1s

################################################################################
                    [1m Learning iteration 4352/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.942s, learning 0.160s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0358
             Mean action noise std: 0.63
                       Mean reward: 25.55
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71319552
                    Iteration time: 8.10s
                        Total time: 38204.05s
                               ETA: 839453.5s

################################################################################
                    [1m Learning iteration 4353/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.804s, learning 0.169s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0289
             Mean action noise std: 0.63
                       Mean reward: 25.57
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71335936
                    Iteration time: 7.97s
                        Total time: 38212.02s
                               ETA: 839427.1s

################################################################################
                    [1m Learning iteration 4354/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.944s, learning 0.185s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0203
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71352320
                    Iteration time: 8.13s
                        Total time: 38220.15s
                               ETA: 839404.1s

################################################################################
                    [1m Learning iteration 4355/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.706s, learning 0.175s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0148
             Mean action noise std: 0.63
                       Mean reward: 25.57
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 7.88s
                        Total time: 38228.03s
                               ETA: 839375.7s

################################################################################
                    [1m Learning iteration 4356/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.866s, learning 0.188s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0223
             Mean action noise std: 0.63
                       Mean reward: 25.56
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71385088
                    Iteration time: 8.05s
                        Total time: 38236.09s
                               ETA: 839351.0s

################################################################################
                    [1m Learning iteration 4357/100000 [0m                    

                       Computation: 2083 steps/s (collection: 7.708s, learning 0.157s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0194
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71401472
                    Iteration time: 7.86s
                        Total time: 38243.95s
                               ETA: 839322.3s

################################################################################
                    [1m Learning iteration 4358/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.907s, learning 0.176s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0365
             Mean action noise std: 0.63
                       Mean reward: 25.54
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71417856
                    Iteration time: 8.08s
                        Total time: 38252.04s
                               ETA: 839298.3s

################################################################################
                    [1m Learning iteration 4359/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.205s, learning 0.164s)
               Value function loss: 5.1387
                    Surrogate loss: 0.1346
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71434240
                    Iteration time: 8.37s
                        Total time: 38260.41s
                               ETA: 839280.6s

################################################################################
                    [1m Learning iteration 4360/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.876s, learning 0.169s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0126
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71450624
                    Iteration time: 8.04s
                        Total time: 38268.45s
                               ETA: 839255.8s

################################################################################
                    [1m Learning iteration 4361/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.811s, learning 0.193s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0069
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 8.00s
                        Total time: 38276.45s
                               ETA: 839230.1s

################################################################################
                    [1m Learning iteration 4362/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.096s, learning 0.157s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0115
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71483392
                    Iteration time: 8.25s
                        Total time: 38284.71s
                               ETA: 839209.9s

################################################################################
                    [1m Learning iteration 4363/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.692s, learning 0.197s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0350
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71499776
                    Iteration time: 7.89s
                        Total time: 38292.60s
                               ETA: 839181.7s

################################################################################
                    [1m Learning iteration 4364/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.029s, learning 0.173s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0387
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71516160
                    Iteration time: 8.20s
                        Total time: 38300.80s
                               ETA: 839160.4s

################################################################################
                    [1m Learning iteration 4365/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.245s, learning 0.161s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0366
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71532544
                    Iteration time: 8.41s
                        Total time: 38309.20s
                               ETA: 839143.6s

################################################################################
                    [1m Learning iteration 4366/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.947s, learning 0.171s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0387
             Mean action noise std: 0.63
                       Mean reward: 25.75
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71548928
                    Iteration time: 8.12s
                        Total time: 38317.32s
                               ETA: 839120.4s

################################################################################
                    [1m Learning iteration 4367/100000 [0m                    

                       Computation: 2126 steps/s (collection: 7.512s, learning 0.195s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0284
             Mean action noise std: 0.63
                       Mean reward: 25.80
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 7.71s
                        Total time: 38325.03s
                               ETA: 839088.2s

################################################################################
                    [1m Learning iteration 4368/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.042s, learning 0.167s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0319
             Mean action noise std: 0.63
                       Mean reward: 25.81
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71581696
                    Iteration time: 8.21s
                        Total time: 38333.24s
                               ETA: 839067.1s

################################################################################
                    [1m Learning iteration 4369/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.831s, learning 0.171s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0156
             Mean action noise std: 0.63
                       Mean reward: 25.78
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71598080
                    Iteration time: 8.00s
                        Total time: 38341.24s
                               ETA: 839041.4s

################################################################################
                    [1m Learning iteration 4370/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.980s, learning 0.183s)
               Value function loss: 0.0143
                    Surrogate loss: -0.0289
             Mean action noise std: 0.63
                       Mean reward: 25.76
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71614464
                    Iteration time: 8.16s
                        Total time: 38349.40s
                               ETA: 839019.3s

################################################################################
                    [1m Learning iteration 4371/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.514s, learning 0.255s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0140
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71630848
                    Iteration time: 8.77s
                        Total time: 38358.17s
                               ETA: 839010.4s

################################################################################
                    [1m Learning iteration 4372/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.974s, learning 0.163s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0318
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71647232
                    Iteration time: 8.14s
                        Total time: 38366.31s
                               ETA: 838987.7s

################################################################################
                    [1m Learning iteration 4373/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.096s, learning 0.160s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0256
             Mean action noise std: 0.63
                       Mean reward: 25.62
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 8.26s
                        Total time: 38374.56s
                               ETA: 838967.6s

################################################################################
                    [1m Learning iteration 4374/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.931s, learning 0.165s)
               Value function loss: 11.7754
                    Surrogate loss: 0.0713
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71680000
                    Iteration time: 8.10s
                        Total time: 38382.66s
                               ETA: 838944.1s

################################################################################
                    [1m Learning iteration 4375/100000 [0m                    

                       Computation: 2094 steps/s (collection: 7.666s, learning 0.157s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0112
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71696384
                    Iteration time: 7.82s
                        Total time: 38390.48s
                               ETA: 838914.5s

################################################################################
                    [1m Learning iteration 4376/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.234s, learning 0.170s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0092
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71712768
                    Iteration time: 8.40s
                        Total time: 38398.89s
                               ETA: 838897.7s

################################################################################
                    [1m Learning iteration 4377/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.067s, learning 0.163s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0084
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71729152
                    Iteration time: 8.23s
                        Total time: 38407.12s
                               ETA: 838877.1s

################################################################################
                    [1m Learning iteration 4378/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.160s, learning 0.161s)
               Value function loss: 0.0079
                    Surrogate loss: -0.0302
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71745536
                    Iteration time: 8.32s
                        Total time: 38415.44s
                               ETA: 838858.4s

################################################################################
                    [1m Learning iteration 4379/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.911s, learning 0.182s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0422
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 8.09s
                        Total time: 38423.53s
                               ETA: 838834.8s

################################################################################
                    [1m Learning iteration 4380/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.097s, learning 0.163s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0297
             Mean action noise std: 0.63
                       Mean reward: 25.36
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71778304
                    Iteration time: 8.26s
                        Total time: 38431.79s
                               ETA: 838814.8s

################################################################################
                    [1m Learning iteration 4381/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.123s, learning 0.178s)
               Value function loss: 0.0290
                    Surrogate loss: -0.0364
             Mean action noise std: 0.63
                       Mean reward: 25.37
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71794688
                    Iteration time: 8.30s
                        Total time: 38440.09s
                               ETA: 838795.8s

################################################################################
                    [1m Learning iteration 4382/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.762s, learning 0.226s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0283
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71811072
                    Iteration time: 7.99s
                        Total time: 38448.08s
                               ETA: 838769.9s

################################################################################
                    [1m Learning iteration 4383/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.908s, learning 0.162s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0326
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71827456
                    Iteration time: 8.07s
                        Total time: 38456.15s
                               ETA: 838745.8s

################################################################################
                    [1m Learning iteration 4384/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.904s, learning 0.165s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0129
             Mean action noise std: 0.63
                       Mean reward: 25.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71843840
                    Iteration time: 8.07s
                        Total time: 38464.22s
                               ETA: 838721.7s

################################################################################
                    [1m Learning iteration 4385/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.771s, learning 0.188s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0302
             Mean action noise std: 0.63
                       Mean reward: 25.28
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 7.96s
                        Total time: 38472.18s
                               ETA: 838695.2s

################################################################################
                    [1m Learning iteration 4386/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.127s, learning 0.199s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0143
             Mean action noise std: 0.63
                       Mean reward: 25.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71876608
                    Iteration time: 8.33s
                        Total time: 38480.50s
                               ETA: 838676.8s

################################################################################
                    [1m Learning iteration 4387/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.941s, learning 0.215s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0202
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71892992
                    Iteration time: 8.16s
                        Total time: 38488.66s
                               ETA: 838654.6s

################################################################################
                    [1m Learning iteration 4388/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.011s, learning 0.249s)
               Value function loss: 0.0267
                    Surrogate loss: -0.0181
             Mean action noise std: 0.63
                       Mean reward: 25.20
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71909376
                    Iteration time: 8.26s
                        Total time: 38496.92s
                               ETA: 838634.7s

################################################################################
                    [1m Learning iteration 4389/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.760s, learning 0.173s)
               Value function loss: 0.0112
                    Surrogate loss: -0.0369
             Mean action noise std: 0.63
                       Mean reward: 25.20
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71925760
                    Iteration time: 7.93s
                        Total time: 38504.85s
                               ETA: 838607.7s

################################################################################
                    [1m Learning iteration 4390/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.247s, learning 0.159s)
               Value function loss: 6.9173
                    Surrogate loss: 0.2304
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71942144
                    Iteration time: 8.41s
                        Total time: 38513.26s
                               ETA: 838590.9s

################################################################################
                    [1m Learning iteration 4391/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.960s, learning 0.201s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0068
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 8.16s
                        Total time: 38521.42s
                               ETA: 838568.9s

################################################################################
                    [1m Learning iteration 4392/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.775s, learning 0.196s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0167
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71974912
                    Iteration time: 7.97s
                        Total time: 38529.39s
                               ETA: 838542.7s

################################################################################
                    [1m Learning iteration 4393/100000 [0m                    

                       Computation: 2110 steps/s (collection: 7.541s, learning 0.220s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0122
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71991296
                    Iteration time: 7.76s
                        Total time: 38537.15s
                               ETA: 838512.0s

################################################################################
                    [1m Learning iteration 4394/100000 [0m                    

                       Computation: 2102 steps/s (collection: 7.630s, learning 0.164s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0355
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72007680
                    Iteration time: 7.79s
                        Total time: 38544.95s
                               ETA: 838482.0s

################################################################################
                    [1m Learning iteration 4395/100000 [0m                    

                       Computation: 2065 steps/s (collection: 7.770s, learning 0.161s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0467
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72024064
                    Iteration time: 7.93s
                        Total time: 38552.88s
                               ETA: 838454.9s

################################################################################
                    [1m Learning iteration 4396/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.925s, learning 0.195s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0333
             Mean action noise std: 0.63
                       Mean reward: 25.33
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72040448
                    Iteration time: 8.12s
                        Total time: 38561.00s
                               ETA: 838432.0s

################################################################################
                    [1m Learning iteration 4397/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.695s, learning 0.217s)
               Value function loss: 0.0450
                    Surrogate loss: -0.0376
             Mean action noise std: 0.63
                       Mean reward: 25.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 7.91s
                        Total time: 38568.91s
                               ETA: 838404.6s

################################################################################
                    [1m Learning iteration 4398/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.795s, learning 0.162s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0399
             Mean action noise std: 0.63
                       Mean reward: 25.31
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72073216
                    Iteration time: 7.96s
                        Total time: 38576.87s
                               ETA: 838378.2s

################################################################################
                    [1m Learning iteration 4399/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.974s, learning 0.173s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0263
             Mean action noise std: 0.63
                       Mean reward: 25.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72089600
                    Iteration time: 8.15s
                        Total time: 38585.01s
                               ETA: 838355.9s

################################################################################
                    [1m Learning iteration 4400/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.804s, learning 0.164s)
               Value function loss: 0.0310
                    Surrogate loss: -0.0208
             Mean action noise std: 0.63
                       Mean reward: 25.30
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72105984
                    Iteration time: 7.97s
                        Total time: 38592.98s
                               ETA: 838329.7s

################################################################################
                    [1m Learning iteration 4401/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.677s, learning 0.354s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 25.26
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72122368
                    Iteration time: 8.03s
                        Total time: 38601.01s
                               ETA: 838304.9s

################################################################################
                    [1m Learning iteration 4402/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.080s, learning 0.160s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0280
             Mean action noise std: 0.63
                       Mean reward: 25.27
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72138752
                    Iteration time: 8.24s
                        Total time: 38609.25s
                               ETA: 838284.7s

################################################################################
                    [1m Learning iteration 4403/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.889s, learning 0.169s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0251
             Mean action noise std: 0.63
                       Mean reward: 25.26
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 8.06s
                        Total time: 38617.31s
                               ETA: 838260.5s

################################################################################
                    [1m Learning iteration 4404/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.054s, learning 0.239s)
               Value function loss: 0.0256
                    Surrogate loss: -0.0231
             Mean action noise std: 0.63
                       Mean reward: 25.27
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72171520
                    Iteration time: 8.29s
                        Total time: 38625.61s
                               ETA: 838241.4s

################################################################################
                    [1m Learning iteration 4405/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.036s, learning 0.171s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0160
             Mean action noise std: 0.63
                       Mean reward: 25.27
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72187904
                    Iteration time: 8.21s
                        Total time: 38633.81s
                               ETA: 838220.5s

################################################################################
                    [1m Learning iteration 4406/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.073s, learning 0.194s)
               Value function loss: 3.2805
                    Surrogate loss: 0.0406
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72204288
                    Iteration time: 8.27s
                        Total time: 38642.08s
                               ETA: 838200.8s

################################################################################
                    [1m Learning iteration 4407/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.942s, learning 0.245s)
               Value function loss: 0.0013
                    Surrogate loss: -0.0143
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72220672
                    Iteration time: 8.19s
                        Total time: 38650.27s
                               ETA: 838179.4s

################################################################################
                    [1m Learning iteration 4408/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.813s, learning 0.173s)
               Value function loss: 0.0014
                    Surrogate loss: -0.0082
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72237056
                    Iteration time: 7.99s
                        Total time: 38658.25s
                               ETA: 838153.7s

################################################################################
                    [1m Learning iteration 4409/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.924s, learning 0.158s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0049
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 8.08s
                        Total time: 38666.33s
                               ETA: 838130.1s

################################################################################
                    [1m Learning iteration 4410/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.873s, learning 0.231s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0405
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72269824
                    Iteration time: 8.10s
                        Total time: 38674.44s
                               ETA: 838106.9s

################################################################################
                    [1m Learning iteration 4411/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.887s, learning 0.242s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0425
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72286208
                    Iteration time: 8.13s
                        Total time: 38682.57s
                               ETA: 838084.3s

################################################################################
                    [1m Learning iteration 4412/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.143s, learning 0.181s)
               Value function loss: 0.0225
                    Surrogate loss: -0.0313
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72302592
                    Iteration time: 8.32s
                        Total time: 38690.89s
                               ETA: 838065.9s

################################################################################
                    [1m Learning iteration 4413/100000 [0m                    

                       Computation: 1992 steps/s (collection: 7.945s, learning 0.277s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0303
             Mean action noise std: 0.63
                       Mean reward: 25.73
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72318976
                    Iteration time: 8.22s
                        Total time: 38699.11s
                               ETA: 838045.4s

################################################################################
                    [1m Learning iteration 4414/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.928s, learning 0.156s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0295
             Mean action noise std: 0.63
                       Mean reward: 25.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72335360
                    Iteration time: 8.08s
                        Total time: 38707.20s
                               ETA: 838021.8s

################################################################################
                    [1m Learning iteration 4415/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.881s, learning 0.171s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0317
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 8.05s
                        Total time: 38715.25s
                               ETA: 837997.6s

################################################################################
                    [1m Learning iteration 4416/100000 [0m                    

                       Computation: 2076 steps/s (collection: 7.679s, learning 0.213s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0256
             Mean action noise std: 0.63
                       Mean reward: 25.75
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72368128
                    Iteration time: 7.89s
                        Total time: 38723.14s
                               ETA: 837969.8s

################################################################################
                    [1m Learning iteration 4417/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.940s, learning 0.169s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0162
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72384512
                    Iteration time: 8.11s
                        Total time: 38731.25s
                               ETA: 837946.9s

################################################################################
                    [1m Learning iteration 4418/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.919s, learning 0.165s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0200
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72400896
                    Iteration time: 8.08s
                        Total time: 38739.34s
                               ETA: 837923.3s

################################################################################
                    [1m Learning iteration 4419/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.772s, learning 0.163s)
               Value function loss: 0.0150
                    Surrogate loss: -0.0204
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72417280
                    Iteration time: 7.93s
                        Total time: 38747.27s
                               ETA: 837896.6s

################################################################################
                    [1m Learning iteration 4420/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.774s, learning 0.175s)
               Value function loss: 0.0092
                    Surrogate loss: -0.0328
             Mean action noise std: 0.63
                       Mean reward: 25.64
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72433664
                    Iteration time: 7.95s
                        Total time: 38755.22s
                               ETA: 837870.1s

################################################################################
                    [1m Learning iteration 4421/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.030s, learning 0.183s)
               Value function loss: 4.9470
                    Surrogate loss: 0.1036
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 8.21s
                        Total time: 38763.43s
                               ETA: 837849.4s

################################################################################
                    [1m Learning iteration 4422/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.005s, learning 0.186s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0017
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72466432
                    Iteration time: 8.19s
                        Total time: 38771.62s
                               ETA: 837828.2s

################################################################################
                    [1m Learning iteration 4423/100000 [0m                    

                       Computation: 2072 steps/s (collection: 7.652s, learning 0.252s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0078
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72482816
                    Iteration time: 7.90s
                        Total time: 38779.53s
                               ETA: 837800.8s

################################################################################
                    [1m Learning iteration 4424/100000 [0m                    

                       Computation: 2004 steps/s (collection: 7.986s, learning 0.190s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0134
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72499200
                    Iteration time: 8.18s
                        Total time: 38787.70s
                               ETA: 837779.3s

################################################################################
                    [1m Learning iteration 4425/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.075s, learning 0.167s)
               Value function loss: 0.0352
                    Surrogate loss: -0.0239
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72515584
                    Iteration time: 8.24s
                        Total time: 38795.95s
                               ETA: 837759.3s

################################################################################
                    [1m Learning iteration 4426/100000 [0m                    

                       Computation: 2077 steps/s (collection: 7.724s, learning 0.162s)
               Value function loss: 0.0957
                    Surrogate loss: -0.0275
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72531968
                    Iteration time: 7.89s
                        Total time: 38803.83s
                               ETA: 837731.5s

################################################################################
                    [1m Learning iteration 4427/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.085s, learning 0.170s)
               Value function loss: 0.0866
                    Surrogate loss: -0.0274
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 8.26s
                        Total time: 38812.09s
                               ETA: 837711.7s

################################################################################
                    [1m Learning iteration 4428/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.191s, learning 0.165s)
               Value function loss: 0.1083
                    Surrogate loss: -0.0350
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72564736
                    Iteration time: 8.36s
                        Total time: 38820.44s
                               ETA: 837694.1s

################################################################################
                    [1m Learning iteration 4429/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.959s, learning 0.168s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0411
             Mean action noise std: 0.63
                       Mean reward: 25.14
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72581120
                    Iteration time: 8.13s
                        Total time: 38828.57s
                               ETA: 837671.6s

################################################################################
                    [1m Learning iteration 4430/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.059s, learning 0.165s)
               Value function loss: 0.0762
                    Surrogate loss: -0.0358
             Mean action noise std: 0.63
                       Mean reward: 25.16
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72597504
                    Iteration time: 8.22s
                        Total time: 38836.79s
                               ETA: 837651.2s

################################################################################
                    [1m Learning iteration 4431/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.985s, learning 0.201s)
               Value function loss: 0.0583
                    Surrogate loss: -0.0293
             Mean action noise std: 0.63
                       Mean reward: 25.25
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72613888
                    Iteration time: 8.19s
                        Total time: 38844.98s
                               ETA: 837629.9s

################################################################################
                    [1m Learning iteration 4432/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.884s, learning 0.277s)
               Value function loss: 0.0505
                    Surrogate loss: -0.0209
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72630272
                    Iteration time: 8.16s
                        Total time: 38853.14s
                               ETA: 837608.2s

################################################################################
                    [1m Learning iteration 4433/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.165s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0172
             Mean action noise std: 0.63
                       Mean reward: 25.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 8.38s
                        Total time: 38861.52s
                               ETA: 837591.2s

################################################################################
                    [1m Learning iteration 4434/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.920s, learning 0.186s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0292
             Mean action noise std: 0.63
                       Mean reward: 25.18
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72663040
                    Iteration time: 8.11s
                        Total time: 38869.63s
                               ETA: 837568.2s

################################################################################
                    [1m Learning iteration 4435/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.110s, learning 0.173s)
               Value function loss: 0.0315
                    Surrogate loss: -0.0239
             Mean action noise std: 0.63
                       Mean reward: 25.17
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72679424
                    Iteration time: 8.28s
                        Total time: 38877.91s
                               ETA: 837549.1s

################################################################################
                    [1m Learning iteration 4436/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.051s, learning 0.182s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0482
             Mean action noise std: 0.63
                       Mean reward: 25.17
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72695808
                    Iteration time: 8.23s
                        Total time: 38886.15s
                               ETA: 837528.9s

################################################################################
                    [1m Learning iteration 4437/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.770s, learning 0.159s)
               Value function loss: 6.0673
                    Surrogate loss: 0.0912
             Mean action noise std: 0.63
                       Mean reward: 24.82
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72712192
                    Iteration time: 7.93s
                        Total time: 38894.07s
                               ETA: 837502.1s

################################################################################
                    [1m Learning iteration 4438/100000 [0m                    

                       Computation: 2106 steps/s (collection: 7.614s, learning 0.166s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0158
             Mean action noise std: 0.63
                       Mean reward: 24.82
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72728576
                    Iteration time: 7.78s
                        Total time: 38901.85s
                               ETA: 837472.2s

################################################################################
                    [1m Learning iteration 4439/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.045s, learning 0.161s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0061
             Mean action noise std: 0.63
                       Mean reward: 24.82
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 8.21s
                        Total time: 38910.06s
                               ETA: 837451.4s

################################################################################
                    [1m Learning iteration 4440/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.925s, learning 0.164s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0036
             Mean action noise std: 0.63
                       Mean reward: 24.82
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72761344
                    Iteration time: 8.09s
                        Total time: 38918.15s
                               ETA: 837428.1s

################################################################################
                    [1m Learning iteration 4441/100000 [0m                    

                       Computation: 2085 steps/s (collection: 7.691s, learning 0.166s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0336
             Mean action noise std: 0.63
                       Mean reward: 24.82
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72777728
                    Iteration time: 7.86s
                        Total time: 38926.00s
                               ETA: 837399.8s

################################################################################
                    [1m Learning iteration 4442/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.312s, learning 0.158s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0433
             Mean action noise std: 0.63
                       Mean reward: 24.82
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72794112
                    Iteration time: 8.47s
                        Total time: 38934.48s
                               ETA: 837384.8s

################################################################################
                    [1m Learning iteration 4443/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.031s, learning 0.168s)
               Value function loss: 0.0245
                    Surrogate loss: -0.0376
             Mean action noise std: 0.63
                       Mean reward: 24.82
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72810496
                    Iteration time: 8.20s
                        Total time: 38942.67s
                               ETA: 837363.9s

################################################################################
                    [1m Learning iteration 4444/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.838s, learning 0.269s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0369
             Mean action noise std: 0.63
                       Mean reward: 24.86
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72826880
                    Iteration time: 8.11s
                        Total time: 38950.78s
                               ETA: 837341.0s

################################################################################
                    [1m Learning iteration 4445/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.034s, learning 0.159s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0396
             Mean action noise std: 0.63
                       Mean reward: 24.87
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 8.19s
                        Total time: 38958.97s
                               ETA: 837320.0s

################################################################################
                    [1m Learning iteration 4446/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.973s, learning 0.167s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0335
             Mean action noise std: 0.63
                       Mean reward: 24.85
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72859648
                    Iteration time: 8.14s
                        Total time: 38967.11s
                               ETA: 837297.9s

################################################################################
                    [1m Learning iteration 4447/100000 [0m                    

                       Computation: 2078 steps/s (collection: 7.719s, learning 0.163s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0180
             Mean action noise std: 0.63
                       Mean reward: 24.86
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72876032
                    Iteration time: 7.88s
                        Total time: 38975.00s
                               ETA: 837270.2s

################################################################################
                    [1m Learning iteration 4448/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.255s, learning 0.398s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0253
             Mean action noise std: 0.63
                       Mean reward: 24.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72892416
                    Iteration time: 8.65s
                        Total time: 38983.65s
                               ETA: 837259.1s

################################################################################
                    [1m Learning iteration 4449/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.167s)
               Value function loss: 0.0157
                    Surrogate loss: -0.0282
             Mean action noise std: 0.63
                       Mean reward: 24.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72908800
                    Iteration time: 8.35s
                        Total time: 38991.99s
                               ETA: 837241.4s

################################################################################
                    [1m Learning iteration 4450/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.909s, learning 0.159s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0256
             Mean action noise std: 0.63
                       Mean reward: 24.93
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72925184
                    Iteration time: 8.07s
                        Total time: 39000.06s
                               ETA: 837217.7s

################################################################################
                    [1m Learning iteration 4451/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.910s, learning 0.163s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0191
             Mean action noise std: 0.63
                       Mean reward: 24.95
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 8.07s
                        Total time: 39008.14s
                               ETA: 837194.1s

################################################################################
                    [1m Learning iteration 4452/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.074s, learning 0.168s)
               Value function loss: 10.1786
                    Surrogate loss: 0.0773
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72957952
                    Iteration time: 8.24s
                        Total time: 39016.38s
                               ETA: 837174.2s

################################################################################
                    [1m Learning iteration 4453/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.815s, learning 0.185s)
               Value function loss: 0.3661
                    Surrogate loss: -0.0057
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72974336
                    Iteration time: 8.00s
                        Total time: 39024.38s
                               ETA: 837149.1s

################################################################################
                    [1m Learning iteration 4454/100000 [0m                    

                       Computation: 2063 steps/s (collection: 7.776s, learning 0.166s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72990720
                    Iteration time: 7.94s
                        Total time: 39032.32s
                               ETA: 837122.8s

################################################################################
                    [1m Learning iteration 4455/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.205s, learning 0.189s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0071
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73007104
                    Iteration time: 8.39s
                        Total time: 39040.71s
                               ETA: 837106.1s

################################################################################
                    [1m Learning iteration 4456/100000 [0m                    

                       Computation: 2067 steps/s (collection: 7.755s, learning 0.169s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0172
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73023488
                    Iteration time: 7.92s
                        Total time: 39048.64s
                               ETA: 837079.4s

################################################################################
                    [1m Learning iteration 4457/100000 [0m                    

                       Computation: 2079 steps/s (collection: 7.710s, learning 0.169s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0449
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 7.88s
                        Total time: 39056.52s
                               ETA: 837051.7s

################################################################################
                    [1m Learning iteration 4458/100000 [0m                    

                       Computation: 2070 steps/s (collection: 7.696s, learning 0.216s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0425
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73056256
                    Iteration time: 7.91s
                        Total time: 39064.43s
                               ETA: 837024.8s

################################################################################
                    [1m Learning iteration 4459/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.876s, learning 0.169s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0478
             Mean action noise std: 0.63
                       Mean reward: 25.66
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73072640
                    Iteration time: 8.05s
                        Total time: 39072.47s
                               ETA: 837000.7s

################################################################################
                    [1m Learning iteration 4460/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.857s, learning 0.163s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0360
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73089024
                    Iteration time: 8.02s
                        Total time: 39080.49s
                               ETA: 836976.1s

################################################################################
                    [1m Learning iteration 4461/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.840s, learning 0.209s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0353
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73105408
                    Iteration time: 8.05s
                        Total time: 39088.54s
                               ETA: 836952.1s

################################################################################
                    [1m Learning iteration 4462/100000 [0m                    

                       Computation: 2064 steps/s (collection: 7.770s, learning 0.167s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0177
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73121792
                    Iteration time: 7.94s
                        Total time: 39096.48s
                               ETA: 836925.7s

################################################################################
                    [1m Learning iteration 4463/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.056s, learning 0.163s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0328
             Mean action noise std: 0.63
                       Mean reward: 25.64
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 8.22s
                        Total time: 39104.70s
                               ETA: 836905.3s

################################################################################
                    [1m Learning iteration 4464/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.892s, learning 0.183s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0132
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73154560
                    Iteration time: 8.07s
                        Total time: 39112.77s
                               ETA: 836881.9s

################################################################################
                    [1m Learning iteration 4465/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.739s, learning 0.217s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0233
             Mean action noise std: 0.63
                       Mean reward: 25.60
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73170944
                    Iteration time: 7.96s
                        Total time: 39120.73s
                               ETA: 836855.9s

################################################################################
                    [1m Learning iteration 4466/100000 [0m                    

                       Computation: 2061 steps/s (collection: 7.762s, learning 0.188s)
               Value function loss: 0.0213
                    Surrogate loss: -0.0188
             Mean action noise std: 0.63
                       Mean reward: 25.59
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73187328
                    Iteration time: 7.95s
                        Total time: 39128.68s
                               ETA: 836829.8s

################################################################################
                    [1m Learning iteration 4467/100000 [0m                    

                       Computation: 2111 steps/s (collection: 7.595s, learning 0.163s)
               Value function loss: 0.0087
                    Surrogate loss: -0.0424
             Mean action noise std: 0.63
                       Mean reward: 25.61
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73203712
                    Iteration time: 7.76s
                        Total time: 39136.43s
                               ETA: 836799.7s

################################################################################
                    [1m Learning iteration 4468/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.079s, learning 0.189s)
               Value function loss: 9.3260
                    Surrogate loss: 0.1508
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73220096
                    Iteration time: 8.27s
                        Total time: 39144.70s
                               ETA: 836780.4s

################################################################################
                    [1m Learning iteration 4469/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.211s, learning 0.188s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0081
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 8.40s
                        Total time: 39153.10s
                               ETA: 836764.0s

################################################################################
                    [1m Learning iteration 4470/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.894s, learning 0.162s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0156
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73252864
                    Iteration time: 8.06s
                        Total time: 39161.16s
                               ETA: 836740.2s

################################################################################
                    [1m Learning iteration 4471/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.910s, learning 0.160s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0009
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73269248
                    Iteration time: 8.07s
                        Total time: 39169.23s
                               ETA: 836716.7s

################################################################################
                    [1m Learning iteration 4472/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.881s, learning 0.193s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0274
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73285632
                    Iteration time: 8.07s
                        Total time: 39177.30s
                               ETA: 836693.3s

################################################################################
                    [1m Learning iteration 4473/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.076s, learning 0.199s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0442
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73302016
                    Iteration time: 8.28s
                        Total time: 39185.58s
                               ETA: 836674.2s

################################################################################
                    [1m Learning iteration 4474/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.920s, learning 0.159s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0427
             Mean action noise std: 0.63
                       Mean reward: 25.15
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73318400
                    Iteration time: 8.08s
                        Total time: 39193.66s
                               ETA: 836651.0s

################################################################################
                    [1m Learning iteration 4475/100000 [0m                    

                       Computation: 2079 steps/s (collection: 7.715s, learning 0.164s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0318
             Mean action noise std: 0.63
                       Mean reward: 25.19
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 7.88s
                        Total time: 39201.54s
                               ETA: 836623.5s

################################################################################
                    [1m Learning iteration 4476/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.769s, learning 0.184s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0312
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73351168
                    Iteration time: 7.95s
                        Total time: 39209.49s
                               ETA: 836597.5s

################################################################################
                    [1m Learning iteration 4477/100000 [0m                    

                       Computation: 2153 steps/s (collection: 7.372s, learning 0.236s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0240
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73367552
                    Iteration time: 7.61s
                        Total time: 39217.10s
                               ETA: 836564.2s

################################################################################
                    [1m Learning iteration 4478/100000 [0m                    

                       Computation: 1982 steps/s (collection: 7.983s, learning 0.283s)
               Value function loss: 0.0251
                    Surrogate loss: -0.0226
             Mean action noise std: 0.63
                       Mean reward: 25.21
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73383936
                    Iteration time: 8.27s
                        Total time: 39225.36s
                               ETA: 836545.0s

################################################################################
                    [1m Learning iteration 4479/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.828s, learning 0.197s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0165
             Mean action noise std: 0.63
                       Mean reward: 25.24
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73400320
                    Iteration time: 8.03s
                        Total time: 39233.39s
                               ETA: 836520.6s

################################################################################
                    [1m Learning iteration 4480/100000 [0m                    

                       Computation: 2092 steps/s (collection: 7.667s, learning 0.162s)
               Value function loss: 0.0194
                    Surrogate loss: -0.0236
             Mean action noise std: 0.63
                       Mean reward: 25.22
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73416704
                    Iteration time: 7.83s
                        Total time: 39241.22s
                               ETA: 836492.1s

################################################################################
                    [1m Learning iteration 4481/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.008s, learning 0.252s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0269
             Mean action noise std: 0.63
                       Mean reward: 25.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 8.26s
                        Total time: 39249.48s
                               ETA: 836472.7s

################################################################################
                    [1m Learning iteration 4482/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.959s, learning 0.207s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0225
             Mean action noise std: 0.63
                       Mean reward: 25.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73449472
                    Iteration time: 8.17s
                        Total time: 39257.64s
                               ETA: 836451.4s

################################################################################
                    [1m Learning iteration 4483/100000 [0m                    

                       Computation: 2082 steps/s (collection: 7.706s, learning 0.162s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0455
             Mean action noise std: 0.63
                       Mean reward: 25.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73465856
                    Iteration time: 7.87s
                        Total time: 39265.51s
                               ETA: 836423.7s

################################################################################
                    [1m Learning iteration 4484/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.908s, learning 0.170s)
               Value function loss: 4.1197
                    Surrogate loss: 0.1811
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73482240
                    Iteration time: 8.08s
                        Total time: 39273.59s
                               ETA: 836400.5s

################################################################################
                    [1m Learning iteration 4485/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.823s, learning 0.216s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0151
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73498624
                    Iteration time: 8.04s
                        Total time: 39281.63s
                               ETA: 836376.5s

################################################################################
                    [1m Learning iteration 4486/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.014s, learning 0.171s)
               Value function loss: 0.0096
                    Surrogate loss: -0.0109
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73515008
                    Iteration time: 8.18s
                        Total time: 39289.81s
                               ETA: 836355.5s

################################################################################
                    [1m Learning iteration 4487/100000 [0m                    

                       Computation: 2059 steps/s (collection: 7.773s, learning 0.183s)
               Value function loss: 0.0277
                    Surrogate loss: -0.0075
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 7.96s
                        Total time: 39297.77s
                               ETA: 836329.7s

################################################################################
                    [1m Learning iteration 4488/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.812s, learning 0.160s)
               Value function loss: 0.0379
                    Surrogate loss: -0.0261
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73547776
                    Iteration time: 7.97s
                        Total time: 39305.74s
                               ETA: 836304.3s

################################################################################
                    [1m Learning iteration 4489/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.018s, learning 0.162s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0394
             Mean action noise std: 0.63
                       Mean reward: 25.63
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73564160
                    Iteration time: 8.18s
                        Total time: 39313.92s
                               ETA: 836283.3s

################################################################################
                    [1m Learning iteration 4490/100000 [0m                    

                       Computation: 2041 steps/s (collection: 7.842s, learning 0.186s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0412
             Mean action noise std: 0.63
                       Mean reward: 25.68
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73580544
                    Iteration time: 8.03s
                        Total time: 39321.95s
                               ETA: 836259.0s

################################################################################
                    [1m Learning iteration 4491/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.804s, learning 0.201s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0402
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73596928
                    Iteration time: 8.00s
                        Total time: 39329.95s
                               ETA: 836234.3s

################################################################################
                    [1m Learning iteration 4492/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.092s, learning 0.238s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0309
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73613312
                    Iteration time: 8.33s
                        Total time: 39338.28s
                               ETA: 836216.5s

################################################################################
                    [1m Learning iteration 4493/100000 [0m                    

                       Computation: 2095 steps/s (collection: 7.658s, learning 0.159s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0337
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 7.82s
                        Total time: 39346.10s
                               ETA: 836187.8s

################################################################################
                    [1m Learning iteration 4494/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.165s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0216
             Mean action noise std: 0.63
                       Mean reward: 25.69
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73646080
                    Iteration time: 8.36s
                        Total time: 39354.46s
                               ETA: 836170.6s

################################################################################
                    [1m Learning iteration 4495/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.946s, learning 0.173s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0215
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73662464
                    Iteration time: 8.12s
                        Total time: 39362.58s
                               ETA: 836148.4s

################################################################################
                    [1m Learning iteration 4496/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.163s, learning 0.263s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0185
             Mean action noise std: 0.63
                       Mean reward: 25.72
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73678848
                    Iteration time: 8.43s
                        Total time: 39371.00s
                               ETA: 836132.6s

################################################################################
                    [1m Learning iteration 4497/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.894s, learning 0.167s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0333
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73695232
                    Iteration time: 8.06s
                        Total time: 39379.06s
                               ETA: 836109.1s

################################################################################
                    [1m Learning iteration 4498/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.115s, learning 0.184s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0202
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73711616
                    Iteration time: 8.30s
                        Total time: 39387.36s
                               ETA: 836090.7s

################################################################################
                    [1m Learning iteration 4499/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.991s, learning 0.170s)
               Value function loss: 9.9950
                    Surrogate loss: 0.0780
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 8.16s
                        Total time: 39395.52s
                               ETA: 836069.3s

################################################################################
                    [1m Learning iteration 4500/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.030s, learning 0.161s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0084
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73744384
                    Iteration time: 8.19s
                        Total time: 39403.72s
                               ETA: 836048.6s

################################################################################
                    [1m Learning iteration 4501/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.086s, learning 0.187s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0106
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73760768
                    Iteration time: 8.27s
                        Total time: 39411.99s
                               ETA: 836029.7s

################################################################################
                    [1m Learning iteration 4502/100000 [0m                    

                       Computation: 2079 steps/s (collection: 7.715s, learning 0.163s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0064
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73777152
                    Iteration time: 7.88s
                        Total time: 39419.87s
                               ETA: 836002.3s

################################################################################
                    [1m Learning iteration 4503/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.967s, learning 0.164s)
               Value function loss: 0.0151
                    Surrogate loss: -0.0212
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73793536
                    Iteration time: 8.13s
                        Total time: 39428.00s
                               ETA: 835980.4s

################################################################################
                    [1m Learning iteration 4504/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.950s, learning 0.170s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0450
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73809920
                    Iteration time: 8.12s
                        Total time: 39436.12s
                               ETA: 835958.2s

################################################################################
                    [1m Learning iteration 4505/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.989s, learning 0.167s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0442
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 8.16s
                        Total time: 39444.27s
                               ETA: 835936.7s

################################################################################
                    [1m Learning iteration 4506/100000 [0m                    

                       Computation: 2055 steps/s (collection: 7.800s, learning 0.171s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0454
             Mean action noise std: 0.63
                       Mean reward: 25.71
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73842688
                    Iteration time: 7.97s
                        Total time: 39452.25s
                               ETA: 835911.4s

################################################################################
                    [1m Learning iteration 4507/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.101s, learning 0.262s)
               Value function loss: 0.0363
                    Surrogate loss: -0.0300
             Mean action noise std: 0.63
                       Mean reward: 25.70
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73859072
                    Iteration time: 8.36s
                        Total time: 39460.61s
                               ETA: 835894.4s

################################################################################
                    [1m Learning iteration 4508/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.965s, learning 0.167s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0326
             Mean action noise std: 0.63
                       Mean reward: 25.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73875456
                    Iteration time: 8.13s
                        Total time: 39468.74s
                               ETA: 835872.5s

################################################################################
                    [1m Learning iteration 4509/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.176s, learning 0.167s)
               Value function loss: 0.0357
                    Surrogate loss: -0.0178
             Mean action noise std: 0.63
                       Mean reward: 25.73
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73891840
                    Iteration time: 8.34s
                        Total time: 39477.08s
                               ETA: 835855.1s

################################################################################
                    [1m Learning iteration 4510/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.823s, learning 0.171s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0360
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73908224
                    Iteration time: 7.99s
                        Total time: 39485.08s
                               ETA: 835830.2s

################################################################################
                    [1m Learning iteration 4511/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.222s, learning 0.179s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0149
             Mean action noise std: 0.63
                       Mean reward: 25.74
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 8.40s
                        Total time: 39493.48s
                               ETA: 835814.0s

################################################################################
                    [1m Learning iteration 4512/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.112s, learning 0.180s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0207
             Mean action noise std: 0.63
                       Mean reward: 25.76
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73940992
                    Iteration time: 8.29s
                        Total time: 39501.77s
                               ETA: 835795.5s

################################################################################
                    [1m Learning iteration 4513/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.828s, learning 0.172s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0143
             Mean action noise std: 0.63
                       Mean reward: 25.78
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73957376
                    Iteration time: 8.00s
                        Total time: 39509.77s
                               ETA: 835770.9s

################################################################################
                    [1m Learning iteration 4514/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.001s, learning 0.163s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0433
             Mean action noise std: 0.63
                       Mean reward: 25.78
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73973760
                    Iteration time: 8.16s
                        Total time: 39517.94s
                               ETA: 835749.7s

################################################################################
                    [1m Learning iteration 4515/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.953s, learning 0.171s)
               Value function loss: 6.4886
                    Surrogate loss: 0.2534
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73990144
                    Iteration time: 8.12s
                        Total time: 39526.06s
                               ETA: 835727.6s

################################################################################
                    [1m Learning iteration 4516/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.733s, learning 0.166s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0108
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74006528
                    Iteration time: 7.90s
                        Total time: 39533.96s
                               ETA: 835700.8s

################################################################################
                    [1m Learning iteration 4517/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.934s, learning 0.190s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0134
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 8.12s
                        Total time: 39542.08s
                               ETA: 835678.8s

################################################################################
                    [1m Learning iteration 4518/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.136s, learning 0.164s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0118
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74039296
                    Iteration time: 8.30s
                        Total time: 39550.38s
                               ETA: 835660.5s

################################################################################
                    [1m Learning iteration 4519/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.860s, learning 0.164s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0262
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74055680
                    Iteration time: 8.02s
                        Total time: 39558.41s
                               ETA: 835636.3s

################################################################################
                    [1m Learning iteration 4520/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.196s, learning 0.170s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0380
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74072064
                    Iteration time: 8.37s
                        Total time: 39566.77s
                               ETA: 835619.4s

################################################################################
                    [1m Learning iteration 4521/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.080s, learning 0.159s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0369
             Mean action noise std: 0.63
                       Mean reward: 26.04
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74088448
                    Iteration time: 8.24s
                        Total time: 39575.01s
                               ETA: 835599.9s

################################################################################
                    [1m Learning iteration 4522/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.971s, learning 0.160s)
               Value function loss: 0.0306
                    Surrogate loss: -0.0328
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74104832
                    Iteration time: 8.13s
                        Total time: 39583.14s
                               ETA: 835578.0s

################################################################################
                    [1m Learning iteration 4523/100000 [0m                    

                       Computation: 2042 steps/s (collection: 7.855s, learning 0.165s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0326
             Mean action noise std: 0.63
                       Mean reward: 26.04
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 8.02s
                        Total time: 39591.16s
                               ETA: 835553.8s

################################################################################
                    [1m Learning iteration 4524/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.014s, learning 0.165s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0172
             Mean action noise std: 0.63
                       Mean reward: 26.04
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74137600
                    Iteration time: 8.18s
                        Total time: 39599.34s
                               ETA: 835533.0s

################################################################################
                    [1m Learning iteration 4525/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.045s, learning 0.261s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0174
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74153984
                    Iteration time: 8.31s
                        Total time: 39607.65s
                               ETA: 835514.9s

################################################################################
                    [1m Learning iteration 4526/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.411s, learning 0.191s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0187
             Mean action noise std: 0.63
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74170368
                    Iteration time: 8.60s
                        Total time: 39616.25s
                               ETA: 835503.0s

################################################################################
                    [1m Learning iteration 4527/100000 [0m                    

                       Computation: 2073 steps/s (collection: 7.741s, learning 0.161s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0211
             Mean action noise std: 0.63
                       Mean reward: 26.06
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74186752
                    Iteration time: 7.90s
                        Total time: 39624.15s
                               ETA: 835476.3s

################################################################################
                    [1m Learning iteration 4528/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.000s, learning 0.163s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0172
             Mean action noise std: 0.62
                       Mean reward: 26.09
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74203136
                    Iteration time: 8.16s
                        Total time: 39632.32s
                               ETA: 835455.2s

################################################################################
                    [1m Learning iteration 4529/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.208s, learning 0.167s)
               Value function loss: 0.0201
                    Surrogate loss: -0.0116
             Mean action noise std: 0.62
                       Mean reward: 26.09
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 8.37s
                        Total time: 39640.69s
                               ETA: 835438.5s

################################################################################
                    [1m Learning iteration 4530/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.062s, learning 0.177s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0141
             Mean action noise std: 0.62
                       Mean reward: 26.08
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74235904
                    Iteration time: 8.24s
                        Total time: 39648.93s
                               ETA: 835419.0s

################################################################################
                    [1m Learning iteration 4531/100000 [0m                    

                       Computation: 2049 steps/s (collection: 7.829s, learning 0.165s)
               Value function loss: 3.5561
                    Surrogate loss: 0.0656
             Mean action noise std: 0.62
                       Mean reward: 26.07
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74252288
                    Iteration time: 7.99s
                        Total time: 39656.92s
                               ETA: 835394.3s

################################################################################
                    [1m Learning iteration 4532/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.971s, learning 0.165s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0232
             Mean action noise std: 0.62
                       Mean reward: 26.07
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74268672
                    Iteration time: 8.14s
                        Total time: 39665.06s
                               ETA: 835372.6s

################################################################################
                    [1m Learning iteration 4533/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.144s, learning 0.167s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0113
             Mean action noise std: 0.62
                       Mean reward: 26.07
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74285056
                    Iteration time: 8.31s
                        Total time: 39673.37s
                               ETA: 835354.6s

################################################################################
                    [1m Learning iteration 4534/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.989s, learning 0.162s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0052
             Mean action noise std: 0.62
                       Mean reward: 26.07
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74301440
                    Iteration time: 8.15s
                        Total time: 39681.52s
                               ETA: 835333.2s

################################################################################
                    [1m Learning iteration 4535/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.186s, learning 0.176s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0418
             Mean action noise std: 0.62
                       Mean reward: 26.07
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 8.36s
                        Total time: 39689.88s
                               ETA: 835316.3s

################################################################################
                    [1m Learning iteration 4536/100000 [0m                    

                       Computation: 2066 steps/s (collection: 7.756s, learning 0.171s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0425
             Mean action noise std: 0.62
                       Mean reward: 26.07
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74334208
                    Iteration time: 7.93s
                        Total time: 39697.81s
                               ETA: 835290.2s

################################################################################
                    [1m Learning iteration 4537/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.785s, learning 0.165s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0268
             Mean action noise std: 0.62
                       Mean reward: 26.06
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74350592
                    Iteration time: 7.95s
                        Total time: 39705.76s
                               ETA: 835264.6s

################################################################################
                    [1m Learning iteration 4538/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.224s, learning 0.166s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0176
             Mean action noise std: 0.62
                       Mean reward: 26.05
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74366976
                    Iteration time: 8.39s
                        Total time: 39714.15s
                               ETA: 835248.3s

################################################################################
                    [1m Learning iteration 4539/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.071s, learning 0.182s)
               Value function loss: 0.0142
                    Surrogate loss: -0.0166
             Mean action noise std: 0.62
                       Mean reward: 26.02
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74383360
                    Iteration time: 8.25s
                        Total time: 39722.40s
                               ETA: 835229.1s

################################################################################
                    [1m Learning iteration 4540/100000 [0m                    

                       Computation: 2071 steps/s (collection: 7.740s, learning 0.169s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0261
             Mean action noise std: 0.62
                       Mean reward: 26.01
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74399744
                    Iteration time: 7.91s
                        Total time: 39730.31s
                               ETA: 835202.7s

################################################################################
                    [1m Learning iteration 4541/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.089s, learning 0.195s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0121
             Mean action noise std: 0.62
                       Mean reward: 26.00
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 8.28s
                        Total time: 39738.60s
                               ETA: 835184.2s

################################################################################
                    [1m Learning iteration 4542/100000 [0m                    

                       Computation: 2048 steps/s (collection: 7.839s, learning 0.161s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0092
             Mean action noise std: 0.62
                       Mean reward: 25.99
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74432512
                    Iteration time: 8.00s
                        Total time: 39746.60s
                               ETA: 835159.7s

################################################################################
                    [1m Learning iteration 4543/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.918s, learning 0.162s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0151
             Mean action noise std: 0.62
                       Mean reward: 25.98
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74448896
                    Iteration time: 8.08s
                        Total time: 39754.68s
                               ETA: 835136.9s

################################################################################
                    [1m Learning iteration 4544/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.063s, learning 0.162s)
               Value function loss: 0.0140
                    Surrogate loss: -0.0153
             Mean action noise std: 0.62
                       Mean reward: 25.99
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74465280
                    Iteration time: 8.23s
                        Total time: 39762.90s
                               ETA: 835117.2s

################################################################################
                    [1m Learning iteration 4545/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.076s, learning 0.173s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0243
             Mean action noise std: 0.62
                       Mean reward: 25.96
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74481664
                    Iteration time: 8.25s
                        Total time: 39771.15s
                               ETA: 835097.9s

################################################################################
                    [1m Learning iteration 4546/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.163s)
               Value function loss: 11.2849
                    Surrogate loss: 0.0876
             Mean action noise std: 0.62
                       Mean reward: 26.24
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74498048
                    Iteration time: 8.29s
                        Total time: 39779.44s
                               ETA: 835079.6s

################################################################################
                    [1m Learning iteration 4547/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.028s, learning 0.216s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0156
             Mean action noise std: 0.62
                       Mean reward: 26.24
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 8.24s
                        Total time: 39787.69s
                               ETA: 835060.3s

################################################################################
                    [1m Learning iteration 4548/100000 [0m                    

                       Computation: 2046 steps/s (collection: 7.847s, learning 0.158s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0210
             Mean action noise std: 0.62
                       Mean reward: 26.24
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74530816
                    Iteration time: 8.00s
                        Total time: 39795.69s
                               ETA: 835036.0s

################################################################################
                    [1m Learning iteration 4549/100000 [0m                    

                       Computation: 2056 steps/s (collection: 7.805s, learning 0.161s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0100
             Mean action noise std: 0.62
                       Mean reward: 26.24
               Mean episode length: 125.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74547200
                    Iteration time: 7.97s
                        Total time: 39803.66s
                               ETA: 835010.8s

################################################################################
                    [1m Learning iteration 4550/100000 [0m                    

                       Computation: 2040 steps/s (collection: 7.861s, learning 0.169s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0361
             Mean action noise std: 0.62
                       Mean reward: 26.24
               Mean episode length: 125.00
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74563584
                    Iteration time: 8.03s
                        Total time: 39811.69s
                               ETA: 834987.0s

################################################################################
                    [1m Learning iteration 4551/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.194s, learning 0.160s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0445
             Mean action noise std: 0.62
                       Mean reward: 26.24
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74579968
                    Iteration time: 8.35s
                        Total time: 39820.04s
                               ETA: 834970.0s

################################################################################
                    [1m Learning iteration 4552/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.191s, learning 0.189s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0377
             Mean action noise std: 0.62
                       Mean reward: 26.24
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74596352
                    Iteration time: 8.38s
                        Total time: 39828.42s
                               ETA: 834953.5s

################################################################################
                    [1m Learning iteration 4553/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.406s, learning 0.183s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0313
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 8.59s
                        Total time: 39837.01s
                               ETA: 834941.4s

################################################################################
                    [1m Learning iteration 4554/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.173s, learning 0.169s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0305
             Mean action noise std: 0.62
                       Mean reward: 26.28
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74629120
                    Iteration time: 8.34s
                        Total time: 39845.35s
                               ETA: 834924.2s

################################################################################
                    [1m Learning iteration 4555/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.918s, learning 0.163s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0283
             Mean action noise std: 0.62
                       Mean reward: 26.28
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74645504
                    Iteration time: 8.08s
                        Total time: 39853.43s
                               ETA: 834901.5s

################################################################################
                    [1m Learning iteration 4556/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.001s, learning 0.247s)
               Value function loss: 0.0287
                    Surrogate loss: -0.0175
             Mean action noise std: 0.62
                       Mean reward: 26.29
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74661888
                    Iteration time: 8.25s
                        Total time: 39861.68s
                               ETA: 834882.2s

################################################################################
                    [1m Learning iteration 4557/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.027s, learning 0.194s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0176
             Mean action noise std: 0.62
                       Mean reward: 26.32
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74678272
                    Iteration time: 8.22s
                        Total time: 39869.90s
                               ETA: 834862.5s

################################################################################
                    [1m Learning iteration 4558/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.827s, learning 0.253s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0103
             Mean action noise std: 0.62
                       Mean reward: 26.35
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74694656
                    Iteration time: 8.08s
                        Total time: 39877.98s
                               ETA: 834839.8s

################################################################################
                    [1m Learning iteration 4559/100000 [0m                    

                       Computation: 2074 steps/s (collection: 7.668s, learning 0.229s)
               Value function loss: 0.0224
                    Surrogate loss: -0.0219
             Mean action noise std: 0.62
                       Mean reward: 26.35
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 7.90s
                        Total time: 39885.88s
                               ETA: 834813.2s

################################################################################
                    [1m Learning iteration 4560/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.163s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0152
             Mean action noise std: 0.62
                       Mean reward: 26.37
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74727424
                    Iteration time: 8.17s
                        Total time: 39894.05s
                               ETA: 834792.4s

################################################################################
                    [1m Learning iteration 4561/100000 [0m                    

                       Computation: 2060 steps/s (collection: 7.776s, learning 0.174s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0350
             Mean action noise std: 0.62
                       Mean reward: 26.37
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74743808
                    Iteration time: 7.95s
                        Total time: 39902.00s
                               ETA: 834767.0s

################################################################################
                    [1m Learning iteration 4562/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.113s, learning 0.196s)
               Value function loss: 6.7780
                    Surrogate loss: 0.1648
             Mean action noise std: 0.62
                       Mean reward: 26.50
               Mean episode length: 125.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74760192
                    Iteration time: 8.31s
                        Total time: 39910.31s
                               ETA: 834749.1s

################################################################################
                    [1m Learning iteration 4563/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.900s, learning 0.223s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0165
             Mean action noise std: 0.62
                       Mean reward: 26.50
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74776576
                    Iteration time: 8.12s
                        Total time: 39918.43s
                               ETA: 834727.3s

################################################################################
                    [1m Learning iteration 4564/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.092s, learning 0.165s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0080
             Mean action noise std: 0.62
                       Mean reward: 26.50
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74792960
                    Iteration time: 8.26s
                        Total time: 39926.69s
                               ETA: 834708.4s

################################################################################
                    [1m Learning iteration 4565/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.270s, learning 0.164s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0130
             Mean action noise std: 0.62
                       Mean reward: 26.50
               Mean episode length: 125.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 8.43s
                        Total time: 39935.13s
                               ETA: 834693.1s

################################################################################
                    [1m Learning iteration 4566/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.342s, learning 0.162s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0465
             Mean action noise std: 0.62
                       Mean reward: 26.50
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74825728
                    Iteration time: 8.50s
                        Total time: 39943.63s
                               ETA: 834679.3s

################################################################################
                    [1m Learning iteration 4567/100000 [0m                    

                       Computation: 2079 steps/s (collection: 7.715s, learning 0.164s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0500
             Mean action noise std: 0.62
                       Mean reward: 26.50
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74842112
                    Iteration time: 7.88s
                        Total time: 39951.51s
                               ETA: 834652.4s

################################################################################
                    [1m Learning iteration 4568/100000 [0m                    

                       Computation: 2080 steps/s (collection: 7.716s, learning 0.161s)
               Value function loss: 0.0423
                    Surrogate loss: -0.0357
             Mean action noise std: 0.62
                       Mean reward: 26.52
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74858496
                    Iteration time: 7.88s
                        Total time: 39959.39s
                               ETA: 834625.5s

################################################################################
                    [1m Learning iteration 4569/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.942s, learning 0.162s)
               Value function loss: 0.0325
                    Surrogate loss: -0.0346
             Mean action noise std: 0.62
                       Mean reward: 26.52
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74874880
                    Iteration time: 8.10s
                        Total time: 39967.49s
                               ETA: 834603.4s

################################################################################
                    [1m Learning iteration 4570/100000 [0m                    

                       Computation: 2053 steps/s (collection: 7.812s, learning 0.168s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0398
             Mean action noise std: 0.62
                       Mean reward: 26.51
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74891264
                    Iteration time: 7.98s
                        Total time: 39975.47s
                               ETA: 834578.7s

################################################################################
                    [1m Learning iteration 4571/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.037s, learning 0.165s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0311
             Mean action noise std: 0.62
                       Mean reward: 26.46
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 8.20s
                        Total time: 39983.67s
                               ETA: 834558.6s

################################################################################
                    [1m Learning iteration 4572/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.037s, learning 0.171s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0185
             Mean action noise std: 0.62
                       Mean reward: 26.48
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74924032
                    Iteration time: 8.21s
                        Total time: 39991.88s
                               ETA: 834538.6s

################################################################################
                    [1m Learning iteration 4573/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.991s, learning 0.169s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0251
             Mean action noise std: 0.62
                       Mean reward: 26.48
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74940416
                    Iteration time: 8.16s
                        Total time: 40000.04s
                               ETA: 834517.7s

################################################################################
                    [1m Learning iteration 4574/100000 [0m                    

                       Computation: 2035 steps/s (collection: 7.881s, learning 0.166s)
               Value function loss: 0.0182
                    Surrogate loss: -0.0227
             Mean action noise std: 0.62
                       Mean reward: 26.47
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74956800
                    Iteration time: 8.05s
                        Total time: 40008.09s
                               ETA: 834494.4s

################################################################################
                    [1m Learning iteration 4575/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.046s, learning 0.173s)
               Value function loss: 0.0148
                    Surrogate loss: -0.0260
             Mean action noise std: 0.62
                       Mean reward: 26.47
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74973184
                    Iteration time: 8.22s
                        Total time: 40016.31s
                               ETA: 834474.7s

################################################################################
                    [1m Learning iteration 4576/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.150s, learning 0.164s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0210
             Mean action noise std: 0.62
                       Mean reward: 26.48
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74989568
                    Iteration time: 8.31s
                        Total time: 40024.62s
                               ETA: 834456.9s

################################################################################
                    [1m Learning iteration 4577/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.054s, learning 0.169s)
               Value function loss: 8.1848
                    Surrogate loss: 0.0646
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 8.22s
                        Total time: 40032.84s
                               ETA: 834437.3s

################################################################################
                    [1m Learning iteration 4578/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.069s, learning 0.169s)
               Value function loss: 0.3792
                    Surrogate loss: -0.0135
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75022336
                    Iteration time: 8.24s
                        Total time: 40041.08s
                               ETA: 834418.0s

################################################################################
                    [1m Learning iteration 4579/100000 [0m                    

                       Computation: 2039 steps/s (collection: 7.832s, learning 0.200s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0195
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75038720
                    Iteration time: 8.03s
                        Total time: 40049.11s
                               ETA: 834394.4s

################################################################################
                    [1m Learning iteration 4580/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.047s, learning 0.166s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0142
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75055104
                    Iteration time: 8.21s
                        Total time: 40057.33s
                               ETA: 834374.6s

################################################################################
                    [1m Learning iteration 4581/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.825s, learning 0.186s)
               Value function loss: 0.0289
                    Surrogate loss: -0.0281
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75071488
                    Iteration time: 8.01s
                        Total time: 40065.34s
                               ETA: 834350.6s

################################################################################
                    [1m Learning iteration 4582/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.030s, learning 0.234s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0508
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75087872
                    Iteration time: 8.26s
                        Total time: 40073.60s
                               ETA: 834331.9s

################################################################################
                    [1m Learning iteration 4583/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.121s, learning 0.175s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0467
             Mean action noise std: 0.62
                       Mean reward: 26.25
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 8.30s
                        Total time: 40081.90s
                               ETA: 834313.8s

################################################################################
                    [1m Learning iteration 4584/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.090s, learning 0.174s)
               Value function loss: 0.0484
                    Surrogate loss: -0.0400
             Mean action noise std: 0.62
                       Mean reward: 26.26
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75120640
                    Iteration time: 8.26s
                        Total time: 40090.16s
                               ETA: 834295.0s

################################################################################
                    [1m Learning iteration 4585/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.867s, learning 0.257s)
               Value function loss: 0.0486
                    Surrogate loss: -0.0366
             Mean action noise std: 0.62
                       Mean reward: 26.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75137024
                    Iteration time: 8.12s
                        Total time: 40098.28s
                               ETA: 834273.4s

################################################################################
                    [1m Learning iteration 4586/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.842s, learning 0.224s)
               Value function loss: 0.0328
                    Surrogate loss: -0.0386
             Mean action noise std: 0.62
                       Mean reward: 26.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75153408
                    Iteration time: 8.07s
                        Total time: 40106.35s
                               ETA: 834250.6s

################################################################################
                    [1m Learning iteration 4587/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.016s, learning 0.222s)
               Value function loss: 0.0329
                    Surrogate loss: -0.0206
             Mean action noise std: 0.62
                       Mean reward: 26.23
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75169792
                    Iteration time: 8.24s
                        Total time: 40114.59s
                               ETA: 834231.3s

################################################################################
                    [1m Learning iteration 4588/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.329s, learning 0.191s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0296
             Mean action noise std: 0.62
                       Mean reward: 26.31
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75186176
                    Iteration time: 8.52s
                        Total time: 40123.11s
                               ETA: 834217.9s

################################################################################
                    [1m Learning iteration 4589/100000 [0m                    

                       Computation: 2054 steps/s (collection: 7.781s, learning 0.195s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0195
             Mean action noise std: 0.62
                       Mean reward: 26.31
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 7.98s
                        Total time: 40131.09s
                               ETA: 834193.2s

################################################################################
                    [1m Learning iteration 4590/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.997s, learning 0.164s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0213
             Mean action noise std: 0.62
                       Mean reward: 26.31
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75218944
                    Iteration time: 8.16s
                        Total time: 40139.25s
                               ETA: 834172.4s

################################################################################
                    [1m Learning iteration 4591/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.969s, learning 0.179s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0252
             Mean action noise std: 0.62
                       Mean reward: 26.30
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75235328
                    Iteration time: 8.15s
                        Total time: 40147.39s
                               ETA: 834151.3s

################################################################################
                    [1m Learning iteration 4592/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.876s, learning 0.197s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0360
             Mean action noise std: 0.62
                       Mean reward: 26.29
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75251712
                    Iteration time: 8.07s
                        Total time: 40155.47s
                               ETA: 834128.6s

################################################################################
                    [1m Learning iteration 4593/100000 [0m                    

                       Computation: 2050 steps/s (collection: 7.825s, learning 0.165s)
               Value function loss: 9.8425
                    Surrogate loss: 0.1484
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75268096
                    Iteration time: 7.99s
                        Total time: 40163.46s
                               ETA: 834104.3s

################################################################################
                    [1m Learning iteration 4594/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.945s, learning 0.173s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0053
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75284480
                    Iteration time: 8.12s
                        Total time: 40171.58s
                               ETA: 834082.6s

################################################################################
                    [1m Learning iteration 4595/100000 [0m                    

                       Computation: 2044 steps/s (collection: 7.846s, learning 0.166s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0226
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 8.01s
                        Total time: 40179.59s
                               ETA: 834058.7s

################################################################################
                    [1m Learning iteration 4596/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.088s, learning 0.172s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0126
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75317248
                    Iteration time: 8.26s
                        Total time: 40187.85s
                               ETA: 834039.9s

################################################################################
                    [1m Learning iteration 4597/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.054s, learning 0.167s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0342
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75333632
                    Iteration time: 8.22s
                        Total time: 40196.07s
                               ETA: 834020.4s

################################################################################
                    [1m Learning iteration 4598/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.005s, learning 0.160s)
               Value function loss: 0.0259
                    Surrogate loss: -0.0501
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75350016
                    Iteration time: 8.16s
                        Total time: 40204.23s
                               ETA: 833999.6s

################################################################################
                    [1m Learning iteration 4599/100000 [0m                    

                       Computation: 2122 steps/s (collection: 7.559s, learning 0.162s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0506
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75366400
                    Iteration time: 7.72s
                        Total time: 40211.96s
                               ETA: 833969.7s

################################################################################
                    [1m Learning iteration 4600/100000 [0m                    

                       Computation: 1988 steps/s (collection: 7.942s, learning 0.299s)
               Value function loss: 0.0246
                    Surrogate loss: -0.0382
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75382784
                    Iteration time: 8.24s
                        Total time: 40220.20s
                               ETA: 833950.6s

################################################################################
                    [1m Learning iteration 4601/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.912s, learning 0.164s)
               Value function loss: 0.0286
                    Surrogate loss: -0.0386
             Mean action noise std: 0.62
                       Mean reward: 25.94
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 8.08s
                        Total time: 40228.27s
                               ETA: 833928.1s

################################################################################
                    [1m Learning iteration 4602/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.149s, learning 0.162s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0288
             Mean action noise std: 0.62
                       Mean reward: 25.91
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75415552
                    Iteration time: 8.31s
                        Total time: 40236.58s
                               ETA: 833910.4s

################################################################################
                    [1m Learning iteration 4603/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.303s, learning 0.198s)
               Value function loss: 0.0261
                    Surrogate loss: -0.0278
             Mean action noise std: 0.62
                       Mean reward: 25.89
               Mean episode length: 125.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75431936
                    Iteration time: 8.50s
                        Total time: 40245.08s
                               ETA: 833896.7s

Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041915-1g18gakk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandOver_ppo_20221020041913
wandb: ⭐️ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: 🚀 View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/1g18gakk
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:3
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=3, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=3, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:3', seed=None, sim_device='cuda:3', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandOver', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_over', 'numEnvs': 2048, 'envSpacing': 0.75, 'episodeLength': 75, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.01, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.01, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.2, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 50, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'egg', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandOver', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandOver_ppo_20221020041913', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 9568
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:3
Sequential(
  (0): Linear(in_features=398, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=40, bias=True)
)
Sequential(
  (0): Linear(in_features=398, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 1104 steps/s (collection: 9.601s, learning 5.231s)
               Value function loss: 1.8261
                    Surrogate loss: 0.0479
             Mean action noise std: 0.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 14.83s
                        Total time: 14.83s
                               ETA: 1483240.0s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 1846 steps/s (collection: 8.321s, learning 0.551s)
               Value function loss: 0.5317
                    Surrogate loss: -0.0178
             Mean action noise std: 0.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 8.87s
                        Total time: 23.70s
                               ETA: 1185169.8s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 1807 steps/s (collection: 8.898s, learning 0.165s)
               Value function loss: 0.2699
                    Surrogate loss: -0.0232
             Mean action noise std: 0.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 9.06s
                        Total time: 32.77s
                               ETA: 1092183.7s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 1476 steps/s (collection: 10.938s, learning 0.158s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0333
             Mean action noise std: 0.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 11.10s
                        Total time: 43.86s
                               ETA: 1096534.8s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 1120 steps/s (collection: 14.433s, learning 0.188s)
               Value function loss: 0.0652
                    Surrogate loss: -0.0420
             Mean action noise std: 0.80
                       Mean reward: 1.63
               Mean episode length: 35.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 14.62s
                        Total time: 58.48s
                               ETA: 1169619.0s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 1039 steps/s (collection: 15.585s, learning 0.171s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0354
             Mean action noise std: 0.80
                       Mean reward: 1.39
               Mean episode length: 42.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 15.76s
                        Total time: 74.24s
                               ETA: 1237253.8s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 1023 steps/s (collection: 15.838s, learning 0.171s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0447
             Mean action noise std: 0.80
                       Mean reward: 1.44
               Mean episode length: 44.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 16.01s
                        Total time: 90.25s
                               ETA: 1289185.8s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 1010 steps/s (collection: 16.047s, learning 0.164s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0205
             Mean action noise std: 0.80
                       Mean reward: 1.54
               Mean episode length: 45.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 16.21s
                        Total time: 106.46s
                               ETA: 1330642.8s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 936 steps/s (collection: 17.306s, learning 0.189s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0290
             Mean action noise std: 0.80
                       Mean reward: 1.58
               Mean episode length: 47.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 17.50s
                        Total time: 123.95s
                               ETA: 1377164.3s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 983 steps/s (collection: 16.473s, learning 0.180s)
               Value function loss: 0.1618
                    Surrogate loss: -0.0017
             Mean action noise std: 0.80
                       Mean reward: 3.11
               Mean episode length: 74.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 4.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 16.65s
                        Total time: 140.61s
                               ETA: 1405945.9s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.097s, learning 0.235s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0332
             Mean action noise std: 0.80
                       Mean reward: 3.11
               Mean episode length: 74.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 17.33s
                        Total time: 157.94s
                               ETA: 1435671.6s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 913 steps/s (collection: 17.779s, learning 0.162s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0377
             Mean action noise std: 0.80
                       Mean reward: 3.07
               Mean episode length: 73.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 17.94s
                        Total time: 175.88s
                               ETA: 1465514.8s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 928 steps/s (collection: 17.456s, learning 0.181s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0453
             Mean action noise std: 0.80
                       Mean reward: 3.05
               Mean episode length: 72.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 17.64s
                        Total time: 193.52s
                               ETA: 1488425.8s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.881s, learning 0.178s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0472
             Mean action noise std: 0.80
                       Mean reward: 3.03
               Mean episode length: 72.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 17.06s
                        Total time: 210.58s
                               ETA: 1503933.2s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 939 steps/s (collection: 17.268s, learning 0.170s)
               Value function loss: 0.0156
                    Surrogate loss: -0.0416
             Mean action noise std: 0.80
                       Mean reward: 2.98
               Mean episode length: 71.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 17.44s
                        Total time: 228.02s
                               ETA: 1519892.2s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 939 steps/s (collection: 17.246s, learning 0.193s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0460
             Mean action noise std: 0.80
                       Mean reward: 2.94
               Mean episode length: 70.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 17.44s
                        Total time: 245.45s
                               ETA: 1533859.8s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 925 steps/s (collection: 17.500s, learning 0.195s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0422
             Mean action noise std: 0.80
                       Mean reward: 2.92
               Mean episode length: 70.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 17.69s
                        Total time: 263.15s
                               ETA: 1547689.8s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.764s, learning 0.193s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0476
             Mean action noise std: 0.80
                       Mean reward: 2.92
               Mean episode length: 70.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 16.96s
                        Total time: 280.11s
                               ETA: 1555877.2s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 939 steps/s (collection: 17.240s, learning 0.195s)
               Value function loss: 0.1529
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 3.27
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 4.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 17.44s
                        Total time: 297.54s
                               ETA: 1565721.1s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.872s, learning 0.177s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0298
             Mean action noise std: 0.80
                       Mean reward: 3.27
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 17.05s
                        Total time: 314.59s
                               ETA: 1572650.5s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 932 steps/s (collection: 17.361s, learning 0.203s)
               Value function loss: 0.0258
                    Surrogate loss: -0.0435
             Mean action noise std: 0.80
                       Mean reward: 3.28
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 17.56s
                        Total time: 332.15s
                               ETA: 1581369.1s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 951 steps/s (collection: 16.913s, learning 0.302s)
               Value function loss: 0.0165
                    Surrogate loss: -0.0385
             Mean action noise std: 0.80
                       Mean reward: 3.27
               Mean episode length: 74.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 17.22s
                        Total time: 349.37s
                               ETA: 1587709.2s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.134s, learning 0.199s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0434
             Mean action noise std: 0.80
                       Mean reward: 3.21
               Mean episode length: 73.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 17.33s
                        Total time: 366.70s
                               ETA: 1594006.9s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.903s, learning 0.175s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0466
             Mean action noise std: 0.80
                       Mean reward: 3.13
               Mean episode length: 71.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 17.08s
                        Total time: 383.78s
                               ETA: 1598716.3s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 948 steps/s (collection: 17.090s, learning 0.179s)
               Value function loss: 0.0131
                    Surrogate loss: -0.0470
             Mean action noise std: 0.80
                       Mean reward: 3.03
               Mean episode length: 69.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 17.27s
                        Total time: 401.05s
                               ETA: 1603813.9s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 938 steps/s (collection: 17.280s, learning 0.174s)
               Value function loss: 0.0099
                    Surrogate loss: -0.0474
             Mean action noise std: 0.80
                       Mean reward: 3.01
               Mean episode length: 69.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 17.45s
                        Total time: 418.50s
                               ETA: 1609226.2s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 948 steps/s (collection: 17.062s, learning 0.208s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0482
             Mean action noise std: 0.80
                       Mean reward: 3.04
               Mean episode length: 68.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 17.27s
                        Total time: 435.77s
                               ETA: 1613555.7s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 943 steps/s (collection: 17.201s, learning 0.167s)
               Value function loss: 0.1674
                    Surrogate loss: -0.0129
             Mean action noise std: 0.80
                       Mean reward: 3.42
               Mean episode length: 75.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 17.37s
                        Total time: 453.14s
                               ETA: 1617925.4s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 947 steps/s (collection: 17.105s, learning 0.187s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0247
             Mean action noise std: 0.80
                       Mean reward: 3.42
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 17.29s
                        Total time: 470.43s
                               ETA: 1621732.4s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 944 steps/s (collection: 17.121s, learning 0.231s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 3.41
               Mean episode length: 74.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 17.35s
                        Total time: 487.79s
                               ETA: 1625482.5s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 953 steps/s (collection: 16.902s, learning 0.286s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0497
             Mean action noise std: 0.80
                       Mean reward: 3.39
               Mean episode length: 74.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 17.19s
                        Total time: 504.97s
                               ETA: 1628460.1s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 953 steps/s (collection: 17.000s, learning 0.179s)
               Value function loss: 0.0094
                    Surrogate loss: -0.0469
             Mean action noise std: 0.80
                       Mean reward: 3.24
               Mean episode length: 71.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 17.18s
                        Total time: 522.15s
                               ETA: 1631223.4s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.937s, learning 0.226s)
               Value function loss: 0.0080
                    Surrogate loss: -0.0500
             Mean action noise std: 0.80
                       Mean reward: 3.20
               Mean episode length: 70.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 17.16s
                        Total time: 539.32s
                               ETA: 1633770.9s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 941 steps/s (collection: 17.234s, learning 0.177s)
               Value function loss: 0.0101
                    Surrogate loss: -0.0408
             Mean action noise std: 0.80
                       Mean reward: 3.14
               Mean episode length: 66.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 17.41s
                        Total time: 556.73s
                               ETA: 1636894.2s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.817s, learning 0.200s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0448
             Mean action noise std: 0.80
                       Mean reward: 3.07
               Mean episode length: 64.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 17.02s
                        Total time: 573.74s
                               ETA: 1638713.5s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 951 steps/s (collection: 17.034s, learning 0.191s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0448
             Mean action noise std: 0.80
                       Mean reward: 3.03
               Mean episode length: 63.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 17.22s
                        Total time: 590.97s
                               ETA: 1641007.4s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 931 steps/s (collection: 17.399s, learning 0.186s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0428
             Mean action noise std: 0.80
                       Mean reward: 3.04
               Mean episode length: 63.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 17.59s
                        Total time: 608.55s
                               ETA: 1644150.7s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1285 steps/s (collection: 12.575s, learning 0.170s)
               Value function loss: 0.1869
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: 3.52
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 4.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 12.74s
                        Total time: 621.30s
                               ETA: 1634393.2s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.465s, learning 0.201s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0287
             Mean action noise std: 0.80
                       Mean reward: 3.52
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 8.67s
                        Total time: 629.97s
                               ETA: 1614681.5s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.753s, learning 0.253s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0512
             Mean action noise std: 0.80
                       Mean reward: 3.54
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 9.01s
                        Total time: 638.97s
                               ETA: 1596804.8s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.419s, learning 0.181s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0505
             Mean action noise std: 0.80
                       Mean reward: 3.46
               Mean episode length: 72.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 8.60s
                        Total time: 647.57s
                               ETA: 1578811.3s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.832s, learning 0.183s)
               Value function loss: 0.0118
                    Surrogate loss: -0.0444
             Mean action noise std: 0.80
                       Mean reward: 3.15
               Mean episode length: 66.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 9.01s
                        Total time: 656.59s
                               ETA: 1562659.7s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.602s, learning 0.179s)
               Value function loss: 0.0111
                    Surrogate loss: -0.0453
             Mean action noise std: 0.80
                       Mean reward: 2.88
               Mean episode length: 61.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 8.78s
                        Total time: 665.37s
                               ETA: 1546714.2s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.483s, learning 0.194s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0435
             Mean action noise std: 0.80
                       Mean reward: 2.88
               Mean episode length: 60.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 8.68s
                        Total time: 674.04s
                               ETA: 1531258.7s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.839s, learning 0.220s)
               Value function loss: 0.0091
                    Surrogate loss: -0.0451
             Mean action noise std: 0.80
                       Mean reward: 3.10
               Mean episode length: 65.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 9.06s
                        Total time: 683.10s
                               ETA: 1517337.4s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.564s, learning 0.238s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0409
             Mean action noise std: 0.80
                       Mean reward: 3.25
               Mean episode length: 67.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 8.80s
                        Total time: 691.90s
                               ETA: 1503463.8s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.422s, learning 0.206s)
               Value function loss: 0.1620
                    Surrogate loss: -0.0275
             Mean action noise std: 0.80
                       Mean reward: 3.63
               Mean episode length: 75.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 4.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 8.63s
                        Total time: 700.53s
                               ETA: 1489809.7s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.555s, learning 0.201s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0350
             Mean action noise std: 0.80
                       Mean reward: 3.60
               Mean episode length: 74.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 8.76s
                        Total time: 709.29s
                               ETA: 1476991.3s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.254s, learning 0.180s)
               Value function loss: 0.0153
                    Surrogate loss: -0.0508
             Mean action noise std: 0.80
                       Mean reward: 3.59
               Mean episode length: 73.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 8.43s
                        Total time: 717.72s
                               ETA: 1464038.7s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1792 steps/s (collection: 8.925s, learning 0.217s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0487
             Mean action noise std: 0.80
                       Mean reward: 3.45
               Mean episode length: 70.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 9.14s
                        Total time: 726.87s
                               ETA: 1453019.1s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1756 steps/s (collection: 9.091s, learning 0.238s)
               Value function loss: 0.0109
                    Surrogate loss: -0.0457
             Mean action noise std: 0.80
                       Mean reward: 3.11
               Mean episode length: 62.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 9.33s
                        Total time: 736.20s
                               ETA: 1442797.9s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.773s, learning 0.180s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0428
             Mean action noise std: 0.80
                       Mean reward: 2.66
               Mean episode length: 52.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 8.95s
                        Total time: 745.15s
                               ETA: 1432246.8s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.914s, learning 0.207s)
               Value function loss: 0.0127
                    Surrogate loss: -0.0415
             Mean action noise std: 0.80
                       Mean reward: 3.05
               Mean episode length: 59.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 9.12s
                        Total time: 754.27s
                               ETA: 1422409.6s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.565s, learning 0.183s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0383
             Mean action noise std: 0.80
                       Mean reward: 3.11
               Mean episode length: 63.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 8.75s
                        Total time: 763.02s
                               ETA: 1412245.8s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.344s, learning 0.184s)
               Value function loss: 0.0110
                    Surrogate loss: -0.0419
             Mean action noise std: 0.80
                       Mean reward: 3.20
               Mean episode length: 64.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 8.53s
                        Total time: 771.55s
                               ETA: 1402051.7s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.444s, learning 0.228s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0375
             Mean action noise std: 0.80
                       Mean reward: 3.42
               Mean episode length: 68.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 8.67s
                        Total time: 780.22s
                               ETA: 1392478.7s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.689s, learning 0.181s)
               Value function loss: 0.0901
                    Surrogate loss: -0.0357
             Mean action noise std: 0.80
                       Mean reward: 3.66
               Mean episode length: 73.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 8.87s
                        Total time: 789.09s
                               ETA: 1383588.2s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.719s, learning 0.215s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0450
             Mean action noise std: 0.80
                       Mean reward: 3.54
               Mean episode length: 71.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 8.93s
                        Total time: 798.02s
                               ETA: 1375113.8s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.687s, learning 0.271s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0474
             Mean action noise std: 0.80
                       Mean reward: 3.35
               Mean episode length: 67.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 8.96s
                        Total time: 806.98s
                               ETA: 1366968.0s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.731s, learning 0.195s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0370
             Mean action noise std: 0.80
                       Mean reward: 2.68
               Mean episode length: 52.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 8.93s
                        Total time: 815.91s
                               ETA: 1359040.5s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.538s, learning 0.194s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0377
             Mean action noise std: 0.80
                       Mean reward: 2.73
               Mean episode length: 53.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 8.73s
                        Total time: 824.64s
                               ETA: 1351054.3s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.619s, learning 0.282s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0330
             Mean action noise std: 0.80
                       Mean reward: 3.24
               Mean episode length: 63.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 8.90s
                        Total time: 833.54s
                               ETA: 1343596.7s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1768 steps/s (collection: 9.017s, learning 0.247s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0356
             Mean action noise std: 0.80
                       Mean reward: 3.40
               Mean episode length: 65.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 9.26s
                        Total time: 842.80s
                               ETA: 1336952.0s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.525s, learning 0.268s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0363
             Mean action noise std: 0.80
                       Mean reward: 3.43
               Mean episode length: 66.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 8.79s
                        Total time: 851.60s
                               ETA: 1329779.3s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.377s, learning 0.215s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0319
             Mean action noise std: 0.80
                       Mean reward: 3.41
               Mean episode length: 67.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 8.59s
                        Total time: 860.19s
                               ETA: 1322517.1s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.580s, learning 0.194s)
               Value function loss: 0.2021
                    Surrogate loss: -0.0346
             Mean action noise std: 0.80
                       Mean reward: 3.81
               Mean episode length: 73.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 4.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 8.77s
                        Total time: 868.96s
                               ETA: 1315750.7s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.568s, learning 0.268s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0549
             Mean action noise std: 0.80
                       Mean reward: 3.55
               Mean episode length: 68.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 8.84s
                        Total time: 877.80s
                               ETA: 1309279.0s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.617s, learning 0.193s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0465
             Mean action noise std: 0.80
                       Mean reward: 3.30
               Mean episode length: 62.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 8.81s
                        Total time: 886.61s
                               ETA: 1302959.8s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.380s, learning 0.179s)
               Value function loss: 0.0124
                    Surrogate loss: -0.0464
             Mean action noise std: 0.80
                       Mean reward: 3.11
               Mean episode length: 58.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 8.56s
                        Total time: 895.17s
                               ETA: 1296459.4s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.505s, learning 0.184s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0389
             Mean action noise std: 0.80
                       Mean reward: 3.27
               Mean episode length: 64.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 8.69s
                        Total time: 903.85s
                               ETA: 1290329.5s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.554s, learning 0.189s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0361
             Mean action noise std: 0.80
                       Mean reward: 3.31
               Mean episode length: 62.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 8.74s
                        Total time: 912.60s
                               ETA: 1284447.9s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.388s, learning 0.190s)
               Value function loss: 0.0203
                    Surrogate loss: -0.0399
             Mean action noise std: 0.80
                       Mean reward: 3.55
               Mean episode length: 66.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 8.58s
                        Total time: 921.17s
                               ETA: 1278500.7s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.447s, learning 0.187s)
               Value function loss: 0.0221
                    Surrogate loss: -0.0354
             Mean action noise std: 0.80
                       Mean reward: 3.64
               Mean episode length: 67.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 8.63s
                        Total time: 929.81s
                               ETA: 1272792.5s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.330s, learning 0.198s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0384
             Mean action noise std: 0.80
                       Mean reward: 3.67
               Mean episode length: 69.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 8.53s
                        Total time: 938.34s
                               ETA: 1267095.8s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.331s, learning 0.336s)
               Value function loss: 0.1776
                    Surrogate loss: -0.0456
             Mean action noise std: 0.80
                       Mean reward: 3.84
               Mean episode length: 73.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 4.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 8.67s
                        Total time: 947.00s
                               ETA: 1261736.5s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.768s, learning 0.290s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0441
             Mean action noise std: 0.80
                       Mean reward: 3.56
               Mean episode length: 66.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 9.06s
                        Total time: 956.06s
                               ETA: 1257032.7s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.612s, learning 0.171s)
               Value function loss: 0.0159
                    Surrogate loss: -0.0450
             Mean action noise std: 0.80
                       Mean reward: 3.20
               Mean episode length: 60.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 8.78s
                        Total time: 964.84s
                               ETA: 1252092.9s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.366s, learning 0.181s)
               Value function loss: 0.0145
                    Surrogate loss: -0.0419
             Mean action noise std: 0.79
                       Mean reward: 3.29
               Mean episode length: 59.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 8.55s
                        Total time: 973.39s
                               ETA: 1246977.2s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.343s, learning 0.269s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0380
             Mean action noise std: 0.80
                       Mean reward: 3.15
               Mean episode length: 59.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 8.61s
                        Total time: 982.00s
                               ETA: 1242072.9s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.638s, learning 0.279s)
               Value function loss: 0.0308
                    Surrogate loss: -0.0400
             Mean action noise std: 0.79
                       Mean reward: 3.37
               Mean episode length: 60.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 8.92s
                        Total time: 990.92s
                               ETA: 1237671.3s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1794 steps/s (collection: 8.871s, learning 0.262s)
               Value function loss: 0.0298
                    Surrogate loss: -0.0382
             Mean action noise std: 0.79
                       Mean reward: 3.46
               Mean episode length: 62.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 9.13s
                        Total time: 1000.05s
                               ETA: 1233644.8s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.395s, learning 0.276s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0395
             Mean action noise std: 0.79
                       Mean reward: 3.66
               Mean episode length: 66.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 8.67s
                        Total time: 1008.72s
                               ETA: 1229154.2s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.770s, learning 0.268s)
               Value function loss: 0.0271
                    Surrogate loss: -0.0374
             Mean action noise std: 0.79
                       Mean reward: 3.61
               Mean episode length: 65.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 9.04s
                        Total time: 1017.76s
                               ETA: 1225212.5s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.542s, learning 0.281s)
               Value function loss: 0.0312
                    Surrogate loss: -0.0355
             Mean action noise std: 0.79
                       Mean reward: 3.76
               Mean episode length: 67.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 8.82s
                        Total time: 1026.58s
                               ETA: 1221109.3s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.750s, learning 0.232s)
               Value function loss: 0.1059
                    Surrogate loss: -0.0384
             Mean action noise std: 0.79
                       Mean reward: 3.31
               Mean episode length: 60.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 5.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 8.98s
                        Total time: 1035.57s
                               ETA: 1217289.1s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.456s, learning 0.384s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0440
             Mean action noise std: 0.79
                       Mean reward: 2.99
               Mean episode length: 52.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 8.84s
                        Total time: 1044.41s
                               ETA: 1213392.4s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.611s, learning 0.173s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0396
             Mean action noise std: 0.79
                       Mean reward: 3.13
               Mean episode length: 55.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 8.78s
                        Total time: 1053.19s
                               ETA: 1209521.1s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.593s, learning 0.287s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0369
             Mean action noise std: 0.79
                       Mean reward: 3.28
               Mean episode length: 58.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 8.88s
                        Total time: 1062.07s
                               ETA: 1205847.7s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.631s, learning 0.268s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0378
             Mean action noise std: 0.79
                       Mean reward: 3.45
               Mean episode length: 59.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 8.90s
                        Total time: 1070.97s
                               ETA: 1202277.3s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.201s, learning 0.178s)
               Value function loss: 0.0402
                    Surrogate loss: -0.0379
             Mean action noise std: 0.79
                       Mean reward: 3.64
               Mean episode length: 64.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 8.38s
                        Total time: 1079.35s
                               ETA: 1198208.4s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.630s, learning 0.391s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0381
             Mean action noise std: 0.79
                       Mean reward: 3.74
               Mean episode length: 64.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 9.02s
                        Total time: 1088.37s
                               ETA: 1194933.5s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.473s, learning 0.242s)
               Value function loss: 0.0345
                    Surrogate loss: -0.0379
             Mean action noise std: 0.79
                       Mean reward: 3.78
               Mean episode length: 65.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 8.71s
                        Total time: 1097.08s
                               ETA: 1191397.0s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.399s, learning 0.409s)
               Value function loss: 0.0343
                    Surrogate loss: -0.0377
             Mean action noise std: 0.79
                       Mean reward: 3.69
               Mean episode length: 62.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 8.81s
                        Total time: 1105.89s
                               ETA: 1188036.8s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.493s, learning 0.166s)
               Value function loss: 0.1214
                    Surrogate loss: -0.0446
             Mean action noise std: 0.79
                       Mean reward: 3.92
               Mean episode length: 68.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 8.66s
                        Total time: 1114.55s
                               ETA: 1184589.6s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.402s, learning 0.212s)
               Value function loss: 0.0280
                    Surrogate loss: -0.0462
             Mean action noise std: 0.79
                       Mean reward: 3.27
               Mean episode length: 55.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 8.61s
                        Total time: 1123.16s
                               ETA: 1181167.2s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.630s, learning 0.260s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0442
             Mean action noise std: 0.79
                       Mean reward: 3.44
               Mean episode length: 57.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 8.89s
                        Total time: 1132.05s
                               ETA: 1178102.4s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.487s, learning 0.200s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0396
             Mean action noise std: 0.79
                       Mean reward: 3.37
               Mean episode length: 56.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 8.69s
                        Total time: 1140.74s
                               ETA: 1174892.4s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1802 steps/s (collection: 8.875s, learning 0.216s)
               Value function loss: 0.0451
                    Surrogate loss: -0.0415
             Mean action noise std: 0.79
                       Mean reward: 3.66
               Mean episode length: 60.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 9.09s
                        Total time: 1149.83s
                               ETA: 1172159.0s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.642s, learning 0.398s)
               Value function loss: 0.0454
                    Surrogate loss: -0.0405
             Mean action noise std: 0.79
                       Mean reward: 3.57
               Mean episode length: 61.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 9.04s
                        Total time: 1158.87s
                               ETA: 1169430.3s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.728s, learning 0.271s)
               Value function loss: 0.0436
                    Surrogate loss: -0.0378
             Mean action noise std: 0.79
                       Mean reward: 3.79
               Mean episode length: 63.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 9.00s
                        Total time: 1167.87s
                               ETA: 1166714.8s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.445s, learning 0.271s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0383
             Mean action noise std: 0.79
                       Mean reward: 3.72
               Mean episode length: 62.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 8.72s
                        Total time: 1176.59s
                               ETA: 1163773.4s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.495s, learning 0.179s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0376
             Mean action noise std: 0.79
                       Mean reward: 3.81
               Mean episode length: 64.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 8.67s
                        Total time: 1185.26s
                               ETA: 1160847.7s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.648s, learning 0.385s)
               Value function loss: 0.0950
                    Surrogate loss: -0.0463
             Mean action noise std: 0.79
                       Mean reward: 4.25
               Mean episode length: 73.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 9.03s
                        Total time: 1194.29s
                               ETA: 1158326.1s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1739 steps/s (collection: 8.976s, learning 0.443s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0452
             Mean action noise std: 0.79
                       Mean reward: 3.51
               Mean episode length: 57.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 9.42s
                        Total time: 1203.71s
                               ETA: 1156223.9s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1786 steps/s (collection: 8.820s, learning 0.353s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0427
             Mean action noise std: 0.79
                       Mean reward: 3.57
               Mean episode length: 56.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 9.17s
                        Total time: 1212.89s
                               ETA: 1153927.9s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.595s, learning 0.291s)
               Value function loss: 0.0324
                    Surrogate loss: -0.0419
             Mean action noise std: 0.79
                       Mean reward: 3.63
               Mean episode length: 59.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 8.89s
                        Total time: 1221.77s
                               ETA: 1151405.3s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.753s, learning 0.271s)
               Value function loss: 0.0413
                    Surrogate loss: -0.0392
             Mean action noise std: 0.79
                       Mean reward: 3.82
               Mean episode length: 61.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 9.02s
                        Total time: 1230.80s
                               ETA: 1149057.9s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.549s, learning 0.278s)
               Value function loss: 0.0460
                    Surrogate loss: -0.0396
             Mean action noise std: 0.79
                       Mean reward: 3.68
               Mean episode length: 60.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 8.83s
                        Total time: 1239.62s
                               ETA: 1146571.3s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.841s, learning 0.311s)
               Value function loss: 0.0416
                    Surrogate loss: -0.0412
             Mean action noise std: 0.79
                       Mean reward: 3.89
               Mean episode length: 63.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 9.15s
                        Total time: 1248.77s
                               ETA: 1144427.4s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1808 steps/s (collection: 8.751s, learning 0.307s)
               Value function loss: 0.0390
                    Surrogate loss: -0.0407
             Mean action noise std: 0.79
                       Mean reward: 3.67
               Mean episode length: 60.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 9.06s
                        Total time: 1257.83s
                               ETA: 1142237.7s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.672s, learning 0.227s)
               Value function loss: 0.0364
                    Surrogate loss: -0.0406
             Mean action noise std: 0.79
                       Mean reward: 3.57
               Mean episode length: 59.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 8.90s
                        Total time: 1266.73s
                               ETA: 1139944.3s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.458s, learning 0.272s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0422
             Mean action noise std: 0.79
                       Mean reward: 3.78
               Mean episode length: 60.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 8.73s
                        Total time: 1275.46s
                               ETA: 1137540.9s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.783s, learning 0.267s)
               Value function loss: 0.0565
                    Surrogate loss: -0.0387
             Mean action noise std: 0.79
                       Mean reward: 3.35
               Mean episode length: 52.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 9.05s
                        Total time: 1284.51s
                               ETA: 1135462.7s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.428s, learning 0.250s)
               Value function loss: 0.0383
                    Surrogate loss: -0.0396
             Mean action noise std: 0.79
                       Mean reward: 3.84
               Mean episode length: 61.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 8.68s
                        Total time: 1293.19s
                               ETA: 1133095.4s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.829s, learning 0.203s)
               Value function loss: 0.0323
                    Surrogate loss: -0.0443
             Mean action noise std: 0.79
                       Mean reward: 3.65
               Mean episode length: 57.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 9.03s
                        Total time: 1302.22s
                               ETA: 1131075.7s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.335s, learning 0.355s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0398
             Mean action noise std: 0.79
                       Mean reward: 3.83
               Mean episode length: 61.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 8.69s
                        Total time: 1310.91s
                               ETA: 1128796.4s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.747s, learning 0.305s)
               Value function loss: 0.0466
                    Surrogate loss: -0.0387
             Mean action noise std: 0.79
                       Mean reward: 3.79
               Mean episode length: 59.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 9.05s
                        Total time: 1319.96s
                               ETA: 1126865.8s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.651s, learning 0.259s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0384
             Mean action noise std: 0.79
                       Mean reward: 3.82
               Mean episode length: 60.27
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 8.91s
                        Total time: 1328.87s
                               ETA: 1124846.5s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.658s, learning 0.358s)
               Value function loss: 0.0438
                    Surrogate loss: -0.0408
             Mean action noise std: 0.79
                       Mean reward: 3.90
               Mean episode length: 61.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 9.02s
                        Total time: 1337.89s
                               ETA: 1122950.7s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1766 steps/s (collection: 8.930s, learning 0.346s)
               Value function loss: 0.0441
                    Surrogate loss: -0.0390
             Mean action noise std: 0.79
                       Mean reward: 3.95
               Mean episode length: 61.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 9.28s
                        Total time: 1347.17s
                               ETA: 1121302.2s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.628s, learning 0.223s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0391
             Mean action noise std: 0.79
                       Mean reward: 3.85
               Mean episode length: 58.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 8.85s
                        Total time: 1356.02s
                               ETA: 1119330.9s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.500s, learning 0.193s)
               Value function loss: 0.0578
                    Surrogate loss: -0.0379
             Mean action noise std: 0.79
                       Mean reward: 4.18
               Mean episode length: 62.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 8.69s
                        Total time: 1364.71s
                               ETA: 1117261.9s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.709s, learning 0.197s)
               Value function loss: 0.0422
                    Surrogate loss: -0.0403
             Mean action noise std: 0.79
                       Mean reward: 3.77
               Mean episode length: 57.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 8.91s
                        Total time: 1373.62s
                               ETA: 1115398.7s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.424s, learning 0.184s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0381
             Mean action noise std: 0.79
                       Mean reward: 3.83
               Mean episode length: 56.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 8.61s
                        Total time: 1382.22s
                               ETA: 1113326.3s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.103s, learning 0.276s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0393
             Mean action noise std: 0.79
                       Mean reward: 3.63
               Mean episode length: 54.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 8.38s
                        Total time: 1390.60s
                               ETA: 1111103.8s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.370s, learning 0.327s)
               Value function loss: 0.0453
                    Surrogate loss: -0.0396
             Mean action noise std: 0.79
                       Mean reward: 3.88
               Mean episode length: 58.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 8.70s
                        Total time: 1399.30s
                               ETA: 1109168.3s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1811 steps/s (collection: 8.772s, learning 0.274s)
               Value function loss: 0.0563
                    Surrogate loss: -0.0438
             Mean action noise std: 0.79
                       Mean reward: 3.96
               Mean episode length: 57.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 9.05s
                        Total time: 1408.35s
                               ETA: 1107537.3s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.549s, learning 0.273s)
               Value function loss: 0.0512
                    Surrogate loss: -0.0436
             Mean action noise std: 0.79
                       Mean reward: 3.81
               Mean episode length: 56.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 8.82s
                        Total time: 1417.17s
                               ETA: 1105757.0s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.473s, learning 0.211s)
               Value function loss: 0.0473
                    Surrogate loss: -0.0475
             Mean action noise std: 0.79
                       Mean reward: 3.73
               Mean episode length: 51.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 8.68s
                        Total time: 1425.85s
                               ETA: 1103896.8s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.541s, learning 0.214s)
               Value function loss: 0.0470
                    Surrogate loss: -0.0433
             Mean action noise std: 0.79
                       Mean reward: 3.69
               Mean episode length: 52.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 8.76s
                        Total time: 1434.61s
                               ETA: 1102120.5s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.559s, learning 0.304s)
               Value function loss: 0.0505
                    Surrogate loss: -0.0404
             Mean action noise std: 0.79
                       Mean reward: 3.71
               Mean episode length: 52.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 8.86s
                        Total time: 1443.47s
                               ETA: 1100453.3s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.451s, learning 0.241s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0424
             Mean action noise std: 0.78
                       Mean reward: 3.72
               Mean episode length: 50.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 8.69s
                        Total time: 1452.16s
                               ETA: 1098681.6s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.700s, learning 0.274s)
               Value function loss: 0.0482
                    Surrogate loss: -0.0433
             Mean action noise std: 0.78
                       Mean reward: 3.73
               Mean episode length: 50.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 8.97s
                        Total time: 1461.14s
                               ETA: 1097148.6s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.630s, learning 0.203s)
               Value function loss: 0.0448
                    Surrogate loss: -0.0427
             Mean action noise std: 0.78
                       Mean reward: 3.79
               Mean episode length: 50.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 8.83s
                        Total time: 1469.97s
                               ETA: 1095532.9s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.621s, learning 0.180s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0437
             Mean action noise std: 0.78
                       Mean reward: 3.89
               Mean episode length: 51.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 8.80s
                        Total time: 1478.77s
                               ETA: 1093917.0s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.430s, learning 0.180s)
               Value function loss: 0.0503
                    Surrogate loss: -0.0451
             Mean action noise std: 0.78
                       Mean reward: 3.94
               Mean episode length: 52.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 8.61s
                        Total time: 1487.38s
                               ETA: 1092184.5s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.375s, learning 0.229s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0431
             Mean action noise std: 0.78
                       Mean reward: 3.75
               Mean episode length: 49.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 8.60s
                        Total time: 1495.98s
                               ETA: 1090473.2s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.775s, learning 0.183s)
               Value function loss: 0.0536
                    Surrogate loss: -0.0433
             Mean action noise std: 0.78
                       Mean reward: 3.97
               Mean episode length: 51.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 8.96s
                        Total time: 1504.94s
                               ETA: 1089042.6s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.851s, learning 0.181s)
               Value function loss: 0.0494
                    Surrogate loss: -0.0409
             Mean action noise std: 0.78
                       Mean reward: 4.10
               Mean episode length: 51.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 9.03s
                        Total time: 1513.97s
                               ETA: 1087685.7s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.563s, learning 0.306s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0400
             Mean action noise std: 0.78
                       Mean reward: 4.07
               Mean episode length: 51.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 8.87s
                        Total time: 1522.84s
                               ETA: 1086231.7s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.538s, learning 0.281s)
               Value function loss: 0.0533
                    Surrogate loss: -0.0422
             Mean action noise std: 0.78
                       Mean reward: 3.98
               Mean episode length: 51.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 8.82s
                        Total time: 1531.66s
                               ETA: 1084762.8s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.662s, learning 0.187s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0409
             Mean action noise std: 0.78
                       Mean reward: 3.93
               Mean episode length: 49.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 8.85s
                        Total time: 1540.51s
                               ETA: 1083336.0s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.506s, learning 0.259s)
               Value function loss: 0.0587
                    Surrogate loss: -0.0422
             Mean action noise std: 0.78
                       Mean reward: 4.18
               Mean episode length: 53.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 8.77s
                        Total time: 1549.27s
                               ETA: 1081870.2s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.470s, learning 0.185s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0426
             Mean action noise std: 0.78
                       Mean reward: 4.28
               Mean episode length: 53.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 8.65s
                        Total time: 1557.93s
                               ETA: 1080347.8s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.921s, learning 0.199s)
               Value function loss: 0.0710
                    Surrogate loss: -0.0387
             Mean action noise std: 0.78
                       Mean reward: 4.45
               Mean episode length: 56.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 9.12s
                        Total time: 1567.05s
                               ETA: 1079166.8s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.551s, learning 0.218s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0372
             Mean action noise std: 0.78
                       Mean reward: 4.27
               Mean episode length: 54.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 8.77s
                        Total time: 1575.82s
                               ETA: 1077761.7s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.533s, learning 0.239s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0417
             Mean action noise std: 0.78
                       Mean reward: 4.53
               Mean episode length: 57.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 8.77s
                        Total time: 1584.59s
                               ETA: 1076377.5s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.555s, learning 0.199s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0387
             Mean action noise std: 0.78
                       Mean reward: 4.37
               Mean episode length: 56.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 8.75s
                        Total time: 1593.34s
                               ETA: 1075000.2s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.710s, learning 0.215s)
               Value function loss: 0.0737
                    Surrogate loss: -0.0400
             Mean action noise std: 0.78
                       Mean reward: 4.36
               Mean episode length: 54.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 8.93s
                        Total time: 1602.27s
                               ETA: 1073755.8s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.491s, learning 0.209s)
               Value function loss: 0.0806
                    Surrogate loss: -0.0393
             Mean action noise std: 0.78
                       Mean reward: 4.28
               Mean episode length: 54.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 8.70s
                        Total time: 1610.97s
                               ETA: 1072378.2s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.338s, learning 0.185s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0423
             Mean action noise std: 0.78
                       Mean reward: 4.37
               Mean episode length: 55.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 8.52s
                        Total time: 1619.49s
                               ETA: 1070901.5s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.532s, learning 0.186s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0427
             Mean action noise std: 0.78
                       Mean reward: 4.49
               Mean episode length: 54.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 8.72s
                        Total time: 1628.21s
                               ETA: 1069572.3s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.346s, learning 0.271s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0397
             Mean action noise std: 0.78
                       Mean reward: 4.47
               Mean episode length: 55.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 8.62s
                        Total time: 1636.82s
                               ETA: 1068194.1s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.563s, learning 0.181s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0406
             Mean action noise std: 0.78
                       Mean reward: 4.42
               Mean episode length: 54.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 8.74s
                        Total time: 1645.57s
                               ETA: 1066916.6s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.627s, learning 0.246s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0399
             Mean action noise std: 0.78
                       Mean reward: 4.37
               Mean episode length: 54.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 8.87s
                        Total time: 1654.44s
                               ETA: 1065737.9s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.664s, learning 0.201s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0359
             Mean action noise std: 0.78
                       Mean reward: 4.44
               Mean episode length: 55.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 8.87s
                        Total time: 1663.31s
                               ETA: 1064569.6s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.268s, learning 0.223s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0385
             Mean action noise std: 0.78
                       Mean reward: 4.56
               Mean episode length: 56.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 8.49s
                        Total time: 1671.80s
                               ETA: 1063178.2s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.548s, learning 0.178s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0401
             Mean action noise std: 0.78
                       Mean reward: 4.54
               Mean episode length: 57.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 8.73s
                        Total time: 1680.52s
                               ETA: 1061952.4s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.592s, learning 0.184s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0389
             Mean action noise std: 0.78
                       Mean reward: 4.37
               Mean episode length: 54.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 8.78s
                        Total time: 1689.30s
                               ETA: 1060773.6s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1793 steps/s (collection: 8.933s, learning 0.201s)
               Value function loss: 0.0839
                    Surrogate loss: -0.0354
             Mean action noise std: 0.78
                       Mean reward: 4.57
               Mean episode length: 55.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 9.13s
                        Total time: 1698.43s
                               ETA: 1059832.4s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.557s, learning 0.326s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0370
             Mean action noise std: 0.78
                       Mean reward: 4.43
               Mean episode length: 53.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 8.88s
                        Total time: 1707.31s
                               ETA: 1058747.2s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1754 steps/s (collection: 8.936s, learning 0.401s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0421
             Mean action noise std: 0.78
                       Mean reward: 4.56
               Mean episode length: 56.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 9.34s
                        Total time: 1716.65s
                               ETA: 1057955.7s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.570s, learning 0.254s)
               Value function loss: 0.0934
                    Surrogate loss: -0.0356
             Mean action noise std: 0.78
                       Mean reward: 4.52
               Mean episode length: 54.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 8.82s
                        Total time: 1725.48s
                               ETA: 1056859.7s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.409s, learning 0.207s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0370
             Mean action noise std: 0.78
                       Mean reward: 4.51
               Mean episode length: 54.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 8.62s
                        Total time: 1734.09s
                               ETA: 1055650.3s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.483s, learning 0.182s)
               Value function loss: 0.0855
                    Surrogate loss: -0.0357
             Mean action noise std: 0.78
                       Mean reward: 4.77
               Mean episode length: 55.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 8.66s
                        Total time: 1742.76s
                               ETA: 1054484.5s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.094s, learning 0.283s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0397
             Mean action noise std: 0.78
                       Mean reward: 4.46
               Mean episode length: 53.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 8.38s
                        Total time: 1751.13s
                               ETA: 1053159.6s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.542s, learning 0.189s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0372
             Mean action noise std: 0.78
                       Mean reward: 4.70
               Mean episode length: 53.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 8.73s
                        Total time: 1759.87s
                               ETA: 1052062.7s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.406s, learning 0.227s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0409
             Mean action noise std: 0.78
                       Mean reward: 4.54
               Mean episode length: 54.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 8.63s
                        Total time: 1768.50s
                               ETA: 1050920.4s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1768 steps/s (collection: 8.882s, learning 0.381s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0369
             Mean action noise std: 0.78
                       Mean reward: 4.38
               Mean episode length: 51.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 9.26s
                        Total time: 1777.76s
                               ETA: 1050163.5s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.436s, learning 0.189s)
               Value function loss: 0.0739
                    Surrogate loss: -0.0378
             Mean action noise std: 0.78
                       Mean reward: 4.81
               Mean episode length: 55.29
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 8.62s
                        Total time: 1786.39s
                               ETA: 1049040.5s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.330s, learning 0.208s)
               Value function loss: 0.0743
                    Surrogate loss: -0.0380
             Mean action noise std: 0.78
                       Mean reward: 4.80
               Mean episode length: 57.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 8.54s
                        Total time: 1794.93s
                               ETA: 1047880.1s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.225s, learning 0.292s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0413
             Mean action noise std: 0.78
                       Mean reward: 4.51
               Mean episode length: 51.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 8.52s
                        Total time: 1803.44s
                               ETA: 1046720.2s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1800 steps/s (collection: 8.789s, learning 0.311s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0378
             Mean action noise std: 0.78
                       Mean reward: 4.68
               Mean episode length: 53.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 9.10s
                        Total time: 1812.54s
                               ETA: 1045910.4s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.789s, learning 0.196s)
               Value function loss: 0.0681
                    Surrogate loss: -0.0389
             Mean action noise std: 0.78
                       Mean reward: 4.53
               Mean episode length: 52.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 8.99s
                        Total time: 1821.53s
                               ETA: 1045044.0s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.545s, learning 0.253s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0381
             Mean action noise std: 0.78
                       Mean reward: 4.59
               Mean episode length: 54.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 8.80s
                        Total time: 1830.33s
                               ETA: 1044080.9s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1834 steps/s (collection: 8.749s, learning 0.182s)
               Value function loss: 0.0625
                    Surrogate loss: -0.0400
             Mean action noise std: 0.78
                       Mean reward: 4.57
               Mean episode length: 53.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 8.93s
                        Total time: 1839.26s
                               ETA: 1043204.0s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.343s, learning 0.241s)
               Value function loss: 0.0751
                    Surrogate loss: -0.0382
             Mean action noise std: 0.78
                       Mean reward: 4.94
               Mean episode length: 57.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 8.58s
                        Total time: 1847.84s
                               ETA: 1042141.5s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.396s, learning 0.200s)
               Value function loss: 17.6503
                    Surrogate loss: -0.0009
             Mean action noise std: 0.78
                       Mean reward: 4.44
               Mean episode length: 51.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 8.60s
                        Total time: 1856.44s
                               ETA: 1041097.0s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.513s, learning 0.179s)
               Value function loss: 18.0437
                    Surrogate loss: -0.0026
             Mean action noise std: 0.78
                       Mean reward: 4.86
               Mean episode length: 54.71
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 8.69s
                        Total time: 1865.13s
                               ETA: 1040117.3s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.249s, learning 0.180s)
               Value function loss: 16.5602
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 4.93
               Mean episode length: 56.81
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 8.43s
                        Total time: 1873.56s
                               ETA: 1039002.7s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.372s, learning 0.187s)
               Value function loss: 0.2548
                    Surrogate loss: -0.0363
             Mean action noise std: 0.78
                       Mean reward: 4.69
               Mean episode length: 53.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 8.56s
                        Total time: 1882.12s
                               ETA: 1037972.2s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.444s, learning 0.347s)
               Value function loss: 0.1316
                    Surrogate loss: -0.0427
             Mean action noise std: 0.78
                       Mean reward: 4.68
               Mean episode length: 55.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 8.79s
                        Total time: 1890.91s
                               ETA: 1037080.4s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.569s, learning 0.181s)
               Value function loss: 0.1076
                    Surrogate loss: -0.0408
             Mean action noise std: 0.78
                       Mean reward: 4.75
               Mean episode length: 55.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 8.75s
                        Total time: 1899.66s
                               ETA: 1036175.6s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.353s, learning 0.229s)
               Value function loss: 0.0959
                    Surrogate loss: -0.0395
             Mean action noise std: 0.78
                       Mean reward: 4.51
               Mean episode length: 51.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 8.58s
                        Total time: 1908.24s
                               ETA: 1035189.5s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.265s, learning 0.175s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0389
             Mean action noise std: 0.77
                       Mean reward: 4.45
               Mean episode length: 49.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 8.44s
                        Total time: 1916.68s
                               ETA: 1034137.3s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.235s, learning 0.323s)
               Value function loss: 0.0726
                    Surrogate loss: -0.0423
             Mean action noise std: 0.77
                       Mean reward: 4.60
               Mean episode length: 53.41
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 8.56s
                        Total time: 1925.24s
                               ETA: 1033159.7s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.556s, learning 0.205s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0418
             Mean action noise std: 0.77
                       Mean reward: 4.37
               Mean episode length: 49.15
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 8.76s
                        Total time: 1934.00s
                               ETA: 1032300.8s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1774 steps/s (collection: 9.024s, learning 0.209s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0383
             Mean action noise std: 0.77
                       Mean reward: 4.06
               Mean episode length: 45.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 9.23s
                        Total time: 1943.23s
                               ETA: 1031701.7s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.173s)
               Value function loss: 0.0642
                    Surrogate loss: -0.0424
             Mean action noise std: 0.77
                       Mean reward: 4.40
               Mean episode length: 48.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 8.82s
                        Total time: 1952.06s
                               ETA: 1030893.0s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.470s, learning 0.172s)
               Value function loss: 0.0678
                    Surrogate loss: -0.0383
             Mean action noise std: 0.77
                       Mean reward: 4.61
               Mean episode length: 51.24
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 8.64s
                        Total time: 1960.70s
                               ETA: 1029996.7s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.446s, learning 0.170s)
               Value function loss: 0.0679
                    Surrogate loss: -0.0372
             Mean action noise std: 0.77
                       Mean reward: 4.51
               Mean episode length: 50.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 8.62s
                        Total time: 1969.32s
                               ETA: 1029096.4s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.378s, learning 0.194s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0395
             Mean action noise std: 0.77
                       Mean reward: 4.49
               Mean episode length: 47.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 8.57s
                        Total time: 1977.89s
                               ETA: 1028182.3s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.368s, learning 0.287s)
               Value function loss: 0.0759
                    Surrogate loss: -0.0414
             Mean action noise std: 0.77
                       Mean reward: 4.55
               Mean episode length: 50.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 8.65s
                        Total time: 1986.54s
                               ETA: 1027320.1s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.658s, learning 0.173s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0404
             Mean action noise std: 0.77
                       Mean reward: 4.60
               Mean episode length: 50.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 8.83s
                        Total time: 1995.37s
                               ETA: 1026558.0s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.516s, learning 0.199s)
               Value function loss: 0.0656
                    Surrogate loss: -0.0387
             Mean action noise std: 0.77
                       Mean reward: 4.38
               Mean episode length: 48.58
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 8.72s
                        Total time: 2004.09s
                               ETA: 1025744.2s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.574s, learning 0.208s)
               Value function loss: 70.9980
                    Surrogate loss: -0.0007
             Mean action noise std: 0.77
                       Mean reward: 4.41
               Mean episode length: 48.35
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 8.78s
                        Total time: 2012.87s
                               ETA: 1024972.5s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.717s, learning 0.175s)
               Value function loss: 0.1415
                    Surrogate loss: -0.0355
             Mean action noise std: 0.77
                       Mean reward: 4.36
               Mean episode length: 50.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 8.89s
                        Total time: 2021.76s
                               ETA: 1024264.0s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.502s, learning 0.247s)
               Value function loss: 34.4024
                    Surrogate loss: -0.0017
             Mean action noise std: 0.77
                       Mean reward: 4.46
               Mean episode length: 49.12
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 8.75s
                        Total time: 2030.51s
                               ETA: 1023490.3s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.417s, learning 0.205s)
               Value function loss: 0.1556
                    Surrogate loss: -0.0353
             Mean action noise std: 0.77
                       Mean reward: 4.54
               Mean episode length: 49.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 8.62s
                        Total time: 2039.13s
                               ETA: 1022660.6s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.596s, learning 0.208s)
               Value function loss: 0.1027
                    Surrogate loss: -0.0413
             Mean action noise std: 0.77
                       Mean reward: 7.36
               Mean episode length: 49.33
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 8.80s
                        Total time: 2047.94s
                               ETA: 1021930.5s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.415s, learning 0.196s)
               Value function loss: 64.3064
                    Surrogate loss: -0.0012
             Mean action noise std: 0.77
                       Mean reward: 4.61
               Mean episode length: 49.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 8.61s
                        Total time: 2056.55s
                               ETA: 1021111.7s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.382s, learning 0.179s)
               Value function loss: 495.7886
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 4.41
               Mean episode length: 47.27
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 8.56s
                        Total time: 2065.11s
                               ETA: 1020276.5s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.494s, learning 0.187s)
               Value function loss: 5.2431
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 4.41
               Mean episode length: 46.03
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 8.68s
                        Total time: 2073.79s
                               ETA: 1019508.2s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.638s, learning 0.189s)
               Value function loss: 0.4496
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 4.42
               Mean episode length: 46.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 8.83s
                        Total time: 2082.62s
                               ETA: 1018818.9s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.377s, learning 0.292s)
               Value function loss: 0.2151
                    Surrogate loss: -0.0262
             Mean action noise std: 0.77
                       Mean reward: 4.52
               Mean episode length: 46.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 8.67s
                        Total time: 2091.29s
                               ETA: 1018059.2s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1793 steps/s (collection: 8.850s, learning 0.286s)
               Value function loss: 0.1612
                    Surrogate loss: -0.0387
             Mean action noise std: 0.77
                       Mean reward: 4.39
               Mean episode length: 46.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 9.14s
                        Total time: 2100.42s
                               ETA: 1017533.0s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1796 steps/s (collection: 8.785s, learning 0.333s)
               Value function loss: 0.1451
                    Surrogate loss: -0.0298
             Mean action noise std: 0.77
                       Mean reward: 4.57
               Mean episode length: 46.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 9.12s
                        Total time: 2109.54s
                               ETA: 1017002.9s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.593s, learning 0.265s)
               Value function loss: 0.1238
                    Surrogate loss: -0.0368
             Mean action noise std: 0.77
                       Mean reward: 4.38
               Mean episode length: 45.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 8.86s
                        Total time: 2118.40s
                               ETA: 1016353.3s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.290s, learning 0.190s)
               Value function loss: 3.9820
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 4.36
               Mean episode length: 46.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 8.48s
                        Total time: 2126.88s
                               ETA: 1015529.3s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.483s, learning 0.178s)
               Value function loss: 0.1184
                    Surrogate loss: -0.0393
             Mean action noise std: 0.77
                       Mean reward: 4.32
               Mean episode length: 47.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 8.66s
                        Total time: 2135.54s
                               ETA: 1014799.2s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.253s, learning 0.212s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 4.53
               Mean episode length: 47.58
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 8.47s
                        Total time: 2144.01s
                               ETA: 1013983.1s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.567s, learning 0.261s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0351
             Mean action noise std: 0.77
                       Mean reward: 4.67
               Mean episode length: 48.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 8.83s
                        Total time: 2152.83s
                               ETA: 1013345.2s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.175s, learning 0.179s)
               Value function loss: 0.0859
                    Surrogate loss: -0.0410
             Mean action noise std: 0.77
                       Mean reward: 4.57
               Mean episode length: 48.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 8.35s
                        Total time: 2161.19s
                               ETA: 1012491.3s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.565s, learning 0.185s)
               Value function loss: 17.8201
                    Surrogate loss: 0.0004
             Mean action noise std: 0.77
                       Mean reward: 4.57
               Mean episode length: 46.83
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 8.75s
                        Total time: 2169.94s
                               ETA: 1011829.9s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.640s, learning 0.177s)
               Value function loss: 0.0973
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 4.22
               Mean episode length: 44.40
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 8.82s
                        Total time: 2178.76s
                               ETA: 1011205.9s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.173s, learning 0.175s)
               Value function loss: 132.7295
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 4.38
               Mean episode length: 46.07
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 8.35s
                        Total time: 2187.10s
                               ETA: 1010370.7s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.337s, learning 0.348s)
               Value function loss: 47.0377
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 4.49
               Mean episode length: 47.40
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 8.69s
                        Total time: 2195.79s
                               ETA: 1009698.5s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.539s, learning 0.187s)
               Value function loss: 0.1944
                    Surrogate loss: -0.0420
             Mean action noise std: 0.77
                       Mean reward: 4.54
               Mean episode length: 48.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 8.73s
                        Total time: 2204.52s
                               ETA: 1009051.1s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.594s, learning 0.198s)
               Value function loss: 131.9989
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 5.12
               Mean episode length: 51.74
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 8.79s
                        Total time: 2213.31s
                               ETA: 1008439.2s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.449s, learning 0.191s)
               Value function loss: 0.2008
                    Surrogate loss: -0.0338
             Mean action noise std: 0.77
                       Mean reward: 4.79
               Mean episode length: 50.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 8.64s
                        Total time: 2221.95s
                               ETA: 1007764.3s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.378s, learning 0.181s)
               Value function loss: 214.7897
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 4.86
               Mean episode length: 51.55
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 8.56s
                        Total time: 2230.51s
                               ETA: 1007058.4s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.692s, learning 0.203s)
               Value function loss: 18.3183
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: 4.75
               Mean episode length: 51.92
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 8.89s
                        Total time: 2239.40s
                               ETA: 1006509.9s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.334s, learning 0.217s)
               Value function loss: 17.8703
                    Surrogate loss: 0.0007
             Mean action noise std: 0.77
                       Mean reward: 5.12
               Mean episode length: 56.65
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 8.55s
                        Total time: 2247.95s
                               ETA: 1005812.5s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.749s, learning 0.224s)
               Value function loss: 47.0618
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 4.89
               Mean episode length: 53.14
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 8.97s
                        Total time: 2256.93s
                               ETA: 1005309.3s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.520s, learning 0.215s)
               Value function loss: 738.4787
                    Surrogate loss: -0.0038
             Mean action noise std: 0.77
                       Mean reward: 4.88
               Mean episode length: 54.41
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 8.74s
                        Total time: 2265.66s
                               ETA: 1004704.7s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.813s, learning 0.215s)
               Value function loss: 95.3548
                    Surrogate loss: -0.0017
             Mean action noise std: 0.77
                       Mean reward: 4.71
               Mean episode length: 58.02
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 9.03s
                        Total time: 2274.69s
                               ETA: 1004235.0s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.768s, learning 0.193s)
               Value function loss: 0.5796
                    Surrogate loss: -0.0302
             Mean action noise std: 0.77
                       Mean reward: 4.86
               Mean episode length: 56.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 8.96s
                        Total time: 2283.65s
                               ETA: 1003739.8s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.433s, learning 0.215s)
               Value function loss: 0.2351
                    Surrogate loss: -0.0259
             Mean action noise std: 0.77
                       Mean reward: 4.94
               Mean episode length: 56.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 8.65s
                        Total time: 2292.30s
                               ETA: 1003111.7s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.298s, learning 0.200s)
               Value function loss: 362.3032
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 4.71
               Mean episode length: 53.70
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 8.50s
                        Total time: 2300.80s
                               ETA: 1002423.6s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.465s, learning 0.199s)
               Value function loss: 0.2705
                    Surrogate loss: -0.0270
             Mean action noise std: 0.77
                       Mean reward: 4.84
               Mean episode length: 59.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 8.66s
                        Total time: 2309.46s
                               ETA: 1001813.6s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.349s, learning 0.168s)
               Value function loss: 0.2324
                    Surrogate loss: -0.0278
             Mean action noise std: 0.77
                       Mean reward: 4.64
               Mean episode length: 57.99
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 8.52s
                        Total time: 2317.98s
                               ETA: 1001145.0s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1801 steps/s (collection: 8.738s, learning 0.356s)
               Value function loss: 0.2205
                    Surrogate loss: -0.0308
             Mean action noise std: 0.77
                       Mean reward: 4.78
               Mean episode length: 60.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 9.09s
                        Total time: 2327.07s
                               ETA: 1000730.3s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.556s, learning 0.190s)
               Value function loss: 0.1657
                    Surrogate loss: -0.0316
             Mean action noise std: 0.77
                       Mean reward: 4.79
               Mean episode length: 57.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 8.75s
                        Total time: 2335.82s
                               ETA: 1000170.0s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.388s, learning 0.223s)
               Value function loss: 0.1346
                    Surrogate loss: -0.0352
             Mean action noise std: 0.77
                       Mean reward: 4.67
               Mean episode length: 55.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 8.61s
                        Total time: 2344.43s
                               ETA: 999557.4s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.638s, learning 0.282s)
               Value function loss: 0.1224
                    Surrogate loss: -0.0325
             Mean action noise std: 0.77
                       Mean reward: 4.65
               Mean episode length: 57.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 8.92s
                        Total time: 2353.35s
                               ETA: 999080.8s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1767 steps/s (collection: 8.998s, learning 0.273s)
               Value function loss: 0.1108
                    Surrogate loss: -0.0375
             Mean action noise std: 0.77
                       Mean reward: 4.78
               Mean episode length: 56.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 9.27s
                        Total time: 2362.62s
                               ETA: 998756.6s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.611s, learning 0.216s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0348
             Mean action noise std: 0.77
                       Mean reward: 5.03
               Mean episode length: 58.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 8.83s
                        Total time: 2371.45s
                               ETA: 998248.4s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.365s, learning 0.269s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0365
             Mean action noise std: 0.77
                       Mean reward: 4.95
               Mean episode length: 58.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 8.63s
                        Total time: 2380.08s
                               ETA: 997663.0s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.338s, learning 0.180s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0365
             Mean action noise std: 0.77
                       Mean reward: 4.90
               Mean episode length: 55.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 8.52s
                        Total time: 2388.60s
                               ETA: 997034.4s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.517s, learning 0.178s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0369
             Mean action noise std: 0.77
                       Mean reward: 4.90
               Mean episode length: 56.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 8.70s
                        Total time: 2397.29s
                               ETA: 996484.5s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.531s, learning 0.280s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0366
             Mean action noise std: 0.77
                       Mean reward: 4.66
               Mean episode length: 56.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 8.81s
                        Total time: 2406.10s
                               ETA: 995987.3s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.634s, learning 0.187s)
               Value function loss: 0.0875
                    Surrogate loss: -0.0355
             Mean action noise std: 0.77
                       Mean reward: 4.96
               Mean episode length: 60.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 8.82s
                        Total time: 2414.92s
                               ETA: 995497.8s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1672 steps/s (collection: 9.581s, learning 0.218s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0361
             Mean action noise std: 0.77
                       Mean reward: 4.84
               Mean episode length: 58.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 9.80s
                        Total time: 2424.72s
                               ETA: 995413.8s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.832s, learning 0.195s)
               Value function loss: 7.2180
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 5.08
               Mean episode length: 60.78
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 17.03s
                        Total time: 2441.75s
                               ETA: 998285.6s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.524s, learning 0.223s)
               Value function loss: 0.0917
                    Surrogate loss: -0.0415
             Mean action noise std: 0.77
                       Mean reward: 4.73
               Mean episode length: 58.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 16.75s
                        Total time: 2458.50s
                               ETA: 1001019.9s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.317s, learning 0.209s)
               Value function loss: 0.0894
                    Surrogate loss: -0.0383
             Mean action noise std: 0.77
                       Mean reward: 4.74
               Mean episode length: 57.96
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 16.53s
                        Total time: 2475.02s
                               ETA: 1003642.2s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.439s, learning 0.190s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0386
             Mean action noise std: 0.77
                       Mean reward: 7.23
               Mean episode length: 59.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 16.63s
                        Total time: 2491.65s
                               ETA: 1006284.7s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.620s, learning 0.185s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0396
             Mean action noise std: 0.77
                       Mean reward: 4.72
               Mean episode length: 55.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 16.81s
                        Total time: 2508.46s
                               ETA: 1008976.6s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.122s, learning 0.200s)
               Value function loss: 0.0793
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 5.12
               Mean episode length: 56.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 16.32s
                        Total time: 2524.78s
                               ETA: 1011452.9s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.751s, learning 0.316s)
               Value function loss: 0.0891
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 4.96
               Mean episode length: 57.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 17.07s
                        Total time: 2541.85s
                               ETA: 1014207.0s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.450s, learning 0.191s)
               Value function loss: 0.0869
                    Surrogate loss: -0.0406
             Mean action noise std: 0.77
                       Mean reward: 4.93
               Mean episode length: 57.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 16.64s
                        Total time: 2558.49s
                               ETA: 1016769.5s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.922s, learning 0.184s)
               Value function loss: 0.0989
                    Surrogate loss: -0.0363
             Mean action noise std: 0.77
                       Mean reward: 4.98
               Mean episode length: 58.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 17.11s
                        Total time: 2575.59s
                               ETA: 1019495.3s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 940 steps/s (collection: 17.080s, learning 0.332s)
               Value function loss: 0.0895
                    Surrogate loss: -0.0374
             Mean action noise std: 0.77
                       Mean reward: 5.05
               Mean episode length: 56.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 17.41s
                        Total time: 2593.00s
                               ETA: 1022320.4s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1012 steps/s (collection: 15.967s, learning 0.216s)
               Value function loss: 161.3701
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: 4.87
               Mean episode length: 56.49
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 16.18s
                        Total time: 2609.19s
                               ETA: 1024640.4s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.707s, learning 0.311s)
               Value function loss: 360.8914
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 22.59
               Mean episode length: 57.01
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 17.02s
                        Total time: 2626.21s
                               ETA: 1027268.9s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.867s, learning 0.193s)
               Value function loss: 0.7163
                    Surrogate loss: -0.0441
             Mean action noise std: 0.77
                       Mean reward: 4.78
               Mean episode length: 52.88
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 17.06s
                        Total time: 2643.27s
                               ETA: 1029892.9s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.502s, learning 0.198s)
               Value function loss: 0.1629
                    Surrogate loss: -0.0433
             Mean action noise std: 0.77
                       Mean reward: 4.79
               Mean episode length: 53.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 16.70s
                        Total time: 2659.97s
                               ETA: 1032356.8s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.468s, learning 0.175s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0406
             Mean action noise std: 0.77
                       Mean reward: 5.04
               Mean episode length: 54.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 16.64s
                        Total time: 2676.61s
                               ETA: 1034779.3s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.620s, learning 0.209s)
               Value function loss: 0.1152
                    Surrogate loss: -0.0342
             Mean action noise std: 0.77
                       Mean reward: 5.05
               Mean episode length: 54.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 16.83s
                        Total time: 2693.44s
                               ETA: 1037254.6s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.628s, learning 0.207s)
               Value function loss: 0.1042
                    Surrogate loss: -0.0384
             Mean action noise std: 0.77
                       Mean reward: 4.86
               Mean episode length: 53.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 16.83s
                        Total time: 2710.27s
                               ETA: 1039712.9s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.644s, learning 0.272s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0382
             Mean action noise std: 0.77
                       Mean reward: 4.61
               Mean episode length: 53.15
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 16.92s
                        Total time: 2727.19s
                               ETA: 1042183.3s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 966 steps/s (collection: 16.746s, learning 0.206s)
               Value function loss: 0.1115
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 4.97
               Mean episode length: 55.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 16.95s
                        Total time: 2744.14s
                               ETA: 1044648.3s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.671s, learning 0.189s)
               Value function loss: 0.1099
                    Surrogate loss: -0.0370
             Mean action noise std: 0.77
                       Mean reward: 5.32
               Mean episode length: 56.02
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 16.86s
                        Total time: 2761.00s
                               ETA: 1047059.8s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.714s, learning 0.182s)
               Value function loss: 107.5344
                    Surrogate loss: -0.0012
             Mean action noise std: 0.77
                       Mean reward: 5.07
               Mean episode length: 56.44
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 16.90s
                        Total time: 2777.90s
                               ETA: 1049466.3s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.710s, learning 0.183s)
               Value function loss: 471.8258
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 4.85
               Mean episode length: 55.49
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 16.89s
                        Total time: 2794.79s
                               ETA: 1051853.4s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.758s, learning 0.171s)
               Value function loss: 131.4438
                    Surrogate loss: -0.0025
             Mean action noise std: 0.77
                       Mean reward: 4.98
               Mean episode length: 53.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 16.93s
                        Total time: 2811.72s
                               ETA: 1054236.0s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.558s, learning 0.180s)
               Value function loss: 99.6152
                    Surrogate loss: -0.0025
             Mean action noise std: 0.77
                       Mean reward: 40.14
               Mean episode length: 54.06
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 16.74s
                        Total time: 2828.46s
                               ETA: 1056529.3s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.819s, learning 0.195s)
               Value function loss: 65.7127
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: 4.94
               Mean episode length: 54.78
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 17.01s
                        Total time: 2845.47s
                               ETA: 1058908.2s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.520s, learning 0.177s)
               Value function loss: 0.3535
                    Surrogate loss: -0.0348
             Mean action noise std: 0.77
                       Mean reward: 4.78
               Mean episode length: 55.78
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 16.70s
                        Total time: 2862.17s
                               ETA: 1061151.7s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.622s, learning 0.179s)
               Value function loss: 0.2140
                    Surrogate loss: -0.0324
             Mean action noise std: 0.77
                       Mean reward: 4.96
               Mean episode length: 55.56
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 16.80s
                        Total time: 2878.97s
                               ETA: 1063416.5s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 956 steps/s (collection: 16.899s, learning 0.229s)
               Value function loss: 0.1594
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 4.67
               Mean episode length: 55.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 17.13s
                        Total time: 2896.10s
                               ETA: 1065785.4s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.612s, learning 0.185s)
               Value function loss: 0.1391
                    Surrogate loss: -0.0364
             Mean action noise std: 0.77
                       Mean reward: 5.42
               Mean episode length: 57.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 16.80s
                        Total time: 2912.89s
                               ETA: 1068014.8s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.122s, learning 0.201s)
               Value function loss: 0.1260
                    Surrogate loss: -0.0382
             Mean action noise std: 0.77
                       Mean reward: 4.97
               Mean episode length: 54.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 16.32s
                        Total time: 2929.22s
                               ETA: 1070055.0s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.696s, learning 0.180s)
               Value function loss: 107.6460
                    Surrogate loss: -0.0006
             Mean action noise std: 0.77
                       Mean reward: 4.94
               Mean episode length: 54.81
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 16.88s
                        Total time: 2946.09s
                               ETA: 1072281.4s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.328s, learning 0.185s)
               Value function loss: 0.1391
                    Surrogate loss: -0.0321
             Mean action noise std: 0.77
                       Mean reward: 5.04
               Mean episode length: 54.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 16.51s
                        Total time: 2962.61s
                               ETA: 1074360.0s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.367s, learning 0.180s)
               Value function loss: 0.1193
                    Surrogate loss: -0.0395
             Mean action noise std: 0.77
                       Mean reward: 4.97
               Mean episode length: 53.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 16.55s
                        Total time: 2979.15s
                               ETA: 1076435.4s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.393s, learning 0.178s)
               Value function loss: 132.7244
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: 4.95
               Mean episode length: 56.07
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 16.57s
                        Total time: 2995.72s
                               ETA: 1078504.2s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.390s, learning 0.278s)
               Value function loss: 0.1620
                    Surrogate loss: -0.0241
             Mean action noise std: 0.77
                       Mean reward: 12.60
               Mean episode length: 54.67
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 16.67s
                        Total time: 3012.39s
                               ETA: 1080592.9s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.537s, learning 0.206s)
               Value function loss: 0.1716
                    Surrogate loss: -0.0264
             Mean action noise std: 0.77
                       Mean reward: 4.84
               Mean episode length: 55.97
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 16.74s
                        Total time: 3029.14s
                               ETA: 1082693.6s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.827s, learning 0.194s)
               Value function loss: 0.1505
                    Surrogate loss: -0.0330
             Mean action noise std: 0.77
                       Mean reward: 4.93
               Mean episode length: 57.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 17.02s
                        Total time: 3046.16s
                               ETA: 1084878.0s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1284 steps/s (collection: 12.570s, learning 0.186s)
               Value function loss: 0.1155
                    Surrogate loss: -0.0395
             Mean action noise std: 0.77
                       Mean reward: 4.93
               Mean episode length: 56.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 12.76s
                        Total time: 3058.91s
                               ETA: 1085533.1s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.281s, learning 0.208s)
               Value function loss: 91.2165
                    Surrogate loss: -0.0010
             Mean action noise std: 0.77
                       Mean reward: 4.80
               Mean episode length: 55.44
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 8.49s
                        Total time: 3067.40s
                               ETA: 1084674.6s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.552s, learning 0.202s)
               Value function loss: 0.1587
                    Surrogate loss: -0.0307
             Mean action noise std: 0.77
                       Mean reward: 4.60
               Mean episode length: 56.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 8.75s
                        Total time: 3076.16s
                               ETA: 1083915.4s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.405s, learning 0.302s)
               Value function loss: 15.6013
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: 4.79
               Mean episode length: 54.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 8.71s
                        Total time: 3084.86s
                               ETA: 1083145.2s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.393s, learning 0.184s)
               Value function loss: 0.1250
                    Surrogate loss: -0.0343
             Mean action noise std: 0.77
                       Mean reward: 4.97
               Mean episode length: 52.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 8.58s
                        Total time: 3093.44s
                               ETA: 1082334.7s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 2003 steps/s (collection: 7.962s, learning 0.217s)
               Value function loss: 15.5526
                    Surrogate loss: 0.0005
             Mean action noise std: 0.77
                       Mean reward: 4.63
               Mean episode length: 52.69
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 8.18s
                        Total time: 3101.62s
                               ETA: 1081391.3s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.256s, learning 0.205s)
               Value function loss: 0.1297
                    Surrogate loss: -0.0344
             Mean action noise std: 0.77
                       Mean reward: 4.71
               Mean episode length: 56.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 8.46s
                        Total time: 3110.08s
                               ETA: 1080552.5s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.218s, learning 0.205s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0433
             Mean action noise std: 0.77
                       Mean reward: 7.79
               Mean episode length: 55.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 8.42s
                        Total time: 3118.50s
                               ETA: 1079706.1s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.345s, learning 0.188s)
               Value function loss: 0.1102
                    Surrogate loss: -0.0387
             Mean action noise std: 0.77
                       Mean reward: 4.86
               Mean episode length: 61.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 8.53s
                        Total time: 3127.04s
                               ETA: 1078903.4s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.427s, learning 0.188s)
               Value function loss: 0.0941
                    Surrogate loss: -0.0376
             Mean action noise std: 0.77
                       Mean reward: 4.79
               Mean episode length: 58.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 8.61s
                        Total time: 3135.65s
                               ETA: 1078134.3s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.440s, learning 0.192s)
               Value function loss: 64.6641
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 5.04
               Mean episode length: 56.54
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 8.63s
                        Total time: 3144.28s
                               ETA: 1077376.3s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.393s, learning 0.193s)
               Value function loss: 94.3663
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 4.84
               Mean episode length: 57.42
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 8.59s
                        Total time: 3152.87s
                               ETA: 1076607.9s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.275s, learning 0.175s)
               Value function loss: 0.1843
                    Surrogate loss: -0.0420
             Mean action noise std: 0.77
                       Mean reward: 4.67
               Mean episode length: 56.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 8.45s
                        Total time: 3161.32s
                               ETA: 1075798.3s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.554s, learning 0.187s)
               Value function loss: 0.1233
                    Surrogate loss: -0.0401
             Mean action noise std: 0.77
                       Mean reward: 20.25
               Mean episode length: 62.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 8.74s
                        Total time: 3170.06s
                               ETA: 1075092.6s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.371s, learning 0.186s)
               Value function loss: 0.1109
                    Surrogate loss: -0.0372
             Mean action noise std: 0.77
                       Mean reward: 4.79
               Mean episode length: 61.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 8.56s
                        Total time: 3178.62s
                               ETA: 1074329.6s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.476s, learning 0.177s)
               Value function loss: 64.2681
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 4.73
               Mean episode length: 58.42
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 8.65s
                        Total time: 3187.27s
                               ETA: 1073603.9s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.694s, learning 0.215s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0332
             Mean action noise std: 0.77
                       Mean reward: 4.85
               Mean episode length: 59.67
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 8.91s
                        Total time: 3196.18s
                               ETA: 1072969.0s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.570s, learning 0.202s)
               Value function loss: 0.1140
                    Surrogate loss: -0.0386
             Mean action noise std: 0.77
                       Mean reward: 4.88
               Mean episode length: 62.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 8.77s
                        Total time: 3204.95s
                               ETA: 1072292.8s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.259s, learning 0.187s)
               Value function loss: 94.2069
                    Surrogate loss: -0.0004
             Mean action noise std: 0.77
                       Mean reward: 4.59
               Mean episode length: 59.10
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 8.45s
                        Total time: 3213.40s
                               ETA: 1071511.9s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.385s, learning 0.167s)
               Value function loss: 11.9186
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 4.96
               Mean episode length: 60.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 8.55s
                        Total time: 3221.95s
                               ETA: 1070771.6s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.627s, learning 0.211s)
               Value function loss: 0.1290
                    Surrogate loss: -0.0376
             Mean action noise std: 0.77
                       Mean reward: 4.68
               Mean episode length: 60.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 8.84s
                        Total time: 3230.79s
                               ETA: 1070130.8s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1795 steps/s (collection: 8.782s, learning 0.346s)
               Value function loss: 0.1231
                    Surrogate loss: -0.0326
             Mean action noise std: 0.77
                       Mean reward: 4.90
               Mean episode length: 59.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 9.13s
                        Total time: 3239.91s
                               ETA: 1069589.8s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.622s, learning 0.196s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0350
             Mean action noise std: 0.77
                       Mean reward: 5.07
               Mean episode length: 60.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 8.82s
                        Total time: 3248.73s
                               ETA: 1068950.3s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.568s, learning 0.184s)
               Value function loss: 0.0989
                    Surrogate loss: -0.0382
             Mean action noise std: 0.77
                       Mean reward: 4.74
               Mean episode length: 60.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 8.75s
                        Total time: 3257.48s
                               ETA: 1068293.6s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.757s, learning 0.193s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0379
             Mean action noise std: 0.77
                       Mean reward: 4.81
               Mean episode length: 60.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 8.95s
                        Total time: 3266.43s
                               ETA: 1067706.1s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.368s, learning 0.312s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0383
             Mean action noise std: 0.77
                       Mean reward: 4.67
               Mean episode length: 57.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 8.68s
                        Total time: 3275.11s
                               ETA: 1067034.2s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.732s, learning 0.271s)
               Value function loss: 0.0941
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 4.64
               Mean episode length: 60.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 9.00s
                        Total time: 3284.12s
                               ETA: 1066471.6s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.320s, learning 0.269s)
               Value function loss: 0.0883
                    Surrogate loss: -0.0373
             Mean action noise std: 0.77
                       Mean reward: 4.91
               Mean episode length: 64.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 8.59s
                        Total time: 3292.71s
                               ETA: 1065778.4s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.354s, learning 0.175s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0368
             Mean action noise std: 0.76
                       Mean reward: 4.77
               Mean episode length: 57.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 8.53s
                        Total time: 3301.24s
                               ETA: 1065070.3s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.694s, learning 0.180s)
               Value function loss: 0.0967
                    Surrogate loss: -0.0395
             Mean action noise std: 0.77
                       Mean reward: 4.77
               Mean episode length: 60.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 8.87s
                        Total time: 3310.11s
                               ETA: 1064477.8s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.248s, learning 0.264s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 4.86
               Mean episode length: 59.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 8.51s
                        Total time: 3318.62s
                               ETA: 1063773.0s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.648s, learning 0.214s)
               Value function loss: 0.0828
                    Surrogate loss: -0.0405
             Mean action noise std: 0.76
                       Mean reward: 4.95
               Mean episode length: 58.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 8.86s
                        Total time: 3327.48s
                               ETA: 1063184.6s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.639s, learning 0.182s)
               Value function loss: 0.0809
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 5.16
               Mean episode length: 61.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 8.82s
                        Total time: 3336.31s
                               ETA: 1062586.8s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.687s, learning 0.181s)
               Value function loss: 0.0823
                    Surrogate loss: -0.0386
             Mean action noise std: 0.76
                       Mean reward: 4.89
               Mean episode length: 59.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 8.87s
                        Total time: 3345.17s
                               ETA: 1062007.5s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.614s, learning 0.401s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0372
             Mean action noise std: 0.76
                       Mean reward: 4.84
               Mean episode length: 59.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 9.01s
                        Total time: 3354.19s
                               ETA: 1061478.3s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.283s, learning 0.193s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0403
             Mean action noise std: 0.76
                       Mean reward: 4.75
               Mean episode length: 60.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 8.48s
                        Total time: 3362.67s
                               ETA: 1060782.6s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.375s, learning 0.326s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0397
             Mean action noise std: 0.76
                       Mean reward: 4.80
               Mean episode length: 60.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 8.70s
                        Total time: 3371.37s
                               ETA: 1060161.9s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.589s, learning 0.194s)
               Value function loss: 317.7215
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 4.99
               Mean episode length: 59.50
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 8.78s
                        Total time: 3380.15s
                               ETA: 1059570.4s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.548s, learning 0.251s)
               Value function loss: 94.0102
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: 5.08
               Mean episode length: 61.44
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 8.80s
                        Total time: 3388.95s
                               ETA: 1058987.9s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.374s, learning 0.187s)
               Value function loss: 39.5389
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 5.02
               Mean episode length: 60.35
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 8.56s
                        Total time: 3397.51s
                               ETA: 1058334.7s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.591s, learning 0.183s)
               Value function loss: 0.2398
                    Surrogate loss: -0.0460
             Mean action noise std: 0.76
                       Mean reward: 5.01
               Mean episode length: 61.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 8.77s
                        Total time: 3406.28s
                               ETA: 1057751.7s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.433s, learning 0.198s)
               Value function loss: 15.5705
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 5.24
               Mean episode length: 58.43
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 8.63s
                        Total time: 3414.91s
                               ETA: 1057128.2s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.604s, learning 0.187s)
               Value function loss: 90.3135
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 5.15
               Mean episode length: 59.27
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 8.79s
                        Total time: 3423.71s
                               ETA: 1056557.7s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.638s, learning 0.211s)
               Value function loss: 192.2829
                    Surrogate loss: -0.0028
             Mean action noise std: 0.76
                       Mean reward: 5.49
               Mean episode length: 61.50
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 8.85s
                        Total time: 3432.55s
                               ETA: 1056008.3s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1792 steps/s (collection: 8.779s, learning 0.364s)
               Value function loss: 40.2458
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 5.47
               Mean episode length: 61.39
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 9.14s
                        Total time: 3441.70s
                               ETA: 1055552.4s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.482s, learning 0.184s)
               Value function loss: 107.5331
                    Surrogate loss: 0.0001
             Mean action noise std: 0.76
                       Mean reward: 5.17
               Mean episode length: 60.08
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 8.67s
                        Total time: 3450.36s
                               ETA: 1054953.6s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.125s, learning 0.217s)
               Value function loss: 18.2381
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 5.37
               Mean episode length: 62.95
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 8.34s
                        Total time: 3458.70s
                               ETA: 1054259.8s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.305s, learning 0.178s)
               Value function loss: 107.8300
                    Surrogate loss: 0.0013
             Mean action noise std: 0.76
                       Mean reward: 10.23
               Mean episode length: 60.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 8.48s
                        Total time: 3467.19s
                               ETA: 1053612.9s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.166s, learning 0.239s)
               Value function loss: 335.6933
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 5.33
               Mean episode length: 61.61
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 8.41s
                        Total time: 3475.59s
                               ETA: 1052946.5s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.348s, learning 0.191s)
               Value function loss: 138.2156
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 5.21
               Mean episode length: 57.87
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 8.54s
                        Total time: 3484.13s
                               ETA: 1052324.4s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.624s, learning 0.206s)
               Value function loss: 1.7659
                    Surrogate loss: -0.0212
             Mean action noise std: 0.76
                       Mean reward: 5.16
               Mean episode length: 59.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 8.83s
                        Total time: 3492.96s
                               ETA: 1051793.4s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.141s, learning 0.206s)
               Value function loss: 0.5188
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 7.75
               Mean episode length: 60.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 8.35s
                        Total time: 3501.31s
                               ETA: 1051120.8s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.393s, learning 0.313s)
               Value function loss: 179.1738
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 5.07
               Mean episode length: 59.58
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 8.71s
                        Total time: 3510.02s
                               ETA: 1050559.3s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.710s, learning 0.177s)
               Value function loss: 114.8656
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 4.87
               Mean episode length: 60.85
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 8.89s
                        Total time: 3518.90s
                               ETA: 1050055.3s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.520s, learning 0.314s)
               Value function loss: 284.8358
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 17.71
               Mean episode length: 61.42
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 8.83s
                        Total time: 3527.74s
                               ETA: 1049538.5s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.533s, learning 0.181s)
               Value function loss: 2.3681
                    Surrogate loss: -0.0283
             Mean action noise std: 0.76
                       Mean reward: 22.90
               Mean episode length: 59.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 8.71s
                        Total time: 3536.45s
                               ETA: 1048989.1s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.254s, learning 0.180s)
               Value function loss: 65.0315
                    Surrogate loss: 0.0022
             Mean action noise std: 0.76
                       Mean reward: 5.21
               Mean episode length: 60.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 8.43s
                        Total time: 3544.89s
                               ETA: 1048360.3s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.389s, learning 0.276s)
               Value function loss: 18.4046
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 5.35
               Mean episode length: 61.92
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 8.67s
                        Total time: 3553.55s
                               ETA: 1047803.4s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.218s, learning 0.199s)
               Value function loss: 0.5206
                    Surrogate loss: -0.0332
             Mean action noise std: 0.76
                       Mean reward: 5.06
               Mean episode length: 60.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 8.42s
                        Total time: 3561.97s
                               ETA: 1047176.4s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.712s, learning 0.175s)
               Value function loss: 0.2528
                    Surrogate loss: -0.0352
             Mean action noise std: 0.76
                       Mean reward: 5.06
               Mean episode length: 59.45
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 8.89s
                        Total time: 3570.85s
                               ETA: 1046691.0s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.579s, learning 0.321s)
               Value function loss: 0.2148
                    Surrogate loss: -0.0353
             Mean action noise std: 0.76
                       Mean reward: 5.33
               Mean episode length: 61.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 8.90s
                        Total time: 3579.75s
                               ETA: 1046212.2s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1787 steps/s (collection: 8.886s, learning 0.280s)
               Value function loss: 0.1616
                    Surrogate loss: -0.0344
             Mean action noise std: 0.76
                       Mean reward: 5.40
               Mean episode length: 61.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 9.17s
                        Total time: 3588.92s
                               ETA: 1045813.6s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.830s, learning 0.175s)
               Value function loss: 0.1476
                    Surrogate loss: -0.0327
             Mean action noise std: 0.76
                       Mean reward: 5.37
               Mean episode length: 61.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 9.01s
                        Total time: 3597.93s
                               ETA: 1045370.5s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.486s, learning 0.337s)
               Value function loss: 0.1297
                    Surrogate loss: -0.0382
             Mean action noise std: 0.76
                       Mean reward: 5.28
               Mean episode length: 60.84
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 8.82s
                        Total time: 3606.75s
                               ETA: 1044877.3s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.507s, learning 0.165s)
               Value function loss: 0.1147
                    Surrogate loss: -0.0411
             Mean action noise std: 0.76
                       Mean reward: 5.45
               Mean episode length: 61.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 8.67s
                        Total time: 3615.42s
                               ETA: 1044343.3s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.682s, learning 0.196s)
               Value function loss: 164.0990
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 5.14
               Mean episode length: 59.17
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 8.88s
                        Total time: 3624.30s
                               ETA: 1043871.7s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.287s, learning 0.172s)
               Value function loss: 0.1827
                    Surrogate loss: -0.0365
             Mean action noise std: 0.76
                       Mean reward: 4.92
               Mean episode length: 59.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 8.46s
                        Total time: 3632.76s
                               ETA: 1043282.3s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.582s, learning 0.168s)
               Value function loss: 0.1432
                    Surrogate loss: -0.0387
             Mean action noise std: 0.76
                       Mean reward: 5.22
               Mean episode length: 59.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 8.75s
                        Total time: 3641.51s
                               ETA: 1042779.4s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.248s, learning 0.286s)
               Value function loss: 18.0214
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 5.07
               Mean episode length: 57.81
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 8.53s
                        Total time: 3650.04s
                               ETA: 1042217.8s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.505s, learning 0.209s)
               Value function loss: 46.9775
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 5.19
               Mean episode length: 61.32
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 8.71s
                        Total time: 3658.76s
                               ETA: 1041710.7s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.430s, learning 0.360s)
               Value function loss: 0.1808
                    Surrogate loss: -0.0357
             Mean action noise std: 0.76
                       Mean reward: 5.22
               Mean episode length: 59.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 8.79s
                        Total time: 3667.55s
                               ETA: 1041227.8s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.028s, learning 0.300s)
               Value function loss: 4.1035
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 5.14
               Mean episode length: 58.24
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 8.33s
                        Total time: 3675.87s
                               ETA: 1040616.9s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.050s, learning 0.214s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0397
             Mean action noise std: 0.76
                       Mean reward: 4.96
               Mean episode length: 58.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 8.26s
                        Total time: 3684.14s
                               ETA: 1039991.5s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1998 steps/s (collection: 7.996s, learning 0.203s)
               Value function loss: 0.1170
                    Surrogate loss: -0.0426
             Mean action noise std: 0.76
                       Mean reward: 5.14
               Mean episode length: 59.97
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 8.20s
                        Total time: 3692.34s
                               ETA: 1039351.4s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.338s, learning 0.192s)
               Value function loss: 0.1189
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 4.90
               Mean episode length: 59.11
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 8.53s
                        Total time: 3700.87s
                               ETA: 1038807.4s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.696s, learning 0.207s)
               Value function loss: 0.1041
                    Surrogate loss: -0.0377
             Mean action noise std: 0.76
                       Mean reward: 5.08
               Mean episode length: 55.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 8.90s
                        Total time: 3709.77s
                               ETA: 1038371.0s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.435s, learning 0.221s)
               Value function loss: 0.1042
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 5.37
               Mean episode length: 57.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 8.66s
                        Total time: 3718.43s
                               ETA: 1037868.0s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.629s, learning 0.285s)
               Value function loss: 0.1003
                    Surrogate loss: -0.0357
             Mean action noise std: 0.76
                       Mean reward: 5.14
               Mean episode length: 59.57
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 8.91s
                        Total time: 3727.34s
                               ETA: 1037439.8s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.497s, learning 0.200s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0364
             Mean action noise std: 0.76
                       Mean reward: 5.37
               Mean episode length: 58.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 8.70s
                        Total time: 3736.04s
                               ETA: 1036953.4s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1806 steps/s (collection: 8.845s, learning 0.226s)
               Value function loss: 64.5290
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 5.23
               Mean episode length: 62.04
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 9.07s
                        Total time: 3745.11s
                               ETA: 1036573.1s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.366s, learning 0.375s)
               Value function loss: 4.0144
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 5.00
               Mean episode length: 61.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 8.74s
                        Total time: 3753.85s
                               ETA: 1036103.9s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.371s, learning 0.221s)
               Value function loss: 121.3390
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 5.24
               Mean episode length: 62.12
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 8.59s
                        Total time: 3762.44s
                               ETA: 1035596.3s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.676s, learning 0.200s)
               Value function loss: 0.3458
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 5.05
               Mean episode length: 60.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 8.88s
                        Total time: 3771.32s
                               ETA: 1035169.3s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.400s, learning 0.185s)
               Value function loss: 0.1617
                    Surrogate loss: -0.0420
             Mean action noise std: 0.76
                       Mean reward: 12.81
               Mean episode length: 59.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 8.59s
                        Total time: 3779.90s
                               ETA: 1034665.2s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.653s, learning 0.338s)
               Value function loss: 0.1353
                    Surrogate loss: -0.0357
             Mean action noise std: 0.76
                       Mean reward: 5.37
               Mean episode length: 62.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 8.99s
                        Total time: 3788.89s
                               ETA: 1034274.4s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.555s, learning 0.223s)
               Value function loss: 0.1061
                    Surrogate loss: -0.0425
             Mean action noise std: 0.76
                       Mean reward: 4.93
               Mean episode length: 57.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 8.78s
                        Total time: 3797.67s
                               ETA: 1033827.8s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.419s, learning 0.201s)
               Value function loss: 259.1403
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 5.19
               Mean episode length: 61.27
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 8.62s
                        Total time: 3806.29s
                               ETA: 1033340.6s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.317s, learning 0.369s)
               Value function loss: 7.8004
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 15.05
               Mean episode length: 58.32
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 8.69s
                        Total time: 3814.98s
                               ETA: 1032873.9s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.343s, learning 0.177s)
               Value function loss: 0.3033
                    Surrogate loss: -0.0315
             Mean action noise std: 0.76
                       Mean reward: 4.73
               Mean episode length: 57.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 8.52s
                        Total time: 3823.50s
                               ETA: 1032364.9s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.720s, learning 0.354s)
               Value function loss: 47.0037
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 5.10
               Mean episode length: 62.09
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 9.07s
                        Total time: 3832.57s
                               ETA: 1032007.7s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.372s, learning 0.194s)
               Value function loss: 39.4919
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 7.68
               Mean episode length: 60.66
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 8.57s
                        Total time: 3841.14s
                               ETA: 1031516.2s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.615s, learning 0.218s)
               Value function loss: 24.7461
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 5.17
               Mean episode length: 62.68
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 8.83s
                        Total time: 3849.97s
                               ETA: 1031098.6s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.209s, learning 0.206s)
               Value function loss: 39.1942
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 12.57
               Mean episode length: 60.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 8.41s
                        Total time: 3858.38s
                               ETA: 1030571.4s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.576s, learning 0.192s)
               Value function loss: 272.7694
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 10.40
               Mean episode length: 60.26
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 8.77s
                        Total time: 3867.15s
                               ETA: 1030141.3s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.452s, learning 0.174s)
               Value function loss: 58.2644
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 4.91
               Mean episode length: 60.46
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 8.63s
                        Total time: 3875.78s
                               ETA: 1029675.7s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.729s, learning 0.208s)
               Value function loss: 0.8527
                    Surrogate loss: -0.0335
             Mean action noise std: 0.76
                       Mean reward: 4.81
               Mean episode length: 60.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 8.94s
                        Total time: 3884.72s
                               ETA: 1029294.9s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.508s, learning 0.187s)
               Value function loss: 0.3175
                    Surrogate loss: -0.0346
             Mean action noise std: 0.76
                       Mean reward: 5.05
               Mean episode length: 59.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 8.69s
                        Total time: 3893.41s
                               ETA: 1028851.9s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.407s, learning 0.228s)
               Value function loss: 0.2181
                    Surrogate loss: -0.0327
             Mean action noise std: 0.76
                       Mean reward: 12.17
               Mean episode length: 58.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 8.64s
                        Total time: 3902.05s
                               ETA: 1028395.6s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.622s, learning 0.172s)
               Value function loss: 71.0146
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 4.98
               Mean episode length: 60.92
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 8.79s
                        Total time: 3910.84s
                               ETA: 1027983.5s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.190s, learning 0.232s)
               Value function loss: 0.1799
                    Surrogate loss: -0.0282
             Mean action noise std: 0.76
                       Mean reward: 4.74
               Mean episode length: 60.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 8.42s
                        Total time: 3919.26s
                               ETA: 1027475.8s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.197s, learning 0.195s)
               Value function loss: 0.1851
                    Surrogate loss: -0.0301
             Mean action noise std: 0.76
                       Mean reward: 5.13
               Mean episode length: 64.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 8.39s
                        Total time: 3927.65s
                               ETA: 1026963.0s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.628s, learning 0.221s)
               Value function loss: 0.1509
                    Surrogate loss: -0.0334
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 59.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 8.85s
                        Total time: 3936.50s
                               ETA: 1026572.1s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.378s, learning 0.208s)
               Value function loss: 0.1455
                    Surrogate loss: -0.0299
             Mean action noise std: 0.76
                       Mean reward: 4.72
               Mean episode length: 59.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 8.59s
                        Total time: 3945.09s
                               ETA: 1026114.6s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.853s, learning 0.178s)
               Value function loss: 0.1266
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 4.76
               Mean episode length: 62.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 9.03s
                        Total time: 3954.12s
                               ETA: 1025775.0s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.512s, learning 0.168s)
               Value function loss: 0.1108
                    Surrogate loss: -0.0359
             Mean action noise std: 0.76
                       Mean reward: 4.73
               Mean episode length: 62.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 8.68s
                        Total time: 3962.80s
                               ETA: 1025346.3s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.672s, learning 0.208s)
               Value function loss: 165.6209
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 4.82
               Mean episode length: 64.23
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 8.88s
                        Total time: 3971.68s
                               ETA: 1024971.4s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.667s, learning 0.191s)
               Value function loss: 50.9698
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 4.86
               Mean episode length: 61.61
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 8.86s
                        Total time: 3980.54s
                               ETA: 1024592.6s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.491s, learning 0.181s)
               Value function loss: 0.2191
                    Surrogate loss: -0.0451
             Mean action noise std: 0.76
                       Mean reward: 4.75
               Mean episode length: 60.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 8.67s
                        Total time: 3989.21s
                               ETA: 1024168.0s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.469s, learning 0.338s)
               Value function loss: 15.5605
                    Surrogate loss: 0.0001
             Mean action noise std: 0.76
                       Mean reward: 4.84
               Mean episode length: 62.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 8.81s
                        Total time: 3998.02s
                               ETA: 1023780.3s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.081s, learning 0.241s)
               Value function loss: 0.1578
                    Surrogate loss: -0.0341
             Mean action noise std: 0.76
                       Mean reward: 4.78
               Mean episode length: 61.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 8.32s
                        Total time: 4006.34s
                               ETA: 1023270.4s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.727s, learning 0.208s)
               Value function loss: 0.1314
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 4.87
               Mean episode length: 62.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 8.94s
                        Total time: 4015.27s
                               ETA: 1022919.4s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.374s, learning 0.171s)
               Value function loss: 0.1079
                    Surrogate loss: -0.0405
             Mean action noise std: 0.76
                       Mean reward: 4.69
               Mean episode length: 62.15
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 8.55s
                        Total time: 4023.82s
                               ETA: 1022471.0s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.498s, learning 0.187s)
               Value function loss: 0.1356
                    Surrogate loss: -0.0270
             Mean action noise std: 0.76
                       Mean reward: 4.78
               Mean episode length: 61.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 8.69s
                        Total time: 4032.50s
                               ETA: 1022060.3s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.897s, learning 0.182s)
               Value function loss: 43.6574
                    Surrogate loss: -0.0011
             Mean action noise std: 0.76
                       Mean reward: 4.87
               Mean episode length: 66.47
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 9.08s
                        Total time: 4041.58s
                               ETA: 1021751.5s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.862s, learning 0.192s)
               Value function loss: 0.1349
                    Surrogate loss: -0.0348
             Mean action noise std: 0.76
                       Mean reward: 4.74
               Mean episode length: 63.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 9.05s
                        Total time: 4050.64s
                               ETA: 1021437.8s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.334s, learning 0.210s)
               Value function loss: 0.1049
                    Surrogate loss: -0.0337
             Mean action noise std: 0.76
                       Mean reward: 4.77
               Mean episode length: 62.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 8.54s
                        Total time: 4059.18s
                               ETA: 1020997.2s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.246s, learning 0.361s)
               Value function loss: 0.0842
                    Surrogate loss: -0.0354
             Mean action noise std: 0.76
                       Mean reward: 4.49
               Mean episode length: 61.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 8.61s
                        Total time: 4067.79s
                               ETA: 1020574.6s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.341s, learning 0.201s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0417
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 62.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 8.54s
                        Total time: 4076.33s
                               ETA: 1020138.0s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.567s, learning 0.188s)
               Value function loss: 0.0895
                    Surrogate loss: -0.0397
             Mean action noise std: 0.76
                       Mean reward: 9.81
               Mean episode length: 66.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 8.76s
                        Total time: 4085.09s
                               ETA: 1019756.6s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.345s, learning 0.181s)
               Value function loss: 17.7841
                    Surrogate loss: -0.0001
             Mean action noise std: 0.76
                       Mean reward: 4.78
               Mean episode length: 65.21
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 8.53s
                        Total time: 4093.61s
                               ETA: 1019319.9s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.314s, learning 0.179s)
               Value function loss: 47.0955
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 64.55
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 8.49s
                        Total time: 4102.11s
                               ETA: 1018877.3s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.422s, learning 0.185s)
               Value function loss: 0.1178
                    Surrogate loss: -0.0478
             Mean action noise std: 0.76
                       Mean reward: 4.79
               Mean episode length: 64.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 8.61s
                        Total time: 4110.71s
                               ETA: 1018465.1s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.446s, learning 0.279s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0396
             Mean action noise std: 0.76
                       Mean reward: 4.66
               Mean episode length: 65.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 8.73s
                        Total time: 4119.44s
                               ETA: 1018084.1s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.407s, learning 0.175s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0366
             Mean action noise std: 0.76
                       Mean reward: 4.42
               Mean episode length: 62.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 8.58s
                        Total time: 4128.02s
                               ETA: 1017669.7s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.706s, learning 0.327s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0347
             Mean action noise std: 0.76
                       Mean reward: 4.68
               Mean episode length: 64.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 9.03s
                        Total time: 4137.05s
                               ETA: 1017368.1s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.449s, learning 0.188s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0382
             Mean action noise std: 0.76
                       Mean reward: 4.52
               Mean episode length: 61.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 8.64s
                        Total time: 4145.69s
                               ETA: 1016970.6s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.570s, learning 0.307s)
               Value function loss: 107.5157
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 4.50
               Mean episode length: 62.57
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 8.88s
                        Total time: 4154.57s
                               ETA: 1016633.8s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.440s, learning 0.200s)
               Value function loss: 17.4697
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 4.44
               Mean episode length: 64.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 8.64s
                        Total time: 4163.21s
                               ETA: 1016241.0s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.425s, learning 0.167s)
               Value function loss: 3.9856
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 4.81
               Mean episode length: 62.91
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 8.59s
                        Total time: 4171.80s
                               ETA: 1015838.2s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.383s, learning 0.182s)
               Value function loss: 0.1873
                    Surrogate loss: -0.0376
             Mean action noise std: 0.76
                       Mean reward: 4.34
               Mean episode length: 64.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 8.56s
                        Total time: 4180.36s
                               ETA: 1015430.7s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.599s, learning 0.233s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0406
             Mean action noise std: 0.76
                       Mean reward: 12.37
               Mean episode length: 65.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 8.83s
                        Total time: 4189.20s
                               ETA: 1015089.9s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.391s, learning 0.212s)
               Value function loss: 0.0974
                    Surrogate loss: -0.0384
             Mean action noise std: 0.76
                       Mean reward: 9.76
               Mean episode length: 61.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 8.60s
                        Total time: 4197.80s
                               ETA: 1014695.5s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.610s, learning 0.204s)
               Value function loss: 215.3308
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 4.36
               Mean episode length: 62.81
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 8.81s
                        Total time: 4206.61s
                               ETA: 1014353.8s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.501s, learning 0.202s)
               Value function loss: 18.0905
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 4.73
               Mean episode length: 64.10
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 8.70s
                        Total time: 4215.32s
                               ETA: 1013987.0s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.483s, learning 0.199s)
               Value function loss: 0.1527
                    Surrogate loss: -0.0363
             Mean action noise std: 0.76
                       Mean reward: 4.55
               Mean episode length: 62.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 8.68s
                        Total time: 4224.00s
                               ETA: 1013617.0s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.674s, learning 0.178s)
               Value function loss: 0.1330
                    Surrogate loss: -0.0369
             Mean action noise std: 0.76
                       Mean reward: 4.86
               Mean episode length: 64.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 8.85s
                        Total time: 4232.85s
                               ETA: 1013289.2s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.615s, learning 0.187s)
               Value function loss: 0.1021
                    Surrogate loss: -0.0407
             Mean action noise std: 0.76
                       Mean reward: 4.57
               Mean episode length: 61.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 8.80s
                        Total time: 4241.65s
                               ETA: 1012951.3s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.499s, learning 0.207s)
               Value function loss: 0.1018
                    Surrogate loss: -0.0343
             Mean action noise std: 0.76
                       Mean reward: 4.49
               Mean episode length: 61.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 8.71s
                        Total time: 4250.36s
                               ETA: 1012591.9s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.533s, learning 0.176s)
               Value function loss: 0.0942
                    Surrogate loss: -0.0442
             Mean action noise std: 0.76
                       Mean reward: 4.65
               Mean episode length: 64.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 8.71s
                        Total time: 4259.07s
                               ETA: 1012234.8s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.354s, learning 0.211s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0427
             Mean action noise std: 0.76
                       Mean reward: 4.81
               Mean episode length: 63.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 8.57s
                        Total time: 4267.63s
                               ETA: 1011845.4s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.266s, learning 0.219s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0421
             Mean action noise std: 0.76
                       Mean reward: 5.09
               Mean episode length: 62.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 8.49s
                        Total time: 4276.12s
                               ETA: 1011438.8s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.419s, learning 0.176s)
               Value function loss: 0.0871
                    Surrogate loss: -0.0417
             Mean action noise std: 0.76
                       Mean reward: 4.65
               Mean episode length: 62.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 8.60s
                        Total time: 4284.71s
                               ETA: 1011060.1s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.426s, learning 0.312s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0400
             Mean action noise std: 0.76
                       Mean reward: 4.36
               Mean episode length: 64.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 8.74s
                        Total time: 4293.45s
                               ETA: 1010716.6s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.386s, learning 0.187s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0407
             Mean action noise std: 0.76
                       Mean reward: 4.96
               Mean episode length: 62.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 8.57s
                        Total time: 4302.02s
                               ETA: 1010336.1s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.965s, learning 0.197s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0409
             Mean action noise std: 0.76
                       Mean reward: 4.52
               Mean episode length: 63.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 8.16s
                        Total time: 4310.18s
                               ETA: 1009861.0s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.687s, learning 0.199s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0418
             Mean action noise std: 0.76
                       Mean reward: 4.48
               Mean episode length: 60.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 8.89s
                        Total time: 4319.07s
                               ETA: 1009557.4s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1839 steps/s (collection: 8.536s, learning 0.372s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0445
             Mean action noise std: 0.76
                       Mean reward: 4.80
               Mean episode length: 63.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 8.91s
                        Total time: 4327.98s
                               ETA: 1009260.3s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.423s, learning 0.183s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0414
             Mean action noise std: 0.76
                       Mean reward: 4.83
               Mean episode length: 65.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 8.61s
                        Total time: 4336.59s
                               ETA: 1008894.5s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.446s, learning 0.220s)
               Value function loss: 0.0804
                    Surrogate loss: -0.0412
             Mean action noise std: 0.76
                       Mean reward: 4.55
               Mean episode length: 63.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 8.67s
                        Total time: 4345.25s
                               ETA: 1008544.2s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.581s, learning 0.233s)
               Value function loss: 0.0800
                    Surrogate loss: -0.0406
             Mean action noise std: 0.76
                       Mean reward: 4.49
               Mean episode length: 62.27
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 8.81s
                        Total time: 4354.07s
                               ETA: 1008229.7s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.366s, learning 0.268s)
               Value function loss: 155.3217
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 4.57
               Mean episode length: 62.98
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 8.63s
                        Total time: 4362.70s
                               ETA: 1007874.9s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.291s, learning 0.199s)
               Value function loss: 47.0216
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 4.58
               Mean episode length: 59.87
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 8.49s
                        Total time: 4371.19s
                               ETA: 1007488.6s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.286s, learning 0.335s)
               Value function loss: 0.1765
                    Surrogate loss: -0.0421
             Mean action noise std: 0.76
                       Mean reward: 17.60
               Mean episode length: 64.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 8.62s
                        Total time: 4379.81s
                               ETA: 1007134.0s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.679s, learning 0.216s)
               Value function loss: 495.8502
                    Surrogate loss: -0.0023
             Mean action noise std: 0.76
                       Mean reward: 4.78
               Mean episode length: 61.54
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 8.90s
                        Total time: 4388.71s
                               ETA: 1006844.0s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.452s, learning 0.184s)
               Value function loss: 0.3094
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 4.75
               Mean episode length: 63.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 8.64s
                        Total time: 4397.34s
                               ETA: 1006496.2s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.502s, learning 0.198s)
               Value function loss: 132.8010
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 4.72
               Mean episode length: 62.88
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 8.70s
                        Total time: 4406.04s
                               ETA: 1006164.1s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.172s, learning 0.293s)
               Value function loss: 0.3656
                    Surrogate loss: -0.0227
             Mean action noise std: 0.76
                       Mean reward: 4.93
               Mean episode length: 59.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 8.46s
                        Total time: 4414.51s
                               ETA: 1005780.2s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.168s, learning 0.178s)
               Value function loss: 198.6903
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 4.84
               Mean episode length: 62.36
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 8.35s
                        Total time: 4422.85s
                               ETA: 1005371.1s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.533s, learning 0.277s)
               Value function loss: 493.4373
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 12.52
               Mean episode length: 60.22
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 8.81s
                        Total time: 4431.66s
                               ETA: 1005068.9s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.138s, learning 0.176s)
               Value function loss: 7.5921
                    Surrogate loss: -0.0250
             Mean action noise std: 0.76
                       Mean reward: 4.86
               Mean episode length: 62.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 8.31s
                        Total time: 4439.98s
                               ETA: 1004656.0s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.181s, learning 0.178s)
               Value function loss: 0.5739
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 4.89
               Mean episode length: 62.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 8.36s
                        Total time: 4448.34s
                               ETA: 1004254.8s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.389s, learning 0.196s)
               Value function loss: 0.2623
                    Surrogate loss: -0.0308
             Mean action noise std: 0.76
                       Mean reward: 4.61
               Mean episode length: 58.96
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 8.59s
                        Total time: 4456.92s
                               ETA: 1003906.5s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.750s, learning 0.287s)
               Value function loss: 71.4594
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 5.02
               Mean episode length: 60.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 9.04s
                        Total time: 4465.96s
                               ETA: 1003661.2s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.248s, learning 0.194s)
               Value function loss: 0.3269
                    Surrogate loss: -0.0322
             Mean action noise std: 0.76
                       Mean reward: 10.44
               Mean episode length: 61.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 8.44s
                        Total time: 4474.40s
                               ETA: 1003283.5s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.466s, learning 0.268s)
               Value function loss: 121.5561
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 10.10
               Mean episode length: 61.45
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 8.73s
                        Total time: 4483.13s
                               ETA: 1002973.0s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.340s, learning 0.361s)
               Value function loss: 0.5271
                    Surrogate loss: -0.0298
             Mean action noise std: 0.76
                       Mean reward: 5.22
               Mean episode length: 62.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 8.70s
                        Total time: 4491.84s
                               ETA: 1002656.4s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.283s, learning 0.183s)
               Value function loss: 250.6848
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 5.21
               Mean episode length: 58.62
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 8.47s
                        Total time: 4500.30s
                               ETA: 1002288.9s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1783 steps/s (collection: 8.922s, learning 0.265s)
               Value function loss: 0.4676
                    Surrogate loss: -0.0272
             Mean action noise std: 0.76
                       Mean reward: 7.91
               Mean episode length: 62.29
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 9.19s
                        Total time: 4509.49s
                               ETA: 1002083.2s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.790s, learning 0.266s)
               Value function loss: 0.3445
                    Surrogate loss: -0.0339
             Mean action noise std: 0.76
                       Mean reward: 5.35
               Mean episode length: 63.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 9.06s
                        Total time: 4518.55s
                               ETA: 1001849.3s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.495s, learning 0.242s)
               Value function loss: 59.2231
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 4.92
               Mean episode length: 59.79
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 8.74s
                        Total time: 4527.28s
                               ETA: 1001545.9s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.608s, learning 0.233s)
               Value function loss: 0.2677
                    Surrogate loss: -0.0342
             Mean action noise std: 0.76
                       Mean reward: 4.76
               Mean episode length: 58.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 8.84s
                        Total time: 4536.12s
                               ETA: 1001266.5s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.779s, learning 0.257s)
               Value function loss: 0.2427
                    Surrogate loss: -0.0276
             Mean action noise std: 0.76
                       Mean reward: 12.49
               Mean episode length: 60.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 9.04s
                        Total time: 4545.16s
                               ETA: 1001031.3s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.494s, learning 0.224s)
               Value function loss: 0.2101
                    Surrogate loss: -0.0259
             Mean action noise std: 0.76
                       Mean reward: 4.52
               Mean episode length: 61.47
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 8.72s
                        Total time: 4553.88s
                               ETA: 1000727.4s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1797 steps/s (collection: 8.850s, learning 0.264s)
               Value function loss: 0.1741
                    Surrogate loss: -0.0385
             Mean action noise std: 0.76
                       Mean reward: 4.83
               Mean episode length: 58.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 9.11s
                        Total time: 4562.99s
                               ETA: 1000511.6s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1829 steps/s (collection: 8.579s, learning 0.378s)
               Value function loss: 0.1486
                    Surrogate loss: -0.0398
             Mean action noise std: 0.76
                       Mean reward: 4.72
               Mean episode length: 58.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 8.96s
                        Total time: 4571.95s
                               ETA: 1000262.4s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.436s, learning 0.290s)
               Value function loss: 0.1405
                    Surrogate loss: -0.0397
             Mean action noise std: 0.76
                       Mean reward: 4.57
               Mean episode length: 57.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 8.73s
                        Total time: 4580.68s
                               ETA: 999963.6s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1833 steps/s (collection: 8.649s, learning 0.288s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0366
             Mean action noise std: 0.76
                       Mean reward: 4.57
               Mean episode length: 58.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 8.94s
                        Total time: 4589.61s
                               ETA: 999712.1s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1806 steps/s (collection: 8.804s, learning 0.266s)
               Value function loss: 192.2632
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 5.00
               Mean episode length: 59.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 9.07s
                        Total time: 4598.68s
                               ETA: 999490.6s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.698s, learning 0.190s)
               Value function loss: 0.1681
                    Surrogate loss: -0.0366
             Mean action noise std: 0.76
                       Mean reward: 5.14
               Mean episode length: 61.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 8.89s
                        Total time: 4607.57s
                               ETA: 999230.5s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.428s, learning 0.370s)
               Value function loss: 494.3207
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 4.76
               Mean episode length: 61.75
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 8.80s
                        Total time: 4616.37s
                               ETA: 998952.0s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.316s, learning 0.282s)
               Value function loss: 4.3151
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 4.55
               Mean episode length: 58.97
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 8.60s
                        Total time: 4624.97s
                               ETA: 998631.5s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1802 steps/s (collection: 8.787s, learning 0.304s)
               Value function loss: 162.4220
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 4.59
               Mean episode length: 60.97
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 9.09s
                        Total time: 4634.06s
                               ETA: 998418.7s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.236s, learning 0.235s)
               Value function loss: 1.2972
                    Surrogate loss: -0.0272
             Mean action noise std: 0.76
                       Mean reward: 30.21
               Mean episode length: 59.13
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 8.47s
                        Total time: 4642.53s
                               ETA: 998073.5s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.436s, learning 0.186s)
               Value function loss: 0.3573
                    Surrogate loss: -0.0260
             Mean action noise std: 0.76
                       Mean reward: 5.03
               Mean episode length: 61.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 8.62s
                        Total time: 4651.15s
                               ETA: 997761.9s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.601s, learning 0.180s)
               Value function loss: 0.2731
                    Surrogate loss: -0.0282
             Mean action noise std: 0.76
                       Mean reward: 4.86
               Mean episode length: 59.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 8.78s
                        Total time: 4659.93s
                               ETA: 997485.7s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.079s, learning 0.178s)
               Value function loss: 0.2034
                    Surrogate loss: -0.0355
             Mean action noise std: 0.76
                       Mean reward: 5.15
               Mean episode length: 57.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 8.26s
                        Total time: 4668.19s
                               ETA: 997098.7s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.172s, learning 0.181s)
               Value function loss: 0.1891
                    Surrogate loss: -0.0286
             Mean action noise std: 0.76
                       Mean reward: 4.95
               Mean episode length: 59.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 8.35s
                        Total time: 4676.54s
                               ETA: 996733.9s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.255s, learning 0.209s)
               Value function loss: 0.1665
                    Surrogate loss: -0.0311
             Mean action noise std: 0.76
                       Mean reward: 5.02
               Mean episode length: 59.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 8.46s
                        Total time: 4685.00s
                               ETA: 996394.2s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.604s, learning 0.183s)
               Value function loss: 0.1462
                    Surrogate loss: -0.0360
             Mean action noise std: 0.76
                       Mean reward: 4.67
               Mean episode length: 60.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 8.79s
                        Total time: 4693.79s
                               ETA: 996124.6s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.270s, learning 0.188s)
               Value function loss: 46.9131
                    Surrogate loss: -0.0009
             Mean action noise std: 0.76
                       Mean reward: 5.02
               Mean episode length: 59.75
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 8.46s
                        Total time: 4702.25s
                               ETA: 995786.3s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.668s, learning 0.228s)
               Value function loss: 0.1396
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 56.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 8.90s
                        Total time: 4711.14s
                               ETA: 995541.9s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.409s, learning 0.196s)
               Value function loss: 0.1406
                    Surrogate loss: -0.0354
             Mean action noise std: 0.76
                       Mean reward: 4.76
               Mean episode length: 62.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 8.61s
                        Total time: 4719.75s
                               ETA: 995237.2s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.633s, learning 0.183s)
               Value function loss: 324.2399
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 4.76
               Mean episode length: 62.40
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 8.82s
                        Total time: 4728.57s
                               ETA: 994978.2s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.248s, learning 0.178s)
               Value function loss: 0.1822
                    Surrogate loss: -0.0283
             Mean action noise std: 0.76
                       Mean reward: 4.75
               Mean episode length: 59.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 8.43s
                        Total time: 4736.99s
                               ETA: 994638.3s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.588s, learning 0.180s)
               Value function loss: 14.0368
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 4.80
               Mean episode length: 59.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 8.77s
                        Total time: 4745.76s
                               ETA: 994371.3s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.376s, learning 0.204s)
               Value function loss: 72.3251
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 5.16
               Mean episode length: 62.34
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 8.58s
                        Total time: 4754.34s
                               ETA: 994066.2s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.307s, learning 0.250s)
               Value function loss: 29.6826
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 25.17
               Mean episode length: 60.47
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 8.56s
                        Total time: 4762.90s
                               ETA: 993757.7s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.690s, learning 0.186s)
               Value function loss: 522.0983
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 4.61
               Mean episode length: 59.57
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 8.88s
                        Total time: 4771.77s
                               ETA: 993516.8s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.635s, learning 0.280s)
               Value function loss: 291.6212
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 7.72
               Mean episode length: 58.64
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 8.91s
                        Total time: 4780.69s
                               ETA: 993284.8s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1074 steps/s (collection: 15.058s, learning 0.192s)
               Value function loss: 146.7149
                    Surrogate loss: -0.0045
             Mean action noise std: 0.76
                       Mean reward: 4.82
               Mean episode length: 57.58
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 15.25s
                        Total time: 4795.94s
                               ETA: 994367.4s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.870s, learning 0.201s)
               Value function loss: 142.0744
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 10.20
               Mean episode length: 60.07
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 17.07s
                        Total time: 4813.01s
                               ETA: 995822.2s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.738s, learning 0.237s)
               Value function loss: 151.3408
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 4.55
               Mean episode length: 57.28
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 16.98s
                        Total time: 4829.98s
                               ETA: 997251.1s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.855s, learning 0.199s)
               Value function loss: 111.4209
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 4.63
               Mean episode length: 56.91
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 17.05s
                        Total time: 4847.04s
                               ETA: 998690.2s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 950 steps/s (collection: 17.020s, learning 0.213s)
               Value function loss: 420.6733
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 25.22
               Mean episode length: 59.49
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 17.23s
                        Total time: 4864.27s
                               ETA: 1000160.0s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.840s, learning 0.179s)
               Value function loss: 183.4111
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 4.82
               Mean episode length: 59.43
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 17.02s
                        Total time: 4881.29s
                               ETA: 1001580.0s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.433s, learning 0.185s)
               Value function loss: 22.7938
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 4.83
               Mean episode length: 60.68
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 16.62s
                        Total time: 4897.91s
                               ETA: 1002911.8s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 952 steps/s (collection: 16.936s, learning 0.260s)
               Value function loss: 0.5069
                    Surrogate loss: -0.0312
             Mean action noise std: 0.76
                       Mean reward: 4.92
               Mean episode length: 58.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 17.20s
                        Total time: 4915.10s
                               ETA: 1004356.2s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.589s, learning 0.179s)
               Value function loss: 192.9473
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 4.92
               Mean episode length: 63.52
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 16.77s
                        Total time: 4931.87s
                               ETA: 1005707.4s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.834s, learning 0.173s)
               Value function loss: 215.9263
                    Surrogate loss: -0.0028
             Mean action noise std: 0.76
                       Mean reward: 7.17
               Mean episode length: 61.70
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 17.01s
                        Total time: 4948.88s
                               ETA: 1007101.8s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 953 steps/s (collection: 17.018s, learning 0.166s)
               Value function loss: 1.2207
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 4.93
               Mean episode length: 62.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 17.18s
                        Total time: 4966.06s
                               ETA: 1008526.3s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.928s, learning 0.176s)
               Value function loss: 18.3387
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 4.89
               Mean episode length: 62.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 17.10s
                        Total time: 4983.17s
                               ETA: 1009928.7s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.462s, learning 0.179s)
               Value function loss: 18.2345
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 4.78
               Mean episode length: 63.25
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 16.64s
                        Total time: 4999.81s
                               ETA: 1011231.5s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.915s, learning 0.199s)
               Value function loss: 9.9502
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 5.02
               Mean episode length: 61.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 17.11s
                        Total time: 5016.92s
                               ETA: 1012624.6s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 955 steps/s (collection: 16.916s, learning 0.233s)
               Value function loss: 0.4963
                    Surrogate loss: -0.0167
             Mean action noise std: 0.76
                       Mean reward: 11.98
               Mean episode length: 61.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 17.15s
                        Total time: 5034.07s
                               ETA: 1014018.9s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 942 steps/s (collection: 17.193s, learning 0.182s)
               Value function loss: 0.3534
                    Surrogate loss: -0.0258
             Mean action noise std: 0.76
                       Mean reward: 4.65
               Mean episode length: 62.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 17.37s
                        Total time: 5051.45s
                               ETA: 1015452.9s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 931 steps/s (collection: 17.394s, learning 0.200s)
               Value function loss: 0.3035
                    Surrogate loss: -0.0201
             Mean action noise std: 0.76
                       Mean reward: 4.84
               Mean episode length: 64.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 17.59s
                        Total time: 5069.04s
                               ETA: 1016925.1s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.367s, learning 0.176s)
               Value function loss: 64.8169
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 7.26
               Mean episode length: 61.04
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 16.54s
                        Total time: 5085.58s
                               ETA: 1018180.7s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.194s, learning 0.186s)
               Value function loss: 29.4486
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 4.41
               Mean episode length: 61.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 16.38s
                        Total time: 5101.96s
                               ETA: 1019398.7s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.711s, learning 0.186s)
               Value function loss: 0.4176
                    Surrogate loss: -0.0346
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 64.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 16.90s
                        Total time: 5118.86s
                               ETA: 1020715.0s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.634s, learning 0.286s)
               Value function loss: 0.2207
                    Surrogate loss: -0.0294
             Mean action noise std: 0.76
                       Mean reward: 4.61
               Mean episode length: 64.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 16.92s
                        Total time: 5135.78s
                               ETA: 1022030.3s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.546s, learning 0.186s)
               Value function loss: 0.2302
                    Surrogate loss: -0.0185
             Mean action noise std: 0.76
                       Mean reward: 14.88
               Mean episode length: 63.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 16.73s
                        Total time: 5152.51s
                               ETA: 1023303.1s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.978s, learning 0.179s)
               Value function loss: 0.1787
                    Surrogate loss: -0.0250
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 65.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 17.16s
                        Total time: 5169.67s
                               ETA: 1024655.1s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.425s, learning 0.203s)
               Value function loss: 0.1359
                    Surrogate loss: -0.0314
             Mean action noise std: 0.76
                       Mean reward: 4.54
               Mean episode length: 64.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 16.63s
                        Total time: 5186.30s
                               ETA: 1025896.9s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 945 steps/s (collection: 17.103s, learning 0.226s)
               Value function loss: 0.1296
                    Surrogate loss: -0.0330
             Mean action noise std: 0.76
                       Mean reward: 4.35
               Mean episode length: 65.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 17.33s
                        Total time: 5203.63s
                               ETA: 1027272.0s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.179s, learning 0.179s)
               Value function loss: 7.5452
                    Surrogate loss: 0.0009
             Mean action noise std: 0.76
                       Mean reward: 4.52
               Mean episode length: 66.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 16.36s
                        Total time: 5219.98s
                               ETA: 1028450.3s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.640s, learning 0.183s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0314
             Mean action noise std: 0.76
                       Mean reward: 4.51
               Mean episode length: 65.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 16.82s
                        Total time: 5236.81s
                               ETA: 1029715.3s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.598s, learning 0.180s)
               Value function loss: 0.1171
                    Surrogate loss: -0.0324
             Mean action noise std: 0.76
                       Mean reward: 4.70
               Mean episode length: 65.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 16.78s
                        Total time: 5253.58s
                               ETA: 1030966.5s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.174s, learning 0.203s)
               Value function loss: 0.1085
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 6.86
               Mean episode length: 66.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 16.38s
                        Total time: 5269.96s
                               ETA: 1032134.2s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 959 steps/s (collection: 16.830s, learning 0.237s)
               Value function loss: 0.0967
                    Surrogate loss: -0.0338
             Mean action noise std: 0.76
                       Mean reward: 4.61
               Mean episode length: 65.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 17.07s
                        Total time: 5287.03s
                               ETA: 1033432.1s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.741s, learning 0.194s)
               Value function loss: 0.0973
                    Surrogate loss: -0.0349
             Mean action noise std: 0.76
                       Mean reward: 4.52
               Mean episode length: 64.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 16.93s
                        Total time: 5303.96s
                               ETA: 1034699.0s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.438s, learning 0.183s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 4.47
               Mean episode length: 62.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 16.62s
                        Total time: 5320.58s
                               ETA: 1035899.8s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 968 steps/s (collection: 16.745s, learning 0.165s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0419
             Mean action noise std: 0.76
                       Mean reward: 4.61
               Mean episode length: 62.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 16.91s
                        Total time: 5337.49s
                               ETA: 1037151.9s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 946 steps/s (collection: 16.980s, learning 0.332s)
               Value function loss: 0.0948
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 4.77
               Mean episode length: 64.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 17.31s
                        Total time: 5354.80s
                               ETA: 1038477.1s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.916s, learning 0.199s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0427
             Mean action noise std: 0.76
                       Mean reward: 4.34
               Mean episode length: 61.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 17.11s
                        Total time: 5371.92s
                               ETA: 1039758.9s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 961 steps/s (collection: 16.852s, learning 0.180s)
               Value function loss: 0.0933
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 4.69
               Mean episode length: 62.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 17.03s
                        Total time: 5388.95s
                               ETA: 1041019.7s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.675s, learning 0.197s)
               Value function loss: 0.1179
                    Surrogate loss: -0.0325
             Mean action noise std: 0.76
                       Mean reward: 4.60
               Mean episode length: 62.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 16.87s
                        Total time: 5405.82s
                               ETA: 1042244.6s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1036 steps/s (collection: 15.634s, learning 0.169s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0372
             Mean action noise std: 0.76
                       Mean reward: 4.53
               Mean episode length: 62.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 15.80s
                        Total time: 5421.63s
                               ETA: 1043259.2s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.480s, learning 0.208s)
               Value function loss: 94.1303
                    Surrogate loss: -0.0011
             Mean action noise std: 0.76
                       Mean reward: 4.63
               Mean episode length: 62.44
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 8.69s
                        Total time: 5430.31s
                               ETA: 1042903.2s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.409s, learning 0.182s)
               Value function loss: 0.0877
                    Surrogate loss: -0.0458
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 60.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 8.59s
                        Total time: 5438.90s
                               ETA: 1042530.0s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.627s, learning 0.203s)
               Value function loss: 0.0836
                    Surrogate loss: -0.0420
             Mean action noise std: 0.76
                       Mean reward: 4.69
               Mean episode length: 61.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 8.83s
                        Total time: 5447.74s
                               ETA: 1042204.1s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.350s, learning 0.285s)
               Value function loss: 0.0774
                    Surrogate loss: -0.0424
             Mean action noise std: 0.76
                       Mean reward: 4.67
               Mean episode length: 60.35
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 8.63s
                        Total time: 5456.37s
                               ETA: 1041842.0s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.861s, learning 0.194s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0356
             Mean action noise std: 0.76
                       Mean reward: 4.62
               Mean episode length: 61.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 9.06s
                        Total time: 5465.43s
                               ETA: 1041561.5s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.563s, learning 0.300s)
               Value function loss: 63.9739
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 4.44
               Mean episode length: 59.09
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 8.86s
                        Total time: 5474.29s
                               ETA: 1041245.2s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1795 steps/s (collection: 8.872s, learning 0.255s)
               Value function loss: 0.1210
                    Surrogate loss: -0.0423
             Mean action noise std: 0.76
                       Mean reward: 4.63
               Mean episode length: 58.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 9.13s
                        Total time: 5483.41s
                               ETA: 1040980.2s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.661s, learning 0.186s)
               Value function loss: 0.1253
                    Surrogate loss: -0.0345
             Mean action noise std: 0.76
                       Mean reward: 4.61
               Mean episode length: 60.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 8.85s
                        Total time: 5492.26s
                               ETA: 1040663.3s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.494s, learning 0.213s)
               Value function loss: 0.0935
                    Surrogate loss: -0.0402
             Mean action noise std: 0.76
                       Mean reward: 4.27
               Mean episode length: 58.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 8.71s
                        Total time: 5500.97s
                               ETA: 1040321.1s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1828 steps/s (collection: 8.778s, learning 0.181s)
               Value function loss: 0.1031
                    Surrogate loss: -0.0424
             Mean action noise std: 0.76
                       Mean reward: 4.52
               Mean episode length: 60.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 8.96s
                        Total time: 5509.93s
                               ETA: 1040027.5s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1837 steps/s (collection: 8.667s, learning 0.247s)
               Value function loss: 84.4113
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: 4.52
               Mean episode length: 58.03
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 8.91s
                        Total time: 5518.84s
                               ETA: 1039726.8s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.410s, learning 0.226s)
               Value function loss: 0.1549
                    Surrogate loss: -0.0419
             Mean action noise std: 0.76
                       Mean reward: 4.60
               Mean episode length: 60.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 8.64s
                        Total time: 5527.48s
                               ETA: 1039374.8s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.674s, learning 0.211s)
               Value function loss: 130.8819
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 9.89
               Mean episode length: 61.53
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 8.89s
                        Total time: 5536.36s
                               ETA: 1039070.9s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.775s, learning 0.175s)
               Value function loss: 26.8171
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 4.38
               Mean episode length: 55.98
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 8.95s
                        Total time: 5545.31s
                               ETA: 1038780.1s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.505s, learning 0.189s)
               Value function loss: 0.5115
                    Surrogate loss: -0.0336
             Mean action noise std: 0.76
                       Mean reward: 4.42
               Mean episode length: 57.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 8.69s
                        Total time: 5554.01s
                               ETA: 1038442.7s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.481s, learning 0.375s)
               Value function loss: 17.6722
                    Surrogate loss: -0.0011
             Mean action noise std: 0.76
                       Mean reward: 7.34
               Mean episode length: 60.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 8.86s
                        Total time: 5562.86s
                               ETA: 1038136.8s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.536s, learning 0.185s)
               Value function loss: 189.4823
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 25.03
               Mean episode length: 58.96
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 8.72s
                        Total time: 5571.59s
                               ETA: 1037806.9s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1803 steps/s (collection: 8.832s, learning 0.252s)
               Value function loss: 0.8090
                    Surrogate loss: -0.0350
             Mean action noise std: 0.76
                       Mean reward: 4.84
               Mean episode length: 61.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 9.08s
                        Total time: 5580.67s
                               ETA: 1037545.4s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.656s, learning 0.358s)
               Value function loss: 0.3417
                    Surrogate loss: -0.0361
             Mean action noise std: 0.76
                       Mean reward: 4.77
               Mean episode length: 60.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 9.01s
                        Total time: 5589.68s
                               ETA: 1037272.0s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.614s, learning 0.263s)
               Value function loss: 0.2950
                    Surrogate loss: -0.0353
             Mean action noise std: 0.76
                       Mean reward: 12.32
               Mean episode length: 60.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 8.88s
                        Total time: 5598.56s
                               ETA: 1036974.2s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.723s, learning 0.220s)
               Value function loss: 0.1871
                    Surrogate loss: -0.0367
             Mean action noise std: 0.76
                       Mean reward: 4.67
               Mean episode length: 62.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 8.94s
                        Total time: 5607.50s
                               ETA: 1036689.5s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.591s, learning 0.276s)
               Value function loss: 0.1681
                    Surrogate loss: -0.0293
             Mean action noise std: 0.76
                       Mean reward: 4.51
               Mean episode length: 60.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 8.87s
                        Total time: 5616.37s
                               ETA: 1036392.0s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1788 steps/s (collection: 8.921s, learning 0.238s)
               Value function loss: 0.1251
                    Surrogate loss: -0.0363
             Mean action noise std: 0.76
                       Mean reward: 4.55
               Mean episode length: 61.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 9.16s
                        Total time: 5625.53s
                               ETA: 1036149.3s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.481s, learning 0.230s)
               Value function loss: 0.1148
                    Surrogate loss: -0.0380
             Mean action noise std: 0.76
                       Mean reward: 4.48
               Mean episode length: 61.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 8.71s
                        Total time: 5634.24s
                               ETA: 1035825.2s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.479s, learning 0.361s)
               Value function loss: 70.8665
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 12.13
               Mean episode length: 59.72
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 8.84s
                        Total time: 5643.08s
                               ETA: 1035525.9s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.587s, learning 0.268s)
               Value function loss: 14.9157
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 4.53
               Mean episode length: 59.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 8.85s
                        Total time: 5651.93s
                               ETA: 1035230.3s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1856 steps/s (collection: 8.568s, learning 0.257s)
               Value function loss: 0.1832
                    Surrogate loss: -0.0415
             Mean action noise std: 0.76
                       Mean reward: 4.65
               Mean episode length: 61.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 8.83s
                        Total time: 5660.76s
                               ETA: 1034930.3s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.601s, learning 0.268s)
               Value function loss: 59.8763
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 4.63
               Mean episode length: 61.34
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 8.87s
                        Total time: 5669.63s
                               ETA: 1034639.6s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.679s, learning 0.196s)
               Value function loss: 45.1235
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 4.47
               Mean episode length: 58.25
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 8.87s
                        Total time: 5678.50s
                               ETA: 1034350.7s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.627s, learning 0.265s)
               Value function loss: 355.2828
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 9.59
               Mean episode length: 59.06
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 8.89s
                        Total time: 5687.39s
                               ETA: 1034066.0s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.610s, learning 0.198s)
               Value function loss: 12.3896
                    Surrogate loss: -0.0260
             Mean action noise std: 0.76
                       Mean reward: 32.58
               Mean episode length: 60.15
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 8.81s
                        Total time: 5696.20s
                               ETA: 1033767.2s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.600s, learning 0.311s)
               Value function loss: 1.7348
                    Surrogate loss: -0.0252
             Mean action noise std: 0.76
                       Mean reward: 4.32
               Mean episode length: 60.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 8.91s
                        Total time: 5705.11s
                               ETA: 1033487.9s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.390s, learning 0.298s)
               Value function loss: 0.5720
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 4.68
               Mean episode length: 57.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 8.69s
                        Total time: 5713.80s
                               ETA: 1033169.5s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.668s, learning 0.382s)
               Value function loss: 0.3305
                    Surrogate loss: -0.0380
             Mean action noise std: 0.76
                       Mean reward: 4.34
               Mean episode length: 58.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 9.05s
                        Total time: 5722.85s
                               ETA: 1032917.5s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.478s, learning 0.269s)
               Value function loss: 0.2050
                    Surrogate loss: -0.0360
             Mean action noise std: 0.76
                       Mean reward: 4.38
               Mean episode length: 61.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 8.75s
                        Total time: 5731.60s
                               ETA: 1032611.7s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.803s, learning 0.252s)
               Value function loss: 0.1513
                    Surrogate loss: -0.0339
             Mean action noise std: 0.76
                       Mean reward: 4.43
               Mean episode length: 58.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 9.05s
                        Total time: 5740.65s
                               ETA: 1032362.4s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.521s, learning 0.271s)
               Value function loss: 0.1479
                    Surrogate loss: -0.0345
             Mean action noise std: 0.76
                       Mean reward: 4.27
               Mean episode length: 58.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 8.79s
                        Total time: 5749.44s
                               ETA: 1032066.7s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.752s, learning 0.270s)
               Value function loss: 175.6038
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 4.21
               Mean episode length: 60.16
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 9.02s
                        Total time: 5758.47s
                               ETA: 1031813.3s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.389s, learning 0.214s)
               Value function loss: 496.3891
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 4.20
               Mean episode length: 62.15
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 8.60s
                        Total time: 5767.07s
                               ETA: 1031486.0s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.561s, learning 0.283s)
               Value function loss: 495.7427
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 4.15
               Mean episode length: 59.09
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 8.84s
                        Total time: 5775.91s
                               ETA: 1031202.9s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.287s, learning 0.214s)
               Value function loss: 243.2198
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 4.39
               Mean episode length: 61.01
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 8.50s
                        Total time: 5784.41s
                               ETA: 1030859.5s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.742s, learning 0.259s)
               Value function loss: 95.2867
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 4.13
               Mean episode length: 59.05
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 9.00s
                        Total time: 5793.42s
                               ETA: 1030606.2s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.743s, learning 0.261s)
               Value function loss: 1.2287
                    Surrogate loss: -0.0349
             Mean action noise std: 0.76
                       Mean reward: 4.32
               Mean episode length: 63.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 9.00s
                        Total time: 5802.42s
                               ETA: 1030354.4s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.577s, learning 0.182s)
               Value function loss: 0.4437
                    Surrogate loss: -0.0315
             Mean action noise std: 0.76
                       Mean reward: 3.85
               Mean episode length: 57.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 8.76s
                        Total time: 5811.18s
                               ETA: 1030060.1s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.379s, learning 0.185s)
               Value function loss: 0.2936
                    Surrogate loss: -0.0334
             Mean action noise std: 0.76
                       Mean reward: 4.08
               Mean episode length: 57.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 8.56s
                        Total time: 5819.74s
                               ETA: 1029732.1s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.173s, learning 0.181s)
               Value function loss: 216.0690
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 16.59
               Mean episode length: 59.73
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 8.35s
                        Total time: 5828.10s
                               ETA: 1029368.3s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1858 steps/s (collection: 8.644s, learning 0.172s)
               Value function loss: 0.3203
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 3.94
               Mean episode length: 63.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 8.82s
                        Total time: 5836.91s
                               ETA: 1029087.1s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.515s, learning 0.206s)
               Value function loss: 0.3318
                    Surrogate loss: -0.0317
             Mean action noise std: 0.76
                       Mean reward: 4.11
               Mean episode length: 61.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 8.72s
                        Total time: 5845.63s
                               ETA: 1028790.3s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.607s, learning 0.184s)
               Value function loss: 0.2907
                    Surrogate loss: -0.0224
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 62.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 8.79s
                        Total time: 5854.43s
                               ETA: 1028506.7s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.351s, learning 0.179s)
               Value function loss: 18.0146
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 3.66
               Mean episode length: 58.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 8.53s
                        Total time: 5862.96s
                               ETA: 1028178.4s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.306s, learning 0.303s)
               Value function loss: 191.6925
                    Surrogate loss: -0.0027
             Mean action noise std: 0.76
                       Mean reward: 3.74
               Mean episode length: 64.11
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 8.61s
                        Total time: 5871.56s
                               ETA: 1027865.0s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.579s, learning 0.279s)
               Value function loss: 0.4617
                    Surrogate loss: -0.0401
             Mean action noise std: 0.76
                       Mean reward: 4.04
               Mean episode length: 65.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 8.86s
                        Total time: 5880.42s
                               ETA: 1027596.1s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.772s, learning 0.276s)
               Value function loss: 0.2322
                    Surrogate loss: -0.0365
             Mean action noise std: 0.76
                       Mean reward: 3.92
               Mean episode length: 60.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 9.05s
                        Total time: 5889.47s
                               ETA: 1027361.4s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.455s, learning 0.410s)
               Value function loss: 0.1732
                    Surrogate loss: -0.0348
             Mean action noise std: 0.76
                       Mean reward: 3.79
               Mean episode length: 60.75
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 8.86s
                        Total time: 5898.34s
                               ETA: 1027095.5s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1877 steps/s (collection: 8.501s, learning 0.224s)
               Value function loss: 0.1333
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 4.11
               Mean episode length: 64.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 8.73s
                        Total time: 5907.06s
                               ETA: 1026806.2s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.680s, learning 0.337s)
               Value function loss: 0.1124
                    Surrogate loss: -0.0340
             Mean action noise std: 0.76
                       Mean reward: 3.72
               Mean episode length: 58.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 9.02s
                        Total time: 5916.08s
                               ETA: 1026568.7s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.215s, learning 0.246s)
               Value function loss: 0.1075
                    Surrogate loss: -0.0372
             Mean action noise std: 0.76
                       Mean reward: 3.86
               Mean episode length: 61.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 8.46s
                        Total time: 5924.54s
                               ETA: 1026235.5s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.447s, learning 0.397s)
               Value function loss: 0.0963
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 3.92
               Mean episode length: 60.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 8.84s
                        Total time: 5933.38s
                               ETA: 1025969.8s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.614s, learning 0.260s)
               Value function loss: 0.0887
                    Surrogate loss: -0.0409
             Mean action noise std: 0.76
                       Mean reward: 3.96
               Mean episode length: 61.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 8.87s
                        Total time: 5942.26s
                               ETA: 1025709.9s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.397s, learning 0.380s)
               Value function loss: 0.0827
                    Surrogate loss: -0.0380
             Mean action noise std: 0.76
                       Mean reward: 4.10
               Mean episode length: 65.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 8.78s
                        Total time: 5951.03s
                               ETA: 1025434.3s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1765 steps/s (collection: 8.855s, learning 0.422s)
               Value function loss: 47.7211
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 3.97
               Mean episode length: 63.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 9.28s
                        Total time: 5960.31s
                               ETA: 1025245.7s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.560s, learning 0.200s)
               Value function loss: 0.0953
                    Surrogate loss: -0.0345
             Mean action noise std: 0.76
                       Mean reward: 3.98
               Mean episode length: 62.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 8.76s
                        Total time: 5969.07s
                               ETA: 1024969.0s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.328s, learning 0.175s)
               Value function loss: 0.0967
                    Surrogate loss: -0.0361
             Mean action noise std: 0.76
                       Mean reward: 3.84
               Mean episode length: 63.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 8.50s
                        Total time: 5977.57s
                               ETA: 1024649.1s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.513s, learning 0.180s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0387
             Mean action noise std: 0.76
                       Mean reward: 4.11
               Mean episode length: 64.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 8.69s
                        Total time: 5986.27s
                               ETA: 1024362.7s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.421s, learning 0.298s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 3.85
               Mean episode length: 62.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 8.72s
                        Total time: 5994.99s
                               ETA: 1024081.9s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.453s, learning 0.179s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0355
             Mean action noise std: 0.76
                       Mean reward: 3.93
               Mean episode length: 62.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 8.63s
                        Total time: 6003.62s
                               ETA: 1023787.1s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.224s, learning 0.356s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0416
             Mean action noise std: 0.76
                       Mean reward: 4.11
               Mean episode length: 66.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 8.58s
                        Total time: 6012.20s
                               ETA: 1023484.4s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.545s, learning 0.196s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0404
             Mean action noise std: 0.76
                       Mean reward: 3.98
               Mean episode length: 59.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 8.74s
                        Total time: 6020.94s
                               ETA: 1023210.1s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.214s, learning 0.272s)
               Value function loss: 0.0540
                    Surrogate loss: -0.0409
             Mean action noise std: 0.76
                       Mean reward: 3.85
               Mean episode length: 63.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 8.49s
                        Total time: 6029.43s
                               ETA: 1022893.5s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.437s, learning 0.183s)
               Value function loss: 0.0577
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 3.70
               Mean episode length: 61.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 8.62s
                        Total time: 6038.05s
                               ETA: 1022600.5s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.297s, learning 0.435s)
               Value function loss: 0.0521
                    Surrogate loss: -0.0381
             Mean action noise std: 0.76
                       Mean reward: 3.91
               Mean episode length: 65.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 8.73s
                        Total time: 6046.78s
                               ETA: 1022327.4s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.426s, learning 0.256s)
               Value function loss: 0.0592
                    Surrogate loss: -0.0404
             Mean action noise std: 0.76
                       Mean reward: 3.94
               Mean episode length: 65.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 8.68s
                        Total time: 6055.46s
                               ETA: 1022046.9s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.340s, learning 0.198s)
               Value function loss: 385.6085
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 4.07
               Mean episode length: 64.30
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 8.54s
                        Total time: 6064.00s
                               ETA: 1021742.8s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1861 steps/s (collection: 8.489s, learning 0.311s)
               Value function loss: 132.4142
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 3.77
               Mean episode length: 62.43
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 8.80s
                        Total time: 6072.80s
                               ETA: 1021484.1s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.469s, learning 0.237s)
               Value function loss: 46.9129
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 4.02
               Mean episode length: 63.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 8.71s
                        Total time: 6081.51s
                               ETA: 1021210.2s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1764 steps/s (collection: 8.840s, learning 0.445s)
               Value function loss: 0.3518
                    Surrogate loss: -0.0386
             Mean action noise std: 0.76
                       Mean reward: 3.73
               Mean episode length: 62.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 9.28s
                        Total time: 6090.79s
                               ETA: 1021034.4s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.675s, learning 0.308s)
               Value function loss: 0.2051
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 3.85
               Mean episode length: 62.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 8.98s
                        Total time: 6099.77s
                               ETA: 1020808.6s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.292s, learning 0.266s)
               Value function loss: 0.1573
                    Surrogate loss: -0.0340
             Mean action noise std: 0.76
                       Mean reward: 3.87
               Mean episode length: 62.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 8.56s
                        Total time: 6108.33s
                               ETA: 1020512.4s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1752 steps/s (collection: 8.965s, learning 0.382s)
               Value function loss: 0.1253
                    Surrogate loss: -0.0340
             Mean action noise std: 0.76
                       Mean reward: 3.73
               Mean episode length: 61.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 9.35s
                        Total time: 6117.68s
                               ETA: 1020348.9s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.603s, learning 0.234s)
               Value function loss: 0.1081
                    Surrogate loss: -0.0291
             Mean action noise std: 0.76
                       Mean reward: 3.93
               Mean episode length: 59.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 8.84s
                        Total time: 6126.52s
                               ETA: 1020101.0s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.427s, learning 0.231s)
               Value function loss: 0.0959
                    Surrogate loss: -0.0303
             Mean action noise std: 0.76
                       Mean reward: 3.92
               Mean episode length: 64.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 8.66s
                        Total time: 6135.18s
                               ETA: 1019824.1s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.378s, learning 0.282s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0313
             Mean action noise std: 0.76
                       Mean reward: 3.51
               Mean episode length: 57.98
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 8.66s
                        Total time: 6143.84s
                               ETA: 1019548.6s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.337s, learning 0.182s)
               Value function loss: 0.0659
                    Surrogate loss: -0.0367
             Mean action noise std: 0.76
                       Mean reward: 3.65
               Mean episode length: 60.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 8.52s
                        Total time: 6152.36s
                               ETA: 1019250.5s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.465s, learning 0.182s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0354
             Mean action noise std: 0.76
                       Mean reward: 3.94
               Mean episode length: 64.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 8.65s
                        Total time: 6161.00s
                               ETA: 1018974.3s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.430s, learning 0.276s)
               Value function loss: 0.0636
                    Surrogate loss: -0.0372
             Mean action noise std: 0.76
                       Mean reward: 3.54
               Mean episode length: 61.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 8.71s
                        Total time: 6169.71s
                               ETA: 1018709.0s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1766 steps/s (collection: 8.904s, learning 0.373s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0392
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 63.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 9.28s
                        Total time: 6178.99s
                               ETA: 1018538.7s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1851 steps/s (collection: 8.537s, learning 0.313s)
               Value function loss: 0.0640
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 3.57
               Mean episode length: 62.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 8.85s
                        Total time: 6187.84s
                               ETA: 1018298.6s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1810 steps/s (collection: 8.760s, learning 0.290s)
               Value function loss: 0.0595
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 3.96
               Mean episode length: 65.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 9.05s
                        Total time: 6196.89s
                               ETA: 1018092.2s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.488s, learning 0.208s)
               Value function loss: 0.0734
                    Surrogate loss: -0.0298
             Mean action noise std: 0.76
                       Mean reward: 3.96
               Mean episode length: 63.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 8.70s
                        Total time: 6205.58s
                               ETA: 1017828.3s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.559s, learning 0.285s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0320
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 63.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 8.84s
                        Total time: 6214.43s
                               ETA: 1017589.3s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.516s, learning 0.289s)
               Value function loss: 386.0472
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 3.77
               Mean episode length: 63.45
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 8.81s
                        Total time: 6223.23s
                               ETA: 1017344.8s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.338s, learning 0.181s)
               Value function loss: 18.1179
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 3.54
               Mean episode length: 60.93
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 8.52s
                        Total time: 6231.75s
                               ETA: 1017054.4s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.613s, learning 0.272s)
               Value function loss: 302.2167
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 3.78
               Mean episode length: 63.71
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 8.89s
                        Total time: 6240.64s
                               ETA: 1016824.6s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1766 steps/s (collection: 8.874s, learning 0.399s)
               Value function loss: 17.9039
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 65.31
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 9.27s
                        Total time: 6249.91s
                               ETA: 1016658.6s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1822 steps/s (collection: 8.582s, learning 0.407s)
               Value function loss: 0.2020
                    Surrogate loss: -0.0415
             Mean action noise std: 0.76
                       Mean reward: 4.08
               Mean episode length: 63.75
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 8.99s
                        Total time: 6258.90s
                               ETA: 1016447.1s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.424s, learning 0.256s)
               Value function loss: 0.1365
                    Surrogate loss: -0.0347
             Mean action noise std: 0.76
                       Mean reward: 4.04
               Mean episode length: 63.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 8.68s
                        Total time: 6267.58s
                               ETA: 1016186.0s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.361s, learning 0.354s)
               Value function loss: 0.1060
                    Surrogate loss: -0.0325
             Mean action noise std: 0.76
                       Mean reward: 3.78
               Mean episode length: 65.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 8.71s
                        Total time: 6276.29s
                               ETA: 1015931.5s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1813 steps/s (collection: 8.731s, learning 0.302s)
               Value function loss: 0.0877
                    Surrogate loss: -0.0309
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 63.27
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 9.03s
                        Total time: 6285.33s
                               ETA: 1015729.1s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1818 steps/s (collection: 8.700s, learning 0.309s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0361
             Mean action noise std: 0.76
                       Mean reward: 3.81
               Mean episode length: 62.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 9.01s
                        Total time: 6294.34s
                               ETA: 1015523.5s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1805 steps/s (collection: 8.852s, learning 0.224s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0346
             Mean action noise std: 0.76
                       Mean reward: 4.17
               Mean episode length: 67.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 9.08s
                        Total time: 6303.41s
                               ETA: 1015329.4s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.455s, learning 0.183s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0423
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 61.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 8.64s
                        Total time: 6312.05s
                               ETA: 1015065.3s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.814s, learning 0.205s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0402
             Mean action noise std: 0.76
                       Mean reward: 3.84
               Mean episode length: 66.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 9.02s
                        Total time: 6321.07s
                               ETA: 1014863.5s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.342s, learning 0.194s)
               Value function loss: 0.0665
                    Surrogate loss: -0.0373
             Mean action noise std: 0.76
                       Mean reward: 3.94
               Mean episode length: 64.72
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 8.54s
                        Total time: 6329.61s
                               ETA: 1014584.6s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1773 steps/s (collection: 8.868s, learning 0.368s)
               Value function loss: 9.5585
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 3.68
               Mean episode length: 63.64
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 9.24s
                        Total time: 6338.84s
                               ETA: 1014418.7s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.361s, learning 0.193s)
               Value function loss: 0.0695
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 3.96
               Mean episode length: 63.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 8.55s
                        Total time: 6347.39s
                               ETA: 1014144.2s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.737s, learning 0.208s)
               Value function loss: 0.0635
                    Surrogate loss: -0.0398
             Mean action noise std: 0.76
                       Mean reward: 4.02
               Mean episode length: 66.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 8.94s
                        Total time: 6356.34s
                               ETA: 1013933.0s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.457s, learning 0.349s)
               Value function loss: 3.9614
                    Surrogate loss: -0.0006
             Mean action noise std: 0.76
                       Mean reward: 4.15
               Mean episode length: 64.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 8.81s
                        Total time: 6365.15s
                               ETA: 1013700.4s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.423s, learning 0.181s)
               Value function loss: 0.0537
                    Surrogate loss: -0.0403
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 63.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 8.60s
                        Total time: 6373.75s
                               ETA: 1013436.3s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.322s, learning 0.217s)
               Value function loss: 0.0565
                    Surrogate loss: -0.0423
             Mean action noise std: 0.76
                       Mean reward: 6.70
               Mean episode length: 65.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 8.54s
                        Total time: 6382.29s
                               ETA: 1013162.7s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.335s, learning 0.270s)
               Value function loss: 0.0552
                    Surrogate loss: -0.0376
             Mean action noise std: 0.76
                       Mean reward: 4.05
               Mean episode length: 66.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 8.60s
                        Total time: 6390.89s
                               ETA: 1012900.4s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.558s, learning 0.182s)
               Value function loss: 0.0560
                    Surrogate loss: -0.0418
             Mean action noise std: 0.76
                       Mean reward: 3.82
               Mean episode length: 61.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 8.74s
                        Total time: 6399.63s
                               ETA: 1012660.4s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.267s, learning 0.224s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0382
             Mean action noise std: 0.76
                       Mean reward: 3.90
               Mean episode length: 65.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 8.49s
                        Total time: 6408.12s
                               ETA: 1012381.7s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.626s, learning 0.195s)
               Value function loss: 0.0513
                    Surrogate loss: -0.0411
             Mean action noise std: 0.76
                       Mean reward: 3.88
               Mean episode length: 61.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 8.82s
                        Total time: 6416.94s
                               ETA: 1012155.9s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.590s, learning 0.177s)
               Value function loss: 0.0568
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 3.63
               Mean episode length: 62.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 8.77s
                        Total time: 6425.71s
                               ETA: 1011922.2s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.558s, learning 0.194s)
               Value function loss: 0.0564
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 4.17
               Mean episode length: 63.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 8.75s
                        Total time: 6434.46s
                               ETA: 1011686.9s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.484s, learning 0.246s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 3.74
               Mean episode length: 59.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 8.73s
                        Total time: 6443.19s
                               ETA: 1011449.0s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.856s, learning 0.294s)
               Value function loss: 0.0630
                    Surrogate loss: -0.0362
             Mean action noise std: 0.76
                       Mean reward: 3.89
               Mean episode length: 63.40
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 9.15s
                        Total time: 6452.34s
                               ETA: 1011277.6s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.305s, learning 0.179s)
               Value function loss: 0.0611
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 4.39
               Mean episode length: 64.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 8.48s
                        Total time: 6460.83s
                               ETA: 1011002.5s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.737s, learning 0.202s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0379
             Mean action noise std: 0.76
                       Mean reward: 3.95
               Mean episode length: 63.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 8.94s
                        Total time: 6469.77s
                               ETA: 1010799.2s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.519s, learning 0.184s)
               Value function loss: 0.0669
                    Surrogate loss: -0.0353
             Mean action noise std: 0.76
                       Mean reward: 4.37
               Mean episode length: 64.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 8.70s
                        Total time: 6478.47s
                               ETA: 1010559.7s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.443s, learning 0.289s)
               Value function loss: 3.9546
                    Surrogate loss: -0.0012
             Mean action noise std: 0.76
                       Mean reward: 4.09
               Mean episode length: 65.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 8.73s
                        Total time: 6487.20s
                               ETA: 1010325.5s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.646s, learning 0.209s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0349
             Mean action noise std: 0.76
                       Mean reward: 4.17
               Mean episode length: 63.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 8.86s
                        Total time: 6496.06s
                               ETA: 1010111.2s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.342s, learning 0.256s)
               Value function loss: 53.8463
                    Surrogate loss: -0.0009
             Mean action noise std: 0.76
                       Mean reward: 4.00
               Mean episode length: 65.03
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 8.60s
                        Total time: 6504.65s
                               ETA: 1009857.6s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.537s, learning 0.304s)
               Value function loss: 0.1324
                    Surrogate loss: -0.0341
             Mean action noise std: 0.76
                       Mean reward: 4.28
               Mean episode length: 63.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 8.84s
                        Total time: 6513.50s
                               ETA: 1009642.6s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1761 steps/s (collection: 9.030s, learning 0.273s)
               Value function loss: 0.1155
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 4.19
               Mean episode length: 63.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 9.30s
                        Total time: 6522.80s
                               ETA: 1009499.7s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.431s, learning 0.270s)
               Value function loss: 0.0980
                    Surrogate loss: -0.0379
             Mean action noise std: 0.76
                       Mean reward: 4.01
               Mean episode length: 61.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 8.70s
                        Total time: 6531.50s
                               ETA: 1009264.0s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.379s, learning 0.319s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0407
             Mean action noise std: 0.76
                       Mean reward: 4.37
               Mean episode length: 65.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 8.70s
                        Total time: 6540.20s
                               ETA: 1009028.6s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.557s, learning 0.214s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0387
             Mean action noise std: 0.76
                       Mean reward: 4.42
               Mean episode length: 62.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 8.77s
                        Total time: 6548.97s
                               ETA: 1008805.2s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.739s, learning 0.274s)
               Value function loss: 0.0811
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 4.21
               Mean episode length: 63.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 9.01s
                        Total time: 6557.98s
                               ETA: 1008619.7s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.695s, learning 0.272s)
               Value function loss: 0.0949
                    Surrogate loss: -0.0355
             Mean action noise std: 0.76
                       Mean reward: 4.49
               Mean episode length: 64.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 8.97s
                        Total time: 6566.95s
                               ETA: 1008427.7s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.706s, learning 0.267s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0362
             Mean action noise std: 0.76
                       Mean reward: 4.61
               Mean episode length: 61.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 8.97s
                        Total time: 6575.92s
                               ETA: 1008237.0s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.545s, learning 0.247s)
               Value function loss: 0.0868
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 4.19
               Mean episode length: 63.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 8.79s
                        Total time: 6584.71s
                               ETA: 1008019.3s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.661s, learning 0.279s)
               Value function loss: 0.0976
                    Surrogate loss: -0.0357
             Mean action noise std: 0.76
                       Mean reward: 4.48
               Mean episode length: 63.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 8.94s
                        Total time: 6593.65s
                               ETA: 1007824.8s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1820 steps/s (collection: 8.796s, learning 0.203s)
               Value function loss: 0.1044
                    Surrogate loss: -0.0369
             Mean action noise std: 0.76
                       Mean reward: 5.10
               Mean episode length: 65.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 9.00s
                        Total time: 6602.65s
                               ETA: 1007639.9s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1786 steps/s (collection: 8.967s, learning 0.205s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 4.66
               Mean episode length: 65.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 9.17s
                        Total time: 6611.83s
                               ETA: 1007482.1s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.662s, learning 0.181s)
               Value function loss: 0.0946
                    Surrogate loss: -0.0339
             Mean action noise std: 0.76
                       Mean reward: 4.62
               Mean episode length: 62.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 8.84s
                        Total time: 6620.67s
                               ETA: 1007274.5s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.817s, learning 0.187s)
               Value function loss: 0.0935
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 4.22
               Mean episode length: 62.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 9.00s
                        Total time: 6629.67s
                               ETA: 1007092.0s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.630s, learning 0.179s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0367
             Mean action noise std: 0.76
                       Mean reward: 4.53
               Mean episode length: 62.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 8.81s
                        Total time: 6638.48s
                               ETA: 1006880.4s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.717s, learning 0.177s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 4.65
               Mean episode length: 61.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 8.89s
                        Total time: 6647.38s
                               ETA: 1006682.3s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.585s, learning 0.222s)
               Value function loss: 155.0008
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 4.22
               Mean episode length: 62.61
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 8.81s
                        Total time: 6656.18s
                               ETA: 1006471.6s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1804 steps/s (collection: 8.827s, learning 0.253s)
               Value function loss: 132.0154
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 4.71
               Mean episode length: 62.24
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 9.08s
                        Total time: 6665.26s
                               ETA: 1006302.7s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.574s, learning 0.200s)
               Value function loss: 0.6731
                    Surrogate loss: -0.0355
             Mean action noise std: 0.76
                       Mean reward: 11.99
               Mean episode length: 63.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 8.77s
                        Total time: 6674.04s
                               ETA: 1006088.2s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1826 steps/s (collection: 8.767s, learning 0.201s)
               Value function loss: 0.3137
                    Surrogate loss: -0.0394
             Mean action noise std: 0.76
                       Mean reward: 4.34
               Mean episode length: 58.74
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 8.97s
                        Total time: 6683.00s
                               ETA: 1005903.5s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1866 steps/s (collection: 8.596s, learning 0.182s)
               Value function loss: 46.2395
                    Surrogate loss: 0.0009
             Mean action noise std: 0.76
                       Mean reward: 9.59
               Mean episode length: 61.81
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 8.78s
                        Total time: 6691.78s
                               ETA: 1005690.8s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.261s, learning 0.180s)
               Value function loss: 0.3058
                    Surrogate loss: -0.0380
             Mean action noise std: 0.76
                       Mean reward: 12.53
               Mean episode length: 60.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 8.44s
                        Total time: 6700.22s
                               ETA: 1005428.2s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.457s, learning 0.211s)
               Value function loss: 10.0309
                    Surrogate loss: 0.0012
             Mean action noise std: 0.76
                       Mean reward: 4.75
               Mean episode length: 60.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 8.67s
                        Total time: 6708.89s
                               ETA: 1005200.3s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1814 steps/s (collection: 8.825s, learning 0.205s)
               Value function loss: 232.3824
                    Surrogate loss: -0.0014
             Mean action noise std: 0.76
                       Mean reward: 4.05
               Mean episode length: 53.70
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 9.03s
                        Total time: 6717.92s
                               ETA: 1005027.2s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.507s, learning 0.209s)
               Value function loss: 39.0604
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 4.38
               Mean episode length: 60.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 8.72s
                        Total time: 6726.64s
                               ETA: 1004807.9s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.584s, learning 0.177s)
               Value function loss: 69.1731
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 9.90
               Mean episode length: 58.19
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 8.76s
                        Total time: 6735.40s
                               ETA: 1004595.9s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.627s, learning 0.180s)
               Value function loss: 1.8484
                    Surrogate loss: -0.0250
             Mean action noise std: 0.76
                       Mean reward: 4.20
               Mean episode length: 59.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 8.81s
                        Total time: 6744.21s
                               ETA: 1004391.2s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.659s, learning 0.187s)
               Value function loss: 59.4322
                    Surrogate loss: 0.0006
             Mean action noise std: 0.76
                       Mean reward: 4.57
               Mean episode length: 58.69
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 8.85s
                        Total time: 6753.05s
                               ETA: 1004193.0s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.650s, learning 0.189s)
               Value function loss: 0.6382
                    Surrogate loss: -0.0235
             Mean action noise std: 0.76
                       Mean reward: 9.31
               Mean episode length: 60.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 8.84s
                        Total time: 6761.89s
                               ETA: 1003994.2s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1842 steps/s (collection: 8.653s, learning 0.239s)
               Value function loss: 18.2699
                    Surrogate loss: 0.0037
             Mean action noise std: 0.76
                       Mean reward: 4.03
               Mean episode length: 59.52
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 8.89s
                        Total time: 6770.78s
                               ETA: 1003804.0s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.323s, learning 0.167s)
               Value function loss: 0.3910
                    Surrogate loss: -0.0284
             Mean action noise std: 0.76
                       Mean reward: 4.11
               Mean episode length: 60.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 8.49s
                        Total time: 6779.27s
                               ETA: 1003554.8s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.557s, learning 0.180s)
               Value function loss: 39.0075
                    Surrogate loss: 0.0010
             Mean action noise std: 0.76
                       Mean reward: 4.13
               Mean episode length: 59.75
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 8.74s
                        Total time: 6788.01s
                               ETA: 1003342.8s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1863 steps/s (collection: 8.611s, learning 0.183s)
               Value function loss: 17.6898
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 4.34
               Mean episode length: 59.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 8.79s
                        Total time: 6796.81s
                               ETA: 1003139.9s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.570s, learning 0.217s)
               Value function loss: 17.9967
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 9.24
               Mean episode length: 61.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 8.79s
                        Total time: 6805.59s
                               ETA: 1002936.4s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.373s, learning 0.189s)
               Value function loss: 0.3340
                    Surrogate loss: -0.0183
             Mean action noise std: 0.76
                       Mean reward: 3.90
               Mean episode length: 58.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 8.56s
                        Total time: 6814.16s
                               ETA: 1002700.5s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.724s, learning 0.195s)
               Value function loss: 0.2915
                    Surrogate loss: -0.0194
             Mean action noise std: 0.76
                       Mean reward: 3.79
               Mean episode length: 60.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 8.92s
                        Total time: 6823.07s
                               ETA: 1002517.6s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.560s, learning 0.179s)
               Value function loss: 0.2224
                    Surrogate loss: -0.0208
             Mean action noise std: 0.76
                       Mean reward: 3.90
               Mean episode length: 61.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 8.74s
                        Total time: 6831.81s
                               ETA: 1002308.8s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.388s, learning 0.186s)
               Value function loss: 0.2157
                    Surrogate loss: -0.0193
             Mean action noise std: 0.76
                       Mean reward: 3.79
               Mean episode length: 62.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 8.57s
                        Total time: 6840.39s
                               ETA: 1002076.5s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.812s, learning 0.172s)
               Value function loss: 0.1931
                    Surrogate loss: -0.0286
             Mean action noise std: 0.76
                       Mean reward: 3.54
               Mean episode length: 58.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 8.98s
                        Total time: 6849.37s
                               ETA: 1001904.8s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.687s, learning 0.187s)
               Value function loss: 0.1634
                    Surrogate loss: -0.0241
             Mean action noise std: 0.76
                       Mean reward: 3.71
               Mean episode length: 60.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 8.87s
                        Total time: 6858.25s
                               ETA: 1001717.5s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.550s, learning 0.182s)
               Value function loss: 0.1368
                    Surrogate loss: -0.0311
             Mean action noise std: 0.76
                       Mean reward: 3.70
               Mean episode length: 63.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 8.73s
                        Total time: 6866.98s
                               ETA: 1001510.0s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.810s, learning 0.173s)
               Value function loss: 131.2918
                    Surrogate loss: -0.0015
             Mean action noise std: 0.76
                       Mean reward: 3.46
               Mean episode length: 60.03
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 8.98s
                        Total time: 6875.96s
                               ETA: 1001339.6s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.514s, learning 0.190s)
               Value function loss: 0.2327
                    Surrogate loss: -0.0295
             Mean action noise std: 0.76
                       Mean reward: 3.47
               Mean episode length: 58.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 8.70s
                        Total time: 6884.67s
                               ETA: 1001129.2s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.606s, learning 0.236s)
               Value function loss: 306.6296
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 3.78
               Mean episode length: 62.29
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 8.84s
                        Total time: 6893.51s
                               ETA: 1000939.4s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.807s, learning 0.210s)
               Value function loss: 51.2111
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 3.42
               Mean episode length: 59.65
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 9.02s
                        Total time: 6902.52s
                               ETA: 1000775.3s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.638s, learning 0.246s)
               Value function loss: 0.8429
                    Surrogate loss: -0.0265
             Mean action noise std: 0.76
                       Mean reward: 3.60
               Mean episode length: 61.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 8.88s
                        Total time: 6911.41s
                               ETA: 1000592.6s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.413s, learning 0.181s)
               Value function loss: 159.9223
                    Surrogate loss: -0.0018
             Mean action noise std: 0.76
                       Mean reward: 13.52
               Mean episode length: 57.42
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 8.59s
                        Total time: 6920.00s
                               ETA: 1000368.4s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.183s, learning 0.170s)
               Value function loss: 0.4204
                    Surrogate loss: -0.0276
             Mean action noise std: 0.76
                       Mean reward: 3.57
               Mean episode length: 57.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 8.35s
                        Total time: 6928.35s
                               ETA: 1000110.0s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.144s, learning 0.177s)
               Value function loss: 17.7372
                    Surrogate loss: 0.0019
             Mean action noise std: 0.76
                       Mean reward: 3.46
               Mean episode length: 59.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 8.32s
                        Total time: 6936.68s
                               ETA: 999847.9s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.469s, learning 0.199s)
               Value function loss: 0.2019
                    Surrogate loss: -0.0340
             Mean action noise std: 0.76
                       Mean reward: 3.54
               Mean episode length: 60.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 8.67s
                        Total time: 6945.35s
                               ETA: 999636.5s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.435s, learning 0.185s)
               Value function loss: 13.5110
                    Surrogate loss: 0.0003
             Mean action noise std: 0.76
                       Mean reward: 3.47
               Mean episode length: 60.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 8.62s
                        Total time: 6953.97s
                               ETA: 999418.7s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.503s, learning 0.180s)
               Value function loss: 0.2382
                    Surrogate loss: -0.0325
             Mean action noise std: 0.76
                       Mean reward: 3.70
               Mean episode length: 61.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 8.68s
                        Total time: 6962.65s
                               ETA: 999210.5s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.474s, learning 0.182s)
               Value function loss: 15.9399
                    Surrogate loss: -0.0003
             Mean action noise std: 0.76
                       Mean reward: 3.51
               Mean episode length: 61.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 8.66s
                        Total time: 6971.30s
                               ETA: 998999.0s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.526s, learning 0.175s)
               Value function loss: 46.1874
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 3.46
               Mean episode length: 60.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 8.70s
                        Total time: 6980.01s
                               ETA: 998794.5s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.454s, learning 0.176s)
               Value function loss: 1.5817
                    Surrogate loss: -0.0210
             Mean action noise std: 0.76
                       Mean reward: 3.37
               Mean episode length: 58.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 8.63s
                        Total time: 6988.64s
                               ETA: 998580.4s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.606s, learning 0.180s)
               Value function loss: 0.3597
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 6.06
               Mean episode length: 57.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 8.79s
                        Total time: 6997.42s
                               ETA: 998389.3s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.565s, learning 0.179s)
               Value function loss: 39.8291
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 3.31
               Mean episode length: 58.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 8.74s
                        Total time: 7006.17s
                               ETA: 998192.6s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.397s, learning 0.173s)
               Value function loss: 0.3129
                    Surrogate loss: -0.0243
             Mean action noise std: 0.76
                       Mean reward: 3.51
               Mean episode length: 61.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 8.57s
                        Total time: 7014.74s
                               ETA: 997971.7s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.301s, learning 0.181s)
               Value function loss: 0.1852
                    Surrogate loss: -0.0244
             Mean action noise std: 0.76
                       Mean reward: 3.48
               Mean episode length: 61.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 8.48s
                        Total time: 7023.22s
                               ETA: 997739.0s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1830 steps/s (collection: 8.736s, learning 0.216s)
               Value function loss: 98.7092
                    Surrogate loss: -0.0022
             Mean action noise std: 0.76
                       Mean reward: 3.33
               Mean episode length: 57.24
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 8.95s
                        Total time: 7032.17s
                               ETA: 997573.5s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.355s, learning 0.183s)
               Value function loss: 17.9146
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 3.38
               Mean episode length: 60.59
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 8.54s
                        Total time: 7040.71s
                               ETA: 997349.8s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.242s, learning 0.200s)
               Value function loss: 0.3482
                    Surrogate loss: -0.0319
             Mean action noise std: 0.76
                       Mean reward: 3.59
               Mean episode length: 59.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 8.44s
                        Total time: 7049.15s
                               ETA: 997113.2s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1042 steps/s (collection: 15.531s, learning 0.181s)
               Value function loss: 0.1984
                    Surrogate loss: -0.0321
             Mean action noise std: 0.76
                       Mean reward: 3.60
               Mean episode length: 64.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 15.71s
                        Total time: 7064.86s
                               ETA: 997904.0s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.452s, learning 0.211s)
               Value function loss: 0.1299
                    Surrogate loss: -0.0329
             Mean action noise std: 0.76
                       Mean reward: 3.46
               Mean episode length: 58.72
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 16.66s
                        Total time: 7081.52s
                               ETA: 998826.8s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.157s, learning 0.178s)
               Value function loss: 0.1302
                    Surrogate loss: -0.0331
             Mean action noise std: 0.76
                       Mean reward: 3.49
               Mean episode length: 62.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 16.33s
                        Total time: 7097.86s
                               ETA: 999700.6s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.377s, learning 0.176s)
               Value function loss: 0.1183
                    Surrogate loss: -0.0323
             Mean action noise std: 0.76
                       Mean reward: 3.47
               Mean episode length: 58.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 16.55s
                        Total time: 7114.41s
                               ETA: 1000602.8s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.412s, learning 0.208s)
               Value function loss: 0.0984
                    Surrogate loss: -0.0339
             Mean action noise std: 0.76
                       Mean reward: 3.57
               Mean episode length: 58.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 16.62s
                        Total time: 7131.03s
                               ETA: 1001511.6s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.191s, learning 0.206s)
               Value function loss: 29.6481
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 3.80
               Mean episode length: 62.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 16.40s
                        Total time: 7147.43s
                               ETA: 1002386.6s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.936s, learning 0.167s)
               Value function loss: 60.8603
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 8.70
               Mean episode length: 58.09
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 17.10s
                        Total time: 7164.53s
                               ETA: 1003357.8s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.561s, learning 0.207s)
               Value function loss: 2.3202
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 3.63
               Mean episode length: 60.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 16.77s
                        Total time: 7181.30s
                               ETA: 1004279.5s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.193s, learning 0.227s)
               Value function loss: 0.9013
                    Surrogate loss: -0.0174
             Mean action noise std: 0.76
                       Mean reward: 3.79
               Mean episode length: 59.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 16.42s
                        Total time: 7197.72s
                               ETA: 1005149.9s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.554s, learning 0.169s)
               Value function loss: 30.7235
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 3.72
               Mean episode length: 58.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 16.72s
                        Total time: 7214.44s
                               ETA: 1006060.0s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1013 steps/s (collection: 15.971s, learning 0.190s)
               Value function loss: 17.7900
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 8.87
               Mean episode length: 59.27
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 16.16s
                        Total time: 7230.60s
                               ETA: 1006889.4s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 969 steps/s (collection: 16.617s, learning 0.279s)
               Value function loss: 38.2972
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.71
               Mean episode length: 61.04
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 16.90s
                        Total time: 7247.50s
                               ETA: 1007818.6s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.601s, learning 0.222s)
               Value function loss: 0.5385
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 3.48
               Mean episode length: 57.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 16.82s
                        Total time: 7264.32s
                               ETA: 1008735.0s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.422s, learning 0.262s)
               Value function loss: 0.3270
                    Surrogate loss: -0.0232
             Mean action noise std: 0.75
                       Mean reward: 6.21
               Mean episode length: 58.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 16.68s
                        Total time: 7281.01s
                               ETA: 1009629.6s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.131s, learning 0.185s)
               Value function loss: 0.1727
                    Surrogate loss: -0.0265
             Mean action noise std: 0.75
                       Mean reward: 3.60
               Mean episode length: 57.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 16.32s
                        Total time: 7297.32s
                               ETA: 1010470.6s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.502s, learning 0.278s)
               Value function loss: 0.1873
                    Surrogate loss: -0.0279
             Mean action noise std: 0.75
                       Mean reward: 3.58
               Mean episode length: 58.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 16.78s
                        Total time: 7314.10s
                               ETA: 1011373.4s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1011 steps/s (collection: 15.979s, learning 0.211s)
               Value function loss: 7.2663
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 3.76
               Mean episode length: 60.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 16.19s
                        Total time: 7330.29s
                               ETA: 1012192.1s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.363s, learning 0.216s)
               Value function loss: 0.1567
                    Surrogate loss: -0.0275
             Mean action noise std: 0.75
                       Mean reward: 3.48
               Mean episode length: 56.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 16.58s
                        Total time: 7346.87s
                               ETA: 1013062.3s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.327s, learning 0.202s)
               Value function loss: 131.6132
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 3.50
               Mean episode length: 60.44
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 16.53s
                        Total time: 7363.40s
                               ETA: 1013923.1s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.291s, learning 0.209s)
               Value function loss: 17.6864
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.27
               Mean episode length: 54.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 16.50s
                        Total time: 7379.90s
                               ETA: 1014777.4s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.458s, learning 0.327s)
               Value function loss: 0.3928
                    Surrogate loss: -0.0254
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 57.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 16.78s
                        Total time: 7396.69s
                               ETA: 1015668.5s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.063s, learning 0.170s)
               Value function loss: 0.2077
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 3.42
               Mean episode length: 59.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 16.23s
                        Total time: 7412.92s
                               ETA: 1016481.3s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 961 steps/s (collection: 16.837s, learning 0.210s)
               Value function loss: 0.1593
                    Surrogate loss: -0.0286
             Mean action noise std: 0.75
                       Mean reward: 3.22
               Mean episode length: 55.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 17.05s
                        Total time: 7429.97s
                               ETA: 1017403.3s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 948 steps/s (collection: 17.097s, learning 0.168s)
               Value function loss: 18.0802
                    Surrogate loss: 0.0002
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 55.82
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 17.27s
                        Total time: 7447.23s
                               ETA: 1018352.5s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.428s, learning 0.171s)
               Value function loss: 0.1279
                    Surrogate loss: -0.0349
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 59.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 16.60s
                        Total time: 7463.83s
                               ETA: 1019208.1s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.670s, learning 0.198s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0299
             Mean action noise std: 0.75
                       Mean reward: 3.45
               Mean episode length: 59.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 16.87s
                        Total time: 7480.70s
                               ETA: 1020097.9s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 960 steps/s (collection: 16.858s, learning 0.198s)
               Value function loss: 0.1049
                    Surrogate loss: -0.0326
             Mean action noise std: 0.75
                       Mean reward: 3.56
               Mean episode length: 59.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 17.06s
                        Total time: 7497.75s
                               ETA: 1021011.0s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.435s, learning 0.213s)
               Value function loss: 0.1238
                    Surrogate loss: -0.0264
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 55.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 16.65s
                        Total time: 7514.40s
                               ETA: 1021866.0s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.068s, learning 0.178s)
               Value function loss: 80.6035
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 55.60
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 16.25s
                        Total time: 7530.65s
                               ETA: 1022664.0s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 965 steps/s (collection: 16.731s, learning 0.233s)
               Value function loss: 284.2369
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 3.38
               Mean episode length: 57.73
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 16.96s
                        Total time: 7547.61s
                               ETA: 1023557.2s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.358s, learning 0.219s)
               Value function loss: 5.6905
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 3.47
               Mean episode length: 59.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 16.58s
                        Total time: 7564.19s
                               ETA: 1024395.4s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.316s, learning 0.196s)
               Value function loss: 1.3297
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 3.44
               Mean episode length: 55.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 16.51s
                        Total time: 7580.70s
                               ETA: 1025222.6s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.143s, learning 0.204s)
               Value function loss: 0.4650
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 16.47
               Mean episode length: 56.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 16.35s
                        Total time: 7597.05s
                               ETA: 1026025.2s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.555s, learning 0.194s)
               Value function loss: 0.2527
                    Surrogate loss: -0.0278
             Mean action noise std: 0.75
                       Mean reward: 3.52
               Mean episode length: 58.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 16.75s
                        Total time: 7613.80s
                               ETA: 1026879.7s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.533s, learning 0.218s)
               Value function loss: 16.6974
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 3.39
               Mean episode length: 58.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 16.75s
                        Total time: 7630.55s
                               ETA: 1027732.1s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.497s, learning 0.179s)
               Value function loss: 13.4392
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.53
               Mean episode length: 55.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 16.68s
                        Total time: 7647.22s
                               ETA: 1028572.0s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.137s, learning 0.284s)
               Value function loss: 59.7470
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 3.52
               Mean episode length: 57.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 16.42s
                        Total time: 7663.64s
                               ETA: 1029375.6s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1105 steps/s (collection: 14.636s, learning 0.184s)
               Value function loss: 0.3317
                    Surrogate loss: -0.0301
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 55.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 14.82s
                        Total time: 7678.46s
                               ETA: 1029962.1s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.505s, learning 0.195s)
               Value function loss: 0.2189
                    Surrogate loss: -0.0322
             Mean action noise std: 0.75
                       Mean reward: 3.56
               Mean episode length: 55.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 8.70s
                        Total time: 7687.16s
                               ETA: 1029727.2s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.307s, learning 0.182s)
               Value function loss: 0.1939
                    Surrogate loss: -0.0272
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 56.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 8.49s
                        Total time: 7695.65s
                               ETA: 1029464.6s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.681s, learning 0.189s)
               Value function loss: 33.7254
                    Surrogate loss: -0.0004
             Mean action noise std: 0.75
                       Mean reward: 3.39
               Mean episode length: 52.65
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 8.87s
                        Total time: 7704.52s
                               ETA: 1029253.7s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.581s, learning 0.184s)
               Value function loss: 52.5639
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 53.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 8.77s
                        Total time: 7713.29s
                               ETA: 1029029.3s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.693s, learning 0.230s)
               Value function loss: 4.0036
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 3.42
               Mean episode length: 55.99
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 8.92s
                        Total time: 7722.21s
                               ETA: 1028826.5s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.458s, learning 0.189s)
               Value function loss: 0.1537
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 6.19
               Mean episode length: 57.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 8.65s
                        Total time: 7730.86s
                               ETA: 1028587.4s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.333s, learning 0.197s)
               Value function loss: 279.0268
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 56.64
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 8.53s
                        Total time: 7739.39s
                               ETA: 1028333.5s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1843 steps/s (collection: 8.486s, learning 0.401s)
               Value function loss: 0.1850
                    Surrogate loss: -0.0346
             Mean action noise std: 0.75
                       Mean reward: 3.49
               Mean episode length: 57.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 8.89s
                        Total time: 7748.27s
                               ETA: 1028127.7s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.349s, learning 0.178s)
               Value function loss: 0.1403
                    Surrogate loss: -0.0332
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 56.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 8.53s
                        Total time: 7756.80s
                               ETA: 1027874.6s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.517s, learning 0.193s)
               Value function loss: 0.1098
                    Surrogate loss: -0.0298
             Mean action noise std: 0.75
                       Mean reward: 3.43
               Mean episode length: 54.98
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 8.71s
                        Total time: 7765.51s
                               ETA: 1027646.5s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.478s, learning 0.198s)
               Value function loss: 29.7857
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 3.41
               Mean episode length: 55.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 8.68s
                        Total time: 7774.19s
                               ETA: 1027414.3s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.319s, learning 0.191s)
               Value function loss: 0.1175
                    Surrogate loss: -0.0334
             Mean action noise std: 0.75
                       Mean reward: 3.36
               Mean episode length: 54.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 8.51s
                        Total time: 7782.70s
                               ETA: 1027160.9s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1879 steps/s (collection: 8.535s, learning 0.181s)
               Value function loss: 0.0998
                    Surrogate loss: -0.0320
             Mean action noise std: 0.75
                       Mean reward: 3.64
               Mean episode length: 56.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 8.72s
                        Total time: 7791.41s
                               ETA: 1026935.3s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.310s, learning 0.323s)
               Value function loss: 0.0995
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 3.45
               Mean episode length: 56.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 8.63s
                        Total time: 7800.05s
                               ETA: 1026699.3s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.450s, learning 0.197s)
               Value function loss: 150.8318
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 3.47
               Mean episode length: 55.79
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 8.65s
                        Total time: 7808.69s
                               ETA: 1026465.8s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.490s, learning 0.291s)
               Value function loss: 321.6071
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 23.94
               Mean episode length: 57.14
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 8.78s
                        Total time: 7817.48s
                               ETA: 1026250.5s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.358s, learning 0.240s)
               Value function loss: 1.4134
                    Surrogate loss: -0.0294
             Mean action noise std: 0.75
                       Mean reward: 3.77
               Mean episode length: 53.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 8.60s
                        Total time: 7826.07s
                               ETA: 1026011.7s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.357s, learning 0.208s)
               Value function loss: 0.8111
                    Surrogate loss: -0.0286
             Mean action noise std: 0.75
                       Mean reward: 8.54
               Mean episode length: 56.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 8.56s
                        Total time: 7834.64s
                               ETA: 1025769.2s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.673s, learning 0.199s)
               Value function loss: 14.0422
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 3.51
               Mean episode length: 55.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 8.87s
                        Total time: 7843.51s
                               ETA: 1025567.5s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.692s, learning 0.180s)
               Value function loss: 0.4466
                    Surrogate loss: -0.0295
             Mean action noise std: 0.75
                       Mean reward: 11.51
               Mean episode length: 57.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 8.87s
                        Total time: 7852.38s
                               ETA: 1025366.1s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.672s, learning 0.226s)
               Value function loss: 59.8112
                    Surrogate loss: -0.0004
             Mean action noise std: 0.75
                       Mean reward: 3.88
               Mean episode length: 57.56
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 8.90s
                        Total time: 7861.28s
                               ETA: 1025168.9s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.289s, learning 0.198s)
               Value function loss: 62.5672
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 3.68
               Mean episode length: 56.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 8.49s
                        Total time: 7869.77s
                               ETA: 1024918.5s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.104s, learning 0.181s)
               Value function loss: 6.1447
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 3.62
               Mean episode length: 57.69
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 8.29s
                        Total time: 7878.05s
                               ETA: 1024642.5s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.281s, learning 0.295s)
               Value function loss: 106.1463
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 8.81
               Mean episode length: 57.48
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 8.58s
                        Total time: 7886.63s
                               ETA: 1024404.9s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1815 steps/s (collection: 8.716s, learning 0.309s)
               Value function loss: 244.1315
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 3.75
               Mean episode length: 58.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 9.03s
                        Total time: 7895.65s
                               ETA: 1024226.2s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.636s, learning 0.192s)
               Value function loss: 11.9930
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 3.61
               Mean episode length: 55.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 8.83s
                        Total time: 7904.48s
                               ETA: 1024022.5s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.342s, learning 0.278s)
               Value function loss: 92.3093
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 3.38
               Mean episode length: 56.58
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 8.62s
                        Total time: 7913.10s
                               ETA: 1023792.4s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.291s, learning 0.210s)
               Value function loss: 2.3876
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 56.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 8.50s
                        Total time: 7921.60s
                               ETA: 1023547.3s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1844 steps/s (collection: 8.708s, learning 0.175s)
               Value function loss: 0.7145
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 59.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 8.88s
                        Total time: 7930.49s
                               ETA: 1023352.3s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.411s, learning 0.182s)
               Value function loss: 127.7704
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.50
               Mean episode length: 58.60
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 8.59s
                        Total time: 7939.08s
                               ETA: 1023120.4s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.474s, learning 0.183s)
               Value function loss: 94.4148
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.23
               Mean episode length: 57.95
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 8.66s
                        Total time: 7947.74s
                               ETA: 1022897.3s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.284s, learning 0.234s)
               Value function loss: 0.9124
                    Surrogate loss: -0.0212
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 59.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 8.52s
                        Total time: 7956.25s
                               ETA: 1022656.8s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.584s, learning 0.164s)
               Value function loss: 0.5567
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 13.39
               Mean episode length: 59.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 8.75s
                        Total time: 7965.00s
                               ETA: 1022446.6s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.398s, learning 0.206s)
               Value function loss: 13.7002
                    Surrogate loss: 0.0009
             Mean action noise std: 0.75
                       Mean reward: 3.26
               Mean episode length: 59.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 8.60s
                        Total time: 7973.61s
                               ETA: 1022218.3s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.591s, learning 0.189s)
               Value function loss: 0.2722
                    Surrogate loss: -0.0243
             Mean action noise std: 0.75
                       Mean reward: 3.31
               Mean episode length: 57.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 8.78s
                        Total time: 7982.39s
                               ETA: 1022013.2s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1809 steps/s (collection: 8.707s, learning 0.347s)
               Value function loss: 7.3193
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 59.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 9.05s
                        Total time: 7991.44s
                               ETA: 1021843.6s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1789 steps/s (collection: 8.903s, learning 0.253s)
               Value function loss: 0.1836
                    Surrogate loss: -0.0244
             Mean action noise std: 0.75
                       Mean reward: 3.23
               Mean episode length: 58.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 9.16s
                        Total time: 8000.60s
                               ETA: 1021687.4s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.499s, learning 0.194s)
               Value function loss: 0.1435
                    Surrogate loss: -0.0263
             Mean action noise std: 0.75
                       Mean reward: 3.00
               Mean episode length: 55.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 8.69s
                        Total time: 8009.29s
                               ETA: 1021472.6s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 2033 steps/s (collection: 7.876s, learning 0.181s)
               Value function loss: 0.1537
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 3.31
               Mean episode length: 59.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 8.06s
                        Total time: 8017.35s
                               ETA: 1021177.2s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.521s, learning 0.177s)
               Value function loss: 0.1206
                    Surrogate loss: -0.0269
             Mean action noise std: 0.75
                       Mean reward: 5.79
               Mean episode length: 61.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 8.70s
                        Total time: 8026.04s
                               ETA: 1020964.1s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.429s, learning 0.218s)
               Value function loss: 0.1118
                    Surrogate loss: -0.0252
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 56.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 8.65s
                        Total time: 8034.69s
                               ETA: 1020745.2s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.662s, learning 0.203s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0323
             Mean action noise std: 0.75
                       Mean reward: 3.08
               Mean episode length: 57.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 8.87s
                        Total time: 8043.56s
                               ETA: 1020554.4s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.314s, learning 0.276s)
               Value function loss: 0.0832
                    Surrogate loss: -0.0307
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 56.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 8.59s
                        Total time: 8052.15s
                               ETA: 1020329.3s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.341s, learning 0.173s)
               Value function loss: 0.0705
                    Surrogate loss: -0.0311
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 62.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 8.51s
                        Total time: 8060.66s
                               ETA: 1020095.0s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.405s, learning 0.167s)
               Value function loss: 0.0575
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 58.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 8.57s
                        Total time: 8069.23s
                               ETA: 1019868.7s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.294s, learning 0.276s)
               Value function loss: 0.0634
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 58.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 8.57s
                        Total time: 8077.80s
                               ETA: 1019642.6s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.314s, learning 0.185s)
               Value function loss: 0.0685
                    Surrogate loss: -0.0320
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 58.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 8.50s
                        Total time: 8086.30s
                               ETA: 1019408.2s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.772s, learning 0.207s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0321
             Mean action noise std: 0.75
                       Mean reward: 2.99
               Mean episode length: 58.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 8.98s
                        Total time: 8095.28s
                               ETA: 1019234.8s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.419s, learning 0.246s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0318
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 58.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 8.67s
                        Total time: 8103.95s
                               ETA: 1019022.3s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.636s, learning 0.203s)
               Value function loss: 16.7709
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 55.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 8.84s
                        Total time: 8112.78s
                               ETA: 1018832.2s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1790 steps/s (collection: 8.817s, learning 0.334s)
               Value function loss: 0.0725
                    Surrogate loss: -0.0389
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 57.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 9.15s
                        Total time: 8121.94s
                               ETA: 1018681.7s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.432s, learning 0.194s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0358
             Mean action noise std: 0.75
                       Mean reward: 6.24
               Mean episode length: 58.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 8.63s
                        Total time: 8130.56s
                               ETA: 1018465.7s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.376s, learning 0.242s)
               Value function loss: 0.0752
                    Surrogate loss: -0.0384
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 53.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 8.62s
                        Total time: 8139.18s
                               ETA: 1018249.4s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1835 steps/s (collection: 8.535s, learning 0.393s)
               Value function loss: 0.0837
                    Surrogate loss: -0.0342
             Mean action noise std: 0.75
                       Mean reward: 3.63
               Mean episode length: 59.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 8.93s
                        Total time: 8148.11s
                               ETA: 1018072.2s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.657s, learning 0.164s)
               Value function loss: 0.0668
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 3.57
               Mean episode length: 56.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 8.82s
                        Total time: 8156.93s
                               ETA: 1017882.2s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.463s, learning 0.206s)
               Value function loss: 0.0661
                    Surrogate loss: -0.0393
             Mean action noise std: 0.75
                       Mean reward: 3.38
               Mean episode length: 57.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 8.67s
                        Total time: 8165.60s
                               ETA: 1017673.6s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.509s, learning 0.204s)
               Value function loss: 132.3967
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.67
               Mean episode length: 56.27
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 8.71s
                        Total time: 8174.31s
                               ETA: 1017471.0s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.309s, learning 0.211s)
               Value function loss: 0.1138
                    Surrogate loss: -0.0339
             Mean action noise std: 0.75
                       Mean reward: 3.59
               Mean episode length: 55.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 8.52s
                        Total time: 8182.83s
                               ETA: 1017245.0s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.563s, learning 0.185s)
               Value function loss: 0.1039
                    Surrogate loss: -0.0304
             Mean action noise std: 0.75
                       Mean reward: 3.57
               Mean episode length: 54.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 8.75s
                        Total time: 8191.58s
                               ETA: 1017047.7s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.170s, learning 0.196s)
               Value function loss: 17.9213
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 14.25
               Mean episode length: 60.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 8.37s
                        Total time: 8199.95s
                               ETA: 1016803.7s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.232s, learning 0.177s)
               Value function loss: 0.1007
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 3.41
               Mean episode length: 55.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 8.41s
                        Total time: 8208.36s
                               ETA: 1016565.4s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.487s, learning 0.195s)
               Value function loss: 59.9177
                    Surrogate loss: -0.0003
             Mean action noise std: 0.75
                       Mean reward: 3.96
               Mean episode length: 59.25
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 8.68s
                        Total time: 8217.04s
                               ETA: 1016361.6s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1819 steps/s (collection: 8.804s, learning 0.199s)
               Value function loss: 63.5652
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 3.83
               Mean episode length: 56.59
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 9.00s
                        Total time: 8226.04s
                               ETA: 1016197.8s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.438s, learning 0.179s)
               Value function loss: 0.2117
                    Surrogate loss: -0.0405
             Mean action noise std: 0.75
                       Mean reward: 3.64
               Mean episode length: 54.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 8.62s
                        Total time: 8234.66s
                               ETA: 1015986.9s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.659s, learning 0.184s)
               Value function loss: 0.1653
                    Surrogate loss: -0.0386
             Mean action noise std: 0.75
                       Mean reward: 3.67
               Mean episode length: 57.12
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 8.84s
                        Total time: 8243.50s
                               ETA: 1015804.3s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.493s, learning 0.210s)
               Value function loss: 0.1396
                    Surrogate loss: -0.0307
             Mean action noise std: 0.75
                       Mean reward: 3.55
               Mean episode length: 54.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 8.70s
                        Total time: 8252.21s
                               ETA: 1015604.8s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.599s, learning 0.229s)
               Value function loss: 63.9742
                    Surrogate loss: -0.0012
             Mean action noise std: 0.75
                       Mean reward: 3.72
               Mean episode length: 56.76
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 8.83s
                        Total time: 8261.03s
                               ETA: 1015421.3s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.393s, learning 0.278s)
               Value function loss: 30.3716
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 4.08
               Mean episode length: 56.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 8.67s
                        Total time: 8269.71s
                               ETA: 1015218.9s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.568s, learning 0.195s)
               Value function loss: 214.9977
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 3.88
               Mean episode length: 57.15
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 8.76s
                        Total time: 8278.47s
                               ETA: 1015028.2s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1792 steps/s (collection: 8.961s, learning 0.179s)
               Value function loss: 55.4220
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 3.64
               Mean episode length: 58.04
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 9.14s
                        Total time: 8287.61s
                               ETA: 1014884.2s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.607s, learning 0.289s)
               Value function loss: 228.7264
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 9.13
               Mean episode length: 56.08
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 8.90s
                        Total time: 8296.51s
                               ETA: 1014710.7s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1840 steps/s (collection: 8.720s, learning 0.184s)
               Value function loss: 4.3685
                    Surrogate loss: -0.0276
             Mean action noise std: 0.75
                       Mean reward: 3.70
               Mean episode length: 55.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 8.90s
                        Total time: 8305.41s
                               ETA: 1014538.4s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.644s, learning 0.215s)
               Value function loss: 348.3327
                    Surrogate loss: 0.0031
             Mean action noise std: 0.75
                       Mean reward: 3.95
               Mean episode length: 57.00
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 8.86s
                        Total time: 8314.27s
                               ETA: 1014361.1s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1823 steps/s (collection: 8.806s, learning 0.176s)
               Value function loss: 2.8527
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 13.89
               Mean episode length: 57.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 8.98s
                        Total time: 8323.25s
                               ETA: 1014199.3s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.293s, learning 0.259s)
               Value function loss: 67.1618
                    Surrogate loss: 0.0015
             Mean action noise std: 0.75
                       Mean reward: 3.54
               Mean episode length: 56.35
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 8.55s
                        Total time: 8331.80s
                               ETA: 1013985.4s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1841 steps/s (collection: 8.677s, learning 0.221s)
               Value function loss: 228.8468
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 11.46
               Mean episode length: 56.50
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 8.90s
                        Total time: 8340.70s
                               ETA: 1013814.1s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.080s, learning 0.218s)
               Value function loss: 52.3211
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 3.80
               Mean episode length: 57.67
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 8.30s
                        Total time: 8349.00s
                               ETA: 1013570.4s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.142s, learning 0.236s)
               Value function loss: 2.6824
                    Surrogate loss: -0.0249
             Mean action noise std: 0.75
                       Mean reward: 3.71
               Mean episode length: 58.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 8.38s
                        Total time: 8357.38s
                               ETA: 1013337.0s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.731s, learning 0.191s)
               Value function loss: 1.0413
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 3.54
               Mean episode length: 55.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 8.92s
                        Total time: 8366.30s
                               ETA: 1013170.0s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.463s, learning 0.180s)
               Value function loss: 0.8266
                    Surrogate loss: -0.0238
             Mean action noise std: 0.75
                       Mean reward: 3.31
               Mean episode length: 56.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 8.64s
                        Total time: 8374.94s
                               ETA: 1012969.6s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.307s, learning 0.197s)
               Value function loss: 24.3891
                    Surrogate loss: -0.0001
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 55.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 8.50s
                        Total time: 8383.44s
                               ETA: 1012752.8s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.301s, learning 0.328s)
               Value function loss: 69.3867
                    Surrogate loss: -0.0024
             Mean action noise std: 0.75
                       Mean reward: 8.47
               Mean episode length: 55.03
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 8.63s
                        Total time: 8392.07s
                               ETA: 1012551.7s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.288s, learning 0.185s)
               Value function loss: 7.3169
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 57.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 8.47s
                        Total time: 8400.55s
                               ETA: 1012332.3s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.530s, learning 0.174s)
               Value function loss: 0.4464
                    Surrogate loss: -0.0272
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 54.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 8.70s
                        Total time: 8409.25s
                               ETA: 1012141.2s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.571s, learning 0.185s)
               Value function loss: 0.3052
                    Surrogate loss: -0.0243
             Mean action noise std: 0.75
                       Mean reward: 3.33
               Mean episode length: 56.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 8.76s
                        Total time: 8418.01s
                               ETA: 1011956.8s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.401s, learning 0.205s)
               Value function loss: 57.5407
                    Surrogate loss: -0.0012
             Mean action noise std: 0.75
                       Mean reward: 3.06
               Mean episode length: 56.00
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 8.61s
                        Total time: 8426.61s
                               ETA: 1011754.7s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.444s, learning 0.222s)
               Value function loss: 448.4152
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 56.70
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 8.67s
                        Total time: 8435.28s
                               ETA: 1011560.4s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1784 steps/s (collection: 8.845s, learning 0.335s)
               Value function loss: 11.1514
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 59.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 9.18s
                        Total time: 8444.46s
                               ETA: 1011428.0s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1846 steps/s (collection: 8.678s, learning 0.196s)
               Value function loss: 33.6607
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 3.63
               Mean episode length: 59.74
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 8.87s
                        Total time: 8453.33s
                               ETA: 1011259.4s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.210s, learning 0.212s)
               Value function loss: 153.2718
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 3.22
               Mean episode length: 56.51
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 8.42s
                        Total time: 8461.76s
                               ETA: 1011037.2s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1816 steps/s (collection: 8.807s, learning 0.210s)
               Value function loss: 43.8297
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 56.57
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 9.02s
                        Total time: 8470.77s
                               ETA: 1010886.5s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.666s, learning 0.188s)
               Value function loss: 318.1916
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 3.27
               Mean episode length: 56.30
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 8.85s
                        Total time: 8479.63s
                               ETA: 1010716.7s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.261s, learning 0.165s)
               Value function loss: 17.2641
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 3.06
               Mean episode length: 56.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 8.43s
                        Total time: 8488.05s
                               ETA: 1010496.3s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.092s, learning 0.186s)
               Value function loss: 3.9461
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 3.29
               Mean episode length: 58.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 8.28s
                        Total time: 8496.33s
                               ETA: 1010258.8s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.576s, learning 0.184s)
               Value function loss: 1.0147
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 56.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 8.76s
                        Total time: 8505.09s
                               ETA: 1010079.0s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.565s, learning 0.297s)
               Value function loss: 0.5444
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 3.28
               Mean episode length: 58.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 8.86s
                        Total time: 8513.95s
                               ETA: 1009911.7s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.146s, learning 0.184s)
               Value function loss: 47.4452
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: 5.75
               Mean episode length: 56.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 8.33s
                        Total time: 8522.28s
                               ETA: 1009681.8s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1855 steps/s (collection: 8.646s, learning 0.185s)
               Value function loss: 0.2769
                    Surrogate loss: -0.0213
             Mean action noise std: 0.75
                       Mean reward: 3.00
               Mean episode length: 56.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 8.83s
                        Total time: 8531.11s
                               ETA: 1009511.8s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.624s, learning 0.197s)
               Value function loss: 0.2245
                    Surrogate loss: -0.0245
             Mean action noise std: 0.75
                       Mean reward: 2.99
               Mean episode length: 52.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 8.82s
                        Total time: 8539.94s
                               ETA: 1009341.0s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.667s, learning 0.257s)
               Value function loss: 0.1504
                    Surrogate loss: -0.0264
             Mean action noise std: 0.75
                       Mean reward: 8.39
               Mean episode length: 56.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 8.92s
                        Total time: 8548.86s
                               ETA: 1009182.6s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.483s, learning 0.182s)
               Value function loss: 0.1584
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 2.75
               Mean episode length: 53.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 8.67s
                        Total time: 8557.52s
                               ETA: 1008994.2s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.570s, learning 0.181s)
               Value function loss: 0.1034
                    Surrogate loss: -0.0226
             Mean action noise std: 0.75
                       Mean reward: 2.87
               Mean episode length: 53.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 8.75s
                        Total time: 8566.28s
                               ETA: 1008816.3s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.134s, learning 0.209s)
               Value function loss: 0.0815
                    Surrogate loss: -0.0245
             Mean action noise std: 0.75
                       Mean reward: 2.96
               Mean episode length: 55.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 8.34s
                        Total time: 8574.62s
                               ETA: 1008590.8s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.200s, learning 0.177s)
               Value function loss: 0.0872
                    Surrogate loss: -0.0253
             Mean action noise std: 0.75
                       Mean reward: 2.92
               Mean episode length: 56.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 8.38s
                        Total time: 8583.00s
                               ETA: 1008369.8s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1831 steps/s (collection: 8.665s, learning 0.281s)
               Value function loss: 64.6691
                    Surrogate loss: 0.0020
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 58.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 8.95s
                        Total time: 8591.94s
                               ETA: 1008216.2s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.479s, learning 0.180s)
               Value function loss: 3.9349
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 10.67
               Mean episode length: 57.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 8.66s
                        Total time: 8600.60s
                               ETA: 1008029.0s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.512s, learning 0.211s)
               Value function loss: 47.6571
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 55.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 8.72s
                        Total time: 8609.32s
                               ETA: 1007849.9s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.385s, learning 0.177s)
               Value function loss: 0.1012
                    Surrogate loss: -0.0279
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 53.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 8.56s
                        Total time: 8617.89s
                               ETA: 1007652.4s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.455s, learning 0.176s)
               Value function loss: 0.1356
                    Surrogate loss: -0.0263
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 58.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 8.63s
                        Total time: 8626.52s
                               ETA: 1007463.3s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.489s, learning 0.243s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0231
             Mean action noise std: 0.75
                       Mean reward: 2.93
               Mean episode length: 55.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 8.73s
                        Total time: 8635.25s
                               ETA: 1007286.5s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.422s, learning 0.187s)
               Value function loss: 0.0839
                    Surrogate loss: -0.0323
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 57.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 8.61s
                        Total time: 8643.86s
                               ETA: 1007095.7s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.400s, learning 0.226s)
               Value function loss: 177.1241
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 3.19
               Mean episode length: 57.33
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 8.63s
                        Total time: 8652.48s
                               ETA: 1006907.3s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.339s, learning 0.175s)
               Value function loss: 123.8407
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 56.31
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 8.51s
                        Total time: 8661.00s
                               ETA: 1006706.4s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.412s, learning 0.172s)
               Value function loss: 134.1085
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 57.05
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 8.58s
                        Total time: 8669.58s
                               ETA: 1006514.1s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1871 steps/s (collection: 8.551s, learning 0.206s)
               Value function loss: 121.7962
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.43
               Mean episode length: 57.69
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 8.76s
                        Total time: 8678.34s
                               ETA: 1006342.2s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.149s, learning 0.234s)
               Value function loss: 64.9129
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 3.18
               Mean episode length: 56.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 8.38s
                        Total time: 8686.72s
                               ETA: 1006127.5s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.453s, learning 0.181s)
               Value function loss: 95.2441
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 3.27
               Mean episode length: 56.18
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 8.63s
                        Total time: 8695.36s
                               ETA: 1005942.2s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.214s, learning 0.185s)
               Value function loss: 517.7381
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 11.33
               Mean episode length: 56.97
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 8.40s
                        Total time: 8703.76s
                               ETA: 1005730.1s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.137s, learning 0.245s)
               Value function loss: 373.9329
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 56.39
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 8.38s
                        Total time: 8712.14s
                               ETA: 1005516.5s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.541s, learning 0.195s)
               Value function loss: 18.3447
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 54.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 8.74s
                        Total time: 8720.87s
                               ETA: 1005344.3s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.346s, learning 0.191s)
               Value function loss: 220.3824
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.57
               Mean episode length: 56.95
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 8.54s
                        Total time: 8729.41s
                               ETA: 1005149.4s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.308s, learning 0.172s)
               Value function loss: 89.4377
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 54.96
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 8.48s
                        Total time: 8737.89s
                               ETA: 1004948.5s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.302s, learning 0.185s)
               Value function loss: 350.6598
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 3.39
               Mean episode length: 56.90
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 8.49s
                        Total time: 8746.38s
                               ETA: 1004748.9s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.462s, learning 0.179s)
               Value function loss: 631.1405
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 16.09
               Mean episode length: 54.41
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 8.64s
                        Total time: 8755.02s
                               ETA: 1004567.4s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.569s, learning 0.191s)
               Value function loss: 106.5365
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 29.09
               Mean episode length: 57.11
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 8.76s
                        Total time: 8763.78s
                               ETA: 1004399.8s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.427s, learning 0.184s)
               Value function loss: 28.2647
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 55.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 8.61s
                        Total time: 8772.39s
                               ETA: 1004215.5s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.657s, learning 0.178s)
               Value function loss: 94.3502
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: 3.63
               Mean episode length: 56.39
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 8.83s
                        Total time: 8781.22s
                               ETA: 1004057.3s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1850 steps/s (collection: 8.677s, learning 0.178s)
               Value function loss: 2.2850
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 3.49
               Mean episode length: 58.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 8.86s
                        Total time: 8790.08s
                               ETA: 1003901.8s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.377s, learning 0.183s)
               Value function loss: 66.5511
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 3.16
               Mean episode length: 57.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 8.56s
                        Total time: 8798.64s
                               ETA: 1003712.9s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1799 steps/s (collection: 8.904s, learning 0.200s)
               Value function loss: 149.1666
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 3.39
               Mean episode length: 56.75
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 9.10s
                        Total time: 8807.74s
                               ETA: 1003586.5s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.438s, learning 0.202s)
               Value function loss: 1.9773
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 3.44
               Mean episode length: 56.18
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 8.64s
                        Total time: 8816.38s
                               ETA: 1003407.5s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.496s, learning 0.206s)
               Value function loss: 47.9715
                    Surrogate loss: 0.0019
             Mean action noise std: 0.75
                       Mean reward: 16.25
               Mean episode length: 56.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 8.70s
                        Total time: 8825.08s
                               ETA: 1003236.0s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.586s, learning 0.177s)
               Value function loss: 14.8180
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 8.86
               Mean episode length: 58.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 8.76s
                        Total time: 8833.85s
                               ETA: 1003071.7s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.476s, learning 0.196s)
               Value function loss: 146.1354
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 3.43
               Mean episode length: 58.93
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 8.67s
                        Total time: 8842.52s
                               ETA: 1002897.5s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1853 steps/s (collection: 8.656s, learning 0.185s)
               Value function loss: 1.7052
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 57.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 8.84s
                        Total time: 8851.36s
                               ETA: 1002742.7s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.530s, learning 0.206s)
               Value function loss: 19.8781
                    Surrogate loss: 0.0019
             Mean action noise std: 0.75
                       Mean reward: 2.97
               Mean episode length: 55.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 8.74s
                        Total time: 8860.10s
                               ETA: 1002576.5s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.390s, learning 0.224s)
               Value function loss: 48.1472
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 2.96
               Mean episode length: 54.15
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 8.61s
                        Total time: 8868.71s
                               ETA: 1002396.9s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.472s, learning 0.199s)
               Value function loss: 64.4621
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 20.70
               Mean episode length: 57.04
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 8.67s
                        Total time: 8877.38s
                               ETA: 1002224.0s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.492s, learning 0.186s)
               Value function loss: 1.7544
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 2.92
               Mean episode length: 58.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 8.68s
                        Total time: 8886.06s
                               ETA: 1002052.3s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.266s, learning 0.181s)
               Value function loss: 0.9920
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 56.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 8.45s
                        Total time: 8894.51s
                               ETA: 1001855.0s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.678s, learning 0.181s)
               Value function loss: 0.6114
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 2.98
               Mean episode length: 59.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 8.86s
                        Total time: 8903.37s
                               ETA: 1001704.4s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1798 steps/s (collection: 8.740s, learning 0.369s)
               Value function loss: 0.5076
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 2.76
               Mean episode length: 55.81
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 9.11s
                        Total time: 8912.47s
                               ETA: 1001582.2s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.709s, learning 0.307s)
               Value function loss: 0.7543
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 2.88
               Mean episode length: 58.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 9.02s
                        Total time: 8921.49s
                               ETA: 1001449.9s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1868 steps/s (collection: 8.588s, learning 0.181s)
               Value function loss: 0.4652
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 2.63
               Mean episode length: 56.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 8.77s
                        Total time: 8930.26s
                               ETA: 1001290.2s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1847 steps/s (collection: 8.671s, learning 0.196s)
               Value function loss: 0.3589
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 2.83
               Mean episode length: 59.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 8.87s
                        Total time: 8939.13s
                               ETA: 1001141.8s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1886 steps/s (collection: 8.454s, learning 0.230s)
               Value function loss: 0.3397
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 2.83
               Mean episode length: 60.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 8.68s
                        Total time: 8947.81s
                               ETA: 1000973.2s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.659s, learning 0.185s)
               Value function loss: 0.3025
                    Surrogate loss: 0.0091
             Mean action noise std: 0.75
                       Mean reward: 2.67
               Mean episode length: 58.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 8.84s
                        Total time: 8956.65s
                               ETA: 1000822.9s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1880 steps/s (collection: 8.531s, learning 0.182s)
               Value function loss: 0.3948
                    Surrogate loss: 0.0036
             Mean action noise std: 0.75
                       Mean reward: 2.98
               Mean episode length: 61.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 8.71s
                        Total time: 8965.37s
                               ETA: 1000658.2s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.453s, learning 0.184s)
               Value function loss: 18.1146
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: 2.94
               Mean episode length: 60.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 8.64s
                        Total time: 8974.00s
                               ETA: 1000485.4s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1794 steps/s (collection: 8.903s, learning 0.226s)
               Value function loss: 47.1618
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 2.69
               Mean episode length: 57.51
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 9.13s
                        Total time: 8983.13s
                               ETA: 1000367.8s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.413s, learning 0.279s)
               Value function loss: 302.7695
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.09
               Mean episode length: 59.50
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 8.69s
                        Total time: 8991.83s
                               ETA: 1000201.8s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.268s, learning 0.175s)
               Value function loss: 217.8838
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 2.80
               Mean episode length: 59.28
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 8.44s
                        Total time: 9000.27s
                               ETA: 1000008.5s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.188s, learning 0.198s)
               Value function loss: 0.3927
                    Surrogate loss: -0.0241
             Mean action noise std: 0.75
                       Mean reward: 2.74
               Mean episode length: 58.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 8.39s
                        Total time: 9008.65s
                               ETA: 999809.3s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.411s, learning 0.181s)
               Value function loss: 0.2641
                    Surrogate loss: -0.0259
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 60.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 8.59s
                        Total time: 9017.25s
                               ETA: 999633.3s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1817 steps/s (collection: 8.830s, learning 0.187s)
               Value function loss: 0.1925
                    Surrogate loss: -0.0250
             Mean action noise std: 0.75
                       Mean reward: 2.94
               Mean episode length: 59.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 9.02s
                        Total time: 9026.26s
                               ETA: 999504.7s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.333s, learning 0.190s)
               Value function loss: 0.1723
                    Surrogate loss: -0.0248
             Mean action noise std: 0.75
                       Mean reward: 2.72
               Mean episode length: 56.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 8.52s
                        Total time: 9034.79s
                               ETA: 999321.9s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.330s, learning 0.194s)
               Value function loss: 0.1300
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 59.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 8.52s
                        Total time: 9043.31s
                               ETA: 999139.5s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.406s, learning 0.327s)
               Value function loss: 0.2066
                    Surrogate loss: -0.0024
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 58.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 8.73s
                        Total time: 9052.04s
                               ETA: 998980.6s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1864 steps/s (collection: 8.522s, learning 0.265s)
               Value function loss: 0.0955
                    Surrogate loss: -0.0265
             Mean action noise std: 0.75
                       Mean reward: 2.95
               Mean episode length: 55.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 8.79s
                        Total time: 9060.83s
                               ETA: 998828.0s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.639s, learning 0.181s)
               Value function loss: 0.0991
                    Surrogate loss: -0.0235
             Mean action noise std: 0.75
                       Mean reward: 2.78
               Mean episode length: 57.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 8.82s
                        Total time: 9069.65s
                               ETA: 998679.3s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.471s, learning 0.200s)
               Value function loss: 0.1030
                    Surrogate loss: -0.0251
             Mean action noise std: 0.75
                       Mean reward: 2.98
               Mean episode length: 60.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 8.67s
                        Total time: 9078.32s
                               ETA: 998514.5s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.166s, learning 0.189s)
               Value function loss: 47.8150
                    Surrogate loss: -0.0001
             Mean action noise std: 0.75
                       Mean reward: 2.89
               Mean episode length: 55.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 8.35s
                        Total time: 9086.68s
                               ETA: 998315.3s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.480s, learning 0.214s)
               Value function loss: 12.1525
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 60.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 8.69s
                        Total time: 9095.37s
                               ETA: 998153.8s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.512s, learning 0.224s)
               Value function loss: 0.1120
                    Surrogate loss: -0.0327
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 59.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 8.74s
                        Total time: 9104.10s
                               ETA: 997997.2s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.205s, learning 0.215s)
               Value function loss: 91.2806
                    Surrogate loss: -0.0001
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 59.70
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 8.42s
                        Total time: 9112.52s
                               ETA: 997806.3s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.529s, learning 0.212s)
               Value function loss: 0.0941
                    Surrogate loss: -0.0286
             Mean action noise std: 0.75
                       Mean reward: 8.45
               Mean episode length: 58.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 8.74s
                        Total time: 9121.27s
                               ETA: 997651.0s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.409s, learning 0.195s)
               Value function loss: 16.5670
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: 2.93
               Mean episode length: 56.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 8.60s
                        Total time: 9129.87s
                               ETA: 997481.0s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1845 steps/s (collection: 8.682s, learning 0.196s)
               Value function loss: 0.1323
                    Surrogate loss: -0.0245
             Mean action noise std: 0.75
                       Mean reward: 10.84
               Mean episode length: 57.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 8.88s
                        Total time: 9138.75s
                               ETA: 997341.3s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.552s, learning 0.193s)
               Value function loss: 59.8666
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 56.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 8.74s
                        Total time: 9147.49s
                               ETA: 997187.2s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.406s, learning 0.295s)
               Value function loss: 0.1519
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 3.09
               Mean episode length: 57.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 8.70s
                        Total time: 9156.19s
                               ETA: 997028.8s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.515s, learning 0.173s)
               Value function loss: 119.3935
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 3.09
               Mean episode length: 54.63
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 8.69s
                        Total time: 9164.88s
                               ETA: 996869.3s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.521s, learning 0.174s)
               Value function loss: 94.3101
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 57.70
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 8.69s
                        Total time: 9173.57s
                               ETA: 996710.9s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.481s, learning 0.178s)
               Value function loss: 96.9882
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 33.71
               Mean episode length: 55.13
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 8.66s
                        Total time: 9182.23s
                               ETA: 996549.0s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.352s, learning 0.187s)
               Value function loss: 2.0113
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 56.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 8.54s
                        Total time: 9190.77s
                               ETA: 996374.5s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1825 steps/s (collection: 8.671s, learning 0.305s)
               Value function loss: 10.9249
                    Surrogate loss: 0.0004
             Mean action noise std: 0.75
                       Mean reward: 2.94
               Mean episode length: 56.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 8.98s
                        Total time: 9199.75s
                               ETA: 996247.5s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.220s, learning 0.275s)
               Value function loss: 65.5324
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 57.26
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 8.50s
                        Total time: 9208.25s
                               ETA: 996068.8s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1836 steps/s (collection: 8.734s, learning 0.189s)
               Value function loss: 0.9719
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 57.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 8.92s
                        Total time: 9217.17s
                               ETA: 995936.7s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.402s, learning 0.174s)
               Value function loss: 1.1297
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 10.92
               Mean episode length: 58.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 8.58s
                        Total time: 9225.75s
                               ETA: 995767.4s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.464s, learning 0.227s)
               Value function loss: 14.8400
                    Surrogate loss: 0.0005
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 56.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 8.69s
                        Total time: 9234.44s
                               ETA: 995610.9s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.347s, learning 0.254s)
               Value function loss: 0.2298
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 54.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 8.60s
                        Total time: 9243.04s
                               ETA: 995445.0s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1874 steps/s (collection: 8.476s, learning 0.267s)
               Value function loss: 51.8814
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 54.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 8.74s
                        Total time: 9251.78s
                               ETA: 995294.6s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1824 steps/s (collection: 8.609s, learning 0.369s)
               Value function loss: 225.4895
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 3.16
               Mean episode length: 56.39
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 8.98s
                        Total time: 9260.76s
                               ETA: 995169.9s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1812 steps/s (collection: 8.771s, learning 0.266s)
               Value function loss: 1.1288
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 10.94
               Mean episode length: 59.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 9.04s
                        Total time: 9269.80s
                               ETA: 995051.8s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.315s, learning 0.407s)
               Value function loss: 64.0200
                    Surrogate loss: 0.0011
             Mean action noise std: 0.75
                       Mean reward: 3.33
               Mean episode length: 57.40
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 8.72s
                        Total time: 9278.52s
                               ETA: 994900.0s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.410s, learning 0.256s)
               Value function loss: 4.8134
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 55.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 8.67s
                        Total time: 9287.18s
                               ETA: 994742.7s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.045s, learning 0.196s)
               Value function loss: 472.4946
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 57.46
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 16.24s
                        Total time: 9303.42s
                               ETA: 995396.1s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 970 steps/s (collection: 16.554s, learning 0.334s)
               Value function loss: 216.1786
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 56.26
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 16.89s
                        Total time: 9320.31s
                               ETA: 996117.1s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.842s, learning 0.184s)
               Value function loss: 364.4423
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 55.25
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 17.03s
                        Total time: 9337.34s
                               ETA: 996851.4s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.106s, learning 0.210s)
               Value function loss: 31.4964
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 3.26
               Mean episode length: 55.99
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 16.32s
                        Total time: 9353.65s
                               ETA: 997508.2s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.411s, learning 0.180s)
               Value function loss: 48.2493
                    Surrogate loss: 0.0042
             Mean action noise std: 0.75
                       Mean reward: 2.78
               Mean episode length: 53.76
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 16.59s
                        Total time: 9370.24s
                               ETA: 998192.9s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.476s, learning 0.225s)
               Value function loss: 18.4037
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 2.88
               Mean episode length: 56.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 16.70s
                        Total time: 9386.94s
                               ETA: 998887.9s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.467s, learning 0.326s)
               Value function loss: 18.7435
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 55.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 16.79s
                        Total time: 9403.74s
                               ETA: 999591.2s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.575s, learning 0.227s)
               Value function loss: 0.8015
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 10.85
               Mean episode length: 55.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 16.80s
                        Total time: 9420.54s
                               ETA: 1000293.8s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 963 steps/s (collection: 16.831s, learning 0.173s)
               Value function loss: 0.7100
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 56.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 17.00s
                        Total time: 9437.54s
                               ETA: 1001016.3s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.552s, learning 0.190s)
               Value function loss: 0.5790
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 58.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 16.74s
                        Total time: 9454.29s
                               ETA: 1001709.5s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 972 steps/s (collection: 16.582s, learning 0.266s)
               Value function loss: 0.3901
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 57.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 16.85s
                        Total time: 9471.13s
                               ETA: 1002412.3s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 954 steps/s (collection: 16.953s, learning 0.217s)
               Value function loss: 4.3590
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 59.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 17.17s
                        Total time: 9488.30s
                               ETA: 1003147.8s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.585s, learning 0.172s)
               Value function loss: 502.5483
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 2.97
               Mean episode length: 57.51
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 16.76s
                        Total time: 9505.06s
                               ETA: 1003838.0s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 976 steps/s (collection: 16.613s, learning 0.170s)
               Value function loss: 20.5731
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 8.33
               Mean episode length: 56.36
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 16.78s
                        Total time: 9521.85s
                               ETA: 1004529.4s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.426s, learning 0.257s)
               Value function loss: 45.3936
                    Surrogate loss: 0.0021
             Mean action noise std: 0.75
                       Mean reward: 3.23
               Mean episode length: 57.01
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 16.68s
                        Total time: 9538.53s
                               ETA: 1005208.7s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.666s, learning 0.167s)
               Value function loss: 0.5251
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 58.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 16.83s
                        Total time: 9555.36s
                               ETA: 1005902.4s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.119s, learning 0.277s)
               Value function loss: 92.7610
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 2.75
               Mean episode length: 55.31
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 16.40s
                        Total time: 9571.76s
                               ETA: 1006548.5s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.563s, learning 0.172s)
               Value function loss: 17.9882
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 7.98
               Mean episode length: 57.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 16.74s
                        Total time: 9588.49s
                               ETA: 1007228.9s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.007s, learning 0.314s)
               Value function loss: 0.4815
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 2.80
               Mean episode length: 53.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 16.32s
                        Total time: 9604.81s
                               ETA: 1007864.4s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 986 steps/s (collection: 16.400s, learning 0.201s)
               Value function loss: 0.4464
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 2.83
               Mean episode length: 58.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 16.60s
                        Total time: 9621.42s
                               ETA: 1008527.9s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.433s, learning 0.185s)
               Value function loss: 0.4397
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 55.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 16.62s
                        Total time: 9638.03s
                               ETA: 1009191.7s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.130s, learning 0.197s)
               Value function loss: 0.3009
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 2.87
               Mean episode length: 55.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 16.33s
                        Total time: 9654.36s
                               ETA: 1009823.6s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 975 steps/s (collection: 16.606s, learning 0.186s)
               Value function loss: 0.3017
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 2.85
               Mean episode length: 55.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 16.79s
                        Total time: 9671.15s
                               ETA: 1010502.8s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.478s, learning 0.178s)
               Value function loss: 47.5367
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 2.91
               Mean episode length: 55.53
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 16.66s
                        Total time: 9687.81s
                               ETA: 1011166.3s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.351s, learning 0.313s)
               Value function loss: 0.1628
                    Surrogate loss: -0.0261
             Mean action noise std: 0.75
                       Mean reward: 3.07
               Mean episode length: 55.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 16.66s
                        Total time: 9704.47s
                               ETA: 1011829.1s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1007 steps/s (collection: 15.917s, learning 0.346s)
               Value function loss: 0.1391
                    Surrogate loss: -0.0285
             Mean action noise std: 0.75
                       Mean reward: 3.06
               Mean episode length: 55.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 16.26s
                        Total time: 9720.73s
                               ETA: 1012448.8s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.138s, learning 0.265s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0239
             Mean action noise std: 0.75
                       Mean reward: 8.44
               Mean episode length: 57.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 16.40s
                        Total time: 9737.14s
                               ETA: 1013081.7s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.567s, learning 0.184s)
               Value function loss: 38.9846
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 3.00
               Mean episode length: 52.77
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 16.75s
                        Total time: 9753.89s
                               ETA: 1013749.5s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.596s, learning 0.167s)
               Value function loss: 0.1356
                    Surrogate loss: -0.0279
             Mean action noise std: 0.75
                       Mean reward: 2.91
               Mean episode length: 51.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 16.76s
                        Total time: 9770.65s
                               ETA: 1014417.1s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.824s, learning 0.201s)
               Value function loss: 0.1855
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 56.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 17.02s
                        Total time: 9787.68s
                               ETA: 1015110.2s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.555s, learning 0.178s)
               Value function loss: 0.2081
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 2.93
               Mean episode length: 55.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 16.73s
                        Total time: 9804.41s
                               ETA: 1015771.8s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 957 steps/s (collection: 16.827s, learning 0.279s)
               Value function loss: 0.1325
                    Surrogate loss: -0.0259
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 55.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 17.11s
                        Total time: 9821.52s
                               ETA: 1016470.4s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.391s, learning 0.189s)
               Value function loss: 47.0016
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 56.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 16.58s
                        Total time: 9838.10s
                               ETA: 1017113.2s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.197s, learning 0.298s)
               Value function loss: 0.1194
                    Surrogate loss: -0.0316
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 55.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 16.49s
                        Total time: 9854.59s
                               ETA: 1017745.9s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.555s, learning 0.212s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 3.07
               Mean episode length: 56.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 16.77s
                        Total time: 9871.36s
                               ETA: 1018405.4s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.436s, learning 0.214s)
               Value function loss: 18.4942
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 55.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 16.65s
                        Total time: 9888.01s
                               ETA: 1019051.3s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.266s, learning 0.167s)
               Value function loss: 252.5355
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 53.95
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 16.43s
                        Total time: 9904.44s
                               ETA: 1019673.5s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 1097 steps/s (collection: 14.752s, learning 0.170s)
               Value function loss: 1.6472
                    Surrogate loss: -0.0234
             Mean action noise std: 0.75
                       Mean reward: 20.69
               Mean episode length: 53.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 14.92s
                        Total time: 9919.36s
                               ETA: 1020139.0s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.380s, learning 0.183s)
               Value function loss: 40.9702
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 55.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 8.56s
                        Total time: 9927.93s
                               ETA: 1019950.2s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1854 steps/s (collection: 8.668s, learning 0.167s)
               Value function loss: 0.6564
                    Surrogate loss: -0.0223
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 53.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 8.84s
                        Total time: 9936.76s
                               ETA: 1019789.7s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.688s, learning 0.224s)
               Value function loss: 63.1477
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 3.00
               Mean episode length: 53.85
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 8.91s
                        Total time: 9945.67s
                               ETA: 1019637.4s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.403s, learning 0.169s)
               Value function loss: 108.4497
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 52.08
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 8.57s
                        Total time: 9954.24s
                               ETA: 1019450.5s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1848 steps/s (collection: 8.642s, learning 0.223s)
               Value function loss: 112.1541
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 54.72
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 8.86s
                        Total time: 9963.11s
                               ETA: 1019294.0s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.423s, learning 0.164s)
               Value function loss: 124.4704
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.64
               Mean episode length: 55.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 8.59s
                        Total time: 9971.70s
                               ETA: 1019109.5s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.334s, learning 0.166s)
               Value function loss: 96.3250
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 3.16
               Mean episode length: 55.20
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 8.50s
                        Total time: 9980.20s
                               ETA: 1018916.4s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1838 steps/s (collection: 8.737s, learning 0.173s)
               Value function loss: 220.0040
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 8.48
               Mean episode length: 55.60
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 8.91s
                        Total time: 9989.11s
                               ETA: 1018765.5s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.279s, learning 0.179s)
               Value function loss: 38.7590
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 3.06
               Mean episode length: 56.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 8.46s
                        Total time: 9997.57s
                               ETA: 1018568.8s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.136s, learning 0.166s)
               Value function loss: 36.2880
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 3.18
               Mean episode length: 54.04
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 8.30s
                        Total time: 10005.87s
                               ETA: 1018356.6s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.245s, learning 0.171s)
               Value function loss: 700.7907
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 20.88
               Mean episode length: 54.22
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 8.42s
                        Total time: 10014.28s
                               ETA: 1018156.5s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1859 steps/s (collection: 8.644s, learning 0.167s)
               Value function loss: 235.2392
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 10.89
               Mean episode length: 53.32
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 8.81s
                        Total time: 10023.09s
                               ETA: 1017996.8s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.333s, learning 0.164s)
               Value function loss: 262.0610
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 55.60
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 8.50s
                        Total time: 10031.59s
                               ETA: 1017805.6s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1827 steps/s (collection: 8.756s, learning 0.211s)
               Value function loss: 360.4053
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 33.58
               Mean episode length: 56.58
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 8.97s
                        Total time: 10040.56s
                               ETA: 1017662.5s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.371s, learning 0.169s)
               Value function loss: 70.5634
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 53.94
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 8.54s
                        Total time: 10049.10s
                               ETA: 1017476.3s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1872 steps/s (collection: 8.577s, learning 0.172s)
               Value function loss: 6.7852
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 54.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 8.75s
                        Total time: 10057.85s
                               ETA: 1017311.7s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1852 steps/s (collection: 8.536s, learning 0.306s)
               Value function loss: 71.9053
                    Surrogate loss: 0.0006
             Mean action noise std: 0.75
                       Mean reward: 5.89
               Mean episode length: 52.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0191
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 8.84s
                        Total time: 10066.69s
                               ETA: 1017156.8s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.512s, learning 0.168s)
               Value function loss: 88.1429
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 56.27
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 8.68s
                        Total time: 10075.37s
                               ETA: 1016985.9s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.530s, learning 0.178s)
               Value function loss: 16.1093
                    Surrogate loss: 0.0084
             Mean action noise std: 0.75
                       Mean reward: 8.18
               Mean episode length: 53.71
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 8.71s
                        Total time: 10084.08s
                               ETA: 1016818.1s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.394s, learning 0.208s)
               Value function loss: 255.9970
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 2.87
               Mean episode length: 52.64
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 8.60s
                        Total time: 10092.68s
                               ETA: 1016639.9s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.175s, learning 0.166s)
               Value function loss: 4.4400
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 55.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 8.34s
                        Total time: 10101.02s
                               ETA: 1016435.7s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.053s, learning 0.170s)
               Value function loss: 47.9338
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 2.96
               Mean episode length: 57.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 8.22s
                        Total time: 10109.24s
                               ETA: 1016220.1s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.275s, learning 0.257s)
               Value function loss: 48.8458
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 5.54
               Mean episode length: 57.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 8.53s
                        Total time: 10117.78s
                               ETA: 1016036.0s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1975 steps/s (collection: 8.119s, learning 0.176s)
               Value function loss: 1.4528
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 10.94
               Mean episode length: 56.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 8.30s
                        Total time: 10126.07s
                               ETA: 1015828.5s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1832 steps/s (collection: 8.621s, learning 0.319s)
               Value function loss: 0.8858
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 54.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 8.94s
                        Total time: 10135.01s
                               ETA: 1015686.0s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.459s, learning 0.171s)
               Value function loss: 0.8032
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 3.08
               Mean episode length: 55.98
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 8.63s
                        Total time: 10143.64s
                               ETA: 1015512.8s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1865 steps/s (collection: 8.606s, learning 0.178s)
               Value function loss: 0.5187
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 3.25
               Mean episode length: 55.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 8.78s
                        Total time: 10152.43s
                               ETA: 1015355.3s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.332s, learning 0.167s)
               Value function loss: 0.3990
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 3.36
               Mean episode length: 52.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 8.50s
                        Total time: 10160.92s
                               ETA: 1015169.6s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.522s, learning 0.170s)
               Value function loss: 0.4689
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 54.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 8.69s
                        Total time: 10169.62s
                               ETA: 1015003.5s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1903 steps/s (collection: 8.443s, learning 0.164s)
               Value function loss: 60.7309
                    Surrogate loss: -0.0004
             Mean action noise std: 0.75
                       Mean reward: 3.87
               Mean episode length: 57.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 8.61s
                        Total time: 10178.22s
                               ETA: 1014829.3s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.170s)
               Value function loss: 142.0877
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 55.36
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 8.54s
                        Total time: 10186.77s
                               ETA: 1014649.0s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.208s, learning 0.164s)
               Value function loss: 101.0377
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 2.98
               Mean episode length: 55.69
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 8.37s
                        Total time: 10195.14s
                               ETA: 1014452.1s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.165s)
               Value function loss: 2.2167
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 56.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 8.38s
                        Total time: 10203.51s
                               ETA: 1014255.8s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.274s, learning 0.163s)
               Value function loss: 94.1400
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 21.31
               Mean episode length: 56.63
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 8.44s
                        Total time: 10211.95s
                               ETA: 1014066.2s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1860 steps/s (collection: 8.568s, learning 0.237s)
               Value function loss: 241.6955
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 55.37
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 8.81s
                        Total time: 10220.76s
                               ETA: 1013913.4s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.296s, learning 0.171s)
               Value function loss: 399.3550
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 8.41
               Mean episode length: 55.48
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 8.47s
                        Total time: 10229.22s
                               ETA: 1013727.3s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.408s, learning 0.169s)
               Value function loss: 14.4139
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 16.36
               Mean episode length: 61.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 8.58s
                        Total time: 10237.80s
                               ETA: 1013552.4s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.480s, learning 0.343s)
               Value function loss: 5.7544
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 3.38
               Mean episode length: 55.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 8.82s
                        Total time: 10246.62s
                               ETA: 1013402.3s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.667s, learning 0.214s)
               Value function loss: 137.7115
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 8.42
               Mean episode length: 53.26
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 8.88s
                        Total time: 10255.50s
                               ETA: 1013258.2s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.375s, learning 0.166s)
               Value function loss: 150.0715
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 57.38
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 8.54s
                        Total time: 10264.05s
                               ETA: 1013080.7s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.425s, learning 0.191s)
               Value function loss: 205.7165
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.57
               Mean episode length: 56.78
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 8.62s
                        Total time: 10272.66s
                               ETA: 1012911.1s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.499s, learning 0.169s)
               Value function loss: 318.8967
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 3.71
               Mean episode length: 57.64
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 8.67s
                        Total time: 10281.33s
                               ETA: 1012746.8s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.352s, learning 0.178s)
               Value function loss: 31.9381
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 18.71
               Mean episode length: 57.07
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 8.53s
                        Total time: 10289.86s
                               ETA: 1012569.3s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.664s, learning 0.162s)
               Value function loss: 74.6044
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 6.26
               Mean episode length: 57.22
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 8.83s
                        Total time: 10298.69s
                               ETA: 1012421.1s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.519s, learning 0.353s)
               Value function loss: 3.9728
                    Surrogate loss: -0.0216
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 57.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 8.87s
                        Total time: 10307.56s
                               ETA: 1012277.8s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.500s, learning 0.168s)
               Value function loss: 3.0148
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 10.99
               Mean episode length: 60.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 8.67s
                        Total time: 10316.23s
                               ETA: 1012114.8s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.285s, learning 0.331s)
               Value function loss: 17.3163
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 57.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 8.62s
                        Total time: 10324.84s
                               ETA: 1011947.0s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.664s, learning 0.190s)
               Value function loss: 29.7458
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 58.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 8.85s
                        Total time: 10333.70s
                               ETA: 1011802.7s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.209s, learning 0.326s)
               Value function loss: 211.5616
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 3.22
               Mean episode length: 59.39
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 8.54s
                        Total time: 10342.23s
                               ETA: 1011627.6s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.537s, learning 0.174s)
               Value function loss: 296.7654
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 16.66
               Mean episode length: 58.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 8.71s
                        Total time: 10350.94s
                               ETA: 1011470.0s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.548s, learning 0.174s)
               Value function loss: 185.5945
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 56.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 8.72s
                        Total time: 10359.67s
                               ETA: 1011313.8s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.717s, learning 0.174s)
               Value function loss: 14.0262
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 3.51
               Mean episode length: 54.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 8.89s
                        Total time: 10368.56s
                               ETA: 1011174.3s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.457s, learning 0.187s)
               Value function loss: 333.4506
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 5.85
               Mean episode length: 60.58
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 8.64s
                        Total time: 10377.20s
                               ETA: 1011011.0s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.632s, learning 0.185s)
               Value function loss: 8.3259
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 3.41
               Mean episode length: 59.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 8.82s
                        Total time: 10386.02s
                               ETA: 1010864.8s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.599s, learning 0.174s)
               Value function loss: 133.7330
                    Surrogate loss: 0.0013
             Mean action noise std: 0.75
                       Mean reward: 3.27
               Mean episode length: 60.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 8.77s
                        Total time: 10394.79s
                               ETA: 1010714.6s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.753s, learning 0.213s)
               Value function loss: 88.2721
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 57.73
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 8.97s
                        Total time: 10403.76s
                               ETA: 1010583.5s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.561s, learning 0.165s)
               Value function loss: 7.0103
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 13.65
               Mean episode length: 60.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 8.73s
                        Total time: 10412.48s
                               ETA: 1010429.3s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.321s, learning 0.173s)
               Value function loss: 2.3895
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 57.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 8.49s
                        Total time: 10420.98s
                               ETA: 1010252.8s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.808s, learning 0.169s)
               Value function loss: 1.2034
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 8.32
               Mean episode length: 55.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 8.98s
                        Total time: 10429.95s
                               ETA: 1010123.5s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.439s, learning 0.166s)
               Value function loss: 0.7666
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 56.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 8.61s
                        Total time: 10438.56s
                               ETA: 1009958.5s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.321s, learning 0.180s)
               Value function loss: 0.7548
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 2.96
               Mean episode length: 59.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 8.50s
                        Total time: 10447.06s
                               ETA: 1009783.7s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.208s, learning 0.167s)
               Value function loss: 0.5777
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 59.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 8.38s
                        Total time: 10455.43s
                               ETA: 1009597.1s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.236s, learning 0.175s)
               Value function loss: 17.6817
                    Surrogate loss: 0.0022
             Mean action noise std: 0.75
                       Mean reward: 2.90
               Mean episode length: 57.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 8.41s
                        Total time: 10463.85s
                               ETA: 1009414.3s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.600s, learning 0.221s)
               Value function loss: 0.3675
                    Surrogate loss: -0.0232
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 56.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 8.82s
                        Total time: 10472.67s
                               ETA: 1009271.3s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.258s, learning 0.175s)
               Value function loss: 14.6783
                    Surrogate loss: 0.0018
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 60.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 8.43s
                        Total time: 10481.10s
                               ETA: 1009091.3s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.453s, learning 0.215s)
               Value function loss: 0.5821
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 2.98
               Mean episode length: 58.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 8.67s
                        Total time: 10489.77s
                               ETA: 1008934.2s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.556s, learning 0.271s)
               Value function loss: 14.2287
                    Surrogate loss: 0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 57.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 8.83s
                        Total time: 10498.59s
                               ETA: 1008792.6s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.310s, learning 0.275s)
               Value function loss: 0.4083
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 5.58
               Mean episode length: 58.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 8.58s
                        Total time: 10507.18s
                               ETA: 1008628.1s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.611s, learning 0.188s)
               Value function loss: 85.1566
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: 2.87
               Mean episode length: 57.60
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 8.80s
                        Total time: 10515.98s
                               ETA: 1008484.4s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.731s, learning 0.230s)
               Value function loss: 109.1091
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 58.71
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 8.96s
                        Total time: 10524.94s
                               ETA: 1008356.5s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.356s, learning 0.182s)
               Value function loss: 130.0735
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 58.62
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 8.54s
                        Total time: 10533.48s
                               ETA: 1008188.3s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.462s, learning 0.164s)
               Value function loss: 36.4318
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 2.85
               Mean episode length: 57.63
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 8.63s
                        Total time: 10542.10s
                               ETA: 1008028.9s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.322s, learning 0.169s)
               Value function loss: 118.8934
                    Surrogate loss: 0.0010
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 57.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 8.49s
                        Total time: 10550.59s
                               ETA: 1007856.8s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.790s, learning 0.171s)
               Value function loss: 216.0806
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 8.29
               Mean episode length: 58.60
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 8.96s
                        Total time: 10559.56s
                               ETA: 1007730.0s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.351s, learning 0.174s)
               Value function loss: 71.7002
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 2.91
               Mean episode length: 52.14
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 8.52s
                        Total time: 10568.08s
                               ETA: 1007561.7s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.515s, learning 0.284s)
               Value function loss: 32.0141
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 54.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 8.80s
                        Total time: 10576.88s
                               ETA: 1007419.9s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.666s, learning 0.177s)
               Value function loss: 87.0898
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: 3.94
               Mean episode length: 58.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 8.84s
                        Total time: 10585.72s
                               ETA: 1007282.5s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.477s, learning 0.170s)
               Value function loss: 3.4082
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 3.11
               Mean episode length: 57.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 8.65s
                        Total time: 10594.37s
                               ETA: 1007126.7s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.450s, learning 0.165s)
               Value function loss: 44.4703
                    Surrogate loss: 0.0001
             Mean action noise std: 0.75
                       Mean reward: 16.30
               Mean episode length: 55.16
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 8.61s
                        Total time: 10602.99s
                               ETA: 1006968.2s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.173s)
               Value function loss: 144.4790
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 3.49
               Mean episode length: 56.52
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 8.59s
                        Total time: 10611.58s
                               ETA: 1006807.6s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.740s, learning 0.347s)
               Value function loss: 271.1273
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 20.76
               Mean episode length: 58.07
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 9.09s
                        Total time: 10620.66s
                               ETA: 1006694.3s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.294s, learning 0.175s)
               Value function loss: 57.7310
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 3.35
               Mean episode length: 57.59
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 8.47s
                        Total time: 10629.13s
                               ETA: 1006522.8s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.078s, learning 0.204s)
               Value function loss: 8.8494
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 3.25
               Mean episode length: 57.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 8.28s
                        Total time: 10637.41s
                               ETA: 1006333.9s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.256s, learning 0.196s)
               Value function loss: 39.2910
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 15.82
               Mean episode length: 59.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 8.45s
                        Total time: 10645.87s
                               ETA: 1006161.4s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.516s, learning 0.169s)
               Value function loss: 219.4493
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 10.82
               Mean episode length: 58.66
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 8.68s
                        Total time: 10654.55s
                               ETA: 1006011.2s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.498s, learning 0.181s)
               Value function loss: 147.3681
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 8.23
               Mean episode length: 57.17
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 8.68s
                        Total time: 10663.23s
                               ETA: 1005860.7s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.679s, learning 0.224s)
               Value function loss: 213.4474
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 15.84
               Mean episode length: 56.74
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 8.90s
                        Total time: 10672.13s
                               ETA: 1005731.7s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.069s, learning 0.170s)
               Value function loss: 132.0049
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 55.95
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 8.24s
                        Total time: 10680.37s
                               ETA: 1005540.3s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.315s, learning 0.175s)
               Value function loss: 97.5181
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 2.99
               Mean episode length: 54.35
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 8.49s
                        Total time: 10688.86s
                               ETA: 1005372.9s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.296s, learning 0.172s)
               Value function loss: 72.7118
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 11.17
               Mean episode length: 60.10
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 8.47s
                        Total time: 10697.33s
                               ETA: 1005203.7s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.557s, learning 0.206s)
               Value function loss: 101.3078
                    Surrogate loss: 0.0006
             Mean action noise std: 0.75
                       Mean reward: 18.44
               Mean episode length: 56.46
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 8.76s
                        Total time: 10706.09s
                               ETA: 1005062.5s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.412s, learning 0.179s)
               Value function loss: 232.0417
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 3.06
               Mean episode length: 54.43
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 8.59s
                        Total time: 10714.68s
                               ETA: 1004905.4s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.391s, learning 0.217s)
               Value function loss: 41.5126
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 3.51
               Mean episode length: 60.19
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 8.61s
                        Total time: 10723.29s
                               ETA: 1004750.2s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.312s, learning 0.188s)
               Value function loss: 260.0328
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 13.33
               Mean episode length: 60.21
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 8.50s
                        Total time: 10731.79s
                               ETA: 1004585.1s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.235s, learning 0.174s)
               Value function loss: 60.7522
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 13.43
               Mean episode length: 58.17
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 8.41s
                        Total time: 10740.20s
                               ETA: 1004411.9s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.395s, learning 0.282s)
               Value function loss: 25.6777
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 3.53
               Mean episode length: 59.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 8.68s
                        Total time: 10748.88s
                               ETA: 1004264.0s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.553s, learning 0.167s)
               Value function loss: 27.3903
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 3.44
               Mean episode length: 61.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 8.72s
                        Total time: 10757.60s
                               ETA: 1004120.4s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.639s, learning 0.170s)
               Value function loss: 3.5694
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 58.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 8.81s
                        Total time: 10766.41s
                               ETA: 1003985.2s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.426s, learning 0.228s)
               Value function loss: 47.1725
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 3.20
               Mean episode length: 61.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 8.65s
                        Total time: 10775.06s
                               ETA: 1003836.0s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.613s, learning 0.166s)
               Value function loss: 6.1914
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 6.00
               Mean episode length: 60.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 8.78s
                        Total time: 10783.84s
                               ETA: 1003698.5s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.613s, learning 0.194s)
               Value function loss: 17.5725
                    Surrogate loss: 0.0035
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 59.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 8.81s
                        Total time: 10792.65s
                               ETA: 1003564.0s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.490s, learning 0.177s)
               Value function loss: 170.2408
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 2.97
               Mean episode length: 64.49
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 8.67s
                        Total time: 10801.31s
                               ETA: 1003416.7s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.336s, learning 0.211s)
               Value function loss: 1.4526
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 3.81
               Mean episode length: 63.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 8.55s
                        Total time: 10809.86s
                               ETA: 1003258.6s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1761 steps/s (collection: 9.025s, learning 0.277s)
               Value function loss: 5.2307
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 2.99
               Mean episode length: 61.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 9.30s
                        Total time: 10819.16s
                               ETA: 1003170.7s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.438s, learning 0.196s)
               Value function loss: 0.8739
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 62.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 8.63s
                        Total time: 10827.80s
                               ETA: 1003021.0s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.779s, learning 0.184s)
               Value function loss: 210.5879
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.68
               Mean episode length: 61.46
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 8.96s
                        Total time: 10836.76s
                               ETA: 1002902.0s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.548s, learning 0.166s)
               Value function loss: 4.5435
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 3.23
               Mean episode length: 60.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 8.71s
                        Total time: 10845.47s
                               ETA: 1002760.3s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.641s, learning 0.186s)
               Value function loss: 46.9162
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 3.22
               Mean episode length: 58.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 8.83s
                        Total time: 10854.30s
                               ETA: 1002629.2s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.394s, learning 0.189s)
               Value function loss: 236.4623
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: 2.95
               Mean episode length: 61.67
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 8.58s
                        Total time: 10862.88s
                               ETA: 1002475.9s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.639s, learning 0.190s)
               Value function loss: 171.9612
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 3.07
               Mean episode length: 60.97
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 8.83s
                        Total time: 10871.71s
                               ETA: 1002345.5s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.792s, learning 0.182s)
               Value function loss: 10.2980
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 3.29
               Mean episode length: 64.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 8.97s
                        Total time: 10880.69s
                               ETA: 1002228.6s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.390s, learning 0.197s)
               Value function loss: 116.6318
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 23.88
               Mean episode length: 64.29
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 8.59s
                        Total time: 10889.27s
                               ETA: 1002076.5s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.797s, learning 0.172s)
               Value function loss: 146.2440
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 61.76
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 8.97s
                        Total time: 10898.24s
                               ETA: 1001959.7s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.375s, learning 0.190s)
               Value function loss: 63.8272
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 3.29
               Mean episode length: 60.64
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 8.57s
                        Total time: 10906.81s
                               ETA: 1001806.0s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.539s, learning 0.173s)
               Value function loss: 132.3803
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 3.51
               Mean episode length: 63.26
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 8.71s
                        Total time: 10915.52s
                               ETA: 1001666.1s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.482s, learning 0.183s)
               Value function loss: 226.1063
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 61.48
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 8.66s
                        Total time: 10924.19s
                               ETA: 1001522.0s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.691s, learning 0.196s)
               Value function loss: 66.2668
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 11.31
               Mean episode length: 65.82
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 8.89s
                        Total time: 10933.07s
                               ETA: 1001398.6s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.568s, learning 0.172s)
               Value function loss: 137.1960
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 61.66
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 8.74s
                        Total time: 10941.81s
                               ETA: 1001261.9s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.283s, learning 0.171s)
               Value function loss: 161.7496
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 8.14
               Mean episode length: 60.79
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 8.45s
                        Total time: 10950.27s
                               ETA: 1001099.4s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.609s, learning 0.244s)
               Value function loss: 114.4603
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 8.46
               Mean episode length: 64.82
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 8.85s
                        Total time: 10959.12s
                               ETA: 1000973.5s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.399s, learning 0.173s)
               Value function loss: 209.3213
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: 21.20
               Mean episode length: 64.04
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 8.57s
                        Total time: 10967.69s
                               ETA: 1000822.2s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.665s, learning 0.179s)
               Value function loss: 130.2845
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 61.35
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 8.84s
                        Total time: 10976.54s
                               ETA: 1000695.9s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.629s, learning 0.209s)
               Value function loss: 125.0529
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 3.00
               Mean episode length: 59.90
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 8.84s
                        Total time: 10985.37s
                               ETA: 1000569.3s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.443s, learning 0.268s)
               Value function loss: 102.6751
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 8.08
               Mean episode length: 63.59
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0170
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 8.71s
                        Total time: 10994.09s
                               ETA: 1000431.5s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.420s, learning 0.167s)
               Value function loss: 7.1291
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 8.25
               Mean episode length: 62.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 8.59s
                        Total time: 11002.67s
                               ETA: 1000282.5s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.770s, learning 0.177s)
               Value function loss: 137.0728
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 63.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 8.95s
                        Total time: 11011.62s
                               ETA: 1000166.5s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.372s, learning 0.197s)
               Value function loss: 3.9947
                    Surrogate loss: -0.0240
             Mean action noise std: 0.75
                       Mean reward: 3.16
               Mean episode length: 62.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 8.57s
                        Total time: 11020.19s
                               ETA: 1000016.5s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.341s, learning 0.209s)
               Value function loss: 19.3592
                    Surrogate loss: 0.0022
             Mean action noise std: 0.75
                       Mean reward: 3.20
               Mean episode length: 63.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 8.55s
                        Total time: 11028.74s
                               ETA: 999864.9s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.750s, learning 0.199s)
               Value function loss: 107.9575
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 3.07
               Mean episode length: 65.67
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 8.95s
                        Total time: 11037.69s
                               ETA: 999749.8s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.626s, learning 0.274s)
               Value function loss: 204.3444
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 2.96
               Mean episode length: 63.64
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 8.90s
                        Total time: 11046.59s
                               ETA: 999630.3s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.750s, learning 0.180s)
               Value function loss: 20.3320
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 20.71
               Mean episode length: 65.20
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 8.93s
                        Total time: 11055.52s
                               ETA: 999513.8s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.287s, learning 0.233s)
               Value function loss: 19.5106
                    Surrogate loss: 0.0005
             Mean action noise std: 0.75
                       Mean reward: 3.43
               Mean episode length: 64.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 8.52s
                        Total time: 11064.04s
                               ETA: 999360.5s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.296s, learning 0.179s)
               Value function loss: 33.1962
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 2.97
               Mean episode length: 62.87
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 8.47s
                        Total time: 11072.51s
                               ETA: 999203.4s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.226s, learning 0.169s)
               Value function loss: 109.8745
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 66.23
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 8.39s
                        Total time: 11080.91s
                               ETA: 999039.3s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.607s, learning 0.200s)
               Value function loss: 129.7701
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 61.62
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 8.81s
                        Total time: 11089.72s
                               ETA: 998912.7s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.694s, learning 0.184s)
               Value function loss: 78.2407
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 3.33
               Mean episode length: 61.75
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 8.88s
                        Total time: 11098.59s
                               ETA: 998792.6s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.759s, learning 0.169s)
               Value function loss: 51.6177
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 3.29
               Mean episode length: 65.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 8.93s
                        Total time: 11107.52s
                               ETA: 998677.3s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.176s, learning 0.222s)
               Value function loss: 12.4035
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 3.44
               Mean episode length: 62.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 8.40s
                        Total time: 11115.92s
                               ETA: 998514.5s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.445s, learning 0.220s)
               Value function loss: 132.8719
                    Surrogate loss: 0.0005
             Mean action noise std: 0.75
                       Mean reward: 2.87
               Mean episode length: 61.77
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 8.66s
                        Total time: 11124.58s
                               ETA: 998375.9s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.351s, learning 0.329s)
               Value function loss: 126.4133
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 62.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 8.68s
                        Total time: 11133.26s
                               ETA: 998239.0s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.379s, learning 0.169s)
               Value function loss: 4.5519
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 18.07
               Mean episode length: 65.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 8.55s
                        Total time: 11141.81s
                               ETA: 998090.4s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.198s, learning 0.182s)
               Value function loss: 59.6521
                    Surrogate loss: 0.0004
             Mean action noise std: 0.75
                       Mean reward: 5.51
               Mean episode length: 68.75
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 8.38s
                        Total time: 11150.19s
                               ETA: 997927.1s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.298s, learning 0.213s)
               Value function loss: 20.9263
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 66.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 8.51s
                        Total time: 11158.70s
                               ETA: 997775.8s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.310s, learning 0.174s)
               Value function loss: 186.5083
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 65.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 8.48s
                        Total time: 11167.19s
                               ETA: 997622.4s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.533s, learning 0.235s)
               Value function loss: 92.2787
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 10.95
               Mean episode length: 66.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 8.77s
                        Total time: 11175.96s
                               ETA: 997494.5s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.431s, learning 0.172s)
               Value function loss: 60.2228
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 8.55
               Mean episode length: 65.73
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 8.60s
                        Total time: 11184.56s
                               ETA: 997352.1s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.508s, learning 0.189s)
               Value function loss: 226.9882
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 11.14
               Mean episode length: 63.86
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 8.70s
                        Total time: 11193.26s
                               ETA: 997218.4s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.280s, learning 0.310s)
               Value function loss: 17.3005
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 3.06
               Mean episode length: 61.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 8.59s
                        Total time: 11201.85s
                               ETA: 997075.3s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.857s, learning 0.169s)
               Value function loss: 115.1018
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 2.97
               Mean episode length: 63.00
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 9.03s
                        Total time: 11210.87s
                               ETA: 996971.3s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.059s, learning 0.204s)
               Value function loss: 22.7106
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 65.93
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 8.26s
                        Total time: 11219.14s
                               ETA: 996799.7s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.367s, learning 0.171s)
               Value function loss: 112.0294
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 8.13
               Mean episode length: 66.25
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 8.54s
                        Total time: 11227.67s
                               ETA: 996652.6s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.110s, learning 0.178s)
               Value function loss: 18.8855
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 21.46
               Mean episode length: 61.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 8.29s
                        Total time: 11235.96s
                               ETA: 996483.7s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.409s, learning 0.235s)
               Value function loss: 81.1543
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 63.98
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 8.64s
                        Total time: 11244.60s
                               ETA: 996346.5s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.638s, learning 0.324s)
               Value function loss: 9.4250
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 6.21
               Mean episode length: 64.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 8.96s
                        Total time: 11253.57s
                               ETA: 996237.9s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.611s, learning 0.204s)
               Value function loss: 53.5712
                    Surrogate loss: 0.0017
             Mean action noise std: 0.75
                       Mean reward: 3.47
               Mean episode length: 66.92
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 8.82s
                        Total time: 11262.38s
                               ETA: 996116.5s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.590s, learning 0.219s)
               Value function loss: 21.3344
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 61.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 8.81s
                        Total time: 11271.19s
                               ETA: 995994.6s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.801s, learning 0.185s)
               Value function loss: 1.9959
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 8.43
               Mean episode length: 68.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 8.99s
                        Total time: 11280.18s
                               ETA: 995888.6s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.664s, learning 0.189s)
               Value function loss: 330.8063
                    Surrogate loss: -0.0003
             Mean action noise std: 0.75
                       Mean reward: 8.66
               Mean episode length: 64.71
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 8.85s
                        Total time: 11289.03s
                               ETA: 995771.0s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.619s, learning 0.170s)
               Value function loss: 660.3646
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 65.39
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 8.79s
                        Total time: 11297.82s
                               ETA: 995648.0s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.500s, learning 0.349s)
               Value function loss: 188.6338
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 20.71
               Mean episode length: 62.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 8.85s
                        Total time: 11306.67s
                               ETA: 995530.6s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.238s, learning 0.169s)
               Value function loss: 146.8651
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 26.73
               Mean episode length: 63.68
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 8.41s
                        Total time: 11315.08s
                               ETA: 995374.3s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.879s, learning 0.201s)
               Value function loss: 603.9579
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 64.54
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 9.08s
                        Total time: 11324.16s
                               ETA: 995277.6s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.554s, learning 0.228s)
               Value function loss: 377.3590
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 16.17
               Mean episode length: 66.62
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 8.78s
                        Total time: 11332.94s
                               ETA: 995154.8s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1774 steps/s (collection: 9.014s, learning 0.220s)
               Value function loss: 7.0310
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 38.21
               Mean episode length: 64.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0237
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 9.23s
                        Total time: 11342.17s
                               ETA: 995071.8s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.409s, learning 0.174s)
               Value function loss: 3.5624
                    Surrogate loss: -0.0221
             Mean action noise std: 0.75
                       Mean reward: 3.31
               Mean episode length: 64.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0218
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 8.58s
                        Total time: 11350.76s
                               ETA: 994932.0s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.611s, learning 0.195s)
               Value function loss: 65.7188
                    Surrogate loss: 0.0038
             Mean action noise std: 0.75
                       Mean reward: 3.38
               Mean episode length: 64.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 8.81s
                        Total time: 11359.56s
                               ETA: 994811.9s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.566s, learning 0.309s)
               Value function loss: 62.9232
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: 3.20
               Mean episode length: 65.93
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 8.87s
                        Total time: 11368.44s
                               ETA: 994698.0s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.740s, learning 0.363s)
               Value function loss: 18.6634
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 8.56
               Mean episode length: 63.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 9.10s
                        Total time: 11377.54s
                               ETA: 994604.2s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.559s, learning 0.183s)
               Value function loss: 19.9094
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.29
               Mean episode length: 67.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 8.74s
                        Total time: 11386.28s
                               ETA: 994479.0s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.132s, learning 0.320s)
               Value function loss: 1.9099
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 8.71
               Mean episode length: 66.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 8.45s
                        Total time: 11394.73s
                               ETA: 994328.7s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.631s, learning 0.177s)
               Value function loss: 54.2278
                    Surrogate loss: 0.0012
             Mean action noise std: 0.75
                       Mean reward: 3.56
               Mean episode length: 61.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 8.81s
                        Total time: 11403.54s
                               ETA: 994209.8s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.664s, learning 0.293s)
               Value function loss: 1.0880
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 3.51
               Mean episode length: 65.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 8.96s
                        Total time: 11412.50s
                               ETA: 994104.0s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1784 steps/s (collection: 8.856s, learning 0.327s)
               Value function loss: 102.4100
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 3.27
               Mean episode length: 65.98
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 9.18s
                        Total time: 11421.68s
                               ETA: 994018.1s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.674s, learning 0.183s)
               Value function loss: 7.1802
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 3.22
               Mean episode length: 61.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 11.86s
                        Total time: 11433.54s
                               ETA: 994164.8s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.616s, learning 0.183s)
               Value function loss: 1.0731
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 8.25
               Mean episode length: 63.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 16.80s
                        Total time: 11450.34s
                               ETA: 994740.6s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.750s, learning 0.172s)
               Value function loss: 31.4799
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: 3.01
               Mean episode length: 65.24
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 16.92s
                        Total time: 11467.26s
                               ETA: 995326.0s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.864s, learning 0.175s)
               Value function loss: 59.6141
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 2.97
               Mean episode length: 68.76
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 17.04s
                        Total time: 11484.30s
                               ETA: 995920.4s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.348s, learning 0.197s)
               Value function loss: 35.6076
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 3.23
               Mean episode length: 62.37
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 16.55s
                        Total time: 11500.84s
                               ETA: 996471.0s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 949 steps/s (collection: 16.905s, learning 0.353s)
               Value function loss: 2.7259
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 8.54
               Mean episode length: 67.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 17.26s
                        Total time: 11518.10s
                               ETA: 997082.4s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.895s, learning 0.178s)
               Value function loss: 16.8226
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 5.63
               Mean episode length: 62.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 17.07s
                        Total time: 11535.18s
                               ETA: 997676.7s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.308s, learning 0.200s)
               Value function loss: 181.1792
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 5.71
               Mean episode length: 64.53
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 16.51s
                        Total time: 11551.68s
                               ETA: 998221.0s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.686s, learning 0.180s)
               Value function loss: 63.5215
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 6.21
               Mean episode length: 63.31
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 16.87s
                        Total time: 11568.55s
                               ETA: 998795.3s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.703s, learning 0.248s)
               Value function loss: 28.7054
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 62.98
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 16.95s
                        Total time: 11585.50s
                               ETA: 999375.8s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.936s, learning 0.229s)
               Value function loss: 144.6382
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: 8.53
               Mean episode length: 66.18
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 17.17s
                        Total time: 11602.67s
                               ETA: 999973.8s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.576s, learning 0.218s)
               Value function loss: 17.6781
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 18.98
               Mean episode length: 64.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 16.79s
                        Total time: 11619.46s
                               ETA: 1000538.7s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.558s, learning 0.176s)
               Value function loss: 2.2470
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 3.31
               Mean episode length: 64.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 16.73s
                        Total time: 11636.19s
                               ETA: 1001097.5s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.455s, learning 0.244s)
               Value function loss: 148.3776
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 61.63
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 16.70s
                        Total time: 11652.89s
                               ETA: 1001652.2s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.675s, learning 0.183s)
               Value function loss: 26.0538
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 2.92
               Mean episode length: 65.00
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 16.86s
                        Total time: 11669.75s
                               ETA: 1002219.6s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.816s, learning 0.190s)
               Value function loss: 17.6770
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 62.12
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 17.01s
                        Total time: 11686.76s
                               ETA: 1002798.7s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.663s, learning 0.292s)
               Value function loss: 357.7584
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 3.26
               Mean episode length: 64.50
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 16.95s
                        Total time: 11703.71s
                               ETA: 1003372.3s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.362s, learning 0.176s)
               Value function loss: 77.0008
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 6.23
               Mean episode length: 65.07
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 16.54s
                        Total time: 11720.25s
                               ETA: 1003909.3s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.516s, learning 0.258s)
               Value function loss: 34.0816
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 3.54
               Mean episode length: 64.71
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 16.77s
                        Total time: 11737.02s
                               ETA: 1004465.6s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.733s, learning 0.198s)
               Value function loss: 16.2876
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 2.87
               Mean episode length: 65.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 16.93s
                        Total time: 11753.95s
                               ETA: 1005034.2s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.612s, learning 0.200s)
               Value function loss: 53.6724
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: 5.72
               Mean episode length: 64.54
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 16.81s
                        Total time: 11770.77s
                               ETA: 1005591.6s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.483s, learning 0.283s)
               Value function loss: 3.2567
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 6.11
               Mean episode length: 64.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 16.77s
                        Total time: 11787.53s
                               ETA: 1006144.2s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.619s, learning 0.214s)
               Value function loss: 48.8114
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 8.88
               Mean episode length: 62.34
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 16.83s
                        Total time: 11804.36s
                               ETA: 1006701.4s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.308s, learning 0.197s)
               Value function loss: 125.8975
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 8.30
               Mean episode length: 65.32
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 16.51s
                        Total time: 11820.87s
                               ETA: 1007229.8s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.728s, learning 0.219s)
               Value function loss: 13.4562
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 3.26
               Mean episode length: 63.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 16.95s
                        Total time: 11837.82s
                               ETA: 1007794.8s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.442s, learning 0.191s)
               Value function loss: 39.8822
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 62.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 16.63s
                        Total time: 11854.45s
                               ETA: 1008332.1s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.668s, learning 0.246s)
               Value function loss: 47.9112
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 9.24
               Mean episode length: 66.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 16.91s
                        Total time: 11871.36s
                               ETA: 1008892.4s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.434s, learning 0.172s)
               Value function loss: 3.7663
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 63.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 16.61s
                        Total time: 11887.97s
                               ETA: 1009425.5s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.340s, learning 0.215s)
               Value function loss: 27.4564
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 3.45
               Mean episode length: 61.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 16.56s
                        Total time: 11904.53s
                               ETA: 1009953.4s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.662s, learning 0.183s)
               Value function loss: 18.6112
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 3.78
               Mean episode length: 62.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 16.85s
                        Total time: 11921.37s
                               ETA: 1010504.9s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.576s, learning 0.193s)
               Value function loss: 22.2302
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 3.45
               Mean episode length: 64.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 16.77s
                        Total time: 11938.14s
                               ETA: 1011048.9s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.739s, learning 0.204s)
               Value function loss: 31.2410
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.31
               Mean episode length: 64.28
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 16.94s
                        Total time: 11955.08s
                               ETA: 1011606.7s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.810s, learning 0.345s)
               Value function loss: 63.8784
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 6.03
               Mean episode length: 61.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 17.15s
                        Total time: 11972.24s
                               ETA: 1012181.4s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.454s, learning 0.243s)
               Value function loss: 65.1142
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 4.14
               Mean episode length: 68.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 16.70s
                        Total time: 11988.93s
                               ETA: 1012716.5s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 940 steps/s (collection: 17.101s, learning 0.315s)
               Value function loss: 282.4973
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 13.62
               Mean episode length: 62.67
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 17.42s
                        Total time: 12006.35s
                               ETA: 1013311.4s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.735s, learning 0.182s)
               Value function loss: 172.7118
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 3.50
               Mean episode length: 65.66
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 16.92s
                        Total time: 12023.27s
                               ETA: 1013863.1s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.581s, learning 0.186s)
               Value function loss: 296.5417
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 3.42
               Mean episode length: 65.73
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 16.77s
                        Total time: 12040.03s
                               ETA: 1014401.1s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.751s, learning 0.276s)
               Value function loss: 13.3147
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 39.07
               Mean episode length: 66.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 17.03s
                        Total time: 12057.06s
                               ETA: 1014960.1s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.844s, learning 0.217s)
               Value function loss: 112.5412
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 3.89
               Mean episode length: 62.46
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 11.06s
                        Total time: 12068.12s
                               ETA: 1015016.4s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.585s, learning 0.190s)
               Value function loss: 21.9840
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 63.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 8.77s
                        Total time: 12076.90s
                               ETA: 1014880.4s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.382s, learning 0.261s)
               Value function loss: 60.7426
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 8.38
               Mean episode length: 62.40
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 8.64s
                        Total time: 12085.54s
                               ETA: 1014733.5s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.483s, learning 0.177s)
               Value function loss: 5.7680
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 8.05
               Mean episode length: 62.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 8.66s
                        Total time: 12094.20s
                               ETA: 1014588.3s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.843s, learning 0.196s)
               Value function loss: 18.2247
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 6.21
               Mean episode length: 61.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 9.04s
                        Total time: 12103.24s
                               ETA: 1014475.1s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.356s, learning 0.213s)
               Value function loss: 31.9446
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 6.14
               Mean episode length: 65.33
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 8.57s
                        Total time: 12111.81s
                               ETA: 1014322.8s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.428s, learning 0.191s)
               Value function loss: 65.0911
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: 8.62
               Mean episode length: 63.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 8.62s
                        Total time: 12120.43s
                               ETA: 1014174.9s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.498s, learning 0.250s)
               Value function loss: 54.1268
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 60.20
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 8.75s
                        Total time: 12129.17s
                               ETA: 1014038.0s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.508s, learning 0.211s)
               Value function loss: 2.9704
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 63.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 8.72s
                        Total time: 12137.89s
                               ETA: 1013898.8s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.352s, learning 0.199s)
               Value function loss: 20.5497
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 10.83
               Mean episode length: 60.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 8.55s
                        Total time: 12146.44s
                               ETA: 1013745.9s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.715s, learning 0.268s)
               Value function loss: 1.8260
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 3.38
               Mean episode length: 62.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 8.98s
                        Total time: 12155.43s
                               ETA: 1013629.3s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.390s, learning 0.170s)
               Value function loss: 158.2309
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 3.48
               Mean episode length: 63.39
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 8.56s
                        Total time: 12163.99s
                               ETA: 1013477.6s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.651s, learning 0.307s)
               Value function loss: 1.8223
                    Surrogate loss: -0.0132
             Mean action noise std: 0.75
                       Mean reward: 16.34
               Mean episode length: 61.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 8.96s
                        Total time: 12172.95s
                               ETA: 1013359.3s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.447s, learning 0.205s)
               Value function loss: 268.7627
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 62.53
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 8.65s
                        Total time: 12181.60s
                               ETA: 1013215.7s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.366s, learning 0.378s)
               Value function loss: 46.9990
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 64.53
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 8.74s
                        Total time: 12190.34s
                               ETA: 1013079.9s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.538s, learning 0.172s)
               Value function loss: 51.5281
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 3.21
               Mean episode length: 61.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 8.71s
                        Total time: 12199.05s
                               ETA: 1012941.6s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.453s, learning 0.167s)
               Value function loss: 215.1815
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 3.03
               Mean episode length: 65.28
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 8.62s
                        Total time: 12207.67s
                               ETA: 1012796.0s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.221s, learning 0.268s)
               Value function loss: 65.0400
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 10.86
               Mean episode length: 62.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 8.49s
                        Total time: 12216.16s
                               ETA: 1012639.9s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.594s, learning 0.209s)
               Value function loss: 22.2891
                    Surrogate loss: -0.0024
             Mean action noise std: 0.75
                       Mean reward: 2.85
               Mean episode length: 61.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 8.80s
                        Total time: 12224.96s
                               ETA: 1012509.9s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.591s, learning 0.207s)
               Value function loss: 94.3771
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 64.62
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 8.80s
                        Total time: 12233.76s
                               ETA: 1012379.7s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.390s, learning 0.170s)
               Value function loss: 101.6783
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 3.25
               Mean episode length: 62.64
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 8.56s
                        Total time: 12242.32s
                               ETA: 1012230.1s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.550s, learning 0.293s)
               Value function loss: 218.8015
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 6.17
               Mean episode length: 62.18
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 8.84s
                        Total time: 12251.17s
                               ETA: 1012104.0s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.379s, learning 0.195s)
               Value function loss: 190.0074
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 3.33
               Mean episode length: 61.15
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 8.57s
                        Total time: 12259.74s
                               ETA: 1011956.0s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.379s, learning 0.186s)
               Value function loss: 50.3010
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: 18.94
               Mean episode length: 61.71
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 8.56s
                        Total time: 12268.30s
                               ETA: 1011807.4s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.543s, learning 0.190s)
               Value function loss: 90.0841
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 59.11
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 8.73s
                        Total time: 12277.04s
                               ETA: 1011672.9s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.724s, learning 0.172s)
               Value function loss: 59.3359
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 8.24
               Mean episode length: 58.15
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 8.90s
                        Total time: 12285.93s
                               ETA: 1011552.0s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.484s, learning 0.190s)
               Value function loss: 4.9386
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 63.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 8.67s
                        Total time: 12294.61s
                               ETA: 1011413.1s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.366s, learning 0.173s)
               Value function loss: 2.8716
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 58.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 8.54s
                        Total time: 12303.15s
                               ETA: 1011263.3s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.527s, learning 0.278s)
               Value function loss: 1.8160
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 3.30
               Mean episode length: 59.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 8.80s
                        Total time: 12311.95s
                               ETA: 1011135.6s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.735s, learning 0.194s)
               Value function loss: 366.6946
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 3.68
               Mean episode length: 60.24
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 8.93s
                        Total time: 12320.88s
                               ETA: 1011018.2s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.540s, learning 0.174s)
               Value function loss: 1.8973
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 3.41
               Mean episode length: 59.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 8.71s
                        Total time: 12329.59s
                               ETA: 1010883.4s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.261s, learning 0.184s)
               Value function loss: 131.5392
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 10.87
               Mean episode length: 61.51
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 8.45s
                        Total time: 12338.04s
                               ETA: 1010726.8s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.571s, learning 0.170s)
               Value function loss: 68.1997
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 3.64
               Mean episode length: 59.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 8.74s
                        Total time: 12346.78s
                               ETA: 1010594.7s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.603s, learning 0.302s)
               Value function loss: 34.4433
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 59.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 8.91s
                        Total time: 12355.69s
                               ETA: 1010476.2s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.412s, learning 0.191s)
               Value function loss: 173.6788
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 3.45
               Mean episode length: 60.17
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 8.60s
                        Total time: 12364.29s
                               ETA: 1010333.2s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.500s, learning 0.191s)
               Value function loss: 4.4105
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 3.36
               Mean episode length: 59.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 8.69s
                        Total time: 12372.98s
                               ETA: 1010197.5s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.653s, learning 0.211s)
               Value function loss: 90.1354
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 58.24
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 8.86s
                        Total time: 12381.84s
                               ETA: 1010076.3s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.479s, learning 0.180s)
               Value function loss: 1.9201
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 3.55
               Mean episode length: 60.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 8.66s
                        Total time: 12390.50s
                               ETA: 1009938.4s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.492s, learning 0.288s)
               Value function loss: 308.6269
                    Surrogate loss: 0.0002
             Mean action noise std: 0.75
                       Mean reward: 3.54
               Mean episode length: 56.26
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 8.78s
                        Total time: 12399.28s
                               ETA: 1009810.6s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.474s, learning 0.175s)
               Value function loss: 1.3633
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 11.18
               Mean episode length: 59.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 8.65s
                        Total time: 12407.93s
                               ETA: 1009672.4s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.423s, learning 0.204s)
               Value function loss: 1.0237
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 3.08
               Mean episode length: 59.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 8.63s
                        Total time: 12416.56s
                               ETA: 1009532.7s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.610s, learning 0.191s)
               Value function loss: 1.1444
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 8.34
               Mean episode length: 57.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 8.80s
                        Total time: 12425.36s
                               ETA: 1009407.2s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.314s, learning 0.175s)
               Value function loss: 80.7057
                    Surrogate loss: -0.0007
             Mean action noise std: 0.75
                       Mean reward: 3.19
               Mean episode length: 59.47
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 8.49s
                        Total time: 12433.85s
                               ETA: 1009256.6s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.301s, learning 0.172s)
               Value function loss: 53.2017
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 2.95
               Mean episode length: 56.93
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 8.47s
                        Total time: 12442.32s
                               ETA: 1009105.0s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.582s, learning 0.183s)
               Value function loss: 270.2693
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 3.26
               Mean episode length: 56.15
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 8.77s
                        Total time: 12451.09s
                               ETA: 1008977.3s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.553s, learning 0.164s)
               Value function loss: 40.0421
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 3.39
               Mean episode length: 61.49
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 8.72s
                        Total time: 12459.80s
                               ETA: 1008845.8s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.270s, learning 0.211s)
               Value function loss: 42.1155
                    Surrogate loss: 0.0013
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 57.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 8.48s
                        Total time: 12468.29s
                               ETA: 1008695.5s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.530s, learning 0.182s)
               Value function loss: 151.6634
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 8.45
               Mean episode length: 55.52
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 8.71s
                        Total time: 12477.00s
                               ETA: 1008564.1s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.592s, learning 0.186s)
               Value function loss: 155.0559
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 16.63
               Mean episode length: 56.71
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 8.78s
                        Total time: 12485.78s
                               ETA: 1008438.2s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.292s, learning 0.374s)
               Value function loss: 146.4709
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 8.71
               Mean episode length: 55.23
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 8.67s
                        Total time: 12494.44s
                               ETA: 1008303.5s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1764 steps/s (collection: 9.065s, learning 0.223s)
               Value function loss: 32.1378
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 13.11
               Mean episode length: 53.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 9.29s
                        Total time: 12503.73s
                               ETA: 1008219.1s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.594s, learning 0.164s)
               Value function loss: 32.9248
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 2.91
               Mean episode length: 55.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 8.76s
                        Total time: 12512.49s
                               ETA: 1008092.1s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.400s, learning 0.395s)
               Value function loss: 23.4676
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 3.27
               Mean episode length: 59.65
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 8.80s
                        Total time: 12521.28s
                               ETA: 1007968.3s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.542s, learning 0.311s)
               Value function loss: 157.3197
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 23.44
               Mean episode length: 58.40
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 8.85s
                        Total time: 12530.13s
                               ETA: 1007849.3s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.631s, learning 0.267s)
               Value function loss: 21.5601
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 3.63
               Mean episode length: 58.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 8.90s
                        Total time: 12539.03s
                               ETA: 1007734.2s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.817s, learning 0.272s)
               Value function loss: 39.5921
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.07
               Mean episode length: 55.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 9.09s
                        Total time: 12548.12s
                               ETA: 1007634.6s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.659s, learning 0.240s)
               Value function loss: 18.5658
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 6.05
               Mean episode length: 57.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 8.90s
                        Total time: 12557.02s
                               ETA: 1007519.9s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.533s, learning 0.249s)
               Value function loss: 110.7413
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 6.37
               Mean episode length: 53.05
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 8.78s
                        Total time: 12565.80s
                               ETA: 1007395.9s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.725s, learning 0.233s)
               Value function loss: 69.5387
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 3.54
               Mean episode length: 53.28
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 8.96s
                        Total time: 12574.76s
                               ETA: 1007286.3s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.743s, learning 0.271s)
               Value function loss: 3.2418
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 3.15
               Mean episode length: 57.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 9.01s
                        Total time: 12583.78s
                               ETA: 1007181.3s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.669s, learning 0.240s)
               Value function loss: 123.2317
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 3.25
               Mean episode length: 55.79
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 8.91s
                        Total time: 12592.68s
                               ETA: 1007068.1s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.056s, learning 0.182s)
               Value function loss: 66.6546
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 3.50
               Mean episode length: 55.85
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 8.24s
                        Total time: 12600.92s
                               ETA: 1006901.3s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.302s, learning 0.202s)
               Value function loss: 108.6174
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 10.90
               Mean episode length: 55.10
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 8.50s
                        Total time: 12609.43s
                               ETA: 1006756.1s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.519s, learning 0.164s)
               Value function loss: 68.3068
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 3.71
               Mean episode length: 56.19
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 8.68s
                        Total time: 12618.11s
                               ETA: 1006625.4s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.492s, learning 0.183s)
               Value function loss: 23.9281
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 11.30
               Mean episode length: 54.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 8.67s
                        Total time: 12626.78s
                               ETA: 1006494.2s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.516s, learning 0.238s)
               Value function loss: 89.4054
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 13.55
               Mean episode length: 53.16
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 8.75s
                        Total time: 12635.54s
                               ETA: 1006369.6s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.798s, learning 0.178s)
               Value function loss: 8.2564
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 3.39
               Mean episode length: 54.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 8.98s
                        Total time: 12644.51s
                               ETA: 1006262.8s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.603s, learning 0.167s)
               Value function loss: 234.1479
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 11.56
               Mean episode length: 53.88
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 8.77s
                        Total time: 12653.28s
                               ETA: 1006139.8s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.815s, learning 0.262s)
               Value function loss: 85.4165
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 2.93
               Mean episode length: 52.55
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 9.08s
                        Total time: 12662.36s
                               ETA: 1006041.3s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.489s, learning 0.278s)
               Value function loss: 264.2058
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 8.23
               Mean episode length: 52.30
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 8.77s
                        Total time: 12671.13s
                               ETA: 1005918.5s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.691s, learning 0.217s)
               Value function loss: 128.9085
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 8.21
               Mean episode length: 49.01
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 8.91s
                        Total time: 12680.04s
                               ETA: 1005806.9s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.585s, learning 0.235s)
               Value function loss: 111.8650
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 5.55
               Mean episode length: 53.45
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 8.82s
                        Total time: 12688.86s
                               ETA: 1005688.5s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.382s, learning 0.189s)
               Value function loss: 220.8390
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 2.96
               Mean episode length: 53.43
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 8.57s
                        Total time: 12697.43s
                               ETA: 1005550.7s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.446s, learning 0.192s)
               Value function loss: 219.2520
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: 3.41
               Mean episode length: 51.95
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 8.64s
                        Total time: 12706.06s
                               ETA: 1005418.3s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.704s, learning 0.182s)
               Value function loss: 212.2481
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 3.31
               Mean episode length: 52.55
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 8.89s
                        Total time: 12714.95s
                               ETA: 1005305.7s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.691s, learning 0.176s)
               Value function loss: 17.0232
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 3.12
               Mean episode length: 51.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 8.87s
                        Total time: 12723.82s
                               ETA: 1005191.8s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.544s, learning 0.271s)
               Value function loss: 97.9544
                    Surrogate loss: 0.0015
             Mean action noise std: 0.75
                       Mean reward: 5.84
               Mean episode length: 51.49
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 8.82s
                        Total time: 12732.63s
                               ETA: 1005074.0s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.386s, learning 0.189s)
               Value function loss: 111.1553
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 3.38
               Mean episode length: 52.20
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 8.57s
                        Total time: 12741.21s
                               ETA: 1004937.4s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.457s, learning 0.178s)
               Value function loss: 40.2879
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 3.61
               Mean episode length: 48.97
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 8.64s
                        Total time: 12749.84s
                               ETA: 1004805.7s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.694s, learning 0.198s)
               Value function loss: 149.8402
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 12.97
               Mean episode length: 51.72
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 8.89s
                        Total time: 12758.74s
                               ETA: 1004694.4s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.571s, learning 0.213s)
               Value function loss: 5.2981
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 15.96
               Mean episode length: 50.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 8.78s
                        Total time: 12767.52s
                               ETA: 1004574.8s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.365s, learning 0.203s)
               Value function loss: 127.0767
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 3.19
               Mean episode length: 52.60
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 8.57s
                        Total time: 12776.09s
                               ETA: 1004438.4s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.585s, learning 0.282s)
               Value function loss: 42.9360
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 2.95
               Mean episode length: 47.93
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 8.87s
                        Total time: 12784.95s
                               ETA: 1004325.8s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.569s, learning 0.193s)
               Value function loss: 145.5498
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 3.20
               Mean episode length: 50.95
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 8.76s
                        Total time: 12793.72s
                               ETA: 1004205.1s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.526s, learning 0.302s)
               Value function loss: 51.7500
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 3.54
               Mean episode length: 51.06
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 8.83s
                        Total time: 12802.54s
                               ETA: 1004089.7s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.485s, learning 0.197s)
               Value function loss: 71.3956
                    Surrogate loss: 0.0020
             Mean action noise std: 0.75
                       Mean reward: 3.68
               Mean episode length: 50.58
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 8.68s
                        Total time: 12811.23s
                               ETA: 1003963.0s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.270s, learning 0.175s)
               Value function loss: 90.6487
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 13.62
               Mean episode length: 47.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 8.44s
                        Total time: 12819.67s
                               ETA: 1003817.9s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.736s, learning 0.291s)
               Value function loss: 59.2294
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.60
               Mean episode length: 53.39
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 9.03s
                        Total time: 12828.70s
                               ETA: 1003718.6s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.235s, learning 0.216s)
               Value function loss: 135.7785
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 3.16
               Mean episode length: 50.79
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 8.45s
                        Total time: 12837.15s
                               ETA: 1003574.4s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.275s, learning 0.193s)
               Value function loss: 155.5924
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 8.43
               Mean episode length: 54.87
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 8.47s
                        Total time: 12845.62s
                               ETA: 1003431.8s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.730s, learning 0.202s)
               Value function loss: 104.8123
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 3.45
               Mean episode length: 47.79
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 8.93s
                        Total time: 12854.55s
                               ETA: 1003325.6s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.478s, learning 0.208s)
               Value function loss: 692.9888
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 18.44
               Mean episode length: 52.44
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 8.69s
                        Total time: 12863.24s
                               ETA: 1003200.3s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.617s, learning 0.223s)
               Value function loss: 196.1194
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 3.70
               Mean episode length: 50.70
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 8.84s
                        Total time: 12872.08s
                               ETA: 1003087.2s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.657s, learning 0.170s)
               Value function loss: 187.0491
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 39.16
               Mean episode length: 53.55
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 8.83s
                        Total time: 12880.90s
                               ETA: 1002973.3s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.543s, learning 0.191s)
               Value function loss: 63.0213
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 10.65
               Mean episode length: 54.56
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 8.73s
                        Total time: 12889.64s
                               ETA: 1002852.3s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.809s, learning 0.174s)
               Value function loss: 78.5153
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 48.77
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 8.98s
                        Total time: 12898.62s
                               ETA: 1002750.8s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.516s, learning 0.176s)
               Value function loss: 220.4959
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 6.68
               Mean episode length: 53.59
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 8.69s
                        Total time: 12907.31s
                               ETA: 1002626.9s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.133s, learning 0.172s)
               Value function loss: 225.0465
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 11.31
               Mean episode length: 52.47
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 8.31s
                        Total time: 12915.62s
                               ETA: 1002473.2s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.088s, learning 0.311s)
               Value function loss: 146.3154
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 3.68
               Mean episode length: 50.12
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 8.40s
                        Total time: 12924.02s
                               ETA: 1002327.0s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.393s, learning 0.181s)
               Value function loss: 60.4351
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 16.06
               Mean episode length: 51.48
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 8.57s
                        Total time: 12932.59s
                               ETA: 1002194.5s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.626s, learning 0.197s)
               Value function loss: 633.8322
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 16.14
               Mean episode length: 53.61
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 8.82s
                        Total time: 12941.41s
                               ETA: 1002081.6s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.436s, learning 0.192s)
               Value function loss: 279.7200
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 18.67
               Mean episode length: 51.43
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 8.63s
                        Total time: 12950.04s
                               ETA: 1001953.6s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.379s, learning 0.174s)
               Value function loss: 208.0662
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 34.00
               Mean episode length: 47.79
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 8.55s
                        Total time: 12958.59s
                               ETA: 1001820.1s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.373s, learning 0.243s)
               Value function loss: 429.1575
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 3.02
               Mean episode length: 47.07
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0208
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 8.62s
                        Total time: 12967.21s
                               ETA: 1001691.7s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.394s, learning 0.284s)
               Value function loss: 23.9563
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 3.53
               Mean episode length: 51.65
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 8.68s
                        Total time: 12975.89s
                               ETA: 1001568.2s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.413s, learning 0.174s)
               Value function loss: 309.1955
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 15.61
               Mean episode length: 46.39
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0218
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 8.59s
                        Total time: 12984.48s
                               ETA: 1001437.9s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.377s, learning 0.238s)
               Value function loss: 58.6210
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 11.07
               Mean episode length: 55.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0222
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 8.62s
                        Total time: 12993.09s
                               ETA: 1001309.9s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.494s, learning 0.178s)
               Value function loss: 164.3213
                    Surrogate loss: -0.0013
             Mean action noise std: 0.75
                       Mean reward: 2.93
               Mean episode length: 48.65
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 8.67s
                        Total time: 13001.76s
                               ETA: 1001186.5s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.599s, learning 0.165s)
               Value function loss: 82.1506
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 3.34
               Mean episode length: 52.58
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 8.76s
                        Total time: 13010.53s
                               ETA: 1001070.3s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.509s, learning 0.234s)
               Value function loss: 315.6858
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 5.90
               Mean episode length: 51.06
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 8.74s
                        Total time: 13019.27s
                               ETA: 1000952.7s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.670s, learning 0.234s)
               Value function loss: 379.4768
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 11.64
               Mean episode length: 52.76
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 8.90s
                        Total time: 13028.17s
                               ETA: 1000847.6s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.387s, learning 0.194s)
               Value function loss: 93.5730
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 49.16
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 8.58s
                        Total time: 13036.75s
                               ETA: 1000717.9s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.500s, learning 0.166s)
               Value function loss: 163.4255
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 16.03
               Mean episode length: 52.56
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 8.67s
                        Total time: 13045.42s
                               ETA: 1000595.0s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.700s, learning 0.184s)
               Value function loss: 77.1169
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 6.23
               Mean episode length: 51.35
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 8.88s
                        Total time: 13054.31s
                               ETA: 1000488.9s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.339s, learning 0.237s)
               Value function loss: 39.0744
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 3.78
               Mean episode length: 51.28
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 8.58s
                        Total time: 13062.88s
                               ETA: 1000359.4s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.709s, learning 0.208s)
               Value function loss: 61.2481
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 8.83
               Mean episode length: 49.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 8.92s
                        Total time: 13071.80s
                               ETA: 1000256.1s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.676s, learning 0.182s)
               Value function loss: 77.7625
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 3.14
               Mean episode length: 51.99
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 8.86s
                        Total time: 13080.66s
                               ETA: 1000148.5s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.610s, learning 0.194s)
               Value function loss: 159.0092
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 5.65
               Mean episode length: 47.11
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 8.80s
                        Total time: 13089.46s
                               ETA: 1000036.9s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.480s, learning 0.169s)
               Value function loss: 164.6888
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 8.07
               Mean episode length: 46.95
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 8.65s
                        Total time: 13098.11s
                               ETA: 999913.6s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.665s, learning 0.222s)
               Value function loss: 92.0757
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 5.58
               Mean episode length: 47.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 8.89s
                        Total time: 13107.00s
                               ETA: 999808.7s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.302s, learning 0.223s)
               Value function loss: 25.8311
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 51.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 8.52s
                        Total time: 13115.52s
                               ETA: 999676.2s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.541s, learning 0.210s)
               Value function loss: 133.0785
                    Surrogate loss: -0.0026
             Mean action noise std: 0.75
                       Mean reward: 4.02
               Mean episode length: 52.90
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 8.75s
                        Total time: 13124.27s
                               ETA: 999561.2s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.548s, learning 0.172s)
               Value function loss: 194.8110
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 11.10
               Mean episode length: 53.45
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 8.72s
                        Total time: 13132.99s
                               ETA: 999444.1s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.022s, learning 0.173s)
               Value function loss: 800.8205
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 3.10
               Mean episode length: 50.68
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 8.19s
                        Total time: 13141.19s
                               ETA: 999287.1s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.314s, learning 0.203s)
               Value function loss: 240.6546
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 3.13
               Mean episode length: 49.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 8.52s
                        Total time: 13149.70s
                               ETA: 999154.9s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.974s, learning 0.175s)
               Value function loss: 361.3594
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 11.46
               Mean episode length: 52.79
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 9.15s
                        Total time: 13158.85s
                               ETA: 999070.8s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.541s, learning 0.203s)
               Value function loss: 43.0804
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 13.24
               Mean episode length: 49.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 8.74s
                        Total time: 13167.60s
                               ETA: 998956.1s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.258s, learning 0.345s)
               Value function loss: 336.4283
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 8.32
               Mean episode length: 47.45
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0203
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 8.60s
                        Total time: 13176.20s
                               ETA: 998830.9s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.645s, learning 0.197s)
               Value function loss: 351.7214
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 3.70
               Mean episode length: 52.05
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 8.84s
                        Total time: 13185.04s
                               ETA: 998724.0s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.144s, learning 0.220s)
               Value function loss: 94.2550
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 13.22
               Mean episode length: 49.18
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 8.36s
                        Total time: 13193.41s
                               ETA: 998581.1s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.737s, learning 0.183s)
               Value function loss: 90.2351
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 48.45
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0226
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 8.92s
                        Total time: 13202.33s
                               ETA: 998480.4s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.133s, learning 0.165s)
               Value function loss: 142.2521
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 3.26
               Mean episode length: 48.62
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0216
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 8.30s
                        Total time: 13210.63s
                               ETA: 998332.9s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.566s, learning 0.240s)
               Value function loss: 90.6217
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 16.27
               Mean episode length: 50.18
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 8.81s
                        Total time: 13219.43s
                               ETA: 998223.9s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.552s, learning 0.195s)
               Value function loss: 11.5402
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 3.24
               Mean episode length: 50.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0211
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 8.75s
                        Total time: 13228.18s
                               ETA: 998110.6s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.411s, learning 0.195s)
               Value function loss: 346.7447
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 5.78
               Mean episode length: 51.16
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 8.61s
                        Total time: 13236.79s
                               ETA: 997986.9s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.334s, learning 0.169s)
               Value function loss: 71.6980
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 8.75
               Mean episode length: 51.30
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 8.50s
                        Total time: 13245.29s
                               ETA: 997855.5s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.571s, learning 0.193s)
               Value function loss: 104.8265
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 3.06
               Mean episode length: 50.63
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 8.76s
                        Total time: 13254.05s
                               ETA: 997744.0s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.490s, learning 0.171s)
               Value function loss: 7.4032
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 6.06
               Mean episode length: 51.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 8.66s
                        Total time: 13262.71s
                               ETA: 997624.9s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.386s, learning 0.180s)
               Value function loss: 46.2606
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 5.90
               Mean episode length: 51.32
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0188
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 8.57s
                        Total time: 13271.28s
                               ETA: 997498.8s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.240s, learning 0.180s)
               Value function loss: 36.3818
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 3.23
               Mean episode length: 46.92
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 8.42s
                        Total time: 13279.70s
                               ETA: 997362.0s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.268s, learning 0.171s)
               Value function loss: 147.0785
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 3.05
               Mean episode length: 55.37
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 8.44s
                        Total time: 13288.14s
                               ETA: 997226.8s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.327s, learning 0.164s)
               Value function loss: 220.2328
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 3.67
               Mean episode length: 48.76
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 8.49s
                        Total time: 13296.63s
                               ETA: 997095.6s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.542s, learning 0.203s)
               Value function loss: 155.7536
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 3.19
               Mean episode length: 51.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 8.74s
                        Total time: 13305.37s
                               ETA: 996983.7s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.568s, learning 0.193s)
               Value function loss: 160.4048
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 11.35
               Mean episode length: 52.97
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 8.76s
                        Total time: 13314.13s
                               ETA: 996873.1s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.499s, learning 0.170s)
               Value function loss: 97.4588
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 11.23
               Mean episode length: 51.73
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 8.67s
                        Total time: 13322.80s
                               ETA: 996755.8s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.184s, learning 0.165s)
               Value function loss: 25.0852
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 11.15
               Mean episode length: 50.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 8.35s
                        Total time: 13331.15s
                               ETA: 996614.8s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.165s)
               Value function loss: 239.4066
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 3.20
               Mean episode length: 44.53
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 8.38s
                        Total time: 13339.54s
                               ETA: 996476.4s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.244s, learning 0.186s)
               Value function loss: 450.1579
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 11.39
               Mean episode length: 50.50
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 8.43s
                        Total time: 13347.97s
                               ETA: 996341.8s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.846s, learning 0.164s)
               Value function loss: 492.7685
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 3.50
               Mean episode length: 50.31
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 8.01s
                        Total time: 13355.97s
                               ETA: 996176.0s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.307s, learning 0.169s)
               Value function loss: 286.4790
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 8.23
               Mean episode length: 50.48
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 8.48s
                        Total time: 13364.45s
                               ETA: 996045.2s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.348s, learning 0.174s)
               Value function loss: 329.6237
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 2.84
               Mean episode length: 48.19
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 8.52s
                        Total time: 13372.97s
                               ETA: 995918.0s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1779 steps/s (collection: 8.991s, learning 0.219s)
               Value function loss: 38.6475
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 23.37
               Mean episode length: 51.26
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0234
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 9.21s
                        Total time: 13382.18s
                               ETA: 995842.2s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.559s, learning 0.193s)
               Value function loss: 82.9511
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 8.65
               Mean episode length: 49.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0221
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 8.75s
                        Total time: 13390.93s
                               ETA: 995732.5s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.496s, learning 0.200s)
               Value function loss: 256.5219
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 13.45
               Mean episode length: 49.82
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0217
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 8.70s
                        Total time: 13399.63s
                               ETA: 995618.7s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.735s, learning 0.195s)
               Value function loss: 256.6098
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 13.36
               Mean episode length: 51.82
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0231
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 8.93s
                        Total time: 13408.56s
                               ETA: 995522.6s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.340s, learning 0.208s)
               Value function loss: 228.3139
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 3.04
               Mean episode length: 47.22
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0218
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 8.55s
                        Total time: 13417.11s
                               ETA: 995398.2s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.603s, learning 0.185s)
               Value function loss: 92.9680
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 23.30
               Mean episode length: 45.74
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0217
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 8.79s
                        Total time: 13425.90s
                               ETA: 995291.7s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.326s, learning 0.192s)
               Value function loss: 126.8097
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 5.53
               Mean episode length: 48.36
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 8.52s
                        Total time: 13434.42s
                               ETA: 995165.4s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1797 steps/s (collection: 8.915s, learning 0.198s)
               Value function loss: 496.5141
                    Surrogate loss: -0.0011
             Mean action noise std: 0.75
                       Mean reward: 3.64
               Mean episode length: 49.82
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0200
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 9.11s
                        Total time: 13443.53s
                               ETA: 995083.3s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.226s, learning 0.174s)
               Value function loss: 420.4030
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 10.91
               Mean episode length: 50.64
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0207
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 8.40s
                        Total time: 13451.93s
                               ETA: 994948.5s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.174s)
               Value function loss: 483.8850
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 3.18
               Mean episode length: 48.35
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 8.67s
                        Total time: 13460.60s
                               ETA: 994834.1s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.310s, learning 0.174s)
               Value function loss: 471.3280
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 8.05
               Mean episode length: 51.19
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 8.48s
                        Total time: 13469.08s
                               ETA: 994705.9s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.328s, learning 0.177s)
               Value function loss: 480.2010
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 3.40
               Mean episode length: 51.35
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 8.51s
                        Total time: 13477.59s
                               ETA: 994579.5s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.285s, learning 0.249s)
               Value function loss: 43.0147
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 13.78
               Mean episode length: 50.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 8.53s
                        Total time: 13486.12s
                               ETA: 994455.4s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1509 steps/s (collection: 10.680s, learning 0.175s)
               Value function loss: 27.6737
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 15.94
               Mean episode length: 52.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 10.85s
                        Total time: 13496.98s
                               ETA: 994502.4s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.525s, learning 0.300s)
               Value function loss: 45.3395
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 6.58
               Mean episode length: 50.96
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0255
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 16.82s
                        Total time: 13513.80s
                               ETA: 994989.0s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.452s, learning 0.167s)
               Value function loss: 653.5149
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 3.09
               Mean episode length: 47.23
                  Mean reward/step: 0.35
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 16.62s
                        Total time: 13530.42s
                               ETA: 995459.7s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.649s, learning 0.168s)
               Value function loss: 1215.4005
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 31.31
               Mean episode length: 48.07
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 16.82s
                        Total time: 13547.24s
                               ETA: 995944.1s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.005s, learning 0.281s)
               Value function loss: 43.1595
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 26.69
               Mean episode length: 53.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0254
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 16.29s
                        Total time: 13563.52s
                               ETA: 996388.8s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.614s, learning 0.216s)
               Value function loss: 141.0175
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 3.61
               Mean episode length: 50.05
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 16.83s
                        Total time: 13580.35s
                               ETA: 996872.8s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.272s, learning 0.173s)
               Value function loss: 662.8866
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 3.76
               Mean episode length: 53.08
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 16.44s
                        Total time: 13596.80s
                               ETA: 997327.7s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.762s, learning 0.170s)
               Value function loss: 147.7667
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 52.76
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0262
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 16.93s
                        Total time: 13613.73s
                               ETA: 997817.7s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.890s, learning 0.228s)
               Value function loss: 133.2301
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 5.51
               Mean episode length: 48.13
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0256
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 17.12s
                        Total time: 13630.85s
                               ETA: 998320.6s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.421s, learning 0.181s)
               Value function loss: 103.1198
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 26.40
               Mean episode length: 52.15
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0274
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 16.60s
                        Total time: 13647.45s
                               ETA: 998785.0s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.642s, learning 0.295s)
               Value function loss: 74.6515
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 19.32
               Mean episode length: 50.99
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 16.94s
                        Total time: 13664.39s
                               ETA: 999273.1s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.803s, learning 0.178s)
               Value function loss: 79.3629
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 3.53
               Mean episode length: 52.47
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0259
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 16.98s
                        Total time: 13681.37s
                               ETA: 999763.6s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.469s, learning 0.178s)
               Value function loss: 334.6194
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 11.45
               Mean episode length: 53.29
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 16.65s
                        Total time: 13698.02s
                               ETA: 1000229.1s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.100s, learning 0.215s)
               Value function loss: 182.2077
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 11.89
               Mean episode length: 54.60
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 16.31s
                        Total time: 13714.33s
                               ETA: 1000669.5s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.526s, learning 0.246s)
               Value function loss: 94.2880
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 16.10
               Mean episode length: 53.87
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 16.77s
                        Total time: 13731.10s
                               ETA: 1001142.6s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.883s, learning 0.266s)
               Value function loss: 121.4329
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 3.89
               Mean episode length: 49.92
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 17.15s
                        Total time: 13748.25s
                               ETA: 1001642.5s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.261s, learning 0.182s)
               Value function loss: 57.3416
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 36.57
               Mean episode length: 50.89
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0250
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 16.44s
                        Total time: 13764.70s
                               ETA: 1002090.2s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.847s, learning 0.202s)
               Value function loss: 261.9795
                    Surrogate loss: -0.0044
             Mean action noise std: 0.75
                       Mean reward: 3.84
               Mean episode length: 52.28
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 17.05s
                        Total time: 13781.75s
                               ETA: 1002581.3s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.813s, learning 0.176s)
               Value function loss: 16.0067
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 3.87
               Mean episode length: 51.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 16.99s
                        Total time: 13798.73s
                               ETA: 1003067.4s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.417s, learning 0.177s)
               Value function loss: 317.5943
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 3.17
               Mean episode length: 50.36
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 16.59s
                        Total time: 13815.33s
                               ETA: 1003523.9s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.411s, learning 0.273s)
               Value function loss: 292.6707
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 9.12
               Mean episode length: 51.55
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0234
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 16.68s
                        Total time: 13832.01s
                               ETA: 1003986.3s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.160s, learning 0.182s)
               Value function loss: 269.5056
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 3.98
               Mean episode length: 49.13
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0222
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 16.34s
                        Total time: 13848.35s
                               ETA: 1004423.2s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.224s, learning 0.228s)
               Value function loss: 35.2345
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 3.96
               Mean episode length: 50.41
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0218
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 16.45s
                        Total time: 13864.81s
                               ETA: 1004867.3s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.790s, learning 0.340s)
               Value function loss: 234.1909
                    Surrogate loss: 0.0000
             Mean action noise std: 0.75
                       Mean reward: 14.24
               Mean episode length: 53.61
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 17.13s
                        Total time: 13881.94s
                               ETA: 1005359.9s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.491s, learning 0.165s)
               Value function loss: 66.1382
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 6.47
               Mean episode length: 51.79
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0229
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 16.66s
                        Total time: 13898.59s
                               ETA: 1005817.5s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.678s, learning 0.186s)
               Value function loss: 8.1880
                    Surrogate loss: -0.0222
             Mean action noise std: 0.75
                       Mean reward: 6.77
               Mean episode length: 49.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 16.86s
                        Total time: 13915.46s
                               ETA: 1006289.4s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.656s, learning 0.202s)
               Value function loss: 379.1283
                    Surrogate loss: -0.0016
             Mean action noise std: 0.75
                       Mean reward: 3.55
               Mean episode length: 46.42
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 16.86s
                        Total time: 13932.31s
                               ETA: 1006760.2s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.582s, learning 0.213s)
               Value function loss: 113.9933
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 4.14
               Mean episode length: 53.03
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0208
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 16.80s
                        Total time: 13949.11s
                               ETA: 1007225.8s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.472s, learning 0.209s)
               Value function loss: 601.0907
                    Surrogate loss: -0.0027
             Mean action noise std: 0.75
                       Mean reward: 3.53
               Mean episode length: 50.96
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0192
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 16.68s
                        Total time: 13965.79s
                               ETA: 1007682.3s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.417s, learning 0.171s)
               Value function loss: 731.4830
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 6.29
               Mean episode length: 51.99
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 16.59s
                        Total time: 13982.38s
                               ETA: 1008131.4s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.500s, learning 0.292s)
               Value function loss: 312.7877
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 19.01
               Mean episode length: 53.77
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 16.79s
                        Total time: 13999.17s
                               ETA: 1008594.6s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.409s, learning 0.175s)
               Value function loss: 879.3838
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 21.30
               Mean episode length: 52.51
                  Mean reward/step: 0.39
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 16.58s
                        Total time: 14015.75s
                               ETA: 1009042.1s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.185s, learning 0.180s)
               Value function loss: 315.0995
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 9.59
               Mean episode length: 52.26
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0271
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 16.37s
                        Total time: 14032.12s
                               ETA: 1009473.2s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.665s, learning 0.207s)
               Value function loss: 383.4533
                    Surrogate loss: 0.0003
             Mean action noise std: 0.75
                       Mean reward: 3.27
               Mean episode length: 50.87
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0284
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 16.87s
                        Total time: 14048.99s
                               ETA: 1009940.0s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.688s, learning 0.288s)
               Value function loss: 559.4965
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 3.72
               Mean episode length: 49.63
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 16.98s
                        Total time: 14065.97s
                               ETA: 1010413.7s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.392s, learning 0.210s)
               Value function loss: 476.2136
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 3.50
               Mean episode length: 54.67
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0305
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 16.60s
                        Total time: 14082.57s
                               ETA: 1010859.8s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.650s, learning 0.185s)
               Value function loss: 150.5693
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 11.12
               Mean episode length: 50.43
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 16.84s
                        Total time: 14099.40s
                               ETA: 1011321.9s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.466s, learning 0.249s)
               Value function loss: 70.2223
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 34.45
               Mean episode length: 52.72
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0303
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 16.72s
                        Total time: 14116.12s
                               ETA: 1011774.8s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.160s, learning 0.167s)
               Value function loss: 92.2279
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 3.32
               Mean episode length: 52.20
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 11.33s
                        Total time: 14127.44s
                               ETA: 1011841.0s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.516s, learning 0.328s)
               Value function loss: 878.2409
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 6.40
               Mean episode length: 52.37
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0290
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 8.84s
                        Total time: 14136.29s
                               ETA: 1011729.5s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.163s, learning 0.171s)
               Value function loss: 555.9168
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 4.21
               Mean episode length: 55.15
                  Mean reward/step: 0.29
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0284
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 8.33s
                        Total time: 14144.62s
                               ETA: 1011581.5s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.635s, learning 0.184s)
               Value function loss: 189.2185
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 11.92
               Mean episode length: 50.60
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0309
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 8.82s
                        Total time: 14153.44s
                               ETA: 1011468.5s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.417s, learning 0.366s)
               Value function loss: 104.4195
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 21.37
               Mean episode length: 51.99
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 8.78s
                        Total time: 14162.22s
                               ETA: 1011353.0s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.235s, learning 0.173s)
               Value function loss: 240.9432
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 21.33
               Mean episode length: 55.18
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0314
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 8.41s
                        Total time: 14170.63s
                               ETA: 1011210.9s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.325s, learning 0.167s)
               Value function loss: 177.0173
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 13.37
               Mean episode length: 52.48
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0309
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 8.49s
                        Total time: 14179.12s
                               ETA: 1011075.0s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.315s, learning 0.181s)
               Value function loss: 213.0315
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 3.72
               Mean episode length: 52.13
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0299
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 8.50s
                        Total time: 14187.62s
                               ETA: 1010939.6s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.558s, learning 0.168s)
               Value function loss: 194.1601
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 3.72
               Mean episode length: 53.12
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 8.73s
                        Total time: 14196.34s
                               ETA: 1010820.7s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.410s, learning 0.215s)
               Value function loss: 266.4764
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 19.13
               Mean episode length: 56.21
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 8.62s
                        Total time: 14204.97s
                               ETA: 1010694.8s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.204s, learning 0.309s)
               Value function loss: 474.8908
                    Surrogate loss: -0.0039
             Mean action noise std: 0.75
                       Mean reward: 16.88
               Mean episode length: 52.89
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 8.51s
                        Total time: 14213.48s
                               ETA: 1010561.2s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.369s, learning 0.200s)
               Value function loss: 603.4626
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 16.85
               Mean episode length: 52.19
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0220
Mean episode consecutive_successes: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 8.57s
                        Total time: 14222.05s
                               ETA: 1010431.7s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.479s, learning 0.194s)
               Value function loss: 547.8355
                    Surrogate loss: -0.0038
             Mean action noise std: 0.75
                       Mean reward: 14.03
               Mean episode length: 52.86
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0220
Mean episode consecutive_successes: 0.0323
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 8.67s
                        Total time: 14230.72s
                               ETA: 1010309.7s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.200s, learning 0.208s)
               Value function loss: 491.9403
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 26.27
               Mean episode length: 54.61
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0367
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 8.41s
                        Total time: 14239.13s
                               ETA: 1010169.1s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.503s, learning 0.327s)
               Value function loss: 98.5559
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 3.36
               Mean episode length: 52.18
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0401
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 8.83s
                        Total time: 14247.96s
                               ETA: 1010058.7s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.797s, learning 0.201s)
               Value function loss: 236.9046
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 12.18
               Mean episode length: 52.63
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0380
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 9.00s
                        Total time: 14256.96s
                               ETA: 1009960.3s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.558s, learning 0.216s)
               Value function loss: 107.4074
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 31.11
               Mean episode length: 51.46
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0377
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 8.77s
                        Total time: 14265.74s
                               ETA: 1009846.1s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.202s, learning 0.181s)
               Value function loss: 415.9144
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 4.43
               Mean episode length: 52.75
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0382
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 8.38s
                        Total time: 14274.12s
                               ETA: 1009704.5s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.552s, learning 0.183s)
               Value function loss: 204.5189
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 3.93
               Mean episode length: 51.90
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0374
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 8.74s
                        Total time: 14282.85s
                               ETA: 1009587.9s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.187s, learning 0.187s)
               Value function loss: 385.1031
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 6.98
               Mean episode length: 54.82
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0352
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 8.37s
                        Total time: 14291.23s
                               ETA: 1009446.0s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.369s, learning 0.173s)
               Value function loss: 213.3606
                    Surrogate loss: -0.0017
             Mean action noise std: 0.75
                       Mean reward: 6.04
               Mean episode length: 54.83
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 8.54s
                        Total time: 14299.77s
                               ETA: 1009316.1s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.475s, learning 0.190s)
               Value function loss: 162.2999
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 28.51
               Mean episode length: 53.54
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0365
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 8.67s
                        Total time: 14308.44s
                               ETA: 1009195.1s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.867s, learning 0.177s)
               Value function loss: 192.6202
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 6.42
               Mean episode length: 53.60
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0357
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 9.04s
                        Total time: 14317.48s
                               ETA: 1009100.9s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.270s, learning 0.376s)
               Value function loss: 213.2203
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 6.50
               Mean episode length: 53.78
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0334
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 8.65s
                        Total time: 14326.13s
                               ETA: 1008978.8s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.527s, learning 0.204s)
               Value function loss: 26.0385
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 6.30
               Mean episode length: 53.99
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0324
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 8.73s
                        Total time: 14334.86s
                               ETA: 1008862.9s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.616s, learning 0.226s)
               Value function loss: 490.0542
                    Surrogate loss: 0.0004
             Mean action noise std: 0.75
                       Mean reward: 3.71
               Mean episode length: 50.63
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 8.84s
                        Total time: 14343.70s
                               ETA: 1008754.8s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.628s, learning 0.184s)
               Value function loss: 142.5499
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 3.91
               Mean episode length: 50.18
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0295
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 8.81s
                        Total time: 14352.51s
                               ETA: 1008644.9s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.664s, learning 0.166s)
               Value function loss: 332.6934
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 21.42
               Mean episode length: 55.25
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0300
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 8.83s
                        Total time: 14361.34s
                               ETA: 1008536.3s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.549s, learning 0.188s)
               Value function loss: 219.9182
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 3.46
               Mean episode length: 53.50
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 8.74s
                        Total time: 14370.08s
                               ETA: 1008421.4s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.795s, learning 0.175s)
               Value function loss: 208.2240
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 16.83
               Mean episode length: 53.13
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 8.97s
                        Total time: 14379.05s
                               ETA: 1008323.0s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.858s, learning 0.172s)
               Value function loss: 178.6975
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 14.02
               Mean episode length: 52.61
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0302
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 9.03s
                        Total time: 14388.08s
                               ETA: 1008228.9s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.896s, learning 0.228s)
               Value function loss: 193.2096
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 6.42
               Mean episode length: 53.43
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 9.12s
                        Total time: 14397.20s
                               ETA: 1008141.5s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.550s, learning 0.213s)
               Value function loss: 152.1678
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 11.65
               Mean episode length: 54.32
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0279
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 8.76s
                        Total time: 14405.96s
                               ETA: 1008028.9s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1800 steps/s (collection: 8.900s, learning 0.199s)
               Value function loss: 375.3830
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 24.42
               Mean episode length: 53.71
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0286
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 9.10s
                        Total time: 14415.06s
                               ETA: 1007940.0s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.732s, learning 0.167s)
               Value function loss: 933.7831
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 23.75
               Mean episode length: 53.48
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0290
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 8.90s
                        Total time: 14423.96s
                               ETA: 1007837.3s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.400s, learning 0.173s)
               Value function loss: 191.1339
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 18.70
               Mean episode length: 55.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0329
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 8.57s
                        Total time: 14432.53s
                               ETA: 1007711.8s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.534s, learning 0.183s)
               Value function loss: 231.0280
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 11.50
               Mean episode length: 53.36
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0320
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 8.72s
                        Total time: 14441.25s
                               ETA: 1007596.7s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.819s, learning 0.226s)
               Value function loss: 111.5866
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 26.77
               Mean episode length: 56.13
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0323
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 9.04s
                        Total time: 14450.30s
                               ETA: 1007504.5s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.589s, learning 0.186s)
               Value function loss: 45.5598
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 8.91
               Mean episode length: 53.36
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 8.77s
                        Total time: 14459.07s
                               ETA: 1007393.7s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.349s, learning 0.167s)
               Value function loss: 110.6407
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 6.43
               Mean episode length: 55.20
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 8.52s
                        Total time: 14467.59s
                               ETA: 1007265.0s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.582s, learning 0.228s)
               Value function loss: 664.7512
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 11.30
               Mean episode length: 50.84
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 8.81s
                        Total time: 14476.40s
                               ETA: 1007156.8s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.476s, learning 0.178s)
               Value function loss: 660.8542
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 21.46
               Mean episode length: 53.60
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0301
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 8.65s
                        Total time: 14485.05s
                               ETA: 1007038.0s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.669s, learning 0.195s)
               Value function loss: 468.0928
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 14.27
               Mean episode length: 54.03
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0316
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 8.86s
                        Total time: 14493.92s
                               ETA: 1006933.9s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.596s, learning 0.205s)
               Value function loss: 109.2548
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 16.57
               Mean episode length: 54.41
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 8.80s
                        Total time: 14502.72s
                               ETA: 1006825.6s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.609s, learning 0.194s)
               Value function loss: 477.1240
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 13.91
               Mean episode length: 54.43
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0326
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 8.80s
                        Total time: 14511.52s
                               ETA: 1006717.6s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.401s, learning 0.340s)
               Value function loss: 232.7277
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 3.49
               Mean episode length: 55.05
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0345
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 8.74s
                        Total time: 14520.26s
                               ETA: 1006605.4s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.420s, learning 0.166s)
               Value function loss: 393.8354
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 3.60
               Mean episode length: 53.14
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0352
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 8.59s
                        Total time: 14528.85s
                               ETA: 1006482.6s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.597s, learning 0.163s)
               Value function loss: 296.3678
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 3.71
               Mean episode length: 51.17
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0337
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 8.76s
                        Total time: 14537.61s
                               ETA: 1006372.0s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.337s, learning 0.177s)
               Value function loss: 529.1674
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 16.36
               Mean episode length: 55.38
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0345
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 8.51s
                        Total time: 14546.12s
                               ETA: 1006244.6s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.388s, learning 0.325s)
               Value function loss: 294.0391
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 16.51
               Mean episode length: 55.69
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0348
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 8.71s
                        Total time: 14554.84s
                               ETA: 1006131.1s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.610s, learning 0.206s)
               Value function loss: 54.4468
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 8.44
               Mean episode length: 53.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0367
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 8.82s
                        Total time: 14563.65s
                               ETA: 1006024.8s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.645s, learning 0.186s)
               Value function loss: 189.7732
                    Surrogate loss: -0.0006
             Mean action noise std: 0.75
                       Mean reward: 3.52
               Mean episode length: 50.93
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0360
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 8.83s
                        Total time: 14572.48s
                               ETA: 1005919.7s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.513s, learning 0.189s)
               Value function loss: 589.5807
                    Surrogate loss: -0.0019
             Mean action noise std: 0.75
                       Mean reward: 4.03
               Mean episode length: 54.74
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0354
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 8.70s
                        Total time: 14581.18s
                               ETA: 1005805.8s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.641s, learning 0.182s)
               Value function loss: 496.1168
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 28.45
               Mean episode length: 53.90
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0376
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 8.82s
                        Total time: 14590.01s
                               ETA: 1005700.4s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.433s, learning 0.185s)
               Value function loss: 919.7769
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 8.99
               Mean episode length: 57.21
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0363
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 8.62s
                        Total time: 14598.63s
                               ETA: 1005581.1s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.595s, learning 0.201s)
               Value function loss: 955.9901
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 11.13
               Mean episode length: 51.64
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0354
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 8.80s
                        Total time: 14607.42s
                               ETA: 1005474.1s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.656s, learning 0.174s)
               Value function loss: 424.2317
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 9.36
               Mean episode length: 53.17
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0363
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 8.83s
                        Total time: 14616.25s
                               ETA: 1005369.6s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.518s, learning 0.237s)
               Value function loss: 957.1979
                    Surrogate loss: -0.0033
             Mean action noise std: 0.75
                       Mean reward: 52.25
               Mean episode length: 51.93
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0478
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 8.75s
                        Total time: 14625.01s
                               ETA: 1005260.1s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.360s, learning 0.181s)
               Value function loss: 563.6201
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 41.90
               Mean episode length: 52.22
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0500
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 8.54s
                        Total time: 14633.55s
                               ETA: 1005136.0s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.401s, learning 0.170s)
               Value function loss: 1018.0500
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 4.18
               Mean episode length: 53.44
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0474
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 8.57s
                        Total time: 14642.12s
                               ETA: 1005014.2s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.375s, learning 0.185s)
               Value function loss: 352.1115
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 6.39
               Mean episode length: 52.02
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0509
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 8.56s
                        Total time: 14650.68s
                               ETA: 1004891.7s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.717s, learning 0.211s)
               Value function loss: 1134.4351
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 46.80
               Mean episode length: 56.55
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0322
Mean episode consecutive_successes: 0.0519
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 8.93s
                        Total time: 14659.61s
                               ETA: 1004794.6s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.554s, learning 0.262s)
               Value function loss: 608.4645
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 46.72
               Mean episode length: 54.28
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0303
Mean episode consecutive_successes: 0.0529
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 8.82s
                        Total time: 14668.42s
                               ETA: 1004690.0s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.501s, learning 0.207s)
               Value function loss: 436.1320
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 34.75
               Mean episode length: 55.73
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0562
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 8.71s
                        Total time: 14677.13s
                               ETA: 1004578.1s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.651s, learning 0.203s)
               Value function loss: 944.5883
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 13.76
               Mean episode length: 54.18
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0547
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 8.85s
                        Total time: 14685.98s
                               ETA: 1004476.4s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.440s, learning 0.198s)
               Value function loss: 161.7325
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 3.22
               Mean episode length: 51.87
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0522
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 8.64s
                        Total time: 14694.62s
                               ETA: 1004360.0s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.203s, learning 0.267s)
               Value function loss: 252.1065
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 11.17
               Mean episode length: 51.90
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0521
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 8.47s
                        Total time: 14703.09s
                               ETA: 1004232.3s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.794s, learning 0.216s)
               Value function loss: 485.0282
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 29.23
               Mean episode length: 53.70
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0532
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 9.01s
                        Total time: 14712.10s
                               ETA: 1004141.6s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.548s, learning 0.170s)
               Value function loss: 334.7896
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 19.08
               Mean episode length: 54.22
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0523
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 8.72s
                        Total time: 14720.82s
                               ETA: 1004031.2s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.500s, learning 0.310s)
               Value function loss: 214.0018
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 6.84
               Mean episode length: 56.06
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0488
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 8.81s
                        Total time: 14729.63s
                               ETA: 1003927.1s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.486s, learning 0.172s)
               Value function loss: 484.6302
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 3.84
               Mean episode length: 54.19
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0492
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 8.66s
                        Total time: 14738.29s
                               ETA: 1003812.8s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.568s, learning 0.218s)
               Value function loss: 522.8218
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 4.07
               Mean episode length: 52.91
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0497
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 8.79s
                        Total time: 14747.07s
                               ETA: 1003707.4s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.706s, learning 0.163s)
               Value function loss: 553.6029
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 18.77
               Mean episode length: 55.23
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0484
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 8.87s
                        Total time: 14755.94s
                               ETA: 1003607.7s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.601s, learning 0.186s)
               Value function loss: 1097.9710
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 31.80
               Mean episode length: 54.46
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0308
Mean episode consecutive_successes: 0.0470
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 8.79s
                        Total time: 14764.73s
                               ETA: 1003502.6s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.506s, learning 0.193s)
               Value function loss: 548.4988
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 46.93
               Mean episode length: 54.46
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0522
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 8.70s
                        Total time: 14773.43s
                               ETA: 1003391.7s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.452s, learning 0.215s)
               Value function loss: 416.5484
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 3.96
               Mean episode length: 57.01
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0536
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 8.67s
                        Total time: 14782.10s
                               ETA: 1003278.7s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.640s, learning 0.180s)
               Value function loss: 532.9092
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 8.89
               Mean episode length: 52.03
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0511
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 8.82s
                        Total time: 14790.92s
                               ETA: 1003176.3s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.388s, learning 0.287s)
               Value function loss: 1095.2195
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 19.10
               Mean episode length: 55.35
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0514
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 8.68s
                        Total time: 14799.59s
                               ETA: 1003064.2s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.686s, learning 0.170s)
               Value function loss: 345.2390
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 3.80
               Mean episode length: 53.73
                  Mean reward/step: 0.23
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0524
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 8.86s
                        Total time: 14808.45s
                               ETA: 1002964.4s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.775s, learning 0.202s)
               Value function loss: 430.8861
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 3.49
               Mean episode length: 51.40
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0531
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 8.98s
                        Total time: 14817.42s
                               ETA: 1002872.9s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.522s, learning 0.200s)
               Value function loss: 749.4761
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 26.85
               Mean episode length: 54.75
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0215
Mean episode consecutive_successes: 0.0545
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 8.72s
                        Total time: 14826.15s
                               ETA: 1002764.4s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.819s, learning 0.224s)
               Value function loss: 284.5632
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 3.37
               Mean episode length: 55.19
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0530
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 9.04s
                        Total time: 14835.19s
                               ETA: 1002677.6s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.542s, learning 0.175s)
               Value function loss: 560.5112
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 3.43
               Mean episode length: 51.87
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0521
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 8.72s
                        Total time: 14843.91s
                               ETA: 1002569.0s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.537s, learning 0.177s)
               Value function loss: 338.2810
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 13.72
               Mean episode length: 53.92
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0264
Mean episode consecutive_successes: 0.0491
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 8.71s
                        Total time: 14852.62s
                               ETA: 1002460.3s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.429s, learning 0.192s)
               Value function loss: 692.8218
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 28.87
               Mean episode length: 53.22
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0273
Mean episode consecutive_successes: 0.0514
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 8.62s
                        Total time: 14861.24s
                               ETA: 1002345.5s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.759s, learning 0.211s)
               Value function loss: 325.7390
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 17.16
               Mean episode length: 55.35
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0215
Mean episode consecutive_successes: 0.0511
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 8.97s
                        Total time: 14870.21s
                               ETA: 1002254.3s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.197s, learning 0.389s)
               Value function loss: 426.2821
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 29.28
               Mean episode length: 53.94
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0533
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 8.59s
                        Total time: 14878.80s
                               ETA: 1002137.4s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.756s, learning 0.211s)
               Value function loss: 183.3793
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 13.91
               Mean episode length: 53.67
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0534
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 8.97s
                        Total time: 14887.77s
                               ETA: 1002046.3s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.642s, learning 0.182s)
               Value function loss: 914.6685
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 3.92
               Mean episode length: 54.01
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0508
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 8.82s
                        Total time: 14896.59s
                               ETA: 1001945.6s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.480s, learning 0.290s)
               Value function loss: 692.1003
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 26.79
               Mean episode length: 54.69
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0510
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 8.77s
                        Total time: 14905.36s
                               ETA: 1001841.5s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.433s, learning 0.174s)
               Value function loss: 953.3754
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 34.40
               Mean episode length: 50.87
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0303
Mean episode consecutive_successes: 0.0510
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 8.61s
                        Total time: 14913.97s
                               ETA: 1001726.5s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.692s, learning 0.171s)
               Value function loss: 721.7668
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 47.74
               Mean episode length: 56.39
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0537
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 8.86s
                        Total time: 14922.83s
                               ETA: 1001628.9s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.425s, learning 0.193s)
               Value function loss: 297.5951
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 16.86
               Mean episode length: 53.59
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0561
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 8.62s
                        Total time: 14931.45s
                               ETA: 1001515.0s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.486s, learning 0.306s)
               Value function loss: 887.7553
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 8.82
               Mean episode length: 51.40
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0548
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 8.79s
                        Total time: 14940.24s
                               ETA: 1001412.8s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.376s, learning 0.231s)
               Value function loss: 1004.9401
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 3.52
               Mean episode length: 51.61
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0361
Mean episode consecutive_successes: 0.0544
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 8.61s
                        Total time: 14948.85s
                               ETA: 1001298.4s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.658s, learning 0.182s)
               Value function loss: 579.7178
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 21.63
               Mean episode length: 54.22
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0342
Mean episode consecutive_successes: 0.0582
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 8.84s
                        Total time: 14957.69s
                               ETA: 1001199.7s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.584s, learning 0.185s)
               Value function loss: 429.3108
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 9.00
               Mean episode length: 55.43
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0573
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 8.77s
                        Total time: 14966.46s
                               ETA: 1001096.4s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.787s, learning 0.200s)
               Value function loss: 274.1110
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 37.82
               Mean episode length: 56.06
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0630
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 8.99s
                        Total time: 14975.44s
                               ETA: 1001007.8s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.532s, learning 0.185s)
               Value function loss: 230.4466
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 8.90
               Mean episode length: 51.68
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0636
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 8.72s
                        Total time: 14984.16s
                               ETA: 1000901.3s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.545s, learning 0.191s)
               Value function loss: 437.3923
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 14.98
               Mean episode length: 54.44
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0614
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 8.74s
                        Total time: 14992.90s
                               ETA: 1000796.1s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.574s, learning 0.174s)
               Value function loss: 359.4079
                    Surrogate loss: -0.0052
             Mean action noise std: 0.75
                       Mean reward: 19.62
               Mean episode length: 55.30
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0613
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 8.75s
                        Total time: 15001.64s
                               ETA: 1000691.9s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.466s, learning 0.184s)
               Value function loss: 1027.6744
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 11.31
               Mean episode length: 52.47
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0587
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 8.65s
                        Total time: 15010.29s
                               ETA: 1000581.3s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.523s, learning 0.176s)
               Value function loss: 369.9222
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 9.33
               Mean episode length: 53.23
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0615
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 8.70s
                        Total time: 15018.99s
                               ETA: 1000474.1s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.817s, learning 0.312s)
               Value function loss: 486.7886
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 16.69
               Mean episode length: 52.85
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0220
Mean episode consecutive_successes: 0.0611
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 9.13s
                        Total time: 15028.12s
                               ETA: 1000395.6s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.563s, learning 0.190s)
               Value function loss: 605.1950
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 40.83
               Mean episode length: 56.14
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0613
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 8.75s
                        Total time: 15036.87s
                               ETA: 1000292.3s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.694s, learning 0.179s)
               Value function loss: 447.5753
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 3.97
               Mean episode length: 51.92
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0283
Mean episode consecutive_successes: 0.0582
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 8.87s
                        Total time: 15045.75s
                               ETA: 1000197.0s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.551s, learning 0.165s)
               Value function loss: 305.4790
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 10.97
               Mean episode length: 52.56
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0317
Mean episode consecutive_successes: 0.0554
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 8.72s
                        Total time: 15054.46s
                               ETA: 1000091.4s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.651s, learning 0.203s)
               Value function loss: 239.1659
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 14.80
               Mean episode length: 54.93
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0239
Mean episode consecutive_successes: 0.0568
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 8.85s
                        Total time: 15063.32s
                               ETA: 999995.1s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.457s, learning 0.191s)
               Value function loss: 418.0937
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 16.84
               Mean episode length: 55.63
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0592
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 8.65s
                        Total time: 15071.96s
                               ETA: 999885.3s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.234s, learning 0.198s)
               Value function loss: 217.2068
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 9.27
               Mean episode length: 54.13
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0576
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 8.43s
                        Total time: 15080.40s
                               ETA: 999761.3s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.573s, learning 0.181s)
               Value function loss: 292.0303
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 16.67
               Mean episode length: 54.71
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0562
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 8.75s
                        Total time: 15089.15s
                               ETA: 999658.7s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.468s, learning 0.276s)
               Value function loss: 173.5324
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 14.17
               Mean episode length: 54.34
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0550
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 8.74s
                        Total time: 15097.89s
                               ETA: 999555.7s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.481s, learning 0.184s)
               Value function loss: 532.0791
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 16.86
               Mean episode length: 58.09
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0536
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 8.67s
                        Total time: 15106.56s
                               ETA: 999447.6s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.707s, learning 0.191s)
               Value function loss: 616.9493
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 47.45
               Mean episode length: 54.97
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0557
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 8.90s
                        Total time: 15115.46s
                               ETA: 999355.0s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.543s, learning 0.284s)
               Value function loss: 385.6923
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 11.57
               Mean episode length: 53.43
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0561
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 8.83s
                        Total time: 15124.29s
                               ETA: 999257.8s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.450s, learning 0.180s)
               Value function loss: 436.3318
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 24.64
               Mean episode length: 53.00
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0538
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 8.63s
                        Total time: 15132.91s
                               ETA: 999147.7s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.417s, learning 0.315s)
               Value function loss: 584.0176
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 9.61
               Mean episode length: 53.69
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0508
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 8.73s
                        Total time: 15141.65s
                               ETA: 999044.5s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.646s, learning 0.190s)
               Value function loss: 528.6600
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 25.01
               Mean episode length: 55.99
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0526
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 8.84s
                        Total time: 15150.48s
                               ETA: 998948.2s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.850s, learning 0.276s)
               Value function loss: 607.4049
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 14.64
               Mean episode length: 53.38
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0264
Mean episode consecutive_successes: 0.0519
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 9.13s
                        Total time: 15159.61s
                               ETA: 998871.2s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.412s, learning 0.205s)
               Value function loss: 675.2725
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 29.54
               Mean episode length: 54.60
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0298
Mean episode consecutive_successes: 0.0548
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 8.62s
                        Total time: 15168.23s
                               ETA: 998760.8s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.283s, learning 0.206s)
               Value function loss: 515.9658
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 6.21
               Mean episode length: 51.18
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0565
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 8.49s
                        Total time: 15176.71s
                               ETA: 998642.0s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.565s, learning 0.199s)
               Value function loss: 778.8610
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 34.87
               Mean episode length: 54.78
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0587
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 8.76s
                        Total time: 15185.48s
                               ETA: 998541.5s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.580s, learning 0.256s)
               Value function loss: 340.4147
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 14.03
               Mean episode length: 53.35
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0586
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 8.84s
                        Total time: 15194.32s
                               ETA: 998445.9s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.623s, learning 0.223s)
               Value function loss: 709.0875
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 34.88
               Mean episode length: 54.37
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0600
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 8.85s
                        Total time: 15203.16s
                               ETA: 998351.0s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.401s, learning 0.203s)
               Value function loss: 417.0638
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 19.75
               Mean episode length: 51.82
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0599
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 8.60s
                        Total time: 15211.77s
                               ETA: 998240.4s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.367s, learning 0.179s)
               Value function loss: 484.0915
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 10.96
               Mean episode length: 54.94
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0592
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 8.55s
                        Total time: 15220.31s
                               ETA: 998126.1s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.498s, learning 0.204s)
               Value function loss: 420.0223
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 3.56
               Mean episode length: 51.28
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0594
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 8.70s
                        Total time: 15229.01s
                               ETA: 998022.1s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.308s, learning 0.178s)
               Value function loss: 197.7981
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 14.75
               Mean episode length: 53.86
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0578
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 8.49s
                        Total time: 15237.50s
                               ETA: 997904.1s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.428s, learning 0.175s)
               Value function loss: 272.6923
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 17.76
               Mean episode length: 50.31
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0584
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 8.60s
                        Total time: 15246.10s
                               ETA: 997794.0s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.745s, learning 0.207s)
               Value function loss: 528.6283
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 19.37
               Mean episode length: 53.63
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0563
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 8.95s
                        Total time: 15255.05s
                               ETA: 997706.8s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.422s, learning 0.197s)
               Value function loss: 641.2234
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 9.56
               Mean episode length: 51.63
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0539
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 8.62s
                        Total time: 15263.67s
                               ETA: 997597.9s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.523s, learning 0.199s)
               Value function loss: 673.2144
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 44.60
               Mean episode length: 52.23
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0317
Mean episode consecutive_successes: 0.0536
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 8.72s
                        Total time: 15272.39s
                               ETA: 997495.9s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.497s, learning 0.166s)
               Value function loss: 487.1264
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 11.61
               Mean episode length: 54.89
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0532
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 8.66s
                        Total time: 15281.06s
                               ETA: 997390.2s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.302s, learning 0.355s)
               Value function loss: 468.3374
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 16.76
               Mean episode length: 52.74
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0530
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 8.66s
                        Total time: 15289.71s
                               ETA: 997284.3s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.628s, learning 0.174s)
               Value function loss: 451.8116
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 17.46
               Mean episode length: 54.21
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0273
Mean episode consecutive_successes: 0.0547
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 8.80s
                        Total time: 15298.52s
                               ETA: 997187.9s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.621s, learning 0.318s)
               Value function loss: 1226.8669
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 22.80
               Mean episode length: 53.06
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0317
Mean episode consecutive_successes: 0.0551
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 8.94s
                        Total time: 15307.46s
                               ETA: 997100.6s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.361s, learning 0.176s)
               Value function loss: 587.1156
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 50.05
               Mean episode length: 57.42
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0298
Mean episode consecutive_successes: 0.0596
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 8.54s
                        Total time: 15315.99s
                               ETA: 996987.1s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.313s, learning 0.200s)
               Value function loss: 1119.2951
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 19.96
               Mean episode length: 54.74
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0391
Mean episode consecutive_successes: 0.0577
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 8.51s
                        Total time: 15324.51s
                               ETA: 996872.3s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.566s, learning 0.175s)
               Value function loss: 222.7050
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 13.79
               Mean episode length: 51.30
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0366
Mean episode consecutive_successes: 0.0569
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 8.74s
                        Total time: 15333.25s
                               ETA: 996772.4s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.562s, learning 0.179s)
               Value function loss: 220.1684
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 20.59
               Mean episode length: 57.59
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0616
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 8.74s
                        Total time: 15341.99s
                               ETA: 996672.7s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.475s, learning 0.164s)
               Value function loss: 481.8559
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 49.99
               Mean episode length: 52.37
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0648
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 8.64s
                        Total time: 15350.63s
                               ETA: 996566.4s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.706s, learning 0.184s)
               Value function loss: 276.0974
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 6.04
               Mean episode length: 50.51
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0617
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 8.89s
                        Total time: 15359.52s
                               ETA: 996476.6s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.468s, learning 0.201s)
               Value function loss: 484.6104
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 19.23
               Mean episode length: 53.26
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0617
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 8.67s
                        Total time: 15368.19s
                               ETA: 996372.5s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.712s, learning 0.202s)
               Value function loss: 346.0371
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 11.85
               Mean episode length: 52.29
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0215
Mean episode consecutive_successes: 0.0603
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 16.91s
                        Total time: 15385.10s
                               ETA: 996802.8s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.783s, learning 0.309s)
               Value function loss: 648.7150
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 30.16
               Mean episode length: 52.31
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0627
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 17.09s
                        Total time: 15402.19s
                               ETA: 997243.9s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 950 steps/s (collection: 16.847s, learning 0.398s)
               Value function loss: 1022.0654
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 24.56
               Mean episode length: 54.18
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0312
Mean episode consecutive_successes: 0.0600
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 17.24s
                        Total time: 15419.44s
                               ETA: 997694.4s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.324s, learning 0.205s)
               Value function loss: 535.5141
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 3.73
               Mean episode length: 53.34
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0347
Mean episode consecutive_successes: 0.0570
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 16.53s
                        Total time: 15435.97s
                               ETA: 998098.0s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.816s, learning 0.195s)
               Value function loss: 680.1828
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 8.81
               Mean episode length: 52.57
                  Mean reward/step: 0.46
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0400
Mean episode consecutive_successes: 0.0559
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 17.01s
                        Total time: 15452.98s
                               ETA: 998532.1s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.682s, learning 0.204s)
               Value function loss: 702.3326
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 21.86
               Mean episode length: 48.73
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0628
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 16.89s
                        Total time: 15469.86s
                               ETA: 998957.6s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.903s, learning 0.197s)
               Value function loss: 557.5007
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 55.67
               Mean episode length: 53.37
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0654
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 17.10s
                        Total time: 15486.96s
                               ETA: 999396.4s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.935s, learning 0.179s)
               Value function loss: 860.1826
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 15.58
               Mean episode length: 53.44
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0283
Mean episode consecutive_successes: 0.0621
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 17.11s
                        Total time: 15504.08s
                               ETA: 999835.4s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.241s, learning 0.314s)
               Value function loss: 314.2052
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 4.06
               Mean episode length: 52.71
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0598
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 16.55s
                        Total time: 15520.63s
                               ETA: 1000237.8s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.733s, learning 0.333s)
               Value function loss: 443.4233
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 4.01
               Mean episode length: 53.32
                  Mean reward/step: 0.37
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0607
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 17.07s
                        Total time: 15537.70s
                               ETA: 1000672.6s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.907s, learning 0.282s)
               Value function loss: 732.2383
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 9.26
               Mean episode length: 51.72
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0293
Mean episode consecutive_successes: 0.0612
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 17.19s
                        Total time: 15554.89s
                               ETA: 1001114.6s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.891s, learning 0.180s)
               Value function loss: 194.2793
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 24.50
               Mean episode length: 51.30
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0604
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 17.07s
                        Total time: 15571.96s
                               ETA: 1001548.5s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.726s, learning 0.182s)
               Value function loss: 165.1201
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 30.33
               Mean episode length: 52.98
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0215
Mean episode consecutive_successes: 0.0606
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 16.91s
                        Total time: 15588.87s
                               ETA: 1001971.3s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.445s, learning 0.334s)
               Value function loss: 851.0753
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 30.47
               Mean episode length: 53.47
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0648
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 16.78s
                        Total time: 15605.64s
                               ETA: 1002385.3s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.557s, learning 0.326s)
               Value function loss: 406.0197
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 4.85
               Mean episode length: 54.17
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0234
Mean episode consecutive_successes: 0.0609
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 16.88s
                        Total time: 15622.53s
                               ETA: 1002805.4s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.506s, learning 0.236s)
               Value function loss: 699.4239
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 6.83
               Mean episode length: 52.73
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0283
Mean episode consecutive_successes: 0.0574
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 16.74s
                        Total time: 15639.27s
                               ETA: 1003215.9s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.539s, learning 0.189s)
               Value function loss: 515.4579
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 21.78
               Mean episode length: 51.25
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0629
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 16.73s
                        Total time: 15656.00s
                               ETA: 1003624.9s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.851s, learning 0.172s)
               Value function loss: 134.2873
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 6.60
               Mean episode length: 50.39
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0617
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 17.02s
                        Total time: 15673.02s
                               ETA: 1004052.3s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.180s, learning 0.277s)
               Value function loss: 469.5961
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 9.25
               Mean episode length: 53.89
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0593
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 16.46s
                        Total time: 15689.48s
                               ETA: 1004442.8s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.794s, learning 0.251s)
               Value function loss: 215.9459
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 30.15
               Mean episode length: 51.73
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0588
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 17.05s
                        Total time: 15706.52s
                               ETA: 1004870.5s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.042s, learning 0.168s)
               Value function loss: 165.9929
                    Surrogate loss: -0.0047
             Mean action noise std: 0.75
                       Mean reward: 20.12
               Mean episode length: 53.19
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0559
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 17.21s
                        Total time: 15723.73s
                               ETA: 1005308.1s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.417s, learning 0.172s)
               Value function loss: 446.5310
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 27.22
               Mean episode length: 52.47
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0573
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 16.59s
                        Total time: 15740.32s
                               ETA: 1005705.5s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.780s, learning 0.183s)
               Value function loss: 244.8400
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 4.42
               Mean episode length: 51.57
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0532
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 16.96s
                        Total time: 15757.29s
                               ETA: 1006126.2s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.391s, learning 0.285s)
               Value function loss: 148.6214
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 9.95
               Mean episode length: 50.13
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0529
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 16.68s
                        Total time: 15773.96s
                               ETA: 1006528.0s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.747s, learning 0.175s)
               Value function loss: 723.5134
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 30.34
               Mean episode length: 54.43
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0520
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 16.92s
                        Total time: 15790.88s
                               ETA: 1006945.0s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.788s, learning 0.180s)
               Value function loss: 222.3097
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 14.78
               Mean episode length: 52.01
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0509
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 16.97s
                        Total time: 15807.85s
                               ETA: 1007364.3s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.453s, learning 0.194s)
               Value function loss: 418.7898
                    Surrogate loss: -0.0036
             Mean action noise std: 0.75
                       Mean reward: 25.01
               Mean episode length: 55.01
                  Mean reward/step: 0.27
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0499
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 16.65s
                        Total time: 15824.50s
                               ETA: 1007762.6s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.200s, learning 0.168s)
               Value function loss: 370.2176
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 11.85
               Mean episode length: 53.19
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0470
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 16.37s
                        Total time: 15840.87s
                               ETA: 1008142.6s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.875s, learning 0.166s)
               Value function loss: 246.1952
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 9.12
               Mean episode length: 52.85
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0452
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 17.04s
                        Total time: 15857.91s
                               ETA: 1008565.0s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.632s, learning 0.316s)
               Value function loss: 389.5291
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 12.54
               Mean episode length: 52.07
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0485
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 16.95s
                        Total time: 15874.85s
                               ETA: 1008980.8s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.741s, learning 0.177s)
               Value function loss: 281.5470
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 4.28
               Mean episode length: 51.53
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0467
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 16.92s
                        Total time: 15891.77s
                               ETA: 1009394.1s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.784s, learning 0.170s)
               Value function loss: 249.0538
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 4.74
               Mean episode length: 50.50
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0453
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 16.95s
                        Total time: 15908.73s
                               ETA: 1009809.2s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.724s, learning 0.235s)
               Value function loss: 606.4286
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 9.51
               Mean episode length: 53.94
                  Mean reward/step: 0.46
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0472
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 16.96s
                        Total time: 15925.69s
                               ETA: 1010224.1s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.687s, learning 0.172s)
               Value function loss: 381.9526
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 20.22
               Mean episode length: 51.93
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0215
Mean episode consecutive_successes: 0.0467
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 16.86s
                        Total time: 15942.55s
                               ETA: 1010632.1s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.955s, learning 0.171s)
               Value function loss: 1124.6337
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 6.27
               Mean episode length: 48.92
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0259
Mean episode consecutive_successes: 0.0478
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 17.13s
                        Total time: 15959.67s
                               ETA: 1011056.4s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.474s, learning 0.165s)
               Value function loss: 628.1866
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 29.69
               Mean episode length: 51.47
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0475
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 16.64s
                        Total time: 15976.31s
                               ETA: 1011449.4s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.618s, learning 0.282s)
               Value function loss: 779.0878
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 4.37
               Mean episode length: 49.95
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0487
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 16.90s
                        Total time: 15993.21s
                               ETA: 1011858.3s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1197 steps/s (collection: 13.497s, learning 0.188s)
               Value function loss: 419.4961
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 17.54
               Mean episode length: 55.32
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0239
Mean episode consecutive_successes: 0.0500
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 13.68s
                        Total time: 16006.89s
                               ETA: 1012063.4s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.411s, learning 0.263s)
               Value function loss: 440.9772
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 19.96
               Mean episode length: 50.32
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0512
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 8.67s
                        Total time: 16015.57s
                               ETA: 1011951.6s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.558s, learning 0.484s)
               Value function loss: 213.2191
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 9.44
               Mean episode length: 53.61
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0531
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 9.04s
                        Total time: 16024.61s
                               ETA: 1011863.2s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.700s, learning 0.230s)
               Value function loss: 335.3495
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 15.48
               Mean episode length: 52.73
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0523
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 8.93s
                        Total time: 16033.54s
                               ETA: 1011767.8s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.373s, learning 0.219s)
               Value function loss: 768.2535
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 5.13
               Mean episode length: 49.36
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0511
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 8.59s
                        Total time: 16042.13s
                               ETA: 1011651.2s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.635s, learning 0.230s)
               Value function loss: 954.7457
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 29.95
               Mean episode length: 55.03
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0504
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 8.87s
                        Total time: 16051.00s
                               ETA: 1011552.0s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1760 steps/s (collection: 8.899s, learning 0.408s)
               Value function loss: 873.5124
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 14.43
               Mean episode length: 52.03
                  Mean reward/step: 0.57
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0532
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 9.31s
                        Total time: 16060.30s
                               ETA: 1011480.7s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.952s, learning 0.172s)
               Value function loss: 645.4900
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 50.98
               Mean episode length: 52.03
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0386
Mean episode consecutive_successes: 0.0554
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 8.12s
                        Total time: 16068.43s
                               ETA: 1011335.0s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.685s, learning 0.265s)
               Value function loss: 478.0698
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 20.26
               Mean episode length: 51.25
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0584
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 8.95s
                        Total time: 16077.38s
                               ETA: 1011241.4s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.329s, learning 0.229s)
               Value function loss: 821.2877
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 30.49
               Mean episode length: 53.99
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0220
Mean episode consecutive_successes: 0.0614
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 8.56s
                        Total time: 16085.94s
                               ETA: 1011123.3s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.541s, learning 0.247s)
               Value function loss: 238.3901
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 35.40
               Mean episode length: 52.91
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0616
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 8.79s
                        Total time: 16094.72s
                               ETA: 1011019.8s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.086s, learning 0.397s)
               Value function loss: 762.1913
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 23.62
               Mean episode length: 51.63
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0596
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 8.48s
                        Total time: 16103.21s
                               ETA: 1010897.3s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1736 steps/s (collection: 9.107s, learning 0.331s)
               Value function loss: 646.6558
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 25.49
               Mean episode length: 53.67
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0288
Mean episode consecutive_successes: 0.0594
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 9.44s
                        Total time: 16112.64s
                               ETA: 1010834.8s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.593s, learning 0.172s)
               Value function loss: 790.6466
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 18.04
               Mean episode length: 51.59
                  Mean reward/step: 0.40
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0220
Mean episode consecutive_successes: 0.0617
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 8.77s
                        Total time: 16121.41s
                               ETA: 1010730.2s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.431s, learning 0.317s)
               Value function loss: 749.4626
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 34.35
               Mean episode length: 48.37
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0609
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 8.75s
                        Total time: 16130.16s
                               ETA: 1010624.7s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.553s, learning 0.312s)
               Value function loss: 1282.2092
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 5.73
               Mean episode length: 53.32
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0327
Mean episode consecutive_successes: 0.0627
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 8.87s
                        Total time: 16139.02s
                               ETA: 1010526.6s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.370s, learning 0.276s)
               Value function loss: 909.4775
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 50.22
               Mean episode length: 54.16
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0322
Mean episode consecutive_successes: 0.0668
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 8.65s
                        Total time: 16147.67s
                               ETA: 1010415.0s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.441s, learning 0.313s)
               Value function loss: 700.5963
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 24.78
               Mean episode length: 50.27
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0356
Mean episode consecutive_successes: 0.0668
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 8.75s
                        Total time: 16156.42s
                               ETA: 1010310.2s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.740s, learning 0.220s)
               Value function loss: 556.1110
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 18.11
               Mean episode length: 53.40
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0669
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 8.96s
                        Total time: 16165.38s
                               ETA: 1010218.4s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.545s, learning 0.225s)
               Value function loss: 274.9349
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 32.54
               Mean episode length: 52.91
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0694
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 8.77s
                        Total time: 16174.15s
                               ETA: 1010114.9s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.627s, learning 0.353s)
               Value function loss: 1113.2544
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 31.01
               Mean episode length: 53.72
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0298
Mean episode consecutive_successes: 0.0718
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 8.98s
                        Total time: 16183.13s
                               ETA: 1010024.5s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.266s, learning 0.177s)
               Value function loss: 1323.1251
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 19.85
               Mean episode length: 53.43
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0308
Mean episode consecutive_successes: 0.0740
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 8.44s
                        Total time: 16191.58s
                               ETA: 1009900.8s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.330s, learning 0.182s)
               Value function loss: 939.9907
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 3.86
               Mean episode length: 52.07
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0396
Mean episode consecutive_successes: 0.0706
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 8.51s
                        Total time: 16200.09s
                               ETA: 1009781.5s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.248s, learning 0.187s)
               Value function loss: 1603.9987
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 128.61
               Mean episode length: 53.27
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0464
Mean episode consecutive_successes: 0.0806
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 8.43s
                        Total time: 16208.52s
                               ETA: 1009657.6s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.227s, learning 0.178s)
               Value function loss: 906.9789
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 17.63
               Mean episode length: 54.39
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0410
Mean episode consecutive_successes: 0.0772
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 8.40s
                        Total time: 16216.93s
                               ETA: 1009531.9s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 2058 steps/s (collection: 7.776s, learning 0.185s)
               Value function loss: 477.5811
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 12.15
               Mean episode length: 49.65
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0317
Mean episode consecutive_successes: 0.0786
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 7.96s
                        Total time: 16224.89s
                               ETA: 1009378.8s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.229s, learning 0.173s)
               Value function loss: 825.7948
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 50.76
               Mean episode length: 54.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0828
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 8.40s
                        Total time: 16233.29s
                               ETA: 1009253.3s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.451s, learning 0.161s)
               Value function loss: 719.3351
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 17.29
               Mean episode length: 51.31
                  Mean reward/step: 0.49
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0229
Mean episode consecutive_successes: 0.0823
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 8.61s
                        Total time: 16241.90s
                               ETA: 1009140.9s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.243s, learning 0.182s)
               Value function loss: 665.4521
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 17.56
               Mean episode length: 53.11
                  Mean reward/step: 0.50
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0254
Mean episode consecutive_successes: 0.0818
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 8.42s
                        Total time: 16250.33s
                               ETA: 1009017.1s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.356s, learning 0.202s)
               Value function loss: 943.9976
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 25.57
               Mean episode length: 51.07
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0303
Mean episode consecutive_successes: 0.0778
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 8.56s
                        Total time: 16258.88s
                               ETA: 1008901.7s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.580s, learning 0.191s)
               Value function loss: 890.2401
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 65.19
               Mean episode length: 54.15
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0298
Mean episode consecutive_successes: 0.0836
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 8.77s
                        Total time: 16267.65s
                               ETA: 1008799.6s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.715s, learning 0.191s)
               Value function loss: 745.8026
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 8.51
               Mean episode length: 51.03
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0312
Mean episode consecutive_successes: 0.0814
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 8.91s
                        Total time: 16276.56s
                               ETA: 1008706.0s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.810s, learning 0.168s)
               Value function loss: 1140.9765
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 25.34
               Mean episode length: 51.38
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0420
Mean episode consecutive_successes: 0.0801
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 8.98s
                        Total time: 16285.54s
                               ETA: 1008617.0s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.478s, learning 0.165s)
               Value function loss: 1001.4791
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 40.29
               Mean episode length: 51.61
                  Mean reward/step: 0.53
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0386
Mean episode consecutive_successes: 0.0819
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 8.64s
                        Total time: 16294.18s
                               ETA: 1008507.4s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.645s, learning 0.177s)
               Value function loss: 1147.7989
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 22.59
               Mean episode length: 53.26
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0449
Mean episode consecutive_successes: 0.0805
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 8.82s
                        Total time: 16303.00s
                               ETA: 1008409.0s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.478s, learning 0.165s)
               Value function loss: 703.8946
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 8.97
               Mean episode length: 52.57
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0420
Mean episode consecutive_successes: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 8.64s
                        Total time: 16311.65s
                               ETA: 1008299.6s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.168s)
               Value function loss: 630.5237
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 4.70
               Mean episode length: 52.95
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0347
Mean episode consecutive_successes: 0.0859
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 8.45s
                        Total time: 16320.10s
                               ETA: 1008178.6s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.334s, learning 0.178s)
               Value function loss: 664.3945
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 27.75
               Mean episode length: 53.71
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0371
Mean episode consecutive_successes: 0.0853
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 8.51s
                        Total time: 16328.61s
                               ETA: 1008061.4s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.502s, learning 0.280s)
               Value function loss: 746.4518
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 39.98
               Mean episode length: 53.42
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0873
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 8.78s
                        Total time: 16337.40s
                               ETA: 1007960.9s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.523s, learning 0.188s)
               Value function loss: 853.4320
                    Surrogate loss: -0.0073
             Mean action noise std: 0.75
                       Mean reward: 32.17
               Mean episode length: 52.13
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0347
Mean episode consecutive_successes: 0.0880
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 8.71s
                        Total time: 16346.11s
                               ETA: 1007856.3s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.236s, learning 0.235s)
               Value function loss: 677.6468
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 11.99
               Mean episode length: 50.28
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0371
Mean episode consecutive_successes: 0.0851
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 8.47s
                        Total time: 16354.58s
                               ETA: 1007736.9s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.380s, learning 0.166s)
               Value function loss: 1258.3598
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 22.67
               Mean episode length: 51.97
                  Mean reward/step: 0.57
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0870
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 8.55s
                        Total time: 16363.12s
                               ETA: 1007622.3s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.607s, learning 0.168s)
               Value function loss: 695.2366
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 27.55
               Mean episode length: 49.16
                  Mean reward/step: 0.55
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0400
Mean episode consecutive_successes: 0.0871
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 8.77s
                        Total time: 16371.90s
                               ETA: 1007521.9s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.329s, learning 0.283s)
               Value function loss: 554.1037
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 15.74
               Mean episode length: 53.31
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0332
Mean episode consecutive_successes: 0.0855
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 8.61s
                        Total time: 16380.51s
                               ETA: 1007411.6s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.470s, learning 0.189s)
               Value function loss: 1158.2098
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 44.52
               Mean episode length: 51.13
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0896
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 8.66s
                        Total time: 16389.17s
                               ETA: 1007304.3s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.391s, learning 0.177s)
               Value function loss: 912.6061
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 7.13
               Mean episode length: 51.70
                  Mean reward/step: 0.58
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0371
Mean episode consecutive_successes: 0.0858
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 8.57s
                        Total time: 16397.74s
                               ETA: 1007191.6s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.653s, learning 0.190s)
               Value function loss: 279.7714
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 14.71
               Mean episode length: 52.38
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0322
Mean episode consecutive_successes: 0.0859
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 8.84s
                        Total time: 16406.58s
                               ETA: 1007095.9s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.502s, learning 0.188s)
               Value function loss: 1085.5300
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 61.69
               Mean episode length: 54.27
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.0887
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 8.69s
                        Total time: 16415.27s
                               ETA: 1006990.9s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.695s, learning 0.168s)
               Value function loss: 1439.1959
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 29.57
               Mean episode length: 54.13
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0391
Mean episode consecutive_successes: 0.0875
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 8.86s
                        Total time: 16424.13s
                               ETA: 1006896.6s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.652s, learning 0.200s)
               Value function loss: 670.5323
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 29.75
               Mean episode length: 52.87
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0366
Mean episode consecutive_successes: 0.0875
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 8.85s
                        Total time: 16432.99s
                               ETA: 1006801.8s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.529s, learning 0.198s)
               Value function loss: 848.9819
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 37.84
               Mean episode length: 53.99
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0444
Mean episode consecutive_successes: 0.0855
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 8.73s
                        Total time: 16441.71s
                               ETA: 1006699.4s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.188s, learning 0.198s)
               Value function loss: 986.8190
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 32.50
               Mean episode length: 52.99
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0464
Mean episode consecutive_successes: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 8.39s
                        Total time: 16450.10s
                               ETA: 1006576.2s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.380s, learning 0.210s)
               Value function loss: 1294.6252
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 19.58
               Mean episode length: 49.86
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0435
Mean episode consecutive_successes: 0.0845
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 8.59s
                        Total time: 16458.69s
                               ETA: 1006465.7s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.312s, learning 0.266s)
               Value function loss: 1211.0271
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 52.90
               Mean episode length: 53.57
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0479
Mean episode consecutive_successes: 0.0875
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 8.58s
                        Total time: 16467.27s
                               ETA: 1006354.6s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.570s, learning 0.192s)
               Value function loss: 1294.7481
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 27.80
               Mean episode length: 53.98
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0557
Mean episode consecutive_successes: 0.0911
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 8.76s
                        Total time: 16476.03s
                               ETA: 1006254.9s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.533s, learning 0.170s)
               Value function loss: 1933.4542
                    Surrogate loss: -0.0088
             Mean action noise std: 0.75
                       Mean reward: 44.77
               Mean episode length: 52.79
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0576
Mean episode consecutive_successes: 0.0940
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 8.70s
                        Total time: 16484.73s
                               ETA: 1006151.6s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.663s, learning 0.177s)
               Value function loss: 885.4121
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 22.83
               Mean episode length: 50.98
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0513
Mean episode consecutive_successes: 0.0979
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 8.84s
                        Total time: 16493.57s
                               ETA: 1006056.8s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.399s, learning 0.168s)
               Value function loss: 755.4364
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 37.50
               Mean episode length: 54.08
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0352
Mean episode consecutive_successes: 0.1050
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 8.57s
                        Total time: 16502.14s
                               ETA: 1005945.5s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.347s, learning 0.171s)
               Value function loss: 985.9082
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 26.97
               Mean episode length: 50.41
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0376
Mean episode consecutive_successes: 0.1031
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 8.52s
                        Total time: 16510.66s
                               ETA: 1005831.3s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.489s, learning 0.171s)
               Value function loss: 1160.3296
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 55.53
               Mean episode length: 55.15
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0430
Mean episode consecutive_successes: 0.1039
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 8.66s
                        Total time: 16519.32s
                               ETA: 1005725.9s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.241s, learning 0.175s)
               Value function loss: 1228.4096
                    Surrogate loss: -0.0029
             Mean action noise std: 0.75
                       Mean reward: 25.37
               Mean episode length: 54.67
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0425
Mean episode consecutive_successes: 0.1021
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 8.42s
                        Total time: 16527.73s
                               ETA: 1005605.7s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.645s, learning 0.195s)
               Value function loss: 1223.7234
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 71.00
               Mean episode length: 54.65
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0474
Mean episode consecutive_successes: 0.1033
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 8.84s
                        Total time: 16536.57s
                               ETA: 1005511.5s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.665s, learning 0.193s)
               Value function loss: 1851.7332
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 40.97
               Mean episode length: 51.73
                  Mean reward/step: 0.82
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0469
Mean episode consecutive_successes: 0.1051
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 8.86s
                        Total time: 16545.43s
                               ETA: 1005418.5s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.438s, learning 0.189s)
               Value function loss: 1150.0172
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 58.00
               Mean episode length: 51.89
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0591
Mean episode consecutive_successes: 0.1060
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 8.63s
                        Total time: 16554.06s
                               ETA: 1005311.6s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.789s, learning 0.166s)
               Value function loss: 2282.3485
                    Surrogate loss: -0.0069
             Mean action noise std: 0.75
                       Mean reward: 27.74
               Mean episode length: 52.10
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0649
Mean episode consecutive_successes: 0.1063
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 8.95s
                        Total time: 16563.01s
                               ETA: 1005224.6s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.321s, learning 0.180s)
               Value function loss: 1511.4187
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 43.18
               Mean episode length: 52.13
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0688
Mean episode consecutive_successes: 0.1082
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 8.50s
                        Total time: 16571.51s
                               ETA: 1005110.3s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.607s, learning 0.211s)
               Value function loss: 2352.1025
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 42.93
               Mean episode length: 53.02
                  Mean reward/step: 1.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0747
Mean episode consecutive_successes: 0.1104
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 8.82s
                        Total time: 16580.33s
                               ETA: 1005015.3s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.155s, learning 0.174s)
               Value function loss: 969.0110
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 36.91
               Mean episode length: 54.02
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0620
Mean episode consecutive_successes: 0.1163
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 8.33s
                        Total time: 16588.66s
                               ETA: 1004890.8s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.576s, learning 0.181s)
               Value function loss: 1370.5134
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 59.82
               Mean episode length: 53.76
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0566
Mean episode consecutive_successes: 0.1232
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 8.76s
                        Total time: 16597.42s
                               ETA: 1004792.4s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.364s, learning 0.299s)
               Value function loss: 829.0899
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 19.57
               Mean episode length: 51.58
                  Mean reward/step: 0.43
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0454
Mean episode consecutive_successes: 0.1210
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 8.66s
                        Total time: 16606.08s
                               ETA: 1004688.3s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1772 steps/s (collection: 9.001s, learning 0.241s)
               Value function loss: 1160.9629
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 34.41
               Mean episode length: 52.06
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0474
Mean episode consecutive_successes: 0.1193
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 9.24s
                        Total time: 16615.32s
                               ETA: 1004619.4s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.469s, learning 0.178s)
               Value function loss: 1094.7801
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 11.71
               Mean episode length: 52.53
                  Mean reward/step: 0.63
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0415
Mean episode consecutive_successes: 0.1206
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 8.65s
                        Total time: 16623.97s
                               ETA: 1004514.6s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.254s, learning 0.248s)
               Value function loss: 1888.9560
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 9.70
               Mean episode length: 51.85
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0522
Mean episode consecutive_successes: 0.1204
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 8.50s
                        Total time: 16632.47s
                               ETA: 1004401.2s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.552s, learning 0.190s)
               Value function loss: 1321.2635
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 13.98
               Mean episode length: 49.75
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0527
Mean episode consecutive_successes: 0.1214
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 8.74s
                        Total time: 16641.22s
                               ETA: 1004302.4s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.719s, learning 0.191s)
               Value function loss: 1992.8826
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 57.73
               Mean episode length: 53.67
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.1233
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 8.91s
                        Total time: 16650.13s
                               ETA: 1004213.9s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.346s, learning 0.322s)
               Value function loss: 1794.1555
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 80.90
               Mean episode length: 56.20
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0532
Mean episode consecutive_successes: 0.1274
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 8.67s
                        Total time: 16658.79s
                               ETA: 1004110.8s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.654s, learning 0.250s)
               Value function loss: 534.8577
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 24.69
               Mean episode length: 49.12
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0410
Mean episode consecutive_successes: 0.1264
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 8.90s
                        Total time: 16667.70s
                               ETA: 1004022.0s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.370s, learning 0.173s)
               Value function loss: 442.6954
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 24.92
               Mean episode length: 55.13
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0415
Mean episode consecutive_successes: 0.1226
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 8.54s
                        Total time: 16676.24s
                               ETA: 1003911.7s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.774s, learning 0.310s)
               Value function loss: 1521.8893
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 31.27
               Mean episode length: 55.60
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0415
Mean episode consecutive_successes: 0.1207
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 9.08s
                        Total time: 16685.32s
                               ETA: 1003834.0s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.594s, learning 0.164s)
               Value function loss: 1427.4661
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 48.79
               Mean episode length: 53.76
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0498
Mean episode consecutive_successes: 0.1201
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 8.76s
                        Total time: 16694.08s
                               ETA: 1003736.8s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.457s, learning 0.199s)
               Value function loss: 1004.6777
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 17.62
               Mean episode length: 56.81
                  Mean reward/step: 0.58
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0474
Mean episode consecutive_successes: 0.1180
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 8.66s
                        Total time: 16702.74s
                               ETA: 1003633.5s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.606s, learning 0.200s)
               Value function loss: 962.6312
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 17.63
               Mean episode length: 55.27
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0396
Mean episode consecutive_successes: 0.1202
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 8.81s
                        Total time: 16711.54s
                               ETA: 1003539.4s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.230s, learning 0.321s)
               Value function loss: 1981.1613
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 28.58
               Mean episode length: 52.95
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0522
Mean episode consecutive_successes: 0.1186
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 8.55s
                        Total time: 16720.09s
                               ETA: 1003430.0s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.572s, learning 0.183s)
               Value function loss: 1833.9200
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 32.04
               Mean episode length: 53.76
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0537
Mean episode consecutive_successes: 0.1233
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 8.75s
                        Total time: 16728.85s
                               ETA: 1003333.1s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.585s, learning 0.194s)
               Value function loss: 1903.7888
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 35.61
               Mean episode length: 54.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0669
Mean episode consecutive_successes: 0.1208
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 8.78s
                        Total time: 16737.63s
                               ETA: 1003237.7s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.606s, learning 0.179s)
               Value function loss: 1852.4622
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 7.82
               Mean episode length: 56.33
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0732
Mean episode consecutive_successes: 0.1177
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 8.78s
                        Total time: 16746.41s
                               ETA: 1003142.7s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.592s, learning 0.286s)
               Value function loss: 1357.1846
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 20.81
               Mean episode length: 54.54
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0630
Mean episode consecutive_successes: 0.1251
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 8.88s
                        Total time: 16755.29s
                               ETA: 1003053.5s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.654s, learning 0.193s)
               Value function loss: 2373.2862
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 19.85
               Mean episode length: 53.88
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0688
Mean episode consecutive_successes: 0.1257
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 8.85s
                        Total time: 16764.14s
                               ETA: 1002962.4s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.487s, learning 0.181s)
               Value function loss: 1199.4296
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 52.59
               Mean episode length: 52.25
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0610
Mean episode consecutive_successes: 0.1305
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 8.67s
                        Total time: 16772.80s
                               ETA: 1002860.8s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.351s, learning 0.319s)
               Value function loss: 980.2043
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 57.41
               Mean episode length: 54.89
                  Mean reward/step: 0.61
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0562
Mean episode consecutive_successes: 0.1318
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 8.67s
                        Total time: 16781.48s
                               ETA: 1002759.4s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.668s, learning 0.193s)
               Value function loss: 1670.5535
                    Surrogate loss: -0.0078
             Mean action noise std: 0.75
                       Mean reward: 35.26
               Mean episode length: 53.74
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0552
Mean episode consecutive_successes: 0.1306
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 8.86s
                        Total time: 16790.34s
                               ETA: 1002669.5s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.645s, learning 0.172s)
               Value function loss: 1455.7303
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 4.18
               Mean episode length: 54.78
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0571
Mean episode consecutive_successes: 0.1287
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 8.82s
                        Total time: 16799.15s
                               ETA: 1002577.1s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.388s, learning 0.185s)
               Value function loss: 1608.0816
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 24.41
               Mean episode length: 54.55
                  Mean reward/step: 0.65
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0562
Mean episode consecutive_successes: 0.1297
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 8.57s
                        Total time: 16807.72s
                               ETA: 1002470.2s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.376s, learning 0.192s)
               Value function loss: 1674.2726
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 22.97
               Mean episode length: 51.56
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0493
Mean episode consecutive_successes: 0.1314
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 8.57s
                        Total time: 16816.29s
                               ETA: 1002363.2s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.474s, learning 0.191s)
               Value function loss: 2697.9380
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 84.83
               Mean episode length: 54.94
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0654
Mean episode consecutive_successes: 0.1344
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 8.66s
                        Total time: 16824.96s
                               ETA: 1002262.0s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.175s, learning 0.233s)
               Value function loss: 2509.9226
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 12.68
               Mean episode length: 55.65
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0825
Mean episode consecutive_successes: 0.1362
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 8.41s
                        Total time: 16833.37s
                               ETA: 1002145.7s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.491s, learning 0.173s)
               Value function loss: 1867.7584
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 30.25
               Mean episode length: 53.51
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0894
Mean episode consecutive_successes: 0.1377
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 8.66s
                        Total time: 16842.03s
                               ETA: 1002044.7s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.385s, learning 0.245s)
               Value function loss: 1717.5978
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 48.16
               Mean episode length: 53.58
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0874
Mean episode consecutive_successes: 0.1414
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 8.63s
                        Total time: 16850.66s
                               ETA: 1001941.8s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.713s, learning 0.205s)
               Value function loss: 1012.4021
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 61.41
               Mean episode length: 55.90
                  Mean reward/step: 0.74
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0737
Mean episode consecutive_successes: 0.1493
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 8.92s
                        Total time: 16859.58s
                               ETA: 1001856.1s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.228s, learning 0.253s)
               Value function loss: 1340.3491
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 53.35
               Mean episode length: 55.24
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0469
Mean episode consecutive_successes: 0.1550
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 8.48s
                        Total time: 16868.06s
                               ETA: 1001744.6s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.590s, learning 0.189s)
               Value function loss: 997.8872
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 15.96
               Mean episode length: 54.73
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0508
Mean episode consecutive_successes: 0.1492
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 8.78s
                        Total time: 16876.84s
                               ETA: 1001650.9s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.397s, learning 0.232s)
               Value function loss: 1723.8243
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 29.72
               Mean episode length: 51.22
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0498
Mean episode consecutive_successes: 0.1514
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 8.63s
                        Total time: 16885.47s
                               ETA: 1001548.4s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.700s, learning 0.220s)
               Value function loss: 1225.5756
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 47.25
               Mean episode length: 51.03
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0547
Mean episode consecutive_successes: 0.1502
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 8.92s
                        Total time: 16894.39s
                               ETA: 1001463.3s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.256s, learning 0.192s)
               Value function loss: 1534.0178
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 66.56
               Mean episode length: 53.71
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0542
Mean episode consecutive_successes: 0.1485
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 8.45s
                        Total time: 16902.83s
                               ETA: 1001350.3s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.024s, learning 0.184s)
               Value function loss: 1588.5217
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 27.80
               Mean episode length: 54.33
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0591
Mean episode consecutive_successes: 0.1449
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 9.21s
                        Total time: 16912.04s
                               ETA: 1001282.5s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.423s, learning 0.195s)
               Value function loss: 861.6234
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 4.97
               Mean episode length: 52.31
                  Mean reward/step: 0.56
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0654
Mean episode consecutive_successes: 0.1377
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 8.62s
                        Total time: 16920.66s
                               ETA: 1001179.8s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.007s, learning 0.202s)
               Value function loss: 1773.3572
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 78.47
               Mean episode length: 54.05
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0552
Mean episode consecutive_successes: 0.1427
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 9.21s
                        Total time: 16929.87s
                               ETA: 1001112.1s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.490s, learning 0.199s)
               Value function loss: 978.8683
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 29.64
               Mean episode length: 53.06
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0396
Mean episode consecutive_successes: 0.1483
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 8.69s
                        Total time: 16938.56s
                               ETA: 1001013.8s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.434s, learning 0.238s)
               Value function loss: 912.8406
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 73.56
               Mean episode length: 56.46
                  Mean reward/step: 0.69
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0449
Mean episode consecutive_successes: 0.1473
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 8.67s
                        Total time: 16947.23s
                               ETA: 1000914.6s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.400s, learning 0.193s)
               Value function loss: 1308.6056
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 35.29
               Mean episode length: 51.55
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0425
Mean episode consecutive_successes: 0.1445
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 8.59s
                        Total time: 16955.82s
                               ETA: 1000810.8s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.281s, learning 0.201s)
               Value function loss: 1053.8992
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 38.04
               Mean episode length: 54.03
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0474
Mean episode consecutive_successes: 0.1408
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 8.48s
                        Total time: 16964.30s
                               ETA: 1000700.6s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.640s, learning 0.178s)
               Value function loss: 1345.7859
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 24.85
               Mean episode length: 51.61
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0508
Mean episode consecutive_successes: 0.1374
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 8.82s
                        Total time: 16973.12s
                               ETA: 1000610.3s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.443s, learning 0.178s)
               Value function loss: 1972.2802
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 25.45
               Mean episode length: 54.17
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0610
Mean episode consecutive_successes: 0.1371
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 8.62s
                        Total time: 16981.74s
                               ETA: 1000508.6s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.645s, learning 0.271s)
               Value function loss: 1077.9053
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 37.31
               Mean episode length: 53.18
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0659
Mean episode consecutive_successes: 0.1342
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 8.92s
                        Total time: 16990.66s
                               ETA: 1000424.3s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.795s, learning 0.300s)
               Value function loss: 2223.4944
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 37.87
               Mean episode length: 53.94
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0767
Mean episode consecutive_successes: 0.1338
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 9.09s
                        Total time: 16999.75s
                               ETA: 1000350.6s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.682s, learning 0.166s)
               Value function loss: 1957.5351
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 42.91
               Mean episode length: 53.20
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0752
Mean episode consecutive_successes: 0.1371
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 8.85s
                        Total time: 17008.60s
                               ETA: 1000262.5s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.581s, learning 0.186s)
               Value function loss: 1219.9851
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 65.51
               Mean episode length: 52.16
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0601
Mean episode consecutive_successes: 0.1441
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 8.77s
                        Total time: 17017.37s
                               ETA: 1000169.7s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.442s, learning 0.180s)
               Value function loss: 1690.4322
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 38.04
               Mean episode length: 53.64
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0547
Mean episode consecutive_successes: 0.1447
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 8.62s
                        Total time: 17025.99s
                               ETA: 1000068.5s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.314s, learning 0.196s)
               Value function loss: 1977.8167
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 76.50
               Mean episode length: 54.30
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0591
Mean episode consecutive_successes: 0.1472
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 8.51s
                        Total time: 17034.50s
                               ETA: 999960.9s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.645s, learning 0.174s)
               Value function loss: 1607.1054
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 27.52
               Mean episode length: 50.89
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0688
Mean episode consecutive_successes: 0.1441
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 8.82s
                        Total time: 17043.32s
                               ETA: 999871.4s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.898s, learning 0.208s)
               Value function loss: 1447.0277
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 50.83
               Mean episode length: 53.28
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0630
Mean episode consecutive_successes: 0.1471
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 9.11s
                        Total time: 17052.43s
                               ETA: 999798.9s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.524s, learning 0.273s)
               Value function loss: 1542.9601
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 79.83
               Mean episode length: 54.89
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.1482
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 8.80s
                        Total time: 17061.22s
                               ETA: 999708.4s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.539s, learning 0.183s)
               Value function loss: 1083.6641
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 32.74
               Mean episode length: 53.76
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0532
Mean episode consecutive_successes: 0.1476
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 8.72s
                        Total time: 17069.95s
                               ETA: 999613.5s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 2002 steps/s (collection: 7.997s, learning 0.185s)
               Value function loss: 2412.7702
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 23.17
               Mean episode length: 56.01
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0562
Mean episode consecutive_successes: 0.1474
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 8.18s
                        Total time: 17078.13s
                               ETA: 999487.3s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.705s, learning 0.258s)
               Value function loss: 1420.1580
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 30.53
               Mean episode length: 53.73
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0532
Mean episode consecutive_successes: 0.1473
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 8.96s
                        Total time: 17087.09s
                               ETA: 999406.8s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.330s, learning 0.188s)
               Value function loss: 2119.0042
                    Surrogate loss: -0.0113
             Mean action noise std: 0.75
                       Mean reward: 30.82
               Mean episode length: 51.76
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0640
Mean episode consecutive_successes: 0.1478
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 8.52s
                        Total time: 17095.61s
                               ETA: 999300.3s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.310s, learning 0.306s)
               Value function loss: 2230.1455
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 28.09
               Mean episode length: 53.73
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0703
Mean episode consecutive_successes: 0.1498
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 8.62s
                        Total time: 17104.22s
                               ETA: 999199.7s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.320s, learning 0.204s)
               Value function loss: 2296.5004
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 34.78
               Mean episode length: 50.11
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0806
Mean episode consecutive_successes: 0.1456
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 8.52s
                        Total time: 17112.75s
                               ETA: 999093.8s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.613s, learning 0.198s)
               Value function loss: 2622.1432
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 30.34
               Mean episode length: 53.08
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0908
Mean episode consecutive_successes: 0.1508
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 8.81s
                        Total time: 17121.56s
                               ETA: 999004.8s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.516s, learning 0.201s)
               Value function loss: 1484.7601
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 51.50
               Mean episode length: 56.08
                  Mean reward/step: 0.92
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0757
Mean episode consecutive_successes: 0.1611
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 8.72s
                        Total time: 17130.28s
                               ETA: 998910.4s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.615s, learning 0.212s)
               Value function loss: 1658.8089
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 55.44
               Mean episode length: 53.84
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0752
Mean episode consecutive_successes: 0.1635
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 8.83s
                        Total time: 17139.10s
                               ETA: 998822.6s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.590s, learning 0.293s)
               Value function loss: 1687.9863
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 45.19
               Mean episode length: 50.95
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0815
Mean episode consecutive_successes: 0.1606
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 8.88s
                        Total time: 17147.99s
                               ETA: 998738.1s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.356s, learning 0.184s)
               Value function loss: 1744.0626
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 63.19
               Mean episode length: 54.04
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0684
Mean episode consecutive_successes: 0.1654
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 8.54s
                        Total time: 17156.53s
                               ETA: 998633.7s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.320s, learning 0.185s)
               Value function loss: 1675.2050
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 28.13
               Mean episode length: 54.02
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0684
Mean episode consecutive_successes: 0.1618
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 8.50s
                        Total time: 17165.03s
                               ETA: 998527.4s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.692s, learning 0.180s)
               Value function loss: 1392.5072
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 28.60
               Mean episode length: 51.56
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.1650
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 8.87s
                        Total time: 17173.90s
                               ETA: 998442.6s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.960s, learning 0.166s)
               Value function loss: 1415.7974
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 99.31
               Mean episode length: 53.48
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.1684
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 9.13s
                        Total time: 17183.03s
                               ETA: 998372.6s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.679s, learning 0.286s)
               Value function loss: 2171.3054
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 71.42
               Mean episode length: 53.35
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0688
Mean episode consecutive_successes: 0.1633
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 8.96s
                        Total time: 17191.99s
                               ETA: 998293.3s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.307s, learning 0.208s)
               Value function loss: 1931.9014
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 76.02
               Mean episode length: 54.83
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0601
Mean episode consecutive_successes: 0.1654
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 8.51s
                        Total time: 17200.51s
                               ETA: 998188.0s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.491s, learning 0.201s)
               Value function loss: 1948.8764
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 48.29
               Mean episode length: 51.37
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0674
Mean episode consecutive_successes: 0.1652
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 8.69s
                        Total time: 17209.20s
                               ETA: 998093.1s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.409s, learning 0.249s)
               Value function loss: 2898.8346
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 45.10
               Mean episode length: 53.40
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0806
Mean episode consecutive_successes: 0.1616
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 8.66s
                        Total time: 17217.86s
                               ETA: 997996.2s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.303s, learning 0.217s)
               Value function loss: 1778.0286
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 71.11
               Mean episode length: 54.36
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0737
Mean episode consecutive_successes: 0.1684
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 8.52s
                        Total time: 17226.38s
                               ETA: 997891.5s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.561s, learning 0.322s)
               Value function loss: 1905.5532
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 103.78
               Mean episode length: 52.11
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0654
Mean episode consecutive_successes: 0.1732
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 8.88s
                        Total time: 17235.26s
                               ETA: 997807.9s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.449s, learning 0.171s)
               Value function loss: 761.1058
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 53.56
               Mean episode length: 53.03
                  Mean reward/step: 0.51
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0605
Mean episode consecutive_successes: 0.1680
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 8.62s
                        Total time: 17243.88s
                               ETA: 997709.2s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.729s, learning 0.186s)
               Value function loss: 1947.4271
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 70.91
               Mean episode length: 51.56
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0542
Mean episode consecutive_successes: 0.1686
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 8.91s
                        Total time: 17252.80s
                               ETA: 997627.7s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.430s, learning 0.283s)
               Value function loss: 2576.4088
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 45.96
               Mean episode length: 53.43
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0776
Mean episode consecutive_successes: 0.1661
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 8.71s
                        Total time: 17261.51s
                               ETA: 997534.6s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.625s, learning 0.186s)
               Value function loss: 1859.8827
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 84.70
               Mean episode length: 56.97
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0776
Mean episode consecutive_successes: 0.1752
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 8.81s
                        Total time: 17270.32s
                               ETA: 997447.2s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1653 steps/s (collection: 9.714s, learning 0.195s)
               Value function loss: 1900.8180
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 41.50
               Mean episode length: 53.69
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0693
Mean episode consecutive_successes: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 9.91s
                        Total time: 17280.23s
                               ETA: 997423.4s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.081s, learning 0.201s)
               Value function loss: 2193.2835
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 67.33
               Mean episode length: 53.82
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0801
Mean episode consecutive_successes: 0.1717
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 17.28s
                        Total time: 17297.51s
                               ETA: 997824.8s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.630s, learning 0.329s)
               Value function loss: 1598.2706
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 73.90
               Mean episode length: 57.07
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0723
Mean episode consecutive_successes: 0.1734
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 16.96s
                        Total time: 17314.47s
                               ETA: 998207.1s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 941 steps/s (collection: 17.095s, learning 0.302s)
               Value function loss: 2321.5776
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 53.63
               Mean episode length: 53.70
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0698
Mean episode consecutive_successes: 0.1786
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 17.40s
                        Total time: 17331.87s
                               ETA: 998614.2s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 935 steps/s (collection: 17.300s, learning 0.209s)
               Value function loss: 1147.8913
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 48.48
               Mean episode length: 52.38
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0723
Mean episode consecutive_successes: 0.1780
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 17.51s
                        Total time: 17349.37s
                               ETA: 999027.2s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.796s, learning 0.228s)
               Value function loss: 867.6959
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 43.73
               Mean episode length: 53.77
                  Mean reward/step: 0.53
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0557
Mean episode consecutive_successes: 0.1771
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 17.02s
                        Total time: 17366.40s
                               ETA: 999411.9s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.073s, learning 0.204s)
               Value function loss: 1485.9687
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 43.30
               Mean episode length: 52.35
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0601
Mean episode consecutive_successes: 0.1741
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 17.28s
                        Total time: 17383.68s
                               ETA: 999810.6s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.613s, learning 0.176s)
               Value function loss: 2118.8514
                    Surrogate loss: -0.0045
             Mean action noise std: 0.75
                       Mean reward: 39.63
               Mean episode length: 54.40
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0659
Mean episode consecutive_successes: 0.1713
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 16.79s
                        Total time: 17400.47s
                               ETA: 1000180.8s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.436s, learning 0.192s)
               Value function loss: 2597.1224
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 56.45
               Mean episode length: 51.16
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0835
Mean episode consecutive_successes: 0.1659
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 16.63s
                        Total time: 17417.09s
                               ETA: 1000541.3s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.953s, learning 0.243s)
               Value function loss: 3011.4939
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 113.66
               Mean episode length: 54.73
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0869
Mean episode consecutive_successes: 0.1744
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 17.20s
                        Total time: 17434.29s
                               ETA: 1000934.0s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.920s, learning 0.224s)
               Value function loss: 1725.3075
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 73.10
               Mean episode length: 54.41
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0786
Mean episode consecutive_successes: 0.1756
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 17.14s
                        Total time: 17451.43s
                               ETA: 1001323.1s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.946s, learning 0.197s)
               Value function loss: 2410.1024
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 10.20
               Mean episode length: 53.21
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0698
Mean episode consecutive_successes: 0.1785
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 17.14s
                        Total time: 17468.58s
                               ETA: 1001711.8s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.722s, learning 0.173s)
               Value function loss: 1830.6169
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 101.04
               Mean episode length: 53.58
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0654
Mean episode consecutive_successes: 0.1784
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 16.89s
                        Total time: 17485.47s
                               ETA: 1002085.7s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.485s, learning 0.277s)
               Value function loss: 2041.3745
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 35.52
               Mean episode length: 53.19
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0762
Mean episode consecutive_successes: 0.1753
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 16.76s
                        Total time: 17502.23s
                               ETA: 1002451.6s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.449s, learning 0.173s)
               Value function loss: 1134.7325
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 46.59
               Mean episode length: 54.02
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0786
Mean episode consecutive_successes: 0.1687
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 16.62s
                        Total time: 17518.86s
                               ETA: 1002809.1s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.788s, learning 0.183s)
               Value function loss: 1004.7882
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 35.60
               Mean episode length: 52.93
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0562
Mean episode consecutive_successes: 0.1749
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 16.97s
                        Total time: 17535.83s
                               ETA: 1003186.0s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.468s, learning 0.305s)
               Value function loss: 1394.2750
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 11.60
               Mean episode length: 56.07
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0625
Mean episode consecutive_successes: 0.1682
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 16.77s
                        Total time: 17552.60s
                               ETA: 1003551.2s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.311s, learning 0.270s)
               Value function loss: 1353.6451
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 36.14
               Mean episode length: 56.38
                  Mean reward/step: 0.70
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0654
Mean episode consecutive_successes: 0.1657
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 16.58s
                        Total time: 17569.18s
                               ETA: 1003905.0s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.592s, learning 0.206s)
               Value function loss: 1245.8600
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 48.99
               Mean episode length: 53.34
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0703
Mean episode consecutive_successes: 0.1647
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 16.80s
                        Total time: 17585.98s
                               ETA: 1004270.7s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.229s, learning 0.257s)
               Value function loss: 2370.5782
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 70.64
               Mean episode length: 55.47
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0664
Mean episode consecutive_successes: 0.1693
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 16.49s
                        Total time: 17602.46s
                               ETA: 1004618.2s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.518s, learning 0.208s)
               Value function loss: 1142.5111
                    Surrogate loss: 0.0001
             Mean action noise std: 0.75
                       Mean reward: 21.35
               Mean episode length: 54.43
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0625
Mean episode consecutive_successes: 0.1644
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 16.73s
                        Total time: 17619.19s
                               ETA: 1004978.9s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.544s, learning 0.189s)
               Value function loss: 3115.3529
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 15.27
               Mean episode length: 51.08
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0825
Mean episode consecutive_successes: 0.1633
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 16.73s
                        Total time: 17635.92s
                               ETA: 1005339.7s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.635s, learning 0.289s)
               Value function loss: 2859.5287
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 74.54
               Mean episode length: 54.50
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0840
Mean episode consecutive_successes: 0.1708
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 16.92s
                        Total time: 17652.85s
                               ETA: 1005710.8s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.593s, learning 0.180s)
               Value function loss: 1924.6336
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 39.08
               Mean episode length: 53.80
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0928
Mean episode consecutive_successes: 0.1681
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 16.77s
                        Total time: 17669.62s
                               ETA: 1006072.9s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.630s, learning 0.183s)
               Value function loss: 830.6799
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 98.81
               Mean episode length: 55.64
                  Mean reward/step: 0.67
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0615
Mean episode consecutive_successes: 0.1803
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 16.81s
                        Total time: 17686.43s
                               ETA: 1006436.9s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.793s, learning 0.173s)
               Value function loss: 1991.7723
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 102.47
               Mean episode length: 55.37
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0708
Mean episode consecutive_successes: 0.1821
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 16.97s
                        Total time: 17703.40s
                               ETA: 1006809.1s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.493s, learning 0.185s)
               Value function loss: 2049.2207
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 38.81
               Mean episode length: 53.38
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0806
Mean episode consecutive_successes: 0.1772
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 16.68s
                        Total time: 17720.08s
                               ETA: 1007164.5s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.863s, learning 0.192s)
               Value function loss: 2580.2429
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 41.24
               Mean episode length: 53.76
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0918
Mean episode consecutive_successes: 0.1772
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 17.06s
                        Total time: 17737.13s
                               ETA: 1007540.9s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.483s, learning 0.166s)
               Value function loss: 2683.4570
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 26.27
               Mean episode length: 50.06
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.02
            Mean episode successes: 0.1108
Mean episode consecutive_successes: 0.1736
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 16.65s
                        Total time: 17753.78s
                               ETA: 1007893.8s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.115s, learning 0.167s)
               Value function loss: 2434.8763
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 33.92
               Mean episode length: 52.20
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1123
Mean episode consecutive_successes: 0.1783
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 17.28s
                        Total time: 17771.06s
                               ETA: 1008282.1s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.849s, learning 0.197s)
               Value function loss: 3237.4819
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 84.20
               Mean episode length: 57.28
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0947
Mean episode consecutive_successes: 0.1944
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 17.05s
                        Total time: 17788.11s
                               ETA: 1008656.6s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.666s, learning 0.272s)
               Value function loss: 2561.5871
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 71.88
               Mean episode length: 54.31
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0830
Mean episode consecutive_successes: 0.2042
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 16.94s
                        Total time: 17805.05s
                               ETA: 1009024.5s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.111s, learning 0.174s)
               Value function loss: 2143.1474
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 69.27
               Mean episode length: 55.11
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0815
Mean episode consecutive_successes: 0.1991
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 17.29s
                        Total time: 17822.33s
                               ETA: 1009411.7s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.664s, learning 0.204s)
               Value function loss: 2204.4250
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 71.79
               Mean episode length: 54.70
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0913
Mean episode consecutive_successes: 0.2005
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 16.87s
                        Total time: 17839.20s
                               ETA: 1009774.8s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.585s, learning 0.194s)
               Value function loss: 2101.0502
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 57.04
               Mean episode length: 53.92
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0854
Mean episode consecutive_successes: 0.2031
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 16.78s
                        Total time: 17855.98s
                               ETA: 1010132.4s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.724s, learning 0.375s)
               Value function loss: 1368.9389
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 71.65
               Mean episode length: 53.70
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0811
Mean episode consecutive_successes: 0.2000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 17.10s
                        Total time: 17873.08s
                               ETA: 1010507.7s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.718s, learning 0.187s)
               Value function loss: 2119.2293
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 66.34
               Mean episode length: 54.68
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0718
Mean episode consecutive_successes: 0.2025
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 16.91s
                        Total time: 17889.98s
                               ETA: 1010871.5s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.430s, learning 0.223s)
               Value function loss: 1787.1535
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 28.58
               Mean episode length: 56.60
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0767
Mean episode consecutive_successes: 0.1969
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 16.65s
                        Total time: 17906.64s
                               ETA: 1011220.8s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1271 steps/s (collection: 12.687s, learning 0.199s)
               Value function loss: 2857.8165
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 51.46
               Mean episode length: 55.19
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0933
Mean episode consecutive_successes: 0.1948
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 12.89s
                        Total time: 17919.52s
                               ETA: 1011356.9s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.153s, learning 0.177s)
               Value function loss: 4144.0897
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 44.84
               Mean episode length: 53.95
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.05
            Mean episode successes: 0.1240
Mean episode consecutive_successes: 0.1917
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 8.33s
                        Total time: 17927.85s
                               ETA: 1011235.9s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.663s, learning 0.231s)
               Value function loss: 3482.0639
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 64.79
               Mean episode length: 57.19
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1333
Mean episode consecutive_successes: 0.1983
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 8.89s
                        Total time: 17936.75s
                               ETA: 1011146.9s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.245s, learning 0.211s)
               Value function loss: 3094.2925
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 46.55
               Mean episode length: 53.64
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.01
            Mean episode successes: 0.1099
Mean episode consecutive_successes: 0.2119
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 8.46s
                        Total time: 17945.20s
                               ETA: 1011033.2s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.366s, learning 0.225s)
               Value function loss: 3361.6279
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 87.55
               Mean episode length: 54.58
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1187
Mean episode consecutive_successes: 0.2180
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 8.59s
                        Total time: 17953.80s
                               ETA: 1010927.3s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.716s, learning 0.203s)
               Value function loss: 2025.1768
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 76.27
               Mean episode length: 55.33
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.07
            Mean episode successes: 0.1162
Mean episode consecutive_successes: 0.2185
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 8.92s
                        Total time: 17962.71s
                               ETA: 1010839.9s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.387s, learning 0.207s)
               Value function loss: 1820.7444
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 76.99
               Mean episode length: 56.23
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0859
Mean episode consecutive_successes: 0.2288
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 8.59s
                        Total time: 17971.31s
                               ETA: 1010734.4s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.495s, learning 0.318s)
               Value function loss: 2086.3585
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 53.81
               Mean episode length: 52.20
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0981
Mean episode consecutive_successes: 0.2237
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 8.81s
                        Total time: 17980.12s
                               ETA: 1010641.2s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.460s, learning 0.179s)
               Value function loss: 3713.5355
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 58.11
               Mean episode length: 51.91
                  Mean reward/step: 1.61
       Mean episode length/episode: 6.97
            Mean episode successes: 0.1084
Mean episode consecutive_successes: 0.2268
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 8.64s
                        Total time: 17988.76s
                               ETA: 1010538.4s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.652s, learning 0.213s)
               Value function loss: 3812.7339
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 40.67
               Mean episode length: 51.54
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.16
            Mean episode successes: 0.1187
Mean episode consecutive_successes: 0.2248
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 8.87s
                        Total time: 17997.63s
                               ETA: 1010448.4s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.678s, learning 0.174s)
               Value function loss: 2453.9445
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 133.64
               Mean episode length: 56.55
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.02
            Mean episode successes: 0.1035
Mean episode consecutive_successes: 0.2342
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 8.85s
                        Total time: 18006.48s
                               ETA: 1010357.7s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.960s, learning 0.176s)
               Value function loss: 2018.2739
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 78.81
               Mean episode length: 52.39
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0879
Mean episode consecutive_successes: 0.2384
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 8.14s
                        Total time: 18014.61s
                               ETA: 1010227.0s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.578s, learning 0.235s)
               Value function loss: 1494.8474
                    Surrogate loss: -0.0121
             Mean action noise std: 0.75
                       Mean reward: 95.67
               Mean episode length: 54.25
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0747
Mean episode consecutive_successes: 0.2408
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 8.81s
                        Total time: 18023.43s
                               ETA: 1010134.3s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.715s, learning 0.201s)
               Value function loss: 2850.1267
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 49.05
               Mean episode length: 52.55
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0820
Mean episode consecutive_successes: 0.2379
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 8.92s
                        Total time: 18032.34s
                               ETA: 1010047.6s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.546s, learning 0.323s)
               Value function loss: 2136.6842
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 71.53
               Mean episode length: 53.03
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0869
Mean episode consecutive_successes: 0.2343
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 8.87s
                        Total time: 18041.21s
                               ETA: 1009958.3s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.506s, learning 0.203s)
               Value function loss: 1788.2587
                    Surrogate loss: -0.0133
             Mean action noise std: 0.75
                       Mean reward: 34.52
               Mean episode length: 52.75
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0835
Mean episode consecutive_successes: 0.2297
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 8.71s
                        Total time: 18049.92s
                               ETA: 1009860.2s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.494s, learning 0.176s)
               Value function loss: 2341.8749
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 72.90
               Mean episode length: 54.72
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0923
Mean episode consecutive_successes: 0.2269
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 8.67s
                        Total time: 18058.59s
                               ETA: 1009759.9s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.366s, learning 0.169s)
               Value function loss: 2324.0690
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 54.60
               Mean episode length: 54.52
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0815
Mean episode consecutive_successes: 0.2292
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 8.54s
                        Total time: 18067.13s
                               ETA: 1009652.3s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1977 steps/s (collection: 7.972s, learning 0.315s)
               Value function loss: 2642.9348
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 39.38
               Mean episode length: 53.41
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0972
Mean episode consecutive_successes: 0.2235
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 8.29s
                        Total time: 18075.41s
                               ETA: 1009530.9s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.620s, learning 0.201s)
               Value function loss: 2493.7167
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 87.93
               Mean episode length: 56.23
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.19
            Mean episode successes: 0.1113
Mean episode consecutive_successes: 0.2223
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 8.82s
                        Total time: 18084.23s
                               ETA: 1009439.4s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.267s, learning 0.202s)
               Value function loss: 3292.3957
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 80.29
               Mean episode length: 56.58
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.16
            Mean episode successes: 0.1284
Mean episode consecutive_successes: 0.2200
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 8.47s
                        Total time: 18092.70s
                               ETA: 1009328.3s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.372s, learning 0.193s)
               Value function loss: 2165.6253
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 94.07
               Mean episode length: 54.14
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.1099
Mean episode consecutive_successes: 0.2261
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 8.57s
                        Total time: 18101.27s
                               ETA: 1009222.8s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.312s, learning 0.257s)
               Value function loss: 3496.8389
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 59.57
               Mean episode length: 55.54
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.06
            Mean episode successes: 0.1294
Mean episode consecutive_successes: 0.2263
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 8.57s
                        Total time: 18109.84s
                               ETA: 1009117.5s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.355s, learning 0.216s)
               Value function loss: 2391.0802
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 105.10
               Mean episode length: 55.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1250
Mean episode consecutive_successes: 0.2278
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 8.57s
                        Total time: 18118.41s
                               ETA: 1009012.5s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.678s, learning 0.298s)
               Value function loss: 3673.5637
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 59.55
               Mean episode length: 54.29
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1294
Mean episode consecutive_successes: 0.2332
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 8.98s
                        Total time: 18127.38s
                               ETA: 1008930.2s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.572s, learning 0.300s)
               Value function loss: 3210.5511
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 98.14
               Mean episode length: 55.62
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.09
            Mean episode successes: 0.1226
Mean episode consecutive_successes: 0.2423
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 8.87s
                        Total time: 18136.26s
                               ETA: 1008842.1s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.408s, learning 0.171s)
               Value function loss: 3501.6563
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 95.23
               Mean episode length: 55.10
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.05
            Mean episode successes: 0.1240
Mean episode consecutive_successes: 0.2466
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 8.58s
                        Total time: 18144.84s
                               ETA: 1008737.9s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.483s, learning 0.168s)
               Value function loss: 4070.2776
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 132.07
               Mean episode length: 51.05
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1357
Mean episode consecutive_successes: 0.2527
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 8.65s
                        Total time: 18153.49s
                               ETA: 1008637.7s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.353s, learning 0.181s)
               Value function loss: 3000.8825
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 64.55
               Mean episode length: 53.18
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1333
Mean episode consecutive_successes: 0.2520
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 8.53s
                        Total time: 18162.02s
                               ETA: 1008531.1s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.668s, learning 0.411s)
               Value function loss: 4217.7835
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 71.26
               Mean episode length: 53.52
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1382
Mean episode consecutive_successes: 0.2565
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 9.08s
                        Total time: 18171.10s
                               ETA: 1008454.9s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.414s, learning 0.179s)
               Value function loss: 3114.8408
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 105.15
               Mean episode length: 53.59
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1392
Mean episode consecutive_successes: 0.2667
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 8.59s
                        Total time: 18179.69s
                               ETA: 1008351.9s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.137s, learning 0.180s)
               Value function loss: 3268.1600
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 65.22
               Mean episode length: 54.76
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1304
Mean episode consecutive_successes: 0.2690
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 8.32s
                        Total time: 18188.01s
                               ETA: 1008233.6s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.267s, learning 0.276s)
               Value function loss: 4233.9028
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 138.77
               Mean episode length: 55.63
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1533
Mean episode consecutive_successes: 0.2694
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 8.54s
                        Total time: 18196.55s
                               ETA: 1008128.0s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.425s, learning 0.310s)
               Value function loss: 2836.5293
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 135.89
               Mean episode length: 55.64
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.09
            Mean episode successes: 0.1377
Mean episode consecutive_successes: 0.2791
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 8.73s
                        Total time: 18205.29s
                               ETA: 1008033.1s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.422s, learning 0.203s)
               Value function loss: 3036.5332
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 103.41
               Mean episode length: 58.87
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.08
            Mean episode successes: 0.1211
Mean episode consecutive_successes: 0.2859
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 8.63s
                        Total time: 18213.91s
                               ETA: 1007932.2s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.371s, learning 0.270s)
               Value function loss: 3391.0534
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 103.44
               Mean episode length: 59.23
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.15
            Mean episode successes: 0.1274
Mean episode consecutive_successes: 0.2883
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 8.64s
                        Total time: 18222.55s
                               ETA: 1007832.4s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.483s, learning 0.179s)
               Value function loss: 2084.9161
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 60.24
               Mean episode length: 56.08
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.14
            Mean episode successes: 0.1196
Mean episode consecutive_successes: 0.2914
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 8.66s
                        Total time: 18231.22s
                               ETA: 1007733.8s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.762s, learning 0.190s)
               Value function loss: 3151.4600
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 67.30
               Mean episode length: 54.23
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.05
            Mean episode successes: 0.1211
Mean episode consecutive_successes: 0.2912
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 8.95s
                        Total time: 18240.17s
                               ETA: 1007651.3s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.625s, learning 0.201s)
               Value function loss: 3430.4802
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 59.70
               Mean episode length: 53.72
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.18
            Mean episode successes: 0.1338
Mean episode consecutive_successes: 0.2871
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 8.83s
                        Total time: 18248.99s
                               ETA: 1007561.9s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.523s, learning 0.194s)
               Value function loss: 2925.1836
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 49.54
               Mean episode length: 53.70
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.07
            Mean episode successes: 0.1255
Mean episode consecutive_successes: 0.2899
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 8.72s
                        Total time: 18257.71s
                               ETA: 1007466.6s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.495s, learning 0.225s)
               Value function loss: 1843.4168
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 118.15
               Mean episode length: 56.37
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1040
Mean episode consecutive_successes: 0.2972
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 8.72s
                        Total time: 18266.43s
                               ETA: 1007371.5s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.446s, learning 0.187s)
               Value function loss: 3087.8341
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 72.74
               Mean episode length: 54.01
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0894
Mean episode consecutive_successes: 0.2958
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 8.63s
                        Total time: 18275.06s
                               ETA: 1007271.8s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.398s, learning 0.316s)
               Value function loss: 3885.1515
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 51.36
               Mean episode length: 55.84
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.08
            Mean episode successes: 0.1172
Mean episode consecutive_successes: 0.2877
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 8.71s
                        Total time: 18283.78s
                               ETA: 1007176.7s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.392s, learning 0.198s)
               Value function loss: 2129.4832
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 69.63
               Mean episode length: 55.48
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1255
Mean episode consecutive_successes: 0.2821
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 8.59s
                        Total time: 18292.37s
                               ETA: 1007074.8s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.457s, learning 0.245s)
               Value function loss: 2825.2615
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 25.70
               Mean episode length: 54.52
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.09
            Mean episode successes: 0.1055
Mean episode consecutive_successes: 0.2879
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 8.70s
                        Total time: 18301.07s
                               ETA: 1006979.2s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.484s, learning 0.194s)
               Value function loss: 3742.8185
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 49.36
               Mean episode length: 53.32
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1270
Mean episode consecutive_successes: 0.2830
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 8.68s
                        Total time: 18309.75s
                               ETA: 1006882.3s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.632s, learning 0.283s)
               Value function loss: 3204.9157
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 45.73
               Mean episode length: 56.14
                  Mean reward/step: 1.72
       Mean episode length/episode: 7.09
            Mean episode successes: 0.1436
Mean episode consecutive_successes: 0.2801
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 8.92s
                        Total time: 18318.66s
                               ETA: 1006798.6s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.475s, learning 0.205s)
               Value function loss: 3387.2704
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 130.33
               Mean episode length: 56.40
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.17
            Mean episode successes: 0.1592
Mean episode consecutive_successes: 0.2832
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 8.68s
                        Total time: 18327.34s
                               ETA: 1006702.0s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.504s, learning 0.223s)
               Value function loss: 2781.4830
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 57.29
               Mean episode length: 54.04
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.08
            Mean episode successes: 0.1558
Mean episode consecutive_successes: 0.2810
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 8.73s
                        Total time: 18336.07s
                               ETA: 1006608.1s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.508s, learning 0.188s)
               Value function loss: 3727.8162
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 155.67
               Mean episode length: 56.84
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.08
            Mean episode successes: 0.1304
Mean episode consecutive_successes: 0.2941
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 8.70s
                        Total time: 18344.77s
                               ETA: 1006512.7s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.759s, learning 0.194s)
               Value function loss: 2944.2282
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 93.47
               Mean episode length: 57.63
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.16
            Mean episode successes: 0.1479
Mean episode consecutive_successes: 0.2974
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 8.95s
                        Total time: 18353.72s
                               ETA: 1006431.4s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.710s, learning 0.166s)
               Value function loss: 3239.7938
                    Surrogate loss: -0.0149
             Mean action noise std: 0.75
                       Mean reward: 136.49
               Mean episode length: 56.86
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.08
            Mean episode successes: 0.1406
Mean episode consecutive_successes: 0.3088
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 8.88s
                        Total time: 18362.59s
                               ETA: 1006346.0s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1771 steps/s (collection: 8.949s, learning 0.299s)
               Value function loss: 3279.1716
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 82.58
               Mean episode length: 54.15
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.15
            Mean episode successes: 0.1284
Mean episode consecutive_successes: 0.3110
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 9.25s
                        Total time: 18371.84s
                               ETA: 1006281.0s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.107s, learning 0.242s)
               Value function loss: 3288.4768
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 54.86
               Mean episode length: 52.89
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.04
            Mean episode successes: 0.1450
Mean episode consecutive_successes: 0.2995
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 8.35s
                        Total time: 18380.19s
                               ETA: 1006167.0s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.215s, learning 0.174s)
               Value function loss: 3665.6415
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 120.74
               Mean episode length: 57.74
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.04
            Mean episode successes: 0.1279
Mean episode consecutive_successes: 0.3106
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 8.39s
                        Total time: 18388.58s
                               ETA: 1006055.1s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.512s, learning 0.190s)
               Value function loss: 2724.2509
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 108.50
               Mean episode length: 57.44
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.06
            Mean episode successes: 0.1235
Mean episode consecutive_successes: 0.3104
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 8.70s
                        Total time: 18397.28s
                               ETA: 1005960.5s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.502s, learning 0.248s)
               Value function loss: 2173.8941
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 93.06
               Mean episode length: 51.77
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.05
            Mean episode successes: 0.1104
Mean episode consecutive_successes: 0.3109
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 8.75s
                        Total time: 18406.03s
                               ETA: 1005868.7s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.497s, learning 0.202s)
               Value function loss: 2780.5045
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 58.35
               Mean episode length: 56.97
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1172
Mean episode consecutive_successes: 0.3069
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 8.70s
                        Total time: 18414.73s
                               ETA: 1005774.1s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.065s, learning 0.211s)
               Value function loss: 3851.9953
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 100.35
               Mean episode length: 56.19
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.17
            Mean episode successes: 0.1284
Mean episode consecutive_successes: 0.3096
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 8.28s
                        Total time: 18423.01s
                               ETA: 1005656.5s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.648s, learning 0.205s)
               Value function loss: 3349.0924
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 79.35
               Mean episode length: 53.40
                  Mean reward/step: 1.71
       Mean episode length/episode: 7.06
            Mean episode successes: 0.1348
Mean episode consecutive_successes: 0.3075
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 8.85s
                        Total time: 18431.86s
                               ETA: 1005570.5s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.515s, learning 0.313s)
               Value function loss: 4091.6022
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 23.82
               Mean episode length: 53.43
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.98
            Mean episode successes: 0.1650
Mean episode consecutive_successes: 0.2973
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 8.83s
                        Total time: 18440.69s
                               ETA: 1005483.3s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.450s, learning 0.188s)
               Value function loss: 4974.6761
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 75.67
               Mean episode length: 54.50
                  Mean reward/step: 2.19
       Mean episode length/episode: 7.17
            Mean episode successes: 0.1763
Mean episode consecutive_successes: 0.3077
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 8.64s
                        Total time: 18449.32s
                               ETA: 1005385.8s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.434s, learning 0.327s)
               Value function loss: 5169.0433
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 210.50
               Mean episode length: 57.98
                  Mean reward/step: 2.15
       Mean episode length/episode: 7.16
            Mean episode successes: 0.1753
Mean episode consecutive_successes: 0.3317
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 8.76s
                        Total time: 18458.09s
                               ETA: 1005295.1s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.464s, learning 0.178s)
               Value function loss: 5191.3820
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 127.94
               Mean episode length: 54.74
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1802
Mean episode consecutive_successes: 0.3369
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 8.64s
                        Total time: 18466.73s
                               ETA: 1005198.1s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.436s, learning 0.307s)
               Value function loss: 5509.5814
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 172.80
               Mean episode length: 54.66
                  Mean reward/step: 2.53
       Mean episode length/episode: 7.12
            Mean episode successes: 0.2036
Mean episode consecutive_successes: 0.3459
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 8.74s
                        Total time: 18475.47s
                               ETA: 1005106.6s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.573s, learning 0.190s)
               Value function loss: 4913.8064
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 131.81
               Mean episode length: 57.40
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.1982
Mean episode consecutive_successes: 0.3490
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 8.76s
                        Total time: 18484.24s
                               ETA: 1005016.3s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.902s, learning 0.192s)
               Value function loss: 4245.0605
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 121.96
               Mean episode length: 56.16
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.1982
Mean episode consecutive_successes: 0.3565
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 9.09s
                        Total time: 18493.33s
                               ETA: 1004944.1s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.562s, learning 0.272s)
               Value function loss: 3014.5958
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 130.78
               Mean episode length: 56.23
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.09
            Mean episode successes: 0.1606
Mean episode consecutive_successes: 0.3704
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 8.83s
                        Total time: 18502.16s
                               ETA: 1004857.8s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.897s, learning 0.179s)
               Value function loss: 3424.0331
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 123.53
               Mean episode length: 56.84
                  Mean reward/step: 1.64
       Mean episode length/episode: 6.97
            Mean episode successes: 0.1328
Mean episode consecutive_successes: 0.3774
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 9.08s
                        Total time: 18511.24s
                               ETA: 1004784.7s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.289s, learning 0.230s)
               Value function loss: 2659.6295
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 70.98
               Mean episode length: 56.87
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.07
            Mean episode successes: 0.1382
Mean episode consecutive_successes: 0.3666
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 8.52s
                        Total time: 18519.76s
                               ETA: 1004681.5s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.505s, learning 0.186s)
               Value function loss: 2906.2310
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 51.82
               Mean episode length: 53.82
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.09
            Mean episode successes: 0.1313
Mean episode consecutive_successes: 0.3612
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 8.69s
                        Total time: 18528.45s
                               ETA: 1004587.7s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1999 steps/s (collection: 7.983s, learning 0.211s)
               Value function loss: 4363.6102
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 101.55
               Mean episode length: 55.93
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.17
            Mean episode successes: 0.1533
Mean episode consecutive_successes: 0.3556
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 8.19s
                        Total time: 18536.64s
                               ETA: 1004467.1s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.584s, learning 0.171s)
               Value function loss: 2594.6944
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 54.11
               Mean episode length: 54.20
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.14
            Mean episode successes: 0.1489
Mean episode consecutive_successes: 0.3460
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 8.75s
                        Total time: 18545.40s
                               ETA: 1004377.0s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.257s, learning 0.197s)
               Value function loss: 3988.3651
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 128.87
               Mean episode length: 54.70
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1533
Mean episode consecutive_successes: 0.3441
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 8.45s
                        Total time: 18553.85s
                               ETA: 1004270.7s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.442s, learning 0.181s)
               Value function loss: 4183.8882
                    Surrogate loss: -0.0156
             Mean action noise std: 0.75
                       Mean reward: 86.83
               Mean episode length: 53.20
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.1440
Mean episode consecutive_successes: 0.3504
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 8.62s
                        Total time: 18562.48s
                               ETA: 1004173.7s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.367s, learning 0.254s)
               Value function loss: 4957.9353
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 115.16
               Mean episode length: 55.88
                  Mean reward/step: 2.13
       Mean episode length/episode: 7.16
            Mean episode successes: 0.1606
Mean episode consecutive_successes: 0.3512
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 8.62s
                        Total time: 18571.10s
                               ETA: 1004076.6s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.518s, learning 0.379s)
               Value function loss: 4312.7920
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 60.62
               Mean episode length: 57.36
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.20
            Mean episode successes: 0.1665
Mean episode consecutive_successes: 0.3523
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 8.90s
                        Total time: 18579.99s
                               ETA: 1003994.5s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.779s, learning 0.273s)
               Value function loss: 4107.0790
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 191.52
               Mean episode length: 59.68
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1553
Mean episode consecutive_successes: 0.3707
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 9.05s
                        Total time: 18589.05s
                               ETA: 1003920.9s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.469s, learning 0.294s)
               Value function loss: 4758.3485
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 91.62
               Mean episode length: 55.42
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.07
            Mean episode successes: 0.1543
Mean episode consecutive_successes: 0.3698
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 8.76s
                        Total time: 18597.81s
                               ETA: 1003831.8s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.801s, learning 0.309s)
               Value function loss: 4936.5350
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 192.71
               Mean episode length: 53.92
                  Mean reward/step: 2.36
       Mean episode length/episode: 6.93
            Mean episode successes: 0.1699
Mean episode consecutive_successes: 0.3735
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 9.11s
                        Total time: 18606.92s
                               ETA: 1003761.5s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.172s, learning 0.236s)
               Value function loss: 4763.0469
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 54.26
               Mean episode length: 54.58
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.17
            Mean episode successes: 0.1753
Mean episode consecutive_successes: 0.3640
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 8.41s
                        Total time: 18615.33s
                               ETA: 1003653.3s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.594s, learning 0.222s)
               Value function loss: 4303.6132
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 113.12
               Mean episode length: 55.83
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.1768
Mean episode consecutive_successes: 0.3676
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 8.82s
                        Total time: 18624.14s
                               ETA: 1003567.3s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.811s, learning 0.257s)
               Value function loss: 5030.6098
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 131.22
               Mean episode length: 55.35
                  Mean reward/step: 2.20
       Mean episode length/episode: 7.16
            Mean episode successes: 0.1904
Mean episode consecutive_successes: 0.3743
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 9.07s
                        Total time: 18633.21s
                               ETA: 1003494.9s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.509s, learning 0.385s)
               Value function loss: 4283.7543
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 167.07
               Mean episode length: 56.39
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1753
Mean episode consecutive_successes: 0.3841
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 8.89s
                        Total time: 18642.10s
                               ETA: 1003413.3s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.558s, learning 0.250s)
               Value function loss: 4605.7761
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 86.24
               Mean episode length: 58.00
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2002
Mean episode consecutive_successes: 0.3735
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 8.81s
                        Total time: 18650.91s
                               ETA: 1003327.1s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.365s, learning 0.378s)
               Value function loss: 4974.5158
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 117.56
               Mean episode length: 54.37
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.15
            Mean episode successes: 0.2065
Mean episode consecutive_successes: 0.3778
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 8.74s
                        Total time: 18659.66s
                               ETA: 1003237.5s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1745 steps/s (collection: 8.957s, learning 0.427s)
               Value function loss: 3430.2979
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 91.52
               Mean episode length: 57.38
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.18
            Mean episode successes: 0.1924
Mean episode consecutive_successes: 0.3869
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 9.38s
                        Total time: 18669.04s
                               ETA: 1003182.4s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.583s, learning 0.265s)
               Value function loss: 3031.8688
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 75.73
               Mean episode length: 54.98
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.06
            Mean episode successes: 0.1802
Mean episode consecutive_successes: 0.3832
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 8.85s
                        Total time: 18677.89s
                               ETA: 1003098.6s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.439s, learning 0.280s)
               Value function loss: 4232.5563
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 156.95
               Mean episode length: 58.83
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.1704
Mean episode consecutive_successes: 0.3965
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 8.72s
                        Total time: 18686.61s
                               ETA: 1003007.9s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.526s, learning 0.392s)
               Value function loss: 5217.1939
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 37.46
               Mean episode length: 56.17
                  Mean reward/step: 2.49
       Mean episode length/episode: 7.05
            Mean episode successes: 0.1880
Mean episode consecutive_successes: 0.3926
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 8.92s
                        Total time: 18695.52s
                               ETA: 1002928.0s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.709s, learning 0.334s)
               Value function loss: 4569.4798
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 176.12
               Mean episode length: 54.15
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.1963
Mean episode consecutive_successes: 0.3919
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 9.04s
                        Total time: 18704.57s
                               ETA: 1002854.9s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.822s, learning 0.269s)
               Value function loss: 5276.7174
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 146.48
               Mean episode length: 57.32
                  Mean reward/step: 2.18
       Mean episode length/episode: 7.18
            Mean episode successes: 0.2144
Mean episode consecutive_successes: 0.3961
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 9.09s
                        Total time: 18713.66s
                               ETA: 1002784.4s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.557s, learning 0.238s)
               Value function loss: 4996.6410
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 165.84
               Mean episode length: 56.30
                  Mean reward/step: 2.37
       Mean episode length/episode: 7.03
            Mean episode successes: 0.1782
Mean episode consecutive_successes: 0.4114
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 8.79s
                        Total time: 18722.45s
                               ETA: 1002698.1s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.392s, learning 0.350s)
               Value function loss: 3966.2466
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 195.61
               Mean episode length: 53.38
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1616
Mean episode consecutive_successes: 0.4212
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 8.74s
                        Total time: 18731.20s
                               ETA: 1002609.2s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.415s, learning 0.192s)
               Value function loss: 3881.8635
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 56.31
               Mean episode length: 52.98
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.12
            Mean episode successes: 0.1616
Mean episode consecutive_successes: 0.4101
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 8.61s
                        Total time: 18739.80s
                               ETA: 1002513.0s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.172s)
               Value function loss: 5379.7786
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 144.71
               Mean episode length: 55.47
                  Mean reward/step: 2.30
       Mean episode length/episode: 7.13
            Mean episode successes: 0.1924
Mean episode consecutive_successes: 0.4044
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 8.55s
                        Total time: 18748.35s
                               ETA: 1002413.6s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.455s, learning 0.164s)
               Value function loss: 2911.5211
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 159.12
               Mean episode length: 57.84
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.07
            Mean episode successes: 0.1553
Mean episode consecutive_successes: 0.4118
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 8.62s
                        Total time: 18756.97s
                               ETA: 1002318.3s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.454s, learning 0.173s)
               Value function loss: 6652.7445
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 137.73
               Mean episode length: 53.14
                  Mean reward/step: 2.76
       Mean episode length/episode: 7.07
            Mean episode successes: 0.1899
Mean episode consecutive_successes: 0.4067
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 8.63s
                        Total time: 18765.59s
                               ETA: 1002223.5s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.690s, learning 0.177s)
               Value function loss: 4881.3431
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 124.40
               Mean episode length: 58.46
                  Mean reward/step: 2.48
       Mean episode length/episode: 7.20
            Mean episode successes: 0.2041
Mean episode consecutive_successes: 0.4133
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 8.87s
                        Total time: 18774.46s
                               ETA: 1002141.7s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.596s, learning 0.306s)
               Value function loss: 5326.0301
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 64.03
               Mean episode length: 53.01
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2148
Mean episode consecutive_successes: 0.4076
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 8.90s
                        Total time: 18783.36s
                               ETA: 1002061.8s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.618s, learning 0.167s)
               Value function loss: 3998.6771
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 104.61
               Mean episode length: 52.81
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.11
            Mean episode successes: 0.1968
Mean episode consecutive_successes: 0.4192
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 8.78s
                        Total time: 18792.15s
                               ETA: 1001975.6s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.240s, learning 0.228s)
               Value function loss: 5354.0822
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 147.05
               Mean episode length: 54.50
                  Mean reward/step: 2.23
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1753
Mean episode consecutive_successes: 0.4343
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 8.47s
                        Total time: 18800.62s
                               ETA: 1001872.7s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.533s, learning 0.169s)
               Value function loss: 6778.2922
                    Surrogate loss: -0.0136
             Mean action noise std: 0.75
                       Mean reward: 73.38
               Mean episode length: 54.83
                  Mean reward/step: 2.78
       Mean episode length/episode: 7.03
            Mean episode successes: 0.2075
Mean episode consecutive_successes: 0.4263
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 8.70s
                        Total time: 18809.32s
                               ETA: 1001782.4s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.392s, learning 0.214s)
               Value function loss: 5477.4657
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 181.77
               Mean episode length: 57.86
                  Mean reward/step: 2.41
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2080
Mean episode consecutive_successes: 0.4373
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 8.61s
                        Total time: 18817.92s
                               ETA: 1001687.0s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.654s, learning 0.178s)
               Value function loss: 5393.5793
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 68.55
               Mean episode length: 57.47
                  Mean reward/step: 2.57
       Mean episode length/episode: 7.16
            Mean episode successes: 0.2378
Mean episode consecutive_successes: 0.4272
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 8.83s
                        Total time: 18826.75s
                               ETA: 1001603.7s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.712s, learning 0.189s)
               Value function loss: 5696.6418
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 335.39
               Mean episode length: 56.37
                  Mean reward/step: 2.46
       Mean episode length/episode: 7.14
            Mean episode successes: 0.2246
Mean episode consecutive_successes: 0.4572
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 8.90s
                        Total time: 18835.66s
                               ETA: 1001524.3s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.359s, learning 0.317s)
               Value function loss: 4512.5544
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 60.92
               Mean episode length: 55.20
                  Mean reward/step: 2.31
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2095
Mean episode consecutive_successes: 0.4540
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 8.68s
                        Total time: 18844.33s
                               ETA: 1001432.9s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.690s, learning 0.291s)
               Value function loss: 7633.8092
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 187.01
               Mean episode length: 58.41
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.20
            Mean episode successes: 0.2563
Mean episode consecutive_successes: 0.4551
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 8.98s
                        Total time: 18853.31s
                               ETA: 1001357.8s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.282s, learning 0.217s)
               Value function loss: 5572.3808
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 124.14
               Mean episode length: 59.18
                  Mean reward/step: 2.66
       Mean episode length/episode: 7.19
            Mean episode successes: 0.2510
Mean episode consecutive_successes: 0.4675
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 8.50s
                        Total time: 18861.81s
                               ETA: 1001257.2s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.735s, learning 0.177s)
               Value function loss: 5745.6932
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 50.49
               Mean episode length: 56.56
                  Mean reward/step: 2.78
       Mean episode length/episode: 7.06
            Mean episode successes: 0.2544
Mean episode consecutive_successes: 0.4654
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 8.91s
                        Total time: 18870.72s
                               ETA: 1001178.6s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.643s, learning 0.231s)
               Value function loss: 5891.3328
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 137.33
               Mean episode length: 56.83
                  Mean reward/step: 2.49
       Mean episode length/episode: 7.09
            Mean episode successes: 0.2363
Mean episode consecutive_successes: 0.4830
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 8.87s
                        Total time: 18879.60s
                               ETA: 1001098.1s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.417s, learning 0.212s)
               Value function loss: 5268.3058
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 133.94
               Mean episode length: 56.18
                  Mean reward/step: 2.32
       Mean episode length/episode: 7.08
            Mean episode successes: 0.2524
Mean episode consecutive_successes: 0.4775
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 8.63s
                        Total time: 18888.23s
                               ETA: 1001004.6s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.412s, learning 0.199s)
               Value function loss: 6068.7993
                    Surrogate loss: -0.0166
             Mean action noise std: 0.75
                       Mean reward: 227.10
               Mean episode length: 54.54
                  Mean reward/step: 2.58
       Mean episode length/episode: 7.09
            Mean episode successes: 0.2197
Mean episode consecutive_successes: 0.4961
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 8.61s
                        Total time: 18896.84s
                               ETA: 1000910.3s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.612s, learning 0.167s)
               Value function loss: 6008.9583
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 77.92
               Mean episode length: 55.88
                  Mean reward/step: 2.51
       Mean episode length/episode: 7.10
            Mean episode successes: 0.1895
Mean episode consecutive_successes: 0.5152
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 8.78s
                        Total time: 18905.62s
                               ETA: 1000825.0s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.431s, learning 0.186s)
               Value function loss: 5350.1077
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 88.02
               Mean episode length: 55.65
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.17
            Mean episode successes: 0.2168
Mean episode consecutive_successes: 0.5024
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 8.62s
                        Total time: 18914.23s
                               ETA: 1000731.1s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1752 steps/s (collection: 9.168s, learning 0.180s)
               Value function loss: 8312.1590
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 126.14
               Mean episode length: 56.14
                  Mean reward/step: 3.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.2451
Mean episode consecutive_successes: 0.4987
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 9.35s
                        Total time: 18923.58s
                               ETA: 1000676.1s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.337s, learning 0.195s)
               Value function loss: 5802.7477
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 245.18
               Mean episode length: 56.59
                  Mean reward/step: 2.67
       Mean episode length/episode: 7.15
            Mean episode successes: 0.2505
Mean episode consecutive_successes: 0.5147
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 8.53s
                        Total time: 18932.11s
                               ETA: 1000578.0s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.138s, learning 0.168s)
               Value function loss: 5633.7255
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 76.65
               Mean episode length: 55.69
                  Mean reward/step: 2.59
       Mean episode length/episode: 7.15
            Mean episode successes: 0.2271
Mean episode consecutive_successes: 0.5213
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 8.31s
                        Total time: 18940.42s
                               ETA: 1000468.0s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.746s, learning 0.191s)
               Value function loss: 6777.7986
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 136.33
               Mean episode length: 56.76
                  Mean reward/step: 2.86
       Mean episode length/episode: 7.06
            Mean episode successes: 0.2451
Mean episode consecutive_successes: 0.5181
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 8.94s
                        Total time: 18949.36s
                               ETA: 1000391.4s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.828s, learning 0.241s)
               Value function loss: 4667.4959
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 187.27
               Mean episode length: 55.62
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.2061
Mean episode consecutive_successes: 0.5304
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 9.07s
                        Total time: 18958.42s
                               ETA: 1000321.9s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.429s, learning 0.180s)
               Value function loss: 6532.7605
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 111.02
               Mean episode length: 56.60
                  Mean reward/step: 2.55
       Mean episode length/episode: 7.12
            Mean episode successes: 0.2222
Mean episode consecutive_successes: 0.5233
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 8.61s
                        Total time: 18967.03s
                               ETA: 1000228.2s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.335s, learning 0.228s)
               Value function loss: 6767.7061
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 93.23
               Mean episode length: 54.99
                  Mean reward/step: 2.71
       Mean episode length/episode: 7.13
            Mean episode successes: 0.2480
Mean episode consecutive_successes: 0.5098
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 8.56s
                        Total time: 18975.60s
                               ETA: 1000132.1s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.411s, learning 0.189s)
               Value function loss: 5774.8144
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 137.91
               Mean episode length: 53.40
                  Mean reward/step: 2.81
       Mean episode length/episode: 7.08
            Mean episode successes: 0.2358
Mean episode consecutive_successes: 0.5248
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 8.60s
                        Total time: 18984.20s
                               ETA: 1000038.1s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.330s, learning 0.174s)
               Value function loss: 6696.2705
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 148.40
               Mean episode length: 54.93
                  Mean reward/step: 2.83
       Mean episode length/episode: 7.13
            Mean episode successes: 0.2500
Mean episode consecutive_successes: 0.5247
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 8.50s
                        Total time: 18992.70s
                               ETA: 999939.1s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.113s, learning 0.178s)
               Value function loss: 4745.8596
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 166.84
               Mean episode length: 56.02
                  Mean reward/step: 2.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.2075
Mean episode consecutive_successes: 0.5412
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 8.29s
                        Total time: 19000.99s
                               ETA: 999829.1s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.522s, learning 0.261s)
               Value function loss: 5489.6214
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 116.96
               Mean episode length: 53.95
                  Mean reward/step: 2.62
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2139
Mean episode consecutive_successes: 0.5332
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 8.78s
                        Total time: 19009.77s
                               ETA: 999745.0s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.412s, learning 0.183s)
               Value function loss: 6340.1084
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 101.86
               Mean episode length: 57.55
                  Mean reward/step: 2.65
       Mean episode length/episode: 7.15
            Mean episode successes: 0.2207
Mean episode consecutive_successes: 0.5272
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 8.59s
                        Total time: 19018.37s
                               ETA: 999651.1s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.195s, learning 0.169s)
               Value function loss: 7338.9611
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 101.74
               Mean episode length: 54.93
                  Mean reward/step: 3.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.2485
Mean episode consecutive_successes: 0.5249
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 8.36s
                        Total time: 19026.73s
                               ETA: 999545.1s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.655s, learning 0.174s)
               Value function loss: 5157.0820
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 82.80
               Mean episode length: 54.31
                  Mean reward/step: 2.51
       Mean episode length/episode: 7.15
            Mean episode successes: 0.2515
Mean episode consecutive_successes: 0.5254
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 8.83s
                        Total time: 19035.56s
                               ETA: 999463.7s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.492s, learning 0.177s)
               Value function loss: 5775.9177
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 104.39
               Mean episode length: 57.87
                  Mean reward/step: 2.48
       Mean episode length/episode: 7.11
            Mean episode successes: 0.2520
Mean episode consecutive_successes: 0.5250
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 8.67s
                        Total time: 19044.23s
                               ETA: 999374.0s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.759s, learning 0.204s)
               Value function loss: 6007.2532
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 178.29
               Mean episode length: 58.26
                  Mean reward/step: 2.51
       Mean episode length/episode: 7.06
            Mean episode successes: 0.2305
Mean episode consecutive_successes: 0.5323
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 8.96s
                        Total time: 19053.19s
                               ETA: 999299.7s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.635s, learning 0.373s)
               Value function loss: 6014.6334
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 116.22
               Mean episode length: 56.43
                  Mean reward/step: 2.48
       Mean episode length/episode: 7.07
            Mean episode successes: 0.2607
Mean episode consecutive_successes: 0.5154
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 9.01s
                        Total time: 19062.20s
                               ETA: 999227.9s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.526s, learning 0.177s)
               Value function loss: 6060.5160
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 187.14
               Mean episode length: 59.64
                  Mean reward/step: 2.45
       Mean episode length/episode: 7.16
            Mean episode successes: 0.2471
Mean episode consecutive_successes: 0.5287
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 8.70s
                        Total time: 19070.90s
                               ETA: 999140.3s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.768s, learning 0.177s)
               Value function loss: 3694.6100
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 105.89
               Mean episode length: 54.59
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.04
            Mean episode successes: 0.1973
Mean episode consecutive_successes: 0.5341
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 8.94s
                        Total time: 19079.85s
                               ETA: 999065.3s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.773s, learning 0.217s)
               Value function loss: 4161.0976
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 109.77
               Mean episode length: 54.43
                  Mean reward/step: 2.27
       Mean episode length/episode: 7.12
            Mean episode successes: 0.1982
Mean episode consecutive_successes: 0.5273
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 8.99s
                        Total time: 19088.84s
                               ETA: 998992.7s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.521s, learning 0.173s)
               Value function loss: 7388.9870
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 117.45
               Mean episode length: 57.16
                  Mean reward/step: 2.94
       Mean episode length/episode: 7.11
            Mean episode successes: 0.2236
Mean episode consecutive_successes: 0.5212
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 8.69s
                        Total time: 19097.53s
                               ETA: 998904.8s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.536s, learning 0.309s)
               Value function loss: 6204.1139
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 108.27
               Mean episode length: 53.10
                  Mean reward/step: 2.73
       Mean episode length/episode: 7.17
            Mean episode successes: 0.2515
Mean episode consecutive_successes: 0.5136
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 8.84s
                        Total time: 19106.38s
                               ETA: 998824.8s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.497s, learning 0.184s)
               Value function loss: 4588.8690
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 149.13
               Mean episode length: 56.13
                  Mean reward/step: 2.24
       Mean episode length/episode: 7.14
            Mean episode successes: 0.2466
Mean episode consecutive_successes: 0.5150
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 8.68s
                        Total time: 19115.06s
                               ETA: 998736.3s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.445s, learning 0.391s)
               Value function loss: 6871.5390
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 133.94
               Mean episode length: 53.39
                  Mean reward/step: 2.82
       Mean episode length/episode: 7.02
            Mean episode successes: 0.2412
Mean episode consecutive_successes: 0.5186
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 8.84s
                        Total time: 19123.89s
                               ETA: 998656.0s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.832s, learning 0.171s)
               Value function loss: 7505.2649
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 126.18
               Mean episode length: 56.52
                  Mean reward/step: 3.17
       Mean episode length/episode: 7.13
            Mean episode successes: 0.2354
Mean episode consecutive_successes: 0.5310
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 9.00s
                        Total time: 19132.90s
                               ETA: 998584.6s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.337s, learning 0.344s)
               Value function loss: 9157.6754
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 253.81
               Mean episode length: 54.22
                  Mean reward/step: 3.71
       Mean episode length/episode: 7.07
            Mean episode successes: 0.2734
Mean episode consecutive_successes: 0.5344
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 8.68s
                        Total time: 19141.58s
                               ETA: 998496.4s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.387s, learning 0.188s)
               Value function loss: 8379.3384
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 204.40
               Mean episode length: 56.55
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.11
            Mean episode successes: 0.2642
Mean episode consecutive_successes: 0.5461
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 8.57s
                        Total time: 19150.15s
                               ETA: 998402.7s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.221s, learning 0.318s)
               Value function loss: 8311.7896
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 152.99
               Mean episode length: 57.25
                  Mean reward/step: 2.95
       Mean episode length/episode: 7.09
            Mean episode successes: 0.3022
Mean episode consecutive_successes: 0.5385
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 8.54s
                        Total time: 19158.69s
                               ETA: 998307.2s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.808s, learning 0.178s)
               Value function loss: 7644.6856
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 132.19
               Mean episode length: 56.23
                  Mean reward/step: 3.26
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2812
Mean episode consecutive_successes: 0.5595
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 8.99s
                        Total time: 19167.68s
                               ETA: 998235.2s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.476s, learning 0.177s)
               Value function loss: 8195.6274
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 183.89
               Mean episode length: 54.99
                  Mean reward/step: 3.37
       Mean episode length/episode: 7.12
            Mean episode successes: 0.2627
Mean episode consecutive_successes: 0.5903
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 8.65s
                        Total time: 19176.33s
                               ETA: 998145.8s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.228s, learning 0.168s)
               Value function loss: 7502.6844
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: 44.52
               Mean episode length: 53.04
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.2720
Mean episode consecutive_successes: 0.5816
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 8.40s
                        Total time: 19184.73s
                               ETA: 998043.2s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.395s, learning 0.177s)
               Value function loss: 8233.9080
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 157.16
               Mean episode length: 55.95
                  Mean reward/step: 3.42
       Mean episode length/episode: 7.11
            Mean episode successes: 0.3037
Mean episode consecutive_successes: 0.5815
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 8.57s
                        Total time: 19193.30s
                               ETA: 997949.8s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.630s, learning 0.196s)
               Value function loss: 10458.7980
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 246.50
               Mean episode length: 56.27
                  Mean reward/step: 4.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.3052
Mean episode consecutive_successes: 0.5942
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 8.83s
                        Total time: 19202.13s
                               ETA: 997869.8s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.829s, learning 0.177s)
               Value function loss: 9163.9083
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 183.89
               Mean episode length: 54.89
                  Mean reward/step: 3.77
       Mean episode length/episode: 7.08
            Mean episode successes: 0.3257
Mean episode consecutive_successes: 0.6096
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 9.01s
                        Total time: 19211.13s
                               ETA: 997799.1s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.319s, learning 0.263s)
               Value function loss: 7826.0850
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 120.49
               Mean episode length: 52.95
                  Mean reward/step: 3.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.3042
Mean episode consecutive_successes: 0.6123
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 8.58s
                        Total time: 19219.71s
                               ETA: 997706.5s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.579s, learning 0.215s)
               Value function loss: 8410.6883
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 334.86
               Mean episode length: 57.44
                  Mean reward/step: 3.51
       Mean episode length/episode: 7.08
            Mean episode successes: 0.2881
Mean episode consecutive_successes: 0.6322
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 8.79s
                        Total time: 19228.51s
                               ETA: 997625.0s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.428s, learning 0.169s)
               Value function loss: 7043.7855
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 206.73
               Mean episode length: 55.17
                  Mean reward/step: 3.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2729
Mean episode consecutive_successes: 0.6422
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 8.60s
                        Total time: 19237.10s
                               ETA: 997533.4s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.821s, learning 0.188s)
               Value function loss: 7755.7715
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 252.12
               Mean episode length: 55.63
                  Mean reward/step: 3.39
       Mean episode length/episode: 7.06
            Mean episode successes: 0.2622
Mean episode consecutive_successes: 0.6494
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 9.01s
                        Total time: 19246.11s
                               ETA: 997463.1s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.266s, learning 0.231s)
               Value function loss: 8757.4683
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 194.06
               Mean episode length: 56.38
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.12
            Mean episode successes: 0.3003
Mean episode consecutive_successes: 0.6450
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 8.50s
                        Total time: 19254.61s
                               ETA: 997366.5s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.632s, learning 0.171s)
               Value function loss: 9010.5362
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 299.65
               Mean episode length: 56.77
                  Mean reward/step: 3.48
       Mean episode length/episode: 7.17
            Mean episode successes: 0.2720
Mean episode consecutive_successes: 0.6663
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 8.80s
                        Total time: 19263.41s
                               ETA: 997285.8s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.412s, learning 0.171s)
               Value function loss: 10641.0353
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 126.83
               Mean episode length: 54.62
                  Mean reward/step: 4.19
       Mean episode length/episode: 7.04
            Mean episode successes: 0.2930
Mean episode consecutive_successes: 0.6666
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 8.58s
                        Total time: 19272.00s
                               ETA: 997193.7s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.164s)
               Value function loss: 9315.9068
                    Surrogate loss: -0.0218
             Mean action noise std: 0.75
                       Mean reward: 132.43
               Mean episode length: 55.37
                  Mean reward/step: 3.82
       Mean episode length/episode: 7.12
            Mean episode successes: 0.3398
Mean episode consecutive_successes: 0.6587
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 8.64s
                        Total time: 19280.63s
                               ETA: 997104.5s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.180s, learning 0.174s)
               Value function loss: 9268.0532
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 110.20
               Mean episode length: 57.15
                  Mean reward/step: 3.87
       Mean episode length/episode: 7.12
            Mean episode successes: 0.3184
Mean episode consecutive_successes: 0.6781
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 8.35s
                        Total time: 19288.99s
                               ETA: 997000.8s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.346s, learning 0.184s)
               Value function loss: 9272.5736
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 209.58
               Mean episode length: 54.78
                  Mean reward/step: 3.64
       Mean episode length/episode: 7.15
            Mean episode successes: 0.3252
Mean episode consecutive_successes: 0.6896
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 8.53s
                        Total time: 19297.52s
                               ETA: 996906.3s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.363s, learning 0.173s)
               Value function loss: 9815.7608
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 308.21
               Mean episode length: 58.60
                  Mean reward/step: 3.84
       Mean episode length/episode: 7.19
            Mean episode successes: 0.3091
Mean episode consecutive_successes: 0.7186
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 8.54s
                        Total time: 19306.05s
                               ETA: 996812.2s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.403s, learning 0.178s)
               Value function loss: 9335.7566
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 168.68
               Mean episode length: 55.07
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.3276
Mean episode consecutive_successes: 0.7186
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 8.58s
                        Total time: 19314.63s
                               ETA: 996720.5s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.319s, learning 0.190s)
               Value function loss: 9556.3826
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 107.17
               Mean episode length: 55.63
                  Mean reward/step: 3.70
       Mean episode length/episode: 7.07
            Mean episode successes: 0.3047
Mean episode consecutive_successes: 0.7289
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 8.51s
                        Total time: 19323.14s
                               ETA: 996625.2s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.721s, learning 0.219s)
               Value function loss: 10092.9484
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 246.98
               Mean episode length: 56.20
                  Mean reward/step: 4.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.3247
Mean episode consecutive_successes: 0.7353
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 8.94s
                        Total time: 19332.08s
                               ETA: 996552.1s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1779 steps/s (collection: 9.031s, learning 0.177s)
               Value function loss: 7392.5825
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 219.34
               Mean episode length: 55.81
                  Mean reward/step: 3.27
       Mean episode length/episode: 7.07
            Mean episode successes: 0.2900
Mean episode consecutive_successes: 0.7489
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 9.21s
                        Total time: 19341.29s
                               ETA: 996493.0s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.642s, learning 0.179s)
               Value function loss: 7487.6689
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 185.17
               Mean episode length: 57.20
                  Mean reward/step: 3.12
       Mean episode length/episode: 7.14
            Mean episode successes: 0.2905
Mean episode consecutive_successes: 0.7401
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 8.82s
                        Total time: 19350.11s
                               ETA: 996413.9s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.411s, learning 0.187s)
               Value function loss: 8981.0536
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 179.57
               Mean episode length: 55.54
                  Mean reward/step: 3.01
       Mean episode length/episode: 7.14
            Mean episode successes: 0.2852
Mean episode consecutive_successes: 0.7362
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 8.60s
                        Total time: 19358.71s
                               ETA: 996323.5s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.560s, learning 0.214s)
               Value function loss: 8245.6735
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 222.51
               Mean episode length: 57.35
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.05
            Mean episode successes: 0.2642
Mean episode consecutive_successes: 0.7320
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 8.77s
                        Total time: 19367.48s
                               ETA: 996242.2s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.438s, learning 0.274s)
               Value function loss: 11415.2386
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 256.32
               Mean episode length: 58.87
                  Mean reward/step: 4.48
       Mean episode length/episode: 7.12
            Mean episode successes: 0.3130
Mean episode consecutive_successes: 0.7371
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 8.71s
                        Total time: 19376.19s
                               ETA: 996157.8s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.396s, learning 0.169s)
               Value function loss: 10146.8099
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: 293.30
               Mean episode length: 56.36
                  Mean reward/step: 4.01
       Mean episode length/episode: 7.18
            Mean episode successes: 0.3496
Mean episode consecutive_successes: 0.7332
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 8.57s
                        Total time: 19384.76s
                               ETA: 996065.9s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.515s, learning 0.203s)
               Value function loss: 10086.5515
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 209.98
               Mean episode length: 55.33
                  Mean reward/step: 3.88
       Mean episode length/episode: 7.13
            Mean episode successes: 0.3618
Mean episode consecutive_successes: 0.7280
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 8.72s
                        Total time: 19393.48s
                               ETA: 995982.0s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.602s, learning 0.184s)
               Value function loss: 10705.2309
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 161.86
               Mean episode length: 53.79
                  Mean reward/step: 3.97
       Mean episode length/episode: 7.02
            Mean episode successes: 0.3413
Mean episode consecutive_successes: 0.7381
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 8.79s
                        Total time: 19402.26s
                               ETA: 995901.6s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.678s, learning 0.169s)
               Value function loss: 8120.3776
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 219.87
               Mean episode length: 55.41
                  Mean reward/step: 3.36
       Mean episode length/episode: 7.04
            Mean episode successes: 0.2944
Mean episode consecutive_successes: 0.7576
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 8.85s
                        Total time: 19411.11s
                               ETA: 995824.5s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.429s, learning 0.276s)
               Value function loss: 8623.6372
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 210.16
               Mean episode length: 57.49
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.09
            Mean episode successes: 0.2671
Mean episode consecutive_successes: 0.7755
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 8.70s
                        Total time: 19419.82s
                               ETA: 995740.1s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.595s, learning 0.177s)
               Value function loss: 7521.4052
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 260.39
               Mean episode length: 57.49
                  Mean reward/step: 3.21
       Mean episode length/episode: 7.16
            Mean episode successes: 0.2690
Mean episode consecutive_successes: 0.7739
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 8.77s
                        Total time: 19428.59s
                               ETA: 995659.3s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.677s, learning 0.167s)
               Value function loss: 7641.6178
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 190.37
               Mean episode length: 56.23
                  Mean reward/step: 2.90
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2734
Mean episode consecutive_successes: 0.7599
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 8.84s
                        Total time: 19437.43s
                               ETA: 995582.2s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.181s, learning 0.215s)
               Value function loss: 7093.3825
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 143.20
               Mean episode length: 56.47
                  Mean reward/step: 3.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.2798
Mean episode consecutive_successes: 0.7469
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 8.40s
                        Total time: 19445.83s
                               ETA: 995482.3s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.089s, learning 0.167s)
               Value function loss: 9944.5013
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 151.99
               Mean episode length: 55.13
                  Mean reward/step: 3.90
       Mean episode length/episode: 7.08
            Mean episode successes: 0.3091
Mean episode consecutive_successes: 0.7327
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 8.26s
                        Total time: 19454.08s
                               ETA: 995375.2s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.506s, learning 0.301s)
               Value function loss: 12304.7376
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 152.78
               Mean episode length: 55.83
                  Mean reward/step: 5.23
       Mean episode length/episode: 7.20
            Mean episode successes: 0.3931
Mean episode consecutive_successes: 0.7250
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 8.81s
                        Total time: 19462.89s
                               ETA: 995296.5s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.630s, learning 0.315s)
               Value function loss: 11152.1937
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 168.54
               Mean episode length: 53.94
                  Mean reward/step: 4.62
       Mean episode length/episode: 7.12
            Mean episode successes: 0.3892
Mean episode consecutive_successes: 0.7477
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 8.95s
                        Total time: 19471.84s
                               ETA: 995224.9s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.690s, learning 0.199s)
               Value function loss: 10062.6480
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 323.16
               Mean episode length: 58.41
                  Mean reward/step: 3.81
       Mean episode length/episode: 7.11
            Mean episode successes: 0.3770
Mean episode consecutive_successes: 0.7555
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 8.89s
                        Total time: 19480.73s
                               ETA: 995150.5s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.184s, learning 0.165s)
               Value function loss: 10079.1473
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 247.67
               Mean episode length: 55.62
                  Mean reward/step: 4.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.3726
Mean episode consecutive_successes: 0.7722
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 8.35s
                        Total time: 19489.07s
                               ETA: 995048.7s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.351s, learning 0.177s)
               Value function loss: 9880.8913
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 262.95
               Mean episode length: 55.51
                  Mean reward/step: 3.65
       Mean episode length/episode: 7.01
            Mean episode successes: 0.3403
Mean episode consecutive_successes: 0.7836
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 8.53s
                        Total time: 19497.60s
                               ETA: 994956.0s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.188s, learning 0.341s)
               Value function loss: 10011.9016
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 204.65
               Mean episode length: 54.53
                  Mean reward/step: 3.74
       Mean episode length/episode: 7.10
            Mean episode successes: 0.2905
Mean episode consecutive_successes: 0.8004
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 8.53s
                        Total time: 19506.13s
                               ETA: 994863.4s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.289s, learning 0.175s)
               Value function loss: 9641.2271
                    Surrogate loss: -0.0213
             Mean action noise std: 0.75
                       Mean reward: 168.88
               Mean episode length: 56.66
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.21
            Mean episode successes: 0.3354
Mean episode consecutive_successes: 0.7838
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 8.46s
                        Total time: 19514.60s
                               ETA: 994767.6s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.778s, learning 0.295s)
               Value function loss: 10734.9697
                    Surrogate loss: -0.0180
             Mean action noise std: 0.75
                       Mean reward: 144.27
               Mean episode length: 53.78
                  Mean reward/step: 4.16
       Mean episode length/episode: 7.09
            Mean episode successes: 0.3472
Mean episode consecutive_successes: 0.7809
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 9.07s
                        Total time: 19523.67s
                               ETA: 994703.0s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.408s, learning 0.205s)
               Value function loss: 8205.5507
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 277.08
               Mean episode length: 55.02
                  Mean reward/step: 3.50
       Mean episode length/episode: 7.07
            Mean episode successes: 0.3354
Mean episode consecutive_successes: 0.7871
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 16.61s
                        Total time: 19540.28s
                               ETA: 995022.4s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.507s, learning 0.181s)
               Value function loss: 11068.7324
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 252.35
               Mean episode length: 56.59
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.3335
Mean episode consecutive_successes: 0.7928
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 16.69s
                        Total time: 19556.97s
                               ETA: 995345.2s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.789s, learning 0.176s)
               Value function loss: 9352.6803
                    Surrogate loss: -0.0206
             Mean action noise std: 0.75
                       Mean reward: 205.16
               Mean episode length: 56.86
                  Mean reward/step: 3.78
       Mean episode length/episode: 7.14
            Mean episode successes: 0.3501
Mean episode consecutive_successes: 0.7850
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 16.96s
                        Total time: 19573.93s
                               ETA: 995681.8s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 953 steps/s (collection: 16.991s, learning 0.183s)
               Value function loss: 10283.6254
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 257.84
               Mean episode length: 57.26
                  Mean reward/step: 3.95
       Mean episode length/episode: 7.16
            Mean episode successes: 0.3735
Mean episode consecutive_successes: 0.7902
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 17.17s
                        Total time: 19591.11s
                               ETA: 996028.6s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.350s, learning 0.219s)
               Value function loss: 10771.4199
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 270.03
               Mean episode length: 56.43
                  Mean reward/step: 3.96
       Mean episode length/episode: 7.13
            Mean episode successes: 0.3491
Mean episode consecutive_successes: 0.8100
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 16.57s
                        Total time: 19607.68s
                               ETA: 996344.3s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.690s, learning 0.182s)
               Value function loss: 11509.7830
                    Surrogate loss: -0.0206
             Mean action noise std: 0.75
                       Mean reward: 234.50
               Mean episode length: 57.74
                  Mean reward/step: 4.22
       Mean episode length/episode: 7.09
            Mean episode successes: 0.3623
Mean episode consecutive_successes: 0.8145
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 16.87s
                        Total time: 19624.55s
                               ETA: 996675.1s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.165s, learning 0.168s)
               Value function loss: 11673.8551
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 175.42
               Mean episode length: 55.75
                  Mean reward/step: 4.30
       Mean episode length/episode: 7.06
            Mean episode successes: 0.3364
Mean episode consecutive_successes: 0.8246
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 17.33s
                        Total time: 19641.88s
                               ETA: 997028.9s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.858s, learning 0.292s)
               Value function loss: 12331.4408
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 184.04
               Mean episode length: 56.31
                  Mean reward/step: 5.09
       Mean episode length/episode: 7.18
            Mean episode successes: 0.3784
Mean episode consecutive_successes: 0.8156
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 17.15s
                        Total time: 19659.03s
                               ETA: 997373.0s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.471s, learning 0.175s)
               Value function loss: 12057.6654
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 179.83
               Mean episode length: 54.67
                  Mean reward/step: 4.47
       Mean episode length/episode: 7.19
            Mean episode successes: 0.4111
Mean episode consecutive_successes: 0.8155
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 16.65s
                        Total time: 19675.68s
                               ETA: 997691.2s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.695s, learning 0.190s)
               Value function loss: 10745.5106
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 201.65
               Mean episode length: 55.71
                  Mean reward/step: 4.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.4048
Mean episode consecutive_successes: 0.8220
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 16.88s
                        Total time: 19692.56s
                               ETA: 998021.2s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.584s, learning 0.181s)
               Value function loss: 12193.2193
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 180.00
               Mean episode length: 56.17
                  Mean reward/step: 4.25
       Mean episode length/episode: 7.11
            Mean episode successes: 0.3740
Mean episode consecutive_successes: 0.8466
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 16.77s
                        Total time: 19709.33s
                               ETA: 998344.7s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.461s, learning 0.173s)
               Value function loss: 11102.7522
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 327.54
               Mean episode length: 55.52
                  Mean reward/step: 4.27
       Mean episode length/episode: 7.09
            Mean episode successes: 0.3320
Mean episode consecutive_successes: 0.8683
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 16.63s
                        Total time: 19725.96s
                               ETA: 998661.2s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.610s, learning 0.171s)
               Value function loss: 11733.1391
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 214.62
               Mean episode length: 54.07
                  Mean reward/step: 4.69
       Mean episode length/episode: 7.09
            Mean episode successes: 0.3853
Mean episode consecutive_successes: 0.8536
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 16.78s
                        Total time: 19742.74s
                               ETA: 998984.9s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 935 steps/s (collection: 17.270s, learning 0.241s)
               Value function loss: 12632.2427
                    Surrogate loss: -0.0179
             Mean action noise std: 0.75
                       Mean reward: 185.73
               Mean episode length: 58.85
                  Mean reward/step: 4.63
       Mean episode length/episode: 7.21
            Mean episode successes: 0.4155
Mean episode consecutive_successes: 0.8629
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 17.51s
                        Total time: 19760.26s
                               ETA: 999345.1s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.806s, learning 0.189s)
               Value function loss: 12600.9559
                    Surrogate loss: -0.0194
             Mean action noise std: 0.75
                       Mean reward: 230.75
               Mean episode length: 58.11
                  Mean reward/step: 5.26
       Mean episode length/episode: 7.10
            Mean episode successes: 0.4321
Mean episode consecutive_successes: 0.8815
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 16.99s
                        Total time: 19777.25s
                               ETA: 999678.8s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.736s, learning 0.201s)
               Value function loss: 12973.9449
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 209.12
               Mean episode length: 56.62
                  Mean reward/step: 4.97
       Mean episode length/episode: 7.15
            Mean episode successes: 0.4375
Mean episode consecutive_successes: 0.8921
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 16.94s
                        Total time: 19794.19s
                               ETA: 1000009.2s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.037s, learning 0.169s)
               Value function loss: 15752.0663
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 196.67
               Mean episode length: 53.22
                  Mean reward/step: 5.58
       Mean episode length/episode: 7.13
            Mean episode successes: 0.4844
Mean episode consecutive_successes: 0.8927
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 17.21s
                        Total time: 19811.39s
                               ETA: 1000352.9s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.508s, learning 0.212s)
               Value function loss: 13152.3654
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 488.16
               Mean episode length: 59.41
                  Mean reward/step: 5.30
       Mean episode length/episode: 7.10
            Mean episode successes: 0.4844
Mean episode consecutive_successes: 0.9213
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 16.72s
                        Total time: 19828.11s
                               ETA: 1000671.7s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.683s, learning 0.165s)
               Value function loss: 11289.2495
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 240.05
               Mean episode length: 57.22
                  Mean reward/step: 3.91
       Mean episode length/episode: 7.05
            Mean episode successes: 0.4185
Mean episode consecutive_successes: 0.9302
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 16.85s
                        Total time: 19844.96s
                               ETA: 1000996.6s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 938 steps/s (collection: 17.242s, learning 0.217s)
               Value function loss: 11672.5184
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 240.96
               Mean episode length: 57.20
                  Mean reward/step: 4.34
       Mean episode length/episode: 7.06
            Mean episode successes: 0.3677
Mean episode consecutive_successes: 0.9589
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 17.46s
                        Total time: 19862.42s
                               ETA: 1001351.9s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.867s, learning 0.191s)
               Value function loss: 11375.5989
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 214.40
               Mean episode length: 57.34
                  Mean reward/step: 4.20
       Mean episode length/episode: 7.26
            Mean episode successes: 0.3984
Mean episode consecutive_successes: 0.9548
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 17.06s
                        Total time: 19879.48s
                               ETA: 1001686.6s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.760s, learning 0.217s)
               Value function loss: 12454.8321
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 201.55
               Mean episode length: 56.48
                  Mean reward/step: 4.71
       Mean episode length/episode: 7.09
            Mean episode successes: 0.4219
Mean episode consecutive_successes: 0.9370
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 16.98s
                        Total time: 19896.46s
                               ETA: 1002017.0s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.633s, learning 0.212s)
               Value function loss: 13247.3931
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 219.61
               Mean episode length: 56.11
                  Mean reward/step: 4.95
       Mean episode length/episode: 7.19
            Mean episode successes: 0.4380
Mean episode consecutive_successes: 0.9428
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 16.84s
                        Total time: 19913.30s
                               ETA: 1002340.2s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.515s, learning 0.229s)
               Value function loss: 11280.5108
                    Surrogate loss: -0.0224
             Mean action noise std: 0.75
                       Mean reward: 182.41
               Mean episode length: 57.59
                  Mean reward/step: 4.76
       Mean episode length/episode: 7.12
            Mean episode successes: 0.4277
Mean episode consecutive_successes: 0.9516
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 16.74s
                        Total time: 19930.04s
                               ETA: 1002658.1s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.853s, learning 0.171s)
               Value function loss: 13322.6645
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 301.79
               Mean episode length: 57.27
                  Mean reward/step: 5.28
       Mean episode length/episode: 7.01
            Mean episode successes: 0.4346
Mean episode consecutive_successes: 0.9585
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 17.02s
                        Total time: 19947.07s
                               ETA: 1002989.7s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.555s, learning 0.194s)
               Value function loss: 13946.4437
                    Surrogate loss: -0.0201
             Mean action noise std: 0.75
                       Mean reward: 483.52
               Mean episode length: 56.35
                  Mean reward/step: 5.25
       Mean episode length/episode: 7.21
            Mean episode successes: 0.4424
Mean episode consecutive_successes: 0.9902
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 16.75s
                        Total time: 19963.82s
                               ETA: 1003307.1s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.568s, learning 0.171s)
               Value function loss: 13084.2342
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 233.39
               Mean episode length: 54.60
                  Mean reward/step: 4.39
       Mean episode length/episode: 7.14
            Mean episode successes: 0.4307
Mean episode consecutive_successes: 0.9776
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 16.74s
                        Total time: 19980.55s
                               ETA: 1003623.7s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.806s, learning 0.262s)
               Value function loss: 10668.2156
                    Surrogate loss: -0.0207
             Mean action noise std: 0.75
                       Mean reward: 168.36
               Mean episode length: 56.27
                  Mean reward/step: 4.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.3965
Mean episode consecutive_successes: 0.9920
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 17.07s
                        Total time: 19997.62s
                               ETA: 1003956.4s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.918s, learning 0.178s)
               Value function loss: 11301.8076
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 407.72
               Mean episode length: 56.29
                  Mean reward/step: 4.24
       Mean episode length/episode: 7.10
            Mean episode successes: 0.3921
Mean episode consecutive_successes: 1.0016
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 17.10s
                        Total time: 20014.72s
                               ETA: 1004290.2s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.827s, learning 0.236s)
               Value function loss: 14595.4387
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 333.37
               Mean episode length: 56.46
                  Mean reward/step: 5.69
       Mean episode length/episode: 7.13
            Mean episode successes: 0.3926
Mean episode consecutive_successes: 1.0092
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 17.06s
                        Total time: 20031.78s
                               ETA: 1004622.0s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.530s, learning 0.182s)
               Value function loss: 13267.5241
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 200.31
               Mean episode length: 58.64
                  Mean reward/step: 5.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.4111
Mean episode consecutive_successes: 0.9964
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 16.71s
                        Total time: 20048.49s
                               ETA: 1004935.9s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.224s, learning 0.262s)
               Value function loss: 12039.5567
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 295.11
               Mean episode length: 55.37
                  Mean reward/step: 4.74
       Mean episode length/episode: 7.13
            Mean episode successes: 0.4209
Mean episode consecutive_successes: 1.0074
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 16.49s
                        Total time: 20064.98s
                               ETA: 1005238.1s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.902s, learning 0.195s)
               Value function loss: 13453.7205
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 298.51
               Mean episode length: 57.93
                  Mean reward/step: 4.78
       Mean episode length/episode: 7.16
            Mean episode successes: 0.4326
Mean episode consecutive_successes: 1.0009
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 17.10s
                        Total time: 20082.08s
                               ETA: 1005570.6s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.012s, learning 0.283s)
               Value function loss: 14259.2417
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 208.79
               Mean episode length: 53.02
                  Mean reward/step: 5.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.4287
Mean episode consecutive_successes: 0.9967
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 17.29s
                        Total time: 20099.37s
                               ETA: 1005912.5s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 950 steps/s (collection: 16.807s, learning 0.425s)
               Value function loss: 14060.5282
                    Surrogate loss: -0.0206
             Mean action noise std: 0.75
                       Mean reward: 82.24
               Mean episode length: 55.84
                  Mean reward/step: 5.15
       Mean episode length/episode: 7.15
            Mean episode successes: 0.4702
Mean episode consecutive_successes: 0.9802
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 17.23s
                        Total time: 20116.60s
                               ETA: 1006251.0s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.143s, learning 0.214s)
               Value function loss: 13332.0753
                    Surrogate loss: -0.0211
             Mean action noise std: 0.75
                       Mean reward: 411.80
               Mean episode length: 55.37
                  Mean reward/step: 5.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.4580
Mean episode consecutive_successes: 1.0003
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 16.36s
                        Total time: 20132.96s
                               ETA: 1006545.4s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.484s, learning 0.208s)
               Value function loss: 16182.8163
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 363.67
               Mean episode length: 58.34
                  Mean reward/step: 6.44
       Mean episode length/episode: 7.13
            Mean episode successes: 0.5039
Mean episode consecutive_successes: 1.0096
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 16.69s
                        Total time: 20149.65s
                               ETA: 1006856.2s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1069 steps/s (collection: 15.109s, learning 0.207s)
               Value function loss: 15496.7046
                    Surrogate loss: -0.0219
             Mean action noise std: 0.75
                       Mean reward: 185.98
               Mean episode length: 55.57
                  Mean reward/step: 5.82
       Mean episode length/episode: 7.12
            Mean episode successes: 0.5146
Mean episode consecutive_successes: 1.0197
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 15.32s
                        Total time: 20164.97s
                               ETA: 1007097.9s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.602s, learning 0.281s)
               Value function loss: 16445.4271
                    Surrogate loss: -0.0253
             Mean action noise std: 0.75
                       Mean reward: 404.44
               Mean episode length: 60.05
                  Mean reward/step: 5.99
       Mean episode length/episode: 7.15
            Mean episode successes: 0.5391
Mean episode consecutive_successes: 1.0504
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 8.88s
                        Total time: 20173.85s
                               ETA: 1007018.3s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.369s, learning 0.184s)
               Value function loss: 16001.5184
                    Surrogate loss: -0.0206
             Mean action noise std: 0.75
                       Mean reward: 381.07
               Mean episode length: 57.24
                  Mean reward/step: 5.93
       Mean episode length/episode: 7.11
            Mean episode successes: 0.5010
Mean episode consecutive_successes: 1.0797
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 8.55s
                        Total time: 20182.40s
                               ETA: 1006922.2s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.655s, learning 0.243s)
               Value function loss: 16979.7352
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 307.36
               Mean episode length: 53.95
                  Mean reward/step: 6.35
       Mean episode length/episode: 7.08
            Mean episode successes: 0.5244
Mean episode consecutive_successes: 1.0938
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 8.90s
                        Total time: 20191.30s
                               ETA: 1006843.5s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.341s, learning 0.254s)
               Value function loss: 16492.6394
                    Surrogate loss: -0.0208
             Mean action noise std: 0.75
                       Mean reward: 252.31
               Mean episode length: 54.53
                  Mean reward/step: 6.50
       Mean episode length/episode: 7.13
            Mean episode successes: 0.4941
Mean episode consecutive_successes: 1.1252
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 8.60s
                        Total time: 20199.90s
                               ETA: 1006749.8s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.463s, learning 0.221s)
               Value function loss: 16021.5999
                    Surrogate loss: -0.0148
             Mean action noise std: 0.75
                       Mean reward: 354.51
               Mean episode length: 60.41
                  Mean reward/step: 5.84
       Mean episode length/episode: 7.14
            Mean episode successes: 0.5000
Mean episode consecutive_successes: 1.1553
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 8.68s
                        Total time: 20208.58s
                               ETA: 1006660.5s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.246s, learning 0.188s)
               Value function loss: 15901.0498
                    Surrogate loss: -0.0227
             Mean action noise std: 0.75
                       Mean reward: 268.41
               Mean episode length: 57.42
                  Mean reward/step: 5.54
       Mean episode length/episode: 7.02
            Mean episode successes: 0.5342
Mean episode consecutive_successes: 1.1298
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 8.43s
                        Total time: 20217.01s
                               ETA: 1006558.9s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.538s, learning 0.339s)
               Value function loss: 14649.7856
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 512.43
               Mean episode length: 59.44
                  Mean reward/step: 5.52
       Mean episode length/episode: 7.20
            Mean episode successes: 0.4541
Mean episode consecutive_successes: 1.2038
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 8.88s
                        Total time: 20225.89s
                               ETA: 1006479.4s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.842s, learning 0.219s)
               Value function loss: 15330.8762
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 217.35
               Mean episode length: 56.75
                  Mean reward/step: 4.86
       Mean episode length/episode: 7.13
            Mean episode successes: 0.4321
Mean episode consecutive_successes: 1.1977
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 9.06s
                        Total time: 20234.95s
                               ETA: 1006409.2s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.491s, learning 0.175s)
               Value function loss: 14445.5683
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 371.72
               Mean episode length: 58.28
                  Mean reward/step: 5.28
       Mean episode length/episode: 7.09
            Mean episode successes: 0.4272
Mean episode consecutive_successes: 1.2046
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 8.67s
                        Total time: 20243.62s
                               ETA: 1006319.3s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.390s, learning 0.371s)
               Value function loss: 17961.7106
                    Surrogate loss: -0.0204
             Mean action noise std: 0.75
                       Mean reward: 302.45
               Mean episode length: 54.82
                  Mean reward/step: 6.19
       Mean episode length/episode: 7.14
            Mean episode successes: 0.4604
Mean episode consecutive_successes: 1.2050
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 8.76s
                        Total time: 20252.38s
                               ETA: 1006234.3s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.326s, learning 0.183s)
               Value function loss: 18219.3375
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 389.00
               Mean episode length: 54.71
                  Mean reward/step: 6.90
       Mean episode length/episode: 7.17
            Mean episode successes: 0.5332
Mean episode consecutive_successes: 1.2002
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 8.51s
                        Total time: 20260.89s
                               ETA: 1006136.9s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.431s, learning 0.307s)
               Value function loss: 19960.1947
                    Surrogate loss: -0.0172
             Mean action noise std: 0.75
                       Mean reward: 260.38
               Mean episode length: 56.33
                  Mean reward/step: 7.42
       Mean episode length/episode: 7.05
            Mean episode successes: 0.5493
Mean episode consecutive_successes: 1.2087
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 8.74s
                        Total time: 20269.63s
                               ETA: 1006050.8s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.616s, learning 0.180s)
               Value function loss: 18764.2974
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 356.87
               Mean episode length: 54.58
                  Mean reward/step: 6.71
       Mean episode length/episode: 7.07
            Mean episode successes: 0.5938
Mean episode consecutive_successes: 1.2041
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 8.80s
                        Total time: 20278.42s
                               ETA: 1005967.8s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.643s, learning 0.270s)
               Value function loss: 17638.9651
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 363.87
               Mean episode length: 53.45
                  Mean reward/step: 6.18
       Mean episode length/episode: 7.16
            Mean episode successes: 0.6118
Mean episode consecutive_successes: 1.2118
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 8.91s
                        Total time: 20287.33s
                               ETA: 1005890.6s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.619s, learning 0.201s)
               Value function loss: 16978.0922
                    Surrogate loss: -0.0092
             Mean action noise std: 0.75
                       Mean reward: 505.19
               Mean episode length: 59.23
                  Mean reward/step: 5.83
       Mean episode length/episode: 7.07
            Mean episode successes: 0.5781
Mean episode consecutive_successes: 1.2276
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 8.82s
                        Total time: 20296.16s
                               ETA: 1005808.9s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.593s, learning 0.203s)
               Value function loss: 17249.7920
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 193.43
               Mean episode length: 52.13
                  Mean reward/step: 6.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.4854
Mean episode consecutive_successes: 1.2700
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 8.80s
                        Total time: 20304.95s
                               ETA: 1005726.1s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.665s, learning 0.340s)
               Value function loss: 15638.8240
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 272.90
               Mean episode length: 55.94
                  Mean reward/step: 5.79
       Mean episode length/episode: 7.10
            Mean episode successes: 0.5122
Mean episode consecutive_successes: 1.2645
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 9.01s
                        Total time: 20313.96s
                               ETA: 1005653.7s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1780 steps/s (collection: 8.904s, learning 0.297s)
               Value function loss: 13847.6579
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 453.32
               Mean episode length: 57.87
                  Mean reward/step: 5.32
       Mean episode length/episode: 7.16
            Mean episode successes: 0.5220
Mean episode consecutive_successes: 1.2752
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 9.20s
                        Total time: 20323.16s
                               ETA: 1005591.0s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.341s, learning 0.242s)
               Value function loss: 14815.7406
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 292.78
               Mean episode length: 55.44
                  Mean reward/step: 5.39
       Mean episode length/episode: 7.20
            Mean episode successes: 0.5259
Mean episode consecutive_successes: 1.2624
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 8.58s
                        Total time: 20331.74s
                               ETA: 1005497.9s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.505s, learning 0.195s)
               Value function loss: 14263.1749
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 288.06
               Mean episode length: 55.92
                  Mean reward/step: 5.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.4458
Mean episode consecutive_successes: 1.2798
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 8.70s
                        Total time: 20340.44s
                               ETA: 1005410.7s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.440s, learning 0.200s)
               Value function loss: 13577.0927
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 185.58
               Mean episode length: 57.89
                  Mean reward/step: 5.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.4868
Mean episode consecutive_successes: 1.2462
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 8.64s
                        Total time: 20349.08s
                               ETA: 1005320.5s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.658s, learning 0.185s)
               Value function loss: 12972.5776
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 327.94
               Mean episode length: 58.19
                  Mean reward/step: 4.93
       Mean episode length/episode: 7.09
            Mean episode successes: 0.4946
Mean episode consecutive_successes: 1.2308
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 8.84s
                        Total time: 20357.92s
                               ETA: 1005240.4s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.544s, learning 0.330s)
               Value function loss: 14240.8162
                    Surrogate loss: -0.0186
             Mean action noise std: 0.75
                       Mean reward: 300.05
               Mean episode length: 56.61
                  Mean reward/step: 5.48
       Mean episode length/episode: 7.10
            Mean episode successes: 0.4585
Mean episode consecutive_successes: 1.2355
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 8.87s
                        Total time: 20366.80s
                               ETA: 1005161.9s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.779s, learning 0.216s)
               Value function loss: 16620.5481
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 453.01
               Mean episode length: 60.51
                  Mean reward/step: 5.75
       Mean episode length/episode: 7.17
            Mean episode successes: 0.5249
Mean episode consecutive_successes: 1.2227
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 8.99s
                        Total time: 20375.79s
                               ETA: 1005089.5s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.731s, learning 0.178s)
               Value function loss: 14586.3069
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 177.95
               Mean episode length: 57.14
                  Mean reward/step: 4.94
       Mean episode length/episode: 7.11
            Mean episode successes: 0.4819
Mean episode consecutive_successes: 1.2118
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 8.91s
                        Total time: 20384.70s
                               ETA: 1005012.9s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.578s, learning 0.184s)
               Value function loss: 14718.3678
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 430.43
               Mean episode length: 56.12
                  Mean reward/step: 5.40
       Mean episode length/episode: 7.17
            Mean episode successes: 0.4966
Mean episode consecutive_successes: 1.2190
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 8.76s
                        Total time: 20393.46s
                               ETA: 1004929.2s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.449s, learning 0.280s)
               Value function loss: 14860.0413
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 373.39
               Mean episode length: 56.93
                  Mean reward/step: 5.56
       Mean episode length/episode: 7.00
            Mean episode successes: 0.4150
Mean episode consecutive_successes: 1.2382
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 8.73s
                        Total time: 20402.19s
                               ETA: 1004843.9s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.355s, learning 0.174s)
               Value function loss: 15358.6314
                    Surrogate loss: -0.0191
             Mean action noise std: 0.75
                       Mean reward: 168.46
               Mean episode length: 52.43
                  Mean reward/step: 5.58
       Mean episode length/episode: 7.17
            Mean episode successes: 0.4927
Mean episode consecutive_successes: 1.1989
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 8.53s
                        Total time: 20410.72s
                               ETA: 1004748.8s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.419s, learning 0.199s)
               Value function loss: 15898.7679
                    Surrogate loss: -0.0071
             Mean action noise std: 0.75
                       Mean reward: 285.12
               Mean episode length: 54.72
                  Mean reward/step: 5.49
       Mean episode length/episode: 7.14
            Mean episode successes: 0.4961
Mean episode consecutive_successes: 1.1967
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 8.62s
                        Total time: 20419.34s
                               ETA: 1004658.1s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.495s, learning 0.284s)
               Value function loss: 14343.3084
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 254.00
               Mean episode length: 59.04
                  Mean reward/step: 5.37
       Mean episode length/episode: 7.14
            Mean episode successes: 0.5029
Mean episode consecutive_successes: 1.2107
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 8.78s
                        Total time: 20428.12s
                               ETA: 1004575.5s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.603s, learning 0.188s)
               Value function loss: 14469.9681
                    Surrogate loss: -0.0214
             Mean action noise std: 0.75
                       Mean reward: 300.02
               Mean episode length: 54.06
                  Mean reward/step: 4.86
       Mean episode length/episode: 7.10
            Mean episode successes: 0.4419
Mean episode consecutive_successes: 1.2245
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 8.79s
                        Total time: 20436.91s
                               ETA: 1004493.5s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.143s, learning 0.174s)
               Value function loss: 15379.8511
                    Surrogate loss: -0.0188
             Mean action noise std: 0.75
                       Mean reward: 226.50
               Mean episode length: 57.60
                  Mean reward/step: 5.44
       Mean episode length/episode: 7.05
            Mean episode successes: 0.4536
Mean episode consecutive_successes: 1.2026
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 8.32s
                        Total time: 20445.23s
                               ETA: 1004388.4s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.694s, learning 0.184s)
               Value function loss: 17413.4008
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 263.34
               Mean episode length: 57.05
                  Mean reward/step: 6.40
       Mean episode length/episode: 7.14
            Mean episode successes: 0.4980
Mean episode consecutive_successes: 1.1896
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 8.88s
                        Total time: 20454.10s
                               ETA: 1004310.9s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.747s, learning 0.232s)
               Value function loss: 16565.8000
                    Surrogate loss: -0.0203
             Mean action noise std: 0.75
                       Mean reward: 396.59
               Mean episode length: 58.61
                  Mean reward/step: 6.08
       Mean episode length/episode: 7.17
            Mean episode successes: 0.5142
Mean episode consecutive_successes: 1.2050
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 8.98s
                        Total time: 20463.08s
                               ETA: 1004238.4s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.873s, learning 0.213s)
               Value function loss: 17803.8806
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 274.71
               Mean episode length: 54.04
                  Mean reward/step: 6.72
       Mean episode length/episode: 7.17
            Mean episode successes: 0.5234
Mean episode consecutive_successes: 1.2208
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 9.09s
                        Total time: 20472.17s
                               ETA: 1004171.2s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.321s, learning 0.322s)
               Value function loss: 17135.9409
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 353.71
               Mean episode length: 57.59
                  Mean reward/step: 7.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.5596
Mean episode consecutive_successes: 1.2257
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 8.64s
                        Total time: 20480.81s
                               ETA: 1004082.4s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.520s, learning 0.176s)
               Value function loss: 21439.5235
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 300.89
               Mean episode length: 55.60
                  Mean reward/step: 7.48
       Mean episode length/episode: 7.14
            Mean episode successes: 0.6260
Mean episode consecutive_successes: 1.2177
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 8.70s
                        Total time: 20489.51s
                               ETA: 1003996.2s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.342s, learning 0.192s)
               Value function loss: 18841.4799
                    Surrogate loss: -0.0205
             Mean action noise std: 0.75
                       Mean reward: 422.00
               Mean episode length: 58.85
                  Mean reward/step: 6.93
       Mean episode length/episode: 7.15
            Mean episode successes: 0.6313
Mean episode consecutive_successes: 1.2474
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 8.53s
                        Total time: 20498.04s
                               ETA: 1003902.1s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.472s, learning 0.177s)
               Value function loss: 18903.7000
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 283.52
               Mean episode length: 57.03
                  Mean reward/step: 6.94
       Mean episode length/episode: 7.16
            Mean episode successes: 0.6196
Mean episode consecutive_successes: 1.2690
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 8.65s
                        Total time: 20506.69s
                               ETA: 1003813.8s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.635s, learning 0.410s)
               Value function loss: 20529.5730
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 477.32
               Mean episode length: 60.50
                  Mean reward/step: 6.97
       Mean episode length/episode: 7.14
            Mean episode successes: 0.6289
Mean episode consecutive_successes: 1.2879
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 9.04s
                        Total time: 20515.74s
                               ETA: 1003744.9s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.381s, learning 0.180s)
               Value function loss: 20970.0690
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 321.56
               Mean episode length: 56.24
                  Mean reward/step: 7.29
       Mean episode length/episode: 7.05
            Mean episode successes: 0.5942
Mean episode consecutive_successes: 1.3323
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 8.56s
                        Total time: 20524.30s
                               ETA: 1003652.5s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.807s, learning 0.281s)
               Value function loss: 20565.5827
                    Surrogate loss: -0.0215
             Mean action noise std: 0.75
                       Mean reward: 440.55
               Mean episode length: 54.90
                  Mean reward/step: 7.54
       Mean episode length/episode: 7.09
            Mean episode successes: 0.5933
Mean episode consecutive_successes: 1.3612
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 9.09s
                        Total time: 20533.38s
                               ETA: 1003585.8s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.594s, learning 0.212s)
               Value function loss: 21160.5212
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 316.36
               Mean episode length: 57.35
                  Mean reward/step: 7.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.6196
Mean episode consecutive_successes: 1.3544
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 8.81s
                        Total time: 20542.19s
                               ETA: 1003505.5s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.292s, learning 0.287s)
               Value function loss: 21857.2595
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 491.35
               Mean episode length: 57.91
                  Mean reward/step: 7.46
       Mean episode length/episode: 7.16
            Mean episode successes: 0.6265
Mean episode consecutive_successes: 1.3995
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 8.58s
                        Total time: 20550.77s
                               ETA: 1003414.1s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.842s, learning 0.218s)
               Value function loss: 23875.7660
                    Surrogate loss: -0.0199
             Mean action noise std: 0.75
                       Mean reward: 410.54
               Mean episode length: 58.27
                  Mean reward/step: 8.88
       Mean episode length/episode: 7.13
            Mean episode successes: 0.6362
Mean episode consecutive_successes: 1.4371
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 9.06s
                        Total time: 20559.83s
                               ETA: 1003346.3s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.565s, learning 0.190s)
               Value function loss: 24143.8048
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 451.22
               Mean episode length: 59.13
                  Mean reward/step: 8.62
       Mean episode length/episode: 7.17
            Mean episode successes: 0.6704
Mean episode consecutive_successes: 1.4589
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 8.76s
                        Total time: 20568.59s
                               ETA: 1003263.8s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.537s, learning 0.185s)
               Value function loss: 24719.8549
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 303.09
               Mean episode length: 55.25
                  Mean reward/step: 8.38
       Mean episode length/episode: 7.14
            Mean episode successes: 0.7114
Mean episode consecutive_successes: 1.4698
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 8.72s
                        Total time: 20577.31s
                               ETA: 1003179.6s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.505s, learning 0.197s)
               Value function loss: 25715.6222
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 422.65
               Mean episode length: 55.03
                  Mean reward/step: 8.97
       Mean episode length/episode: 7.06
            Mean episode successes: 0.7271
Mean episode consecutive_successes: 1.4953
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 8.70s
                        Total time: 20586.01s
                               ETA: 1003094.5s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.242s, learning 0.203s)
               Value function loss: 23854.4013
                    Surrogate loss: -0.0150
             Mean action noise std: 0.75
                       Mean reward: 490.77
               Mean episode length: 57.22
                  Mean reward/step: 7.93
       Mean episode length/episode: 7.20
            Mean episode successes: 0.7329
Mean episode consecutive_successes: 1.5284
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 8.45s
                        Total time: 20594.45s
                               ETA: 1002997.0s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.496s, learning 0.218s)
               Value function loss: 24990.3512
                    Surrogate loss: -0.0210
             Mean action noise std: 0.75
                       Mean reward: 466.66
               Mean episode length: 56.70
                  Mean reward/step: 8.60
       Mean episode length/episode: 7.12
            Mean episode successes: 0.7480
Mean episode consecutive_successes: 1.5484
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 8.71s
                        Total time: 20603.17s
                               ETA: 1002912.7s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.650s, learning 0.200s)
               Value function loss: 23944.8206
                    Surrogate loss: -0.0200
             Mean action noise std: 0.75
                       Mean reward: 551.61
               Mean episode length: 57.50
                  Mean reward/step: 7.79
       Mean episode length/episode: 7.18
            Mean episode successes: 0.6919
Mean episode consecutive_successes: 1.5947
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 8.85s
                        Total time: 20612.02s
                               ETA: 1002835.0s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.293s, learning 0.201s)
               Value function loss: 23569.8808
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 504.71
               Mean episode length: 56.25
                  Mean reward/step: 8.54
       Mean episode length/episode: 7.02
            Mean episode successes: 0.6357
Mean episode consecutive_successes: 1.6447
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 8.49s
                        Total time: 20620.51s
                               ETA: 1002740.2s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.665s, learning 0.328s)
               Value function loss: 26382.1476
                    Surrogate loss: -0.0193
             Mean action noise std: 0.75
                       Mean reward: 474.27
               Mean episode length: 57.66
                  Mean reward/step: 8.53
       Mean episode length/episode: 7.18
            Mean episode successes: 0.6958
Mean episode consecutive_successes: 1.6314
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.99s
                        Total time: 20629.51s
                               ETA: 1002669.7s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.554s, learning 0.197s)
               Value function loss: 24771.4393
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 376.94
               Mean episode length: 54.77
                  Mean reward/step: 8.34
       Mean episode length/episode: 7.10
            Mean episode successes: 0.7031
Mean episode consecutive_successes: 1.6454
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 8.75s
                        Total time: 20638.26s
                               ETA: 1002587.5s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1780 steps/s (collection: 8.916s, learning 0.286s)
               Value function loss: 29313.7266
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 614.16
               Mean episode length: 57.05
                  Mean reward/step: 9.83
       Mean episode length/episode: 7.14
            Mean episode successes: 0.7773
Mean episode consecutive_successes: 1.6644
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 9.20s
                        Total time: 20647.46s
                               ETA: 1002527.2s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.907s, learning 0.190s)
               Value function loss: 28225.7408
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 628.97
               Mean episode length: 58.16
                  Mean reward/step: 9.35
       Mean episode length/episode: 7.15
            Mean episode successes: 0.7163
Mean episode consecutive_successes: 1.7348
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 9.10s
                        Total time: 20656.56s
                               ETA: 1002461.9s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.688s, learning 0.192s)
               Value function loss: 29548.0866
                    Surrogate loss: -0.0231
             Mean action noise std: 0.75
                       Mean reward: 521.40
               Mean episode length: 58.64
                  Mean reward/step: 9.26
       Mean episode length/episode: 7.16
            Mean episode successes: 0.7490
Mean episode consecutive_successes: 1.7524
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 8.88s
                        Total time: 20665.43s
                               ETA: 1002386.1s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.576s, learning 0.309s)
               Value function loss: 28378.8510
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 475.75
               Mean episode length: 56.22
                  Mean reward/step: 8.73
       Mean episode length/episode: 7.17
            Mean episode successes: 0.7334
Mean episode consecutive_successes: 1.7794
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 8.88s
                        Total time: 20674.32s
                               ETA: 1002310.7s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.821s, learning 0.177s)
               Value function loss: 26896.5337
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 739.72
               Mean episode length: 59.19
                  Mean reward/step: 8.27
       Mean episode length/episode: 7.16
            Mean episode successes: 0.7764
Mean episode consecutive_successes: 1.7985
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 9.00s
                        Total time: 20683.32s
                               ETA: 1002240.7s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.424s, learning 0.195s)
               Value function loss: 26263.2008
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 441.29
               Mean episode length: 57.84
                  Mean reward/step: 8.65
       Mean episode length/episode: 7.12
            Mean episode successes: 0.7271
Mean episode consecutive_successes: 1.8037
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 8.62s
                        Total time: 20691.94s
                               ETA: 1002152.5s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.456s, learning 0.276s)
               Value function loss: 24356.2941
                    Surrogate loss: -0.0177
             Mean action noise std: 0.75
                       Mean reward: 845.89
               Mean episode length: 58.96
                  Mean reward/step: 8.36
       Mean episode length/episode: 7.10
            Mean episode successes: 0.6890
Mean episode consecutive_successes: 1.8400
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 8.73s
                        Total time: 20700.67s
                               ETA: 1002069.9s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.557s, learning 0.189s)
               Value function loss: 22959.4802
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 395.88
               Mean episode length: 58.49
                  Mean reward/step: 7.59
       Mean episode length/episode: 7.14
            Mean episode successes: 0.6587
Mean episode consecutive_successes: 1.8354
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 8.75s
                        Total time: 20709.41s
                               ETA: 1001988.0s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.768s, learning 0.178s)
               Value function loss: 22062.2554
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 437.41
               Mean episode length: 58.67
                  Mean reward/step: 7.95
       Mean episode length/episode: 7.19
            Mean episode successes: 0.6289
Mean episode consecutive_successes: 1.8541
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 8.95s
                        Total time: 20718.36s
                               ETA: 1001915.8s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.725s, learning 0.187s)
               Value function loss: 20943.7462
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 360.14
               Mean episode length: 56.44
                  Mean reward/step: 7.11
       Mean episode length/episode: 7.14
            Mean episode successes: 0.6226
Mean episode consecutive_successes: 1.8293
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 8.91s
                        Total time: 20727.27s
                               ETA: 1001842.0s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.683s, learning 0.211s)
               Value function loss: 23282.4651
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 330.77
               Mean episode length: 57.77
                  Mean reward/step: 7.87
       Mean episode length/episode: 7.23
            Mean episode successes: 0.6987
Mean episode consecutive_successes: 1.7921
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 8.89s
                        Total time: 20736.17s
                               ETA: 1001767.5s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.868s, learning 0.184s)
               Value function loss: 25138.1604
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 413.56
               Mean episode length: 57.64
                  Mean reward/step: 8.96
       Mean episode length/episode: 7.10
            Mean episode successes: 0.7305
Mean episode consecutive_successes: 1.7938
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 9.05s
                        Total time: 20745.22s
                               ETA: 1001700.6s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.966s, learning 0.222s)
               Value function loss: 26458.0659
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 396.00
               Mean episode length: 59.00
                  Mean reward/step: 9.32
       Mean episode length/episode: 7.10
            Mean episode successes: 0.7651
Mean episode consecutive_successes: 1.7977
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 8.19s
                        Total time: 20753.41s
                               ETA: 1001592.1s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.610s, learning 0.175s)
               Value function loss: 23356.5419
                    Surrogate loss: -0.0187
             Mean action noise std: 0.75
                       Mean reward: 422.95
               Mean episode length: 56.77
                  Mean reward/step: 8.30
       Mean episode length/episode: 7.17
            Mean episode successes: 0.7778
Mean episode consecutive_successes: 1.8008
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 8.78s
                        Total time: 20762.19s
                               ETA: 1001512.4s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.478s, learning 0.218s)
               Value function loss: 22992.5768
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 674.53
               Mean episode length: 58.40
                  Mean reward/step: 7.22
       Mean episode length/episode: 7.09
            Mean episode successes: 0.6646
Mean episode consecutive_successes: 1.8319
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 8.70s
                        Total time: 20770.89s
                               ETA: 1001428.6s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.516s, learning 0.187s)
               Value function loss: 21303.5831
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 395.44
               Mean episode length: 55.31
                  Mean reward/step: 7.05
       Mean episode length/episode: 7.17
            Mean episode successes: 0.6572
Mean episode consecutive_successes: 1.8244
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 8.70s
                        Total time: 20779.59s
                               ETA: 1001345.2s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.441s, learning 0.335s)
               Value function loss: 23628.2301
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 407.72
               Mean episode length: 60.84
                  Mean reward/step: 7.48
       Mean episode length/episode: 7.18
            Mean episode successes: 0.6465
Mean episode consecutive_successes: 1.8193
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 8.78s
                        Total time: 20788.37s
                               ETA: 1001265.4s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.342s, learning 0.265s)
               Value function loss: 23807.9851
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 323.81
               Mean episode length: 56.10
                  Mean reward/step: 8.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.6406
Mean episode consecutive_successes: 1.8350
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 8.61s
                        Total time: 20796.97s
                               ETA: 1001177.5s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.462s, learning 0.204s)
               Value function loss: 26300.7061
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 428.40
               Mean episode length: 57.24
                  Mean reward/step: 8.94
       Mean episode length/episode: 7.08
            Mean episode successes: 0.6753
Mean episode consecutive_successes: 1.8218
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 8.67s
                        Total time: 20805.64s
                               ETA: 1001092.5s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.529s, learning 0.313s)
               Value function loss: 26007.5288
                    Surrogate loss: -0.0178
             Mean action noise std: 0.75
                       Mean reward: 186.48
               Mean episode length: 55.05
                  Mean reward/step: 8.61
       Mean episode length/episode: 7.18
            Mean episode successes: 0.7085
Mean episode consecutive_successes: 1.8041
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 8.84s
                        Total time: 20814.48s
                               ETA: 1001016.1s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.118s, learning 0.205s)
               Value function loss: 26845.2107
                    Surrogate loss: -0.0190
             Mean action noise std: 0.75
                       Mean reward: 593.37
               Mean episode length: 61.18
                  Mean reward/step: 9.28
       Mean episode length/episode: 7.14
            Mean episode successes: 0.7085
Mean episode consecutive_successes: 1.8647
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 8.32s
                        Total time: 20822.80s
                               ETA: 1000914.8s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.675s, learning 0.375s)
               Value function loss: 23904.5580
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 460.08
               Mean episode length: 55.33
                  Mean reward/step: 7.48
       Mean episode length/episode: 7.22
            Mean episode successes: 0.6880
Mean episode consecutive_successes: 1.8481
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 9.05s
                        Total time: 20831.86s
                               ETA: 1000848.6s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.224s, learning 0.192s)
               Value function loss: 25814.4123
                    Surrogate loss: -0.0189
             Mean action noise std: 0.75
                       Mean reward: 514.66
               Mean episode length: 58.77
                  Mean reward/step: 8.98
       Mean episode length/episode: 7.10
            Mean episode successes: 0.7383
Mean episode consecutive_successes: 1.8286
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 8.42s
                        Total time: 20840.27s
                               ETA: 1000751.9s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.490s, learning 0.337s)
               Value function loss: 27110.7408
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 642.04
               Mean episode length: 60.15
                  Mean reward/step: 8.76
       Mean episode length/episode: 7.13
            Mean episode successes: 0.7227
Mean episode consecutive_successes: 1.8488
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 8.83s
                        Total time: 20849.10s
                               ETA: 1000675.0s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.507s, learning 0.191s)
               Value function loss: 22984.7967
                    Surrogate loss: -0.0181
             Mean action noise std: 0.75
                       Mean reward: 628.85
               Mean episode length: 57.00
                  Mean reward/step: 8.68
       Mean episode length/episode: 7.15
            Mean episode successes: 0.6797
Mean episode consecutive_successes: 1.8839
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 8.70s
                        Total time: 20857.80s
                               ETA: 1000592.0s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.682s, learning 0.212s)
               Value function loss: 24028.6433
                    Surrogate loss: -0.0174
             Mean action noise std: 0.75
                       Mean reward: 516.67
               Mean episode length: 57.13
                  Mean reward/step: 8.57
       Mean episode length/episode: 7.15
            Mean episode successes: 0.6748
Mean episode consecutive_successes: 1.8918
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 8.89s
                        Total time: 20866.69s
                               ETA: 1000518.5s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.584s, learning 0.191s)
               Value function loss: 25351.4754
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 511.69
               Mean episode length: 59.14
                  Mean reward/step: 8.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.6992
Mean episode consecutive_successes: 1.8741
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 8.77s
                        Total time: 20875.46s
                               ETA: 1000439.3s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.490s, learning 0.327s)
               Value function loss: 25587.9724
                    Surrogate loss: -0.0196
             Mean action noise std: 0.75
                       Mean reward: 496.51
               Mean episode length: 59.45
                  Mean reward/step: 8.53
       Mean episode length/episode: 7.15
            Mean episode successes: 0.6509
Mean episode consecutive_successes: 1.9223
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 8.82s
                        Total time: 20884.28s
                               ETA: 1000362.2s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.379s, learning 0.219s)
               Value function loss: 25723.6343
                    Surrogate loss: -0.0097
             Mean action noise std: 0.75
                       Mean reward: 388.63
               Mean episode length: 58.08
                  Mean reward/step: 8.70
       Mean episode length/episode: 7.15
            Mean episode successes: 0.6885
Mean episode consecutive_successes: 1.8996
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 8.60s
                        Total time: 20892.88s
                               ETA: 1000274.7s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.326s, learning 0.177s)
               Value function loss: 27071.8806
                    Surrogate loss: -0.0145
             Mean action noise std: 0.75
                       Mean reward: 382.37
               Mean episode length: 56.48
                  Mean reward/step: 9.43
       Mean episode length/episode: 7.14
            Mean episode successes: 0.7378
Mean episode consecutive_successes: 1.8799
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 8.50s
                        Total time: 20901.38s
                               ETA: 1000182.7s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.710s, learning 0.208s)
               Value function loss: 27471.9175
                    Surrogate loss: -0.0198
             Mean action noise std: 0.75
                       Mean reward: 339.06
               Mean episode length: 55.69
                  Mean reward/step: 9.35
       Mean episode length/episode: 7.11
            Mean episode successes: 0.7642
Mean episode consecutive_successes: 1.8779
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 8.92s
                        Total time: 20910.30s
                               ETA: 1000110.7s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.470s, learning 0.270s)
               Value function loss: 31597.1876
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 543.30
               Mean episode length: 57.46
                  Mean reward/step: 9.43
       Mean episode length/episode: 7.10
            Mean episode successes: 0.8062
Mean episode consecutive_successes: 1.8916
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 8.74s
                        Total time: 20919.04s
                               ETA: 1000030.2s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.549s, learning 0.223s)
               Value function loss: 28001.6396
                    Surrogate loss: -0.0162
             Mean action noise std: 0.75
                       Mean reward: 651.68
               Mean episode length: 58.90
                  Mean reward/step: 9.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.7358
Mean episode consecutive_successes: 1.9311
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 8.77s
                        Total time: 20927.81s
                               ETA: 999951.3s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.410s, learning 0.191s)
               Value function loss: 30130.7590
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 422.99
               Mean episode length: 57.58
                  Mean reward/step: 10.06
       Mean episode length/episode: 7.18
            Mean episode successes: 0.8091
Mean episode consecutive_successes: 1.9317
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 8.60s
                        Total time: 20936.41s
                               ETA: 999864.3s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.675s, learning 0.305s)
               Value function loss: 30352.4469
                    Surrogate loss: -0.0209
             Mean action noise std: 0.75
                       Mean reward: 294.06
               Mean episode length: 56.69
                  Mean reward/step: 9.34
       Mean episode length/episode: 7.11
            Mean episode successes: 0.7295
Mean episode consecutive_successes: 1.9655
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 8.98s
                        Total time: 20945.39s
                               ETA: 999795.5s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.282s, learning 0.198s)
               Value function loss: 32801.4064
                    Surrogate loss: -0.0202
             Mean action noise std: 0.75
                       Mean reward: 684.44
               Mean episode length: 57.45
                  Mean reward/step: 8.87
       Mean episode length/episode: 7.20
            Mean episode successes: 0.7007
Mean episode consecutive_successes: 2.0056
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 8.48s
                        Total time: 20953.87s
                               ETA: 999702.8s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.453s, learning 0.249s)
               Value function loss: 28025.3861
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 412.75
               Mean episode length: 54.93
                  Mean reward/step: 9.26
       Mean episode length/episode: 7.18
            Mean episode successes: 0.7969
Mean episode consecutive_successes: 1.9656
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 8.70s
                        Total time: 20962.58s
                               ETA: 999620.9s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.314s, learning 0.174s)
               Value function loss: 32102.3084
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 436.77
               Mean episode length: 57.94
                  Mean reward/step: 10.38
       Mean episode length/episode: 7.11
            Mean episode successes: 0.8257
Mean episode consecutive_successes: 1.9725
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 8.49s
                        Total time: 20971.06s
                               ETA: 999528.8s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1766 steps/s (collection: 8.937s, learning 0.337s)
               Value function loss: 28637.5583
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 632.18
               Mean episode length: 59.63
                  Mean reward/step: 9.52
       Mean episode length/episode: 7.09
            Mean episode successes: 0.8066
Mean episode consecutive_successes: 2.0044
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 9.27s
                        Total time: 20980.34s
                               ETA: 999474.3s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.565s, learning 0.182s)
               Value function loss: 26848.4203
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 364.78
               Mean episode length: 60.02
                  Mean reward/step: 8.70
       Mean episode length/episode: 7.11
            Mean episode successes: 0.7510
Mean episode consecutive_successes: 2.0108
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 8.75s
                        Total time: 20989.09s
                               ETA: 999394.7s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.487s, learning 0.181s)
               Value function loss: 26180.5817
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 611.95
               Mean episode length: 58.40
                  Mean reward/step: 8.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.7007
Mean episode consecutive_successes: 2.0323
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 8.67s
                        Total time: 20997.75s
                               ETA: 999311.5s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.369s, learning 0.199s)
               Value function loss: 24732.6188
                    Surrogate loss: -0.0225
             Mean action noise std: 0.75
                       Mean reward: 295.77
               Mean episode length: 59.65
                  Mean reward/step: 8.40
       Mean episode length/episode: 7.12
            Mean episode successes: 0.6592
Mean episode consecutive_successes: 2.0411
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 8.57s
                        Total time: 21006.32s
                               ETA: 999223.5s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.475s, learning 0.175s)
               Value function loss: 26184.9943
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 259.78
               Mean episode length: 56.19
                  Mean reward/step: 8.85
       Mean episode length/episode: 7.17
            Mean episode successes: 0.6885
Mean episode consecutive_successes: 2.0326
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 8.65s
                        Total time: 21014.97s
                               ETA: 999139.5s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.853s, learning 0.174s)
               Value function loss: 28626.4554
                    Surrogate loss: -0.0164
             Mean action noise std: 0.75
                       Mean reward: 566.34
               Mean episode length: 56.81
                  Mean reward/step: 10.10
       Mean episode length/episode: 7.12
            Mean episode successes: 0.7661
Mean episode consecutive_successes: 2.0226
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 9.03s
                        Total time: 21024.00s
                               ETA: 999073.5s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.326s, learning 0.181s)
               Value function loss: 26831.6417
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 472.32
               Mean episode length: 59.68
                  Mean reward/step: 9.63
       Mean episode length/episode: 7.15
            Mean episode successes: 0.7759
Mean episode consecutive_successes: 2.0380
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 8.51s
                        Total time: 21032.51s
                               ETA: 998982.8s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.527s, learning 0.172s)
               Value function loss: 29303.2545
                    Surrogate loss: -0.0170
             Mean action noise std: 0.75
                       Mean reward: 559.09
               Mean episode length: 57.00
                  Mean reward/step: 9.57
       Mean episode length/episode: 7.17
            Mean episode successes: 0.8188
Mean episode consecutive_successes: 2.0100
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 8.70s
                        Total time: 21041.21s
                               ETA: 998901.4s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.545s, learning 0.201s)
               Value function loss: 29781.1301
                    Surrogate loss: -0.0155
             Mean action noise std: 0.75
                       Mean reward: 401.07
               Mean episode length: 56.47
                  Mean reward/step: 9.76
       Mean episode length/episode: 7.18
            Mean episode successes: 0.8643
Mean episode consecutive_successes: 2.0207
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 8.75s
                        Total time: 21049.95s
                               ETA: 998822.2s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.599s, learning 0.221s)
               Value function loss: 32799.7824
                    Surrogate loss: -0.0185
             Mean action noise std: 0.75
                       Mean reward: 473.76
               Mean episode length: 56.28
                  Mean reward/step: 10.90
       Mean episode length/episode: 7.12
            Mean episode successes: 0.8511
Mean episode consecutive_successes: 2.0536
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 8.82s
                        Total time: 21058.77s
                               ETA: 998746.7s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.493s, learning 0.167s)
               Value function loss: 34988.5317
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 477.69
               Mean episode length: 56.66
                  Mean reward/step: 12.24
       Mean episode length/episode: 7.09
            Mean episode successes: 0.9341
Mean episode consecutive_successes: 2.0539
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 8.66s
                        Total time: 21067.43s
                               ETA: 998663.6s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.240s, learning 0.182s)
               Value function loss: 28152.7925
                    Surrogate loss: -0.0167
             Mean action noise std: 0.75
                       Mean reward: 553.25
               Mean episode length: 60.40
                  Mean reward/step: 10.64
       Mean episode length/episode: 7.10
            Mean episode successes: 0.8765
Mean episode consecutive_successes: 2.0981
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 8.42s
                        Total time: 21075.85s
                               ETA: 998569.3s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.384s, learning 0.285s)
               Value function loss: 32210.1797
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 410.21
               Mean episode length: 56.76
                  Mean reward/step: 10.25
       Mean episode length/episode: 7.18
            Mean episode successes: 0.8735
Mean episode consecutive_successes: 2.1214
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 8.67s
                        Total time: 21084.52s
                               ETA: 998486.8s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.791s, learning 0.199s)
               Value function loss: 32415.9373
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 715.31
               Mean episode length: 55.87
                  Mean reward/step: 10.32
       Mean episode length/episode: 7.19
            Mean episode successes: 0.8848
Mean episode consecutive_successes: 2.1682
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 8.99s
                        Total time: 21093.51s
                               ETA: 998419.5s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.312s, learning 0.167s)
               Value function loss: 30441.1157
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 529.39
               Mean episode length: 52.58
                  Mean reward/step: 9.95
       Mean episode length/episode: 7.06
            Mean episode successes: 0.7324
Mean episode consecutive_successes: 2.2094
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 8.48s
                        Total time: 21101.99s
                               ETA: 998328.1s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.268s, learning 0.166s)
               Value function loss: 26305.5898
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 560.60
               Mean episode length: 56.43
                  Mean reward/step: 8.85
       Mean episode length/episode: 7.07
            Mean episode successes: 0.7178
Mean episode consecutive_successes: 2.2162
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 8.43s
                        Total time: 21110.43s
                               ETA: 998234.7s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.705s, learning 0.201s)
               Value function loss: 28280.3191
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 425.63
               Mean episode length: 56.59
                  Mean reward/step: 9.61
       Mean episode length/episode: 7.15
            Mean episode successes: 0.7168
Mean episode consecutive_successes: 2.1907
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 8.91s
                        Total time: 21119.33s
                               ETA: 998163.7s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.781s, learning 0.182s)
               Value function loss: 31084.9720
                    Surrogate loss: -0.0139
             Mean action noise std: 0.75
                       Mean reward: 562.42
               Mean episode length: 58.64
                  Mean reward/step: 9.23
       Mean episode length/episode: 7.15
            Mean episode successes: 0.7300
Mean episode consecutive_successes: 2.1932
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 8.96s
                        Total time: 21128.30s
                               ETA: 998095.4s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.237s, learning 0.161s)
               Value function loss: 31302.4725
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 554.55
               Mean episode length: 57.68
                  Mean reward/step: 10.17
       Mean episode length/episode: 7.19
            Mean episode successes: 0.7842
Mean episode consecutive_successes: 2.1806
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 8.40s
                        Total time: 21136.69s
                               ETA: 998000.5s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.109s, learning 0.174s)
               Value function loss: 31842.9198
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 504.61
               Mean episode length: 59.22
                  Mean reward/step: 9.32
       Mean episode length/episode: 7.14
            Mean episode successes: 0.7573
Mean episode consecutive_successes: 2.1845
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 8.28s
                        Total time: 21144.98s
                               ETA: 997900.3s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.358s, learning 0.259s)
               Value function loss: 34448.8760
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 549.97
               Mean episode length: 58.09
                  Mean reward/step: 10.58
       Mean episode length/episode: 7.16
            Mean episode successes: 0.8447
Mean episode consecutive_successes: 2.1803
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 8.62s
                        Total time: 21153.59s
                               ETA: 997815.9s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1779 steps/s (collection: 8.893s, learning 0.316s)
               Value function loss: 38252.2818
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 502.52
               Mean episode length: 58.58
                  Mean reward/step: 10.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.8193
Mean episode consecutive_successes: 2.1800
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 9.21s
                        Total time: 21162.80s
                               ETA: 997759.4s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.178s, learning 0.228s)
               Value function loss: 34408.9416
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 390.72
               Mean episode length: 54.97
                  Mean reward/step: 10.40
       Mean episode length/episode: 7.15
            Mean episode successes: 0.8784
Mean episode consecutive_successes: 2.1665
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 8.41s
                        Total time: 21171.21s
                               ETA: 997665.2s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.603s, learning 0.191s)
               Value function loss: 30231.8271
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 348.64
               Mean episode length: 56.62
                  Mean reward/step: 9.95
       Mean episode length/episode: 7.14
            Mean episode successes: 0.8345
Mean episode consecutive_successes: 2.1726
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 8.79s
                        Total time: 21180.00s
                               ETA: 997589.4s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.418s, learning 0.183s)
               Value function loss: 30312.9113
                    Surrogate loss: -0.0068
             Mean action noise std: 0.75
                       Mean reward: 559.41
               Mean episode length: 57.75
                  Mean reward/step: 9.46
       Mean episode length/episode: 7.08
            Mean episode successes: 0.7637
Mean episode consecutive_successes: 2.2165
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 8.60s
                        Total time: 21188.60s
                               ETA: 997504.5s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.459s, learning 0.186s)
               Value function loss: 29265.7785
                    Surrogate loss: -0.0160
             Mean action noise std: 0.75
                       Mean reward: 462.75
               Mean episode length: 54.96
                  Mean reward/step: 8.72
       Mean episode length/episode: 7.11
            Mean episode successes: 0.7168
Mean episode consecutive_successes: 2.2115
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 8.65s
                        Total time: 21197.25s
                               ETA: 997421.8s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.551s, learning 0.188s)
               Value function loss: 28101.6603
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 334.52
               Mean episode length: 56.08
                  Mean reward/step: 9.41
       Mean episode length/episode: 7.11
            Mean episode successes: 0.6924
Mean episode consecutive_successes: 2.2116
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 8.74s
                        Total time: 21205.99s
                               ETA: 997343.5s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.394s, learning 0.265s)
               Value function loss: 31567.3434
                    Surrogate loss: -0.0144
             Mean action noise std: 0.75
                       Mean reward: 372.96
               Mean episode length: 53.19
                  Mean reward/step: 11.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.8032
Mean episode consecutive_successes: 2.1827
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 8.66s
                        Total time: 21214.65s
                               ETA: 997261.5s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.110s, learning 0.169s)
               Value function loss: 33989.7753
                    Surrogate loss: -0.0153
             Mean action noise std: 0.75
                       Mean reward: 601.01
               Mean episode length: 56.80
                  Mean reward/step: 10.21
       Mean episode length/episode: 7.11
            Mean episode successes: 0.8223
Mean episode consecutive_successes: 2.1979
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 8.28s
                        Total time: 21222.93s
                               ETA: 997161.8s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.497s, learning 0.183s)
               Value function loss: 32430.4750
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 882.56
               Mean episode length: 57.52
                  Mean reward/step: 9.39
       Mean episode length/episode: 7.15
            Mean episode successes: 0.8120
Mean episode consecutive_successes: 2.1943
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 8.68s
                        Total time: 21231.61s
                               ETA: 997081.0s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.519s, learning 0.184s)
               Value function loss: 30628.0193
                    Surrogate loss: -0.0192
             Mean action noise std: 0.75
                       Mean reward: 501.57
               Mean episode length: 58.00
                  Mean reward/step: 9.95
       Mean episode length/episode: 7.11
            Mean episode successes: 0.7773
Mean episode consecutive_successes: 2.1922
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 8.70s
                        Total time: 21240.31s
                               ETA: 997001.3s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.706s, learning 0.166s)
               Value function loss: 35510.1354
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 297.26
               Mean episode length: 56.48
                  Mean reward/step: 11.50
       Mean episode length/episode: 7.18
            Mean episode successes: 0.7979
Mean episode consecutive_successes: 2.2155
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 8.87s
                        Total time: 21249.18s
                               ETA: 996929.6s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.382s, learning 0.187s)
               Value function loss: 34226.2270
                    Surrogate loss: -0.0026
             Mean action noise std: 0.74
                       Mean reward: 554.67
               Mean episode length: 59.67
                  Mean reward/step: 11.41
       Mean episode length/episode: 7.19
            Mean episode successes: 0.9067
Mean episode consecutive_successes: 2.1949
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 8.57s
                        Total time: 21257.75s
                               ETA: 996843.9s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.572s, learning 0.195s)
               Value function loss: 34017.2073
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 635.92
               Mean episode length: 57.25
                  Mean reward/step: 10.80
       Mean episode length/episode: 7.13
            Mean episode successes: 0.9702
Mean episode consecutive_successes: 2.2084
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 8.77s
                        Total time: 21266.52s
                               ETA: 996767.4s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.294s, learning 0.300s)
               Value function loss: 39024.1720
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 803.65
               Mean episode length: 59.65
                  Mean reward/step: 13.03
       Mean episode length/episode: 7.14
            Mean episode successes: 1.0479
Mean episode consecutive_successes: 2.2388
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 8.59s
                        Total time: 21275.11s
                               ETA: 996682.9s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.125s, learning 0.166s)
               Value function loss: 39731.2527
                    Surrogate loss: -0.0187
             Mean action noise std: 0.74
                       Mean reward: 445.09
               Mean episode length: 56.22
                  Mean reward/step: 12.38
       Mean episode length/episode: 7.17
            Mean episode successes: 0.9390
Mean episode consecutive_successes: 2.3308
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 8.29s
                        Total time: 21283.40s
                               ETA: 996584.3s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.389s, learning 0.228s)
               Value function loss: 34775.4569
                    Surrogate loss: -0.0004
             Mean action noise std: 0.74
                       Mean reward: 633.68
               Mean episode length: 58.77
                  Mean reward/step: 11.66
       Mean episode length/episode: 7.10
            Mean episode successes: 0.9780
Mean episode consecutive_successes: 2.3476
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 8.62s
                        Total time: 21292.02s
                               ETA: 996501.1s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.202s, learning 0.185s)
               Value function loss: 33440.4739
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 662.11
               Mean episode length: 56.88
                  Mean reward/step: 10.88
       Mean episode length/episode: 7.18
            Mean episode successes: 0.9473
Mean episode consecutive_successes: 2.3754
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 8.39s
                        Total time: 21300.41s
                               ETA: 996407.1s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.350s, learning 0.166s)
               Value function loss: 33183.2711
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 486.63
               Mean episode length: 56.73
                  Mean reward/step: 10.73
       Mean episode length/episode: 7.12
            Mean episode successes: 0.9297
Mean episode consecutive_successes: 2.3749
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 8.52s
                        Total time: 21308.92s
                               ETA: 996319.3s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.145s, learning 0.252s)
               Value function loss: 30605.9928
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 663.27
               Mean episode length: 57.20
                  Mean reward/step: 10.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.8535
Mean episode consecutive_successes: 2.4222
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 8.40s
                        Total time: 21317.32s
                               ETA: 996226.0s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.591s, learning 0.171s)
               Value function loss: 39472.5266
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 561.61
               Mean episode length: 57.01
                  Mean reward/step: 10.50
       Mean episode length/episode: 7.18
            Mean episode successes: 0.8696
Mean episode consecutive_successes: 2.4212
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 8.76s
                        Total time: 21326.08s
                               ETA: 996149.8s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.144s, learning 0.331s)
               Value function loss: 40634.0981
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 601.92
               Mean episode length: 58.41
                  Mean reward/step: 11.40
       Mean episode length/episode: 7.11
            Mean episode successes: 0.9541
Mean episode consecutive_successes: 2.4042
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 8.47s
                        Total time: 21334.56s
                               ETA: 996060.3s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.362s, learning 0.165s)
               Value function loss: 42011.2464
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 825.78
               Mean episode length: 58.94
                  Mean reward/step: 10.93
       Mean episode length/episode: 7.11
            Mean episode successes: 0.8721
Mean episode consecutive_successes: 2.4238
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 8.53s
                        Total time: 21343.08s
                               ETA: 995973.3s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.737s, learning 0.272s)
               Value function loss: 31875.1914
                    Surrogate loss: -0.0183
             Mean action noise std: 0.74
                       Mean reward: 664.43
               Mean episode length: 56.21
                  Mean reward/step: 9.60
       Mean episode length/episode: 7.08
            Mean episode successes: 0.8193
Mean episode consecutive_successes: 2.4098
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 9.01s
                        Total time: 21352.09s
                               ETA: 995908.8s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.634s, learning 0.166s)
               Value function loss: 33359.3394
                    Surrogate loss: 0.0022
             Mean action noise std: 0.74
                       Mean reward: 410.59
               Mean episode length: 56.41
                  Mean reward/step: 8.58
       Mean episode length/episode: 7.22
            Mean episode successes: 0.8047
Mean episode consecutive_successes: 2.4045
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 8.80s
                        Total time: 21360.89s
                               ETA: 995834.6s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.588s, learning 0.179s)
               Value function loss: 28607.5811
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 742.56
               Mean episode length: 57.46
                  Mean reward/step: 9.52
       Mean episode length/episode: 7.05
            Mean episode successes: 0.7866
Mean episode consecutive_successes: 2.4119
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 8.77s
                        Total time: 21369.66s
                               ETA: 995759.0s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.200s, learning 0.180s)
               Value function loss: 28745.8667
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 394.85
               Mean episode length: 54.45
                  Mean reward/step: 8.70
       Mean episode length/episode: 7.13
            Mean episode successes: 0.7729
Mean episode consecutive_successes: 2.3735
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 8.38s
                        Total time: 21378.04s
                               ETA: 995665.4s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.481s, learning 0.222s)
               Value function loss: 31814.2691
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 641.71
               Mean episode length: 57.37
                  Mean reward/step: 10.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.8022
Mean episode consecutive_successes: 2.3583
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 8.70s
                        Total time: 21386.74s
                               ETA: 995586.9s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.349s, learning 0.370s)
               Value function loss: 30861.6559
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 535.75
               Mean episode length: 59.14
                  Mean reward/step: 10.17
       Mean episode length/episode: 7.14
            Mean episode successes: 0.8833
Mean episode consecutive_successes: 2.3098
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 8.72s
                        Total time: 21395.46s
                               ETA: 995509.2s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.639s, learning 0.188s)
               Value function loss: 31151.6852
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 570.65
               Mean episode length: 55.89
                  Mean reward/step: 10.69
       Mean episode length/episode: 7.20
            Mean episode successes: 0.8047
Mean episode consecutive_successes: 2.3700
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 8.83s
                        Total time: 21404.29s
                               ETA: 995436.7s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.255s, learning 0.342s)
               Value function loss: 34147.0915
                    Surrogate loss: 0.0027
             Mean action noise std: 0.74
                       Mean reward: 671.25
               Mean episode length: 55.25
                  Mean reward/step: 10.99
       Mean episode length/episode: 7.10
            Mean episode successes: 0.8745
Mean episode consecutive_successes: 2.3451
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 8.60s
                        Total time: 21412.89s
                               ETA: 995353.5s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.516s, learning 0.169s)
               Value function loss: 35949.0691
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 631.22
               Mean episode length: 58.55
                  Mean reward/step: 11.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.8843
Mean episode consecutive_successes: 2.3530
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 8.69s
                        Total time: 21421.57s
                               ETA: 995274.4s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.352s, learning 0.207s)
               Value function loss: 34660.8084
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 653.74
               Mean episode length: 56.44
                  Mean reward/step: 10.96
       Mean episode length/episode: 7.19
            Mean episode successes: 0.9243
Mean episode consecutive_successes: 2.3414
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 8.56s
                        Total time: 21430.13s
                               ETA: 995189.6s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.326s, learning 0.180s)
               Value function loss: 37346.5578
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 798.99
               Mean episode length: 60.51
                  Mean reward/step: 10.86
       Mean episode length/episode: 7.20
            Mean episode successes: 0.8779
Mean episode consecutive_successes: 2.3988
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 8.51s
                        Total time: 21438.64s
                               ETA: 995102.4s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.464s, learning 0.211s)
               Value function loss: 33778.1693
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 916.02
               Mean episode length: 58.63
                  Mean reward/step: 11.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.8545
Mean episode consecutive_successes: 2.4319
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 8.68s
                        Total time: 21447.31s
                               ETA: 995023.1s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.211s, learning 0.176s)
               Value function loss: 34507.8298
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 649.34
               Mean episode length: 59.47
                  Mean reward/step: 11.27
       Mean episode length/episode: 7.18
            Mean episode successes: 0.8408
Mean episode consecutive_successes: 2.4369
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 8.39s
                        Total time: 21455.70s
                               ETA: 994930.5s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.515s, learning 0.329s)
               Value function loss: 36765.1897
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 428.42
               Mean episode length: 57.64
                  Mean reward/step: 11.88
       Mean episode length/episode: 7.15
            Mean episode successes: 0.9819
Mean episode consecutive_successes: 2.3829
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 8.84s
                        Total time: 21464.54s
                               ETA: 994859.1s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.574s, learning 0.183s)
               Value function loss: 33522.4411
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 678.70
               Mean episode length: 60.59
                  Mean reward/step: 11.25
       Mean episode length/episode: 7.03
            Mean episode successes: 0.8696
Mean episode consecutive_successes: 2.4234
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 8.76s
                        Total time: 21473.30s
                               ETA: 994783.8s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.451s, learning 0.183s)
               Value function loss: 33000.6050
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 350.37
               Mean episode length: 56.94
                  Mean reward/step: 10.63
       Mean episode length/episode: 7.03
            Mean episode successes: 0.8755
Mean episode consecutive_successes: 2.4099
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 8.63s
                        Total time: 21481.93s
                               ETA: 994702.9s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.291s, learning 0.174s)
               Value function loss: 31251.2296
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 778.79
               Mean episode length: 56.28
                  Mean reward/step: 10.20
       Mean episode length/episode: 7.12
            Mean episode successes: 0.8779
Mean episode consecutive_successes: 2.3902
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 8.47s
                        Total time: 21490.40s
                               ETA: 994614.2s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.517s, learning 0.191s)
               Value function loss: 32735.9371
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 906.20
               Mean episode length: 56.14
                  Mean reward/step: 10.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.8198
Mean episode consecutive_successes: 2.4241
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 8.71s
                        Total time: 21499.10s
                               ETA: 994536.8s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.496s, learning 0.259s)
               Value function loss: 33949.6984
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 526.00
               Mean episode length: 56.77
                  Mean reward/step: 9.29
       Mean episode length/episode: 7.19
            Mean episode successes: 0.7949
Mean episode consecutive_successes: 2.4210
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 8.75s
                        Total time: 21507.86s
                               ETA: 994461.7s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.634s, learning 0.223s)
               Value function loss: 35853.6390
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 525.24
               Mean episode length: 54.84
                  Mean reward/step: 10.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.7832
Mean episode consecutive_successes: 2.4011
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 8.86s
                        Total time: 21516.72s
                               ETA: 994391.3s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.581s, learning 0.167s)
               Value function loss: 36263.6390
                    Surrogate loss: -0.0182
             Mean action noise std: 0.74
                       Mean reward: 916.78
               Mean episode length: 56.62
                  Mean reward/step: 10.78
       Mean episode length/episode: 7.16
            Mean episode successes: 0.7720
Mean episode consecutive_successes: 2.3890
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 8.75s
                        Total time: 21525.46s
                               ETA: 994316.0s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.797s, learning 0.261s)
               Value function loss: 39032.0546
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 425.72
               Mean episode length: 55.00
                  Mean reward/step: 12.28
       Mean episode length/episode: 7.11
            Mean episode successes: 0.8560
Mean episode consecutive_successes: 2.3590
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 9.06s
                        Total time: 21534.52s
                               ETA: 994255.0s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.198s, learning 0.170s)
               Value function loss: 48912.5026
                    Surrogate loss: -0.0018
             Mean action noise std: 0.74
                       Mean reward: 583.45
               Mean episode length: 57.43
                  Mean reward/step: 13.90
       Mean episode length/episode: 7.18
            Mean episode successes: 1.0459
Mean episode consecutive_successes: 2.3382
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 8.37s
                        Total time: 21542.89s
                               ETA: 994162.2s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1782 steps/s (collection: 8.877s, learning 0.318s)
               Value function loss: 42289.6977
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 784.73
               Mean episode length: 58.83
                  Mean reward/step: 12.73
       Mean episode length/episode: 7.13
            Mean episode successes: 1.0054
Mean episode consecutive_successes: 2.4048
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 9.19s
                        Total time: 21552.08s
                               ETA: 994107.7s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.133s, learning 0.172s)
               Value function loss: 44172.0190
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 734.01
               Mean episode length: 61.04
                  Mean reward/step: 11.89
       Mean episode length/episode: 7.17
            Mean episode successes: 1.0796
Mean episode consecutive_successes: 2.4078
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 8.31s
                        Total time: 21560.39s
                               ETA: 994012.2s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.614s, learning 0.230s)
               Value function loss: 43546.2999
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 926.10
               Mean episode length: 59.03
                  Mean reward/step: 10.89
       Mean episode length/episode: 7.07
            Mean episode successes: 0.9526
Mean episode consecutive_successes: 2.4721
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 8.84s
                        Total time: 21569.23s
                               ETA: 993941.6s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.670s, learning 0.168s)
               Value function loss: 40834.7663
                    Surrogate loss: -0.0177
             Mean action noise std: 0.74
                       Mean reward: 710.50
               Mean episode length: 58.62
                  Mean reward/step: 12.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.9180
Mean episode consecutive_successes: 2.5128
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 8.84s
                        Total time: 21578.07s
                               ETA: 993870.7s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.722s, learning 0.196s)
               Value function loss: 39623.5349
                    Surrogate loss: -0.0209
             Mean action noise std: 0.74
                       Mean reward: 525.80
               Mean episode length: 56.11
                  Mean reward/step: 12.25
       Mean episode length/episode: 7.09
            Mean episode successes: 0.9712
Mean episode consecutive_successes: 2.5006
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 8.92s
                        Total time: 21586.99s
                               ETA: 993803.7s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.169s)
               Value function loss: 48767.2357
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 712.02
               Mean episode length: 56.79
                  Mean reward/step: 12.65
       Mean episode length/episode: 7.16
            Mean episode successes: 0.9561
Mean episode consecutive_successes: 2.5386
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 8.53s
                        Total time: 21595.52s
                               ETA: 993718.8s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.263s, learning 0.170s)
               Value function loss: 49669.6434
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 787.43
               Mean episode length: 56.82
                  Mean reward/step: 12.10
       Mean episode length/episode: 7.21
            Mean episode successes: 0.9692
Mean episode consecutive_successes: 2.5851
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 8.43s
                        Total time: 21603.95s
                               ETA: 993629.5s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.174s, learning 0.220s)
               Value function loss: 34549.7298
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 607.40
               Mean episode length: 57.59
                  Mean reward/step: 10.14
       Mean episode length/episode: 7.06
            Mean episode successes: 0.8447
Mean episode consecutive_successes: 2.6056
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 8.39s
                        Total time: 21612.35s
                               ETA: 993538.6s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.261s, learning 0.181s)
               Value function loss: 33353.8238
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 652.12
               Mean episode length: 58.19
                  Mean reward/step: 10.90
       Mean episode length/episode: 7.08
            Mean episode successes: 0.8345
Mean episode consecutive_successes: 2.5812
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 8.44s
                        Total time: 21620.79s
                               ETA: 993449.9s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.431s, learning 0.210s)
               Value function loss: 36537.3028
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 639.90
               Mean episode length: 59.60
                  Mean reward/step: 11.56
       Mean episode length/episode: 7.07
            Mean episode successes: 0.8892
Mean episode consecutive_successes: 2.5538
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 8.64s
                        Total time: 21629.43s
                               ETA: 993370.4s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1137 steps/s (collection: 14.054s, learning 0.345s)
               Value function loss: 33265.1663
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 443.37
               Mean episode length: 56.21
                  Mean reward/step: 12.54
       Mean episode length/episode: 7.16
            Mean episode successes: 0.9531
Mean episode consecutive_successes: 2.5367
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 14.40s
                        Total time: 21643.83s
                               ETA: 993555.3s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.249s, learning 0.166s)
               Value function loss: 37545.7488
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 1027.22
               Mean episode length: 60.08
                  Mean reward/step: 12.14
       Mean episode length/episode: 7.12
            Mean episode successes: 1.0303
Mean episode consecutive_successes: 2.5519
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 16.42s
                        Total time: 21660.24s
                               ETA: 993832.5s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.533s, learning 0.222s)
               Value function loss: 36490.3147
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 468.97
               Mean episode length: 56.88
                  Mean reward/step: 10.89
       Mean episode length/episode: 7.20
            Mean episode successes: 0.9502
Mean episode consecutive_successes: 2.5677
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 16.75s
                        Total time: 21677.00s
                               ETA: 994125.0s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.415s, learning 0.169s)
               Value function loss: 37180.1960
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 547.90
               Mean episode length: 55.17
                  Mean reward/step: 10.95
       Mean episode length/episode: 7.05
            Mean episode successes: 0.9009
Mean episode consecutive_successes: 2.5717
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 16.58s
                        Total time: 21693.58s
                               ETA: 994409.5s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.726s, learning 0.179s)
               Value function loss: 40127.9665
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 650.82
               Mean episode length: 56.13
                  Mean reward/step: 12.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.9331
Mean episode consecutive_successes: 2.5763
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 16.90s
                        Total time: 21710.49s
                               ETA: 994708.3s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.287s, learning 0.177s)
               Value function loss: 51811.3915
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 455.62
               Mean episode length: 56.82
                  Mean reward/step: 12.28
       Mean episode length/episode: 7.12
            Mean episode successes: 0.9790
Mean episode consecutive_successes: 2.5425
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 16.46s
                        Total time: 21726.95s
                               ETA: 994986.6s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.487s, learning 0.197s)
               Value function loss: 45166.8817
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 886.86
               Mean episode length: 57.31
                  Mean reward/step: 13.67
       Mean episode length/episode: 7.16
            Mean episode successes: 1.0361
Mean episode consecutive_successes: 2.5702
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 16.68s
                        Total time: 21743.64s
                               ETA: 995274.7s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.689s, learning 0.181s)
               Value function loss: 45686.3929
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 702.07
               Mean episode length: 59.41
                  Mean reward/step: 11.86
       Mean episode length/episode: 7.18
            Mean episode successes: 1.0723
Mean episode consecutive_successes: 2.5812
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 16.87s
                        Total time: 21760.51s
                               ETA: 995571.1s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.552s, learning 0.166s)
               Value function loss: 43957.4435
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 392.35
               Mean episode length: 58.07
                  Mean reward/step: 12.37
       Mean episode length/episode: 7.11
            Mean episode successes: 1.0078
Mean episode consecutive_successes: 2.5940
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 16.72s
                        Total time: 21777.22s
                               ETA: 995860.2s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.203s, learning 0.215s)
               Value function loss: 40687.7552
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 1038.61
               Mean episode length: 58.86
                  Mean reward/step: 11.31
       Mean episode length/episode: 7.13
            Mean episode successes: 0.8994
Mean episode consecutive_successes: 2.6691
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 16.42s
                        Total time: 21793.64s
                               ETA: 996135.4s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.265s, learning 0.189s)
               Value function loss: 39271.4528
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 701.08
               Mean episode length: 56.44
                  Mean reward/step: 11.50
       Mean episode length/episode: 7.07
            Mean episode successes: 0.9180
Mean episode consecutive_successes: 2.6558
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 16.45s
                        Total time: 21810.10s
                               ETA: 996411.9s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.383s, learning 0.170s)
               Value function loss: 36925.3037
                    Surrogate loss: -0.0181
             Mean action noise std: 0.74
                       Mean reward: 466.53
               Mean episode length: 58.24
                  Mean reward/step: 10.52
       Mean episode length/episode: 7.18
            Mean episode successes: 0.9302
Mean episode consecutive_successes: 2.6172
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 16.55s
                        Total time: 21826.65s
                               ETA: 996692.6s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.753s, learning 0.248s)
               Value function loss: 37827.6999
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 820.87
               Mean episode length: 57.90
                  Mean reward/step: 11.94
       Mean episode length/episode: 7.11
            Mean episode successes: 0.9150
Mean episode consecutive_successes: 2.6409
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 17.00s
                        Total time: 21843.65s
                               ETA: 996993.5s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.712s, learning 0.399s)
               Value function loss: 34317.2041
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 489.41
               Mean episode length: 57.99
                  Mean reward/step: 10.72
       Mean episode length/episode: 7.08
            Mean episode successes: 0.8975
Mean episode consecutive_successes: 2.6171
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 17.11s
                        Total time: 21860.76s
                               ETA: 997299.1s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 948 steps/s (collection: 16.906s, learning 0.372s)
               Value function loss: 33248.1224
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 539.40
               Mean episode length: 57.62
                  Mean reward/step: 9.66
       Mean episode length/episode: 7.13
            Mean episode successes: 0.8311
Mean episode consecutive_successes: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 17.28s
                        Total time: 21878.04s
                               ETA: 997612.0s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 940 steps/s (collection: 17.088s, learning 0.337s)
               Value function loss: 32788.9609
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 512.27
               Mean episode length: 58.65
                  Mean reward/step: 9.80
       Mean episode length/episode: 7.17
            Mean episode successes: 0.8252
Mean episode consecutive_successes: 2.6152
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 17.43s
                        Total time: 21895.46s
                               ETA: 997931.4s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.334s, learning 0.301s)
               Value function loss: 35452.2893
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 331.65
               Mean episode length: 56.47
                  Mean reward/step: 11.24
       Mean episode length/episode: 7.17
            Mean episode successes: 0.8398
Mean episode consecutive_successes: 2.5953
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 16.64s
                        Total time: 21912.10s
                               ETA: 998214.4s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.138s, learning 0.179s)
               Value function loss: 34144.6640
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 523.53
               Mean episode length: 57.34
                  Mean reward/step: 11.33
       Mean episode length/episode: 7.04
            Mean episode successes: 0.8906
Mean episode consecutive_successes: 2.5689
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 16.32s
                        Total time: 21928.42s
                               ETA: 998482.7s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.934s, learning 0.166s)
               Value function loss: 35554.9458
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 529.38
               Mean episode length: 58.03
                  Mean reward/step: 10.50
       Mean episode length/episode: 7.15
            Mean episode successes: 0.9233
Mean episode consecutive_successes: 2.5310
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 17.10s
                        Total time: 21945.52s
                               ETA: 998786.4s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.728s, learning 0.202s)
               Value function loss: 36522.8100
                    Surrogate loss: 0.0070
             Mean action noise std: 0.74
                       Mean reward: 802.55
               Mean episode length: 59.64
                  Mean reward/step: 11.44
       Mean episode length/episode: 7.10
            Mean episode successes: 0.9736
Mean episode consecutive_successes: 2.5209
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 16.93s
                        Total time: 21962.45s
                               ETA: 999082.0s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.514s, learning 0.194s)
               Value function loss: 39194.8339
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 684.96
               Mean episode length: 57.69
                  Mean reward/step: 11.35
       Mean episode length/episode: 7.14
            Mean episode successes: 0.8848
Mean episode consecutive_successes: 2.5574
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 16.71s
                        Total time: 21979.15s
                               ETA: 999367.2s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.899s, learning 0.224s)
               Value function loss: 35579.2969
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 337.47
               Mean episode length: 56.34
                  Mean reward/step: 10.52
       Mean episode length/episode: 7.15
            Mean episode successes: 0.8638
Mean episode consecutive_successes: 2.5675
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 17.12s
                        Total time: 21996.28s
                               ETA: 999671.0s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.411s, learning 0.183s)
               Value function loss: 34286.9098
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 592.64
               Mean episode length: 57.95
                  Mean reward/step: 11.31
       Mean episode length/episode: 7.07
            Mean episode successes: 0.9004
Mean episode consecutive_successes: 2.5484
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 16.59s
                        Total time: 22012.87s
                               ETA: 999950.5s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.554s, learning 0.175s)
               Value function loss: 34586.9510
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 635.98
               Mean episode length: 60.35
                  Mean reward/step: 10.33
       Mean episode length/episode: 7.19
            Mean episode successes: 0.9360
Mean episode consecutive_successes: 2.5135
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 16.73s
                        Total time: 22029.60s
                               ETA: 1000235.9s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.292s, learning 0.165s)
               Value function loss: 36833.7319
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 556.42
               Mean episode length: 57.13
                  Mean reward/step: 10.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.9653
Mean episode consecutive_successes: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 16.46s
                        Total time: 22046.06s
                               ETA: 1000508.6s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.820s, learning 0.168s)
               Value function loss: 34443.5058
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 588.53
               Mean episode length: 56.65
                  Mean reward/step: 10.51
       Mean episode length/episode: 7.12
            Mean episode successes: 0.9077
Mean episode consecutive_successes: 2.5090
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 16.99s
                        Total time: 22063.05s
                               ETA: 1000805.1s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.728s, learning 0.215s)
               Value function loss: 38564.1099
                    Surrogate loss: -0.0159
             Mean action noise std: 0.74
                       Mean reward: 856.04
               Mean episode length: 59.38
                  Mean reward/step: 11.54
       Mean episode length/episode: 7.07
            Mean episode successes: 0.8726
Mean episode consecutive_successes: 2.5382
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 16.94s
                        Total time: 22079.99s
                               ETA: 1001099.3s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.038s, learning 0.178s)
               Value function loss: 42933.9409
                    Surrogate loss: -0.0003
             Mean action noise std: 0.74
                       Mean reward: 706.21
               Mean episode length: 55.44
                  Mean reward/step: 11.39
       Mean episode length/episode: 7.13
            Mean episode successes: 0.9429
Mean episode consecutive_successes: 2.5229
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 17.22s
                        Total time: 22097.20s
                               ETA: 1001405.6s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.546s, learning 0.214s)
               Value function loss: 38664.6251
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 716.54
               Mean episode length: 56.58
                  Mean reward/step: 11.72
       Mean episode length/episode: 7.17
            Mean episode successes: 0.9355
Mean episode consecutive_successes: 2.5162
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 16.76s
                        Total time: 22113.96s
                               ETA: 1001690.9s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.003s, learning 0.181s)
               Value function loss: 35604.8883
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 685.64
               Mean episode length: 56.23
                  Mean reward/step: 11.50
       Mean episode length/episode: 7.13
            Mean episode successes: 0.9443
Mean episode consecutive_successes: 2.5195
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 17.18s
                        Total time: 22131.15s
                               ETA: 1001995.1s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.481s, learning 0.172s)
               Value function loss: 43961.8837
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 327.75
               Mean episode length: 54.07
                  Mean reward/step: 12.15
       Mean episode length/episode: 7.13
            Mean episode successes: 0.9800
Mean episode consecutive_successes: 2.5102
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 16.65s
                        Total time: 22147.80s
                               ETA: 1002275.1s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.519s, learning 0.210s)
               Value function loss: 47098.9006
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 709.37
               Mean episode length: 60.40
                  Mean reward/step: 13.46
       Mean episode length/episode: 7.10
            Mean episode successes: 1.0356
Mean episode consecutive_successes: 2.5404
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 16.73s
                        Total time: 22164.53s
                               ETA: 1002558.1s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.762s, learning 0.216s)
               Value function loss: 61197.9036
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 627.93
               Mean episode length: 56.65
                  Mean reward/step: 14.49
       Mean episode length/episode: 7.10
            Mean episode successes: 1.1406
Mean episode consecutive_successes: 2.5345
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 16.98s
                        Total time: 22181.51s
                               ETA: 1002852.2s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.228s, learning 0.175s)
               Value function loss: 78586.1211
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 516.64
               Mean episode length: 57.93
                  Mean reward/step: 15.06
       Mean episode length/episode: 7.19
            Mean episode successes: 1.2344
Mean episode consecutive_successes: 2.5482
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 16.40s
                        Total time: 22197.91s
                               ETA: 1003120.0s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.579s, learning 0.266s)
               Value function loss: 54045.1716
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 1201.01
               Mean episode length: 57.46
                  Mean reward/step: 14.38
       Mean episode length/episode: 7.16
            Mean episode successes: 1.2856
Mean episode consecutive_successes: 2.6243
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 16.85s
                        Total time: 22214.76s
                               ETA: 1003407.5s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.714s, learning 0.165s)
               Value function loss: 57612.9052
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 798.84
               Mean episode length: 60.33
                  Mean reward/step: 12.93
       Mean episode length/episode: 7.09
            Mean episode successes: 1.1245
Mean episode consecutive_successes: 2.6965
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 16.88s
                        Total time: 22231.64s
                               ETA: 1003696.3s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.510s, learning 0.172s)
               Value function loss: 44482.1901
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 835.18
               Mean episode length: 58.59
                  Mean reward/step: 11.63
       Mean episode length/episode: 7.14
            Mean episode successes: 1.0571
Mean episode consecutive_successes: 2.7231
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 16.68s
                        Total time: 22248.32s
                               ETA: 1003975.9s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.574s, learning 0.168s)
               Value function loss: 33061.5268
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 761.77
               Mean episode length: 56.26
                  Mean reward/step: 10.54
       Mean episode length/episode: 7.10
            Mean episode successes: 0.9141
Mean episode consecutive_successes: 2.7789
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 16.74s
                        Total time: 22265.06s
                               ETA: 1004257.9s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.249s, learning 0.179s)
               Value function loss: 34765.5000
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 560.94
               Mean episode length: 56.83
                  Mean reward/step: 11.86
       Mean episode length/episode: 7.07
            Mean episode successes: 0.8472
Mean episode consecutive_successes: 2.7782
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 8.43s
                        Total time: 22273.49s
                               ETA: 1004164.8s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.284s, learning 0.261s)
               Value function loss: 36626.7069
                    Surrogate loss: -0.0024
             Mean action noise std: 0.74
                       Mean reward: 267.59
               Mean episode length: 56.05
                  Mean reward/step: 12.46
       Mean episode length/episode: 7.23
            Mean episode successes: 0.9893
Mean episode consecutive_successes: 2.7150
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 8.54s
                        Total time: 22282.03s
                               ETA: 1004077.0s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.869s, learning 0.219s)
               Value function loss: 42324.5733
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 696.70
               Mean episode length: 61.13
                  Mean reward/step: 13.03
       Mean episode length/episode: 7.15
            Mean episode successes: 1.1040
Mean episode consecutive_successes: 2.6866
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 9.09s
                        Total time: 22291.12s
                               ETA: 1004013.8s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.431s, learning 0.246s)
               Value function loss: 48663.1702
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 779.39
               Mean episode length: 56.21
                  Mean reward/step: 14.44
       Mean episode length/episode: 7.14
            Mean episode successes: 1.0923
Mean episode consecutive_successes: 2.7555
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 8.68s
                        Total time: 22299.80s
                               ETA: 1003932.1s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.862s, learning 0.266s)
               Value function loss: 44821.5631
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 1043.55
               Mean episode length: 57.30
                  Mean reward/step: 13.86
       Mean episode length/episode: 7.17
            Mean episode successes: 1.0933
Mean episode consecutive_successes: 2.7876
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 9.13s
                        Total time: 22308.92s
                               ETA: 1003870.8s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.681s, learning 0.258s)
               Value function loss: 46947.2688
                    Surrogate loss: -0.0155
             Mean action noise std: 0.74
                       Mean reward: 717.07
               Mean episode length: 57.76
                  Mean reward/step: 12.85
       Mean episode length/episode: 7.12
            Mean episode successes: 1.0635
Mean episode consecutive_successes: 2.7965
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 8.94s
                        Total time: 22317.86s
                               ETA: 1003801.1s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1769 steps/s (collection: 9.026s, learning 0.235s)
               Value function loss: 53066.2603
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 536.50
               Mean episode length: 60.10
                  Mean reward/step: 12.79
       Mean episode length/episode: 7.19
            Mean episode successes: 1.1333
Mean episode consecutive_successes: 2.7695
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 9.26s
                        Total time: 22327.12s
                               ETA: 1003745.8s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.180s, learning 0.314s)
               Value function loss: 46613.4718
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 930.87
               Mean episode length: 59.64
                  Mean reward/step: 14.09
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1147
Mean episode consecutive_successes: 2.8358
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 8.49s
                        Total time: 22335.62s
                               ETA: 1003656.2s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.455s, learning 0.215s)
               Value function loss: 47186.1689
                    Surrogate loss: -0.0167
             Mean action noise std: 0.74
                       Mean reward: 765.14
               Mean episode length: 58.49
                  Mean reward/step: 14.32
       Mean episode length/episode: 7.11
            Mean episode successes: 1.1274
Mean episode consecutive_successes: 2.8462
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 8.67s
                        Total time: 22344.29s
                               ETA: 1003574.5s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.480s, learning 0.179s)
               Value function loss: 49389.1252
                    Surrogate loss: -0.0018
             Mean action noise std: 0.74
                       Mean reward: 1033.90
               Mean episode length: 59.61
                  Mean reward/step: 14.06
       Mean episode length/episode: 7.10
            Mean episode successes: 1.0327
Mean episode consecutive_successes: 2.9078
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 8.66s
                        Total time: 22352.95s
                               ETA: 1003492.4s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.712s, learning 0.420s)
               Value function loss: 45795.0221
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 704.22
               Mean episode length: 56.52
                  Mean reward/step: 13.89
       Mean episode length/episode: 7.11
            Mean episode successes: 1.0586
Mean episode consecutive_successes: 2.8863
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 9.13s
                        Total time: 22362.08s
                               ETA: 1003431.6s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.583s, learning 0.178s)
               Value function loss: 40859.9881
                    Surrogate loss: -0.0171
             Mean action noise std: 0.74
                       Mean reward: 879.76
               Mean episode length: 56.32
                  Mean reward/step: 12.70
       Mean episode length/episode: 7.22
            Mean episode successes: 1.1353
Mean episode consecutive_successes: 2.9020
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 8.76s
                        Total time: 22370.84s
                               ETA: 1003354.2s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.675s, learning 0.245s)
               Value function loss: 41316.2866
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 742.92
               Mean episode length: 60.27
                  Mean reward/step: 13.15
       Mean episode length/episode: 7.15
            Mean episode successes: 1.0571
Mean episode consecutive_successes: 2.9307
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 8.92s
                        Total time: 22379.76s
                               ETA: 1003284.0s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.524s, learning 0.276s)
               Value function loss: 48474.5450
                    Surrogate loss: -0.0003
             Mean action noise std: 0.74
                       Mean reward: 793.03
               Mean episode length: 58.50
                  Mean reward/step: 14.31
       Mean episode length/episode: 7.14
            Mean episode successes: 1.2085
Mean episode consecutive_successes: 2.8882
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 8.80s
                        Total time: 22388.56s
                               ETA: 1003208.5s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.657s, learning 0.262s)
               Value function loss: 54005.2095
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 1091.16
               Mean episode length: 59.43
                  Mean reward/step: 14.42
       Mean episode length/episode: 7.11
            Mean episode successes: 1.0649
Mean episode consecutive_successes: 2.9837
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 8.92s
                        Total time: 22397.48s
                               ETA: 1003138.4s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.577s, learning 0.267s)
               Value function loss: 47422.3365
                    Surrogate loss: -0.0163
             Mean action noise std: 0.74
                       Mean reward: 534.82
               Mean episode length: 57.47
                  Mean reward/step: 13.56
       Mean episode length/episode: 7.18
            Mean episode successes: 1.0806
Mean episode consecutive_successes: 2.9873
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 8.84s
                        Total time: 22406.32s
                               ETA: 1003064.9s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.495s, learning 0.204s)
               Value function loss: 53303.4537
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 865.51
               Mean episode length: 57.26
                  Mean reward/step: 12.93
       Mean episode length/episode: 7.15
            Mean episode successes: 1.0723
Mean episode consecutive_successes: 3.0016
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 8.70s
                        Total time: 22415.02s
                               ETA: 1002985.1s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.795s, learning 0.292s)
               Value function loss: 51528.6208
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 635.57
               Mean episode length: 58.00
                  Mean reward/step: 14.35
       Mean episode length/episode: 7.08
            Mean episode successes: 1.1860
Mean episode consecutive_successes: 2.9505
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 9.09s
                        Total time: 22424.11s
                               ETA: 1002922.6s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.892s, learning 0.265s)
               Value function loss: 52268.3630
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 1100.75
               Mean episode length: 58.25
                  Mean reward/step: 13.99
       Mean episode length/episode: 7.15
            Mean episode successes: 1.1411
Mean episode consecutive_successes: 2.9979
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 9.16s
                        Total time: 22433.27s
                               ETA: 1002863.4s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.564s, learning 0.239s)
               Value function loss: 53935.9754
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 930.55
               Mean episode length: 58.11
                  Mean reward/step: 12.61
       Mean episode length/episode: 7.15
            Mean episode successes: 1.1201
Mean episode consecutive_successes: 3.0215
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 8.80s
                        Total time: 22442.07s
                               ETA: 1002788.3s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.381s, learning 0.197s)
               Value function loss: 57699.8370
                    Surrogate loss: -0.0164
             Mean action noise std: 0.74
                       Mean reward: 441.86
               Mean episode length: 58.78
                  Mean reward/step: 11.21
       Mean episode length/episode: 7.19
            Mean episode successes: 1.0625
Mean episode consecutive_successes: 2.9871
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 8.58s
                        Total time: 22450.65s
                               ETA: 1002703.3s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.429s, learning 0.370s)
               Value function loss: 78960.7615
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 986.32
               Mean episode length: 59.01
                  Mean reward/step: 13.20
       Mean episode length/episode: 7.19
            Mean episode successes: 1.0537
Mean episode consecutive_successes: 3.0237
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 8.80s
                        Total time: 22459.45s
                               ETA: 1002628.2s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.229s, learning 0.196s)
               Value function loss: 71406.9540
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 848.50
               Mean episode length: 58.04
                  Mean reward/step: 13.93
       Mean episode length/episode: 7.09
            Mean episode successes: 1.0889
Mean episode consecutive_successes: 3.0295
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 8.42s
                        Total time: 22467.87s
                               ETA: 1002536.5s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.700s, learning 0.344s)
               Value function loss: 57030.1073
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 461.74
               Mean episode length: 57.96
                  Mean reward/step: 14.96
       Mean episode length/episode: 7.17
            Mean episode successes: 1.1055
Mean episode consecutive_successes: 3.0034
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 9.04s
                        Total time: 22476.92s
                               ETA: 1002472.5s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.452s, learning 0.240s)
               Value function loss: 64222.9015
                    Surrogate loss: 0.0107
             Mean action noise std: 0.74
                       Mean reward: 541.92
               Mean episode length: 56.37
                  Mean reward/step: 14.95
       Mean episode length/episode: 7.11
            Mean episode successes: 1.2041
Mean episode consecutive_successes: 2.9855
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 8.69s
                        Total time: 22485.61s
                               ETA: 1002392.8s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1758 steps/s (collection: 9.029s, learning 0.286s)
               Value function loss: 57378.1384
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 661.22
               Mean episode length: 55.39
                  Mean reward/step: 14.31
       Mean episode length/episode: 7.15
            Mean episode successes: 1.2412
Mean episode consecutive_successes: 2.9865
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 9.32s
                        Total time: 22494.92s
                               ETA: 1002340.9s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.440s, learning 0.211s)
               Value function loss: 62020.6020
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 954.59
               Mean episode length: 60.01
                  Mean reward/step: 15.65
       Mean episode length/episode: 7.11
            Mean episode successes: 1.2490
Mean episode consecutive_successes: 3.0189
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 8.65s
                        Total time: 22503.57s
                               ETA: 1002259.6s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.686s, learning 0.239s)
               Value function loss: 58842.3168
                    Surrogate loss: 0.0061
             Mean action noise std: 0.74
                       Mean reward: 1097.57
               Mean episode length: 59.06
                  Mean reward/step: 14.54
       Mean episode length/episode: 7.07
            Mean episode successes: 1.1885
Mean episode consecutive_successes: 3.0631
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 8.92s
                        Total time: 22512.50s
                               ETA: 1002190.4s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.437s, learning 0.359s)
               Value function loss: 57742.8297
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 978.74
               Mean episode length: 59.84
                  Mean reward/step: 15.45
       Mean episode length/episode: 7.24
            Mean episode successes: 1.2134
Mean episode consecutive_successes: 3.1149
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 8.80s
                        Total time: 22521.30s
                               ETA: 1002115.7s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1729 steps/s (collection: 9.041s, learning 0.433s)
               Value function loss: 52323.7495
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 911.89
               Mean episode length: 56.65
                  Mean reward/step: 14.77
       Mean episode length/episode: 7.17
            Mean episode successes: 1.2002
Mean episode consecutive_successes: 3.1621
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 9.47s
                        Total time: 22530.77s
                               ETA: 1002071.1s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1772 steps/s (collection: 8.849s, learning 0.393s)
               Value function loss: 55018.7091
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 634.24
               Mean episode length: 54.92
                  Mean reward/step: 12.39
       Mean episode length/episode: 7.09
            Mean episode successes: 1.0928
Mean episode consecutive_successes: 3.1628
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 9.24s
                        Total time: 22540.01s
                               ETA: 1002016.2s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.511s, learning 0.177s)
               Value function loss: 53535.7454
                    Surrogate loss: 0.0035
             Mean action noise std: 0.74
                       Mean reward: 951.90
               Mean episode length: 59.98
                  Mean reward/step: 12.74
       Mean episode length/episode: 7.20
            Mean episode successes: 1.0635
Mean episode consecutive_successes: 3.2102
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 8.69s
                        Total time: 22548.70s
                               ETA: 1001936.8s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.754s, learning 0.328s)
               Value function loss: 49354.8416
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 851.99
               Mean episode length: 60.68
                  Mean reward/step: 12.61
       Mean episode length/episode: 7.06
            Mean episode successes: 0.9468
Mean episode consecutive_successes: 3.1842
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 9.08s
                        Total time: 22557.78s
                               ETA: 1001874.9s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.800s, learning 0.362s)
               Value function loss: 44150.7617
                    Surrogate loss: 0.0005
             Mean action noise std: 0.74
                       Mean reward: 829.62
               Mean episode length: 58.81
                  Mean reward/step: 11.74
       Mean episode length/episode: 7.15
            Mean episode successes: 1.0303
Mean episode consecutive_successes: 3.1369
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 9.16s
                        Total time: 22566.94s
                               ETA: 1001816.6s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.350s, learning 0.187s)
               Value function loss: 44732.6487
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 781.22
               Mean episode length: 60.38
                  Mean reward/step: 11.58
       Mean episode length/episode: 7.24
            Mean episode successes: 0.9922
Mean episode consecutive_successes: 3.1284
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 8.54s
                        Total time: 22575.48s
                               ETA: 1001730.6s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.244s, learning 0.170s)
               Value function loss: 45389.2680
                    Surrogate loss: -0.0180
             Mean action noise std: 0.74
                       Mean reward: 755.92
               Mean episode length: 60.41
                  Mean reward/step: 12.36
       Mean episode length/episode: 7.10
            Mean episode successes: 1.0288
Mean episode consecutive_successes: 3.0742
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 8.41s
                        Total time: 22583.89s
                               ETA: 1001639.3s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.786s, learning 0.287s)
               Value function loss: 54233.1884
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 891.09
               Mean episode length: 58.60
                  Mean reward/step: 13.19
       Mean episode length/episode: 7.14
            Mean episode successes: 1.0464
Mean episode consecutive_successes: 3.0625
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 9.07s
                        Total time: 22592.97s
                               ETA: 1001577.2s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.436s, learning 0.229s)
               Value function loss: 66183.7152
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 439.12
               Mean episode length: 57.03
                  Mean reward/step: 15.06
       Mean episode length/episode: 7.19
            Mean episode successes: 1.1748
Mean episode consecutive_successes: 2.9901
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 8.67s
                        Total time: 22601.63s
                               ETA: 1001497.1s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.585s, learning 0.214s)
               Value function loss: 60725.0289
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 890.88
               Mean episode length: 58.27
                  Mean reward/step: 14.55
       Mean episode length/episode: 7.09
            Mean episode successes: 1.2051
Mean episode consecutive_successes: 3.0204
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 8.80s
                        Total time: 22610.43s
                               ETA: 1001423.0s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.598s, learning 0.224s)
               Value function loss: 60105.0886
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 802.35
               Mean episode length: 56.80
                  Mean reward/step: 13.56
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1592
Mean episode consecutive_successes: 3.0570
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 8.82s
                        Total time: 22619.25s
                               ETA: 1001350.0s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.581s, learning 0.189s)
               Value function loss: 47166.5237
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 652.77
               Mean episode length: 59.20
                  Mean reward/step: 13.79
       Mean episode length/episode: 7.08
            Mean episode successes: 1.1187
Mean episode consecutive_successes: 3.0883
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 8.77s
                        Total time: 22628.02s
                               ETA: 1001274.7s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.314s, learning 0.173s)
               Value function loss: 47938.8738
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 959.65
               Mean episode length: 59.71
                  Mean reward/step: 15.24
       Mean episode length/episode: 7.13
            Mean episode successes: 1.1406
Mean episode consecutive_successes: 3.1340
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 8.49s
                        Total time: 22636.51s
                               ETA: 1001187.0s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.346s, learning 0.176s)
               Value function loss: 54281.1358
                    Surrogate loss: 0.0070
             Mean action noise std: 0.74
                       Mean reward: 721.31
               Mean episode length: 55.37
                  Mean reward/step: 14.52
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1899
Mean episode consecutive_successes: 3.0888
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 8.52s
                        Total time: 22645.03s
                               ETA: 1001100.9s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.207s, learning 0.174s)
               Value function loss: 47724.7884
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 874.52
               Mean episode length: 56.35
                  Mean reward/step: 14.63
       Mean episode length/episode: 7.24
            Mean episode successes: 1.2100
Mean episode consecutive_successes: 3.1329
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 8.38s
                        Total time: 22653.41s
                               ETA: 1001008.6s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.375s, learning 0.319s)
               Value function loss: 49985.3086
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 1537.73
               Mean episode length: 59.14
                  Mean reward/step: 14.34
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1011
Mean episode consecutive_successes: 3.2070
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 8.69s
                        Total time: 22662.11s
                               ETA: 1000930.3s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.470s, learning 0.215s)
               Value function loss: 51496.2658
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 935.32
               Mean episode length: 58.82
                  Mean reward/step: 13.38
       Mean episode length/episode: 7.17
            Mean episode successes: 1.1587
Mean episode consecutive_successes: 3.2098
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 8.68s
                        Total time: 22670.79s
                               ETA: 1000851.6s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.550s, learning 0.382s)
               Value function loss: 88438.4410
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 475.95
               Mean episode length: 59.68
                  Mean reward/step: 13.89
       Mean episode length/episode: 7.15
            Mean episode successes: 1.0811
Mean episode consecutive_successes: 3.1864
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 8.93s
                        Total time: 22679.73s
                               ETA: 1000783.8s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.779s, learning 0.167s)
               Value function loss: 83470.8191
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 712.89
               Mean episode length: 59.82
                  Mean reward/step: 15.00
       Mean episode length/episode: 7.19
            Mean episode successes: 1.1997
Mean episode consecutive_successes: 3.1776
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 8.95s
                        Total time: 22688.67s
                               ETA: 1000716.7s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.716s, learning 0.252s)
               Value function loss: 53106.2473
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 726.42
               Mean episode length: 61.48
                  Mean reward/step: 13.56
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1519
Mean episode consecutive_successes: 3.1983
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 8.97s
                        Total time: 22697.64s
                               ETA: 1000650.7s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.586s, learning 0.288s)
               Value function loss: 43681.2465
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 854.24
               Mean episode length: 56.28
                  Mean reward/step: 13.74
       Mean episode length/episode: 7.16
            Mean episode successes: 1.1543
Mean episode consecutive_successes: 3.2067
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 8.87s
                        Total time: 22706.51s
                               ETA: 1000580.5s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.518s, learning 0.236s)
               Value function loss: 46616.2099
                    Surrogate loss: -0.0001
             Mean action noise std: 0.74
                       Mean reward: 1073.66
               Mean episode length: 58.86
                  Mean reward/step: 12.98
       Mean episode length/episode: 7.11
            Mean episode successes: 1.0342
Mean episode consecutive_successes: 3.2292
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 8.75s
                        Total time: 22715.27s
                               ETA: 1000505.2s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.698s, learning 0.186s)
               Value function loss: 45066.2897
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 808.64
               Mean episode length: 58.81
                  Mean reward/step: 13.49
       Mean episode length/episode: 7.10
            Mean episode successes: 1.0093
Mean episode consecutive_successes: 3.2289
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 8.88s
                        Total time: 22724.15s
                               ETA: 1000435.6s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.528s, learning 0.189s)
               Value function loss: 45791.5449
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 673.54
               Mean episode length: 60.52
                  Mean reward/step: 13.75
       Mean episode length/episode: 7.19
            Mean episode successes: 1.0400
Mean episode consecutive_successes: 3.2005
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 8.72s
                        Total time: 22732.87s
                               ETA: 1000358.8s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.904s, learning 0.187s)
               Value function loss: 45794.6236
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 1097.30
               Mean episode length: 60.45
                  Mean reward/step: 13.67
       Mean episode length/episode: 7.16
            Mean episode successes: 1.0962
Mean episode consecutive_successes: 3.2054
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 9.09s
                        Total time: 22741.96s
                               ETA: 1000298.4s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.760s, learning 0.170s)
               Value function loss: 47395.3495
                    Surrogate loss: 0.0107
             Mean action noise std: 0.74
                       Mean reward: 665.82
               Mean episode length: 60.62
                  Mean reward/step: 12.51
       Mean episode length/episode: 7.16
            Mean episode successes: 1.0679
Mean episode consecutive_successes: 3.1600
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 8.93s
                        Total time: 22750.89s
                               ETA: 1000231.0s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.900s, learning 0.187s)
               Value function loss: 48409.4177
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 1055.95
               Mean episode length: 57.88
                  Mean reward/step: 12.44
       Mean episode length/episode: 7.12
            Mean episode successes: 1.0107
Mean episode consecutive_successes: 3.1803
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 9.09s
                        Total time: 22759.98s
                               ETA: 1000170.5s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.432s, learning 0.176s)
               Value function loss: 43312.7371
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 928.38
               Mean episode length: 58.61
                  Mean reward/step: 11.72
       Mean episode length/episode: 7.13
            Mean episode successes: 0.9302
Mean episode consecutive_successes: 3.1672
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 8.61s
                        Total time: 22768.59s
                               ETA: 1000089.1s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.791s, learning 0.232s)
               Value function loss: 45333.2496
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: 435.73
               Mean episode length: 56.50
                  Mean reward/step: 11.96
       Mean episode length/episode: 7.16
            Mean episode successes: 0.9429
Mean episode consecutive_successes: 3.1258
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 9.02s
                        Total time: 22777.61s
                               ETA: 1000026.0s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.429s, learning 0.164s)
               Value function loss: 44078.8010
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 521.58
               Mean episode length: 57.98
                  Mean reward/step: 13.25
       Mean episode length/episode: 7.06
            Mean episode successes: 0.9810
Mean episode consecutive_successes: 3.0617
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 8.59s
                        Total time: 22786.20s
                               ETA: 999944.0s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.524s, learning 0.216s)
               Value function loss: 44625.4282
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 506.78
               Mean episode length: 57.02
                  Mean reward/step: 13.87
       Mean episode length/episode: 7.18
            Mean episode successes: 1.0996
Mean episode consecutive_successes: 3.0287
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 8.74s
                        Total time: 22794.94s
                               ETA: 999868.6s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.945s, learning 0.176s)
               Value function loss: 45498.4108
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 696.21
               Mean episode length: 56.68
                  Mean reward/step: 14.04
       Mean episode length/episode: 7.15
            Mean episode successes: 1.1484
Mean episode consecutive_successes: 3.0255
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 9.12s
                        Total time: 22804.06s
                               ETA: 999809.9s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.452s, learning 0.206s)
               Value function loss: 55642.8714
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 912.02
               Mean episode length: 60.28
                  Mean reward/step: 13.51
       Mean episode length/episode: 7.11
            Mean episode successes: 1.0610
Mean episode consecutive_successes: 3.0546
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 8.66s
                        Total time: 22812.72s
                               ETA: 999730.9s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.048s, learning 0.168s)
               Value function loss: 63892.2689
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 770.36
               Mean episode length: 59.15
                  Mean reward/step: 14.47
       Mean episode length/episode: 7.22
            Mean episode successes: 1.1279
Mean episode consecutive_successes: 3.0784
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 9.22s
                        Total time: 22821.94s
                               ETA: 999676.5s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.363s, learning 0.239s)
               Value function loss: 64634.3964
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 737.59
               Mean episode length: 58.66
                  Mean reward/step: 13.66
       Mean episode length/episode: 7.13
            Mean episode successes: 1.0698
Mean episode consecutive_successes: 3.0697
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 8.60s
                        Total time: 22830.54s
                               ETA: 999595.2s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.750s, learning 0.209s)
               Value function loss: 49506.1783
                    Surrogate loss: 0.0025
             Mean action noise std: 0.74
                       Mean reward: 861.26
               Mean episode length: 57.38
                  Mean reward/step: 13.32
       Mean episode length/episode: 7.12
            Mean episode successes: 1.0581
Mean episode consecutive_successes: 3.1199
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 8.96s
                        Total time: 22839.50s
                               ETA: 999529.6s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.669s, learning 0.263s)
               Value function loss: 49379.2366
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 1089.87
               Mean episode length: 56.78
                  Mean reward/step: 13.93
       Mean episode length/episode: 7.04
            Mean episode successes: 1.0391
Mean episode consecutive_successes: 3.1238
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 8.93s
                        Total time: 22848.43s
                               ETA: 999462.9s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.447s, learning 0.209s)
               Value function loss: 45076.8788
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 697.84
               Mean episode length: 54.70
                  Mean reward/step: 12.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.9526
Mean episode consecutive_successes: 3.1348
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 8.66s
                        Total time: 22857.09s
                               ETA: 999384.2s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.464s, learning 0.224s)
               Value function loss: 46639.1299
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 1102.51
               Mean episode length: 63.15
                  Mean reward/step: 12.12
       Mean episode length/episode: 7.15
            Mean episode successes: 1.0161
Mean episode consecutive_successes: 3.1018
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 8.69s
                        Total time: 22865.77s
                               ETA: 999306.9s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.667s, learning 0.172s)
               Value function loss: 44208.5611
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 1175.08
               Mean episode length: 60.02
                  Mean reward/step: 11.88
       Mean episode length/episode: 7.25
            Mean episode successes: 0.9932
Mean episode consecutive_successes: 3.1215
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 8.84s
                        Total time: 22874.61s
                               ETA: 999236.2s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.323s, learning 0.355s)
               Value function loss: 42761.1087
                    Surrogate loss: -0.0166
             Mean action noise std: 0.74
                       Mean reward: 511.24
               Mean episode length: 58.27
                  Mean reward/step: 12.35
       Mean episode length/episode: 7.03
            Mean episode successes: 0.9463
Mean episode consecutive_successes: 3.0742
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 8.68s
                        Total time: 22883.29s
                               ETA: 999158.7s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.215s, learning 0.261s)
               Value function loss: 48895.6340
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 606.54
               Mean episode length: 59.01
                  Mean reward/step: 11.86
       Mean episode length/episode: 7.20
            Mean episode successes: 0.9561
Mean episode consecutive_successes: 3.0590
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 8.48s
                        Total time: 22891.77s
                               ETA: 999072.3s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.352s, learning 0.177s)
               Value function loss: 50603.0484
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 737.54
               Mean episode length: 58.16
                  Mean reward/step: 13.13
       Mean episode length/episode: 7.09
            Mean episode successes: 1.0410
Mean episode consecutive_successes: 3.0064
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 8.53s
                        Total time: 22900.29s
                               ETA: 998988.3s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.246s, learning 0.165s)
               Value function loss: 75419.4715
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 504.38
               Mean episode length: 56.74
                  Mean reward/step: 13.54
       Mean episode length/episode: 7.11
            Mean episode successes: 1.1011
Mean episode consecutive_successes: 2.9818
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 8.41s
                        Total time: 22908.71s
                               ETA: 998899.3s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.296s, learning 0.184s)
               Value function loss: 61382.0859
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 881.17
               Mean episode length: 57.88
                  Mean reward/step: 13.32
       Mean episode length/episode: 7.21
            Mean episode successes: 1.0581
Mean episode consecutive_successes: 3.0313
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 8.48s
                        Total time: 22917.19s
                               ETA: 998813.3s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.596s, learning 0.164s)
               Value function loss: 58177.3029
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: 852.49
               Mean episode length: 61.90
                  Mean reward/step: 12.22
       Mean episode length/episode: 7.14
            Mean episode successes: 0.9912
Mean episode consecutive_successes: 3.0406
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 8.76s
                        Total time: 22925.95s
                               ETA: 998739.6s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.780s, learning 0.182s)
               Value function loss: 44188.7805
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 812.42
               Mean episode length: 56.78
                  Mean reward/step: 11.79
       Mean episode length/episode: 7.16
            Mean episode successes: 0.9443
Mean episode consecutive_successes: 3.0566
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 8.96s
                        Total time: 22934.91s
                               ETA: 998674.7s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.482s, learning 0.199s)
               Value function loss: 44968.6156
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 789.60
               Mean episode length: 59.09
                  Mean reward/step: 12.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.9478
Mean episode consecutive_successes: 3.0338
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 8.68s
                        Total time: 22943.59s
                               ETA: 998597.7s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.163s, learning 0.249s)
               Value function loss: 48933.4876
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 896.94
               Mean episode length: 58.85
                  Mean reward/step: 12.13
       Mean episode length/episode: 7.19
            Mean episode successes: 0.8862
Mean episode consecutive_successes: 3.0390
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 8.41s
                        Total time: 22952.00s
                               ETA: 998509.0s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.659s, learning 0.325s)
               Value function loss: 48088.5744
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 739.66
               Mean episode length: 59.25
                  Mean reward/step: 12.46
       Mean episode length/episode: 7.14
            Mean episode successes: 0.9492
Mean episode consecutive_successes: 2.9873
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 8.98s
                        Total time: 22960.98s
                               ETA: 998445.3s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.177s)
               Value function loss: 47085.0985
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 605.33
               Mean episode length: 57.25
                  Mean reward/step: 13.33
       Mean episode length/episode: 7.22
            Mean episode successes: 1.0742
Mean episode consecutive_successes: 2.9201
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 8.56s
                        Total time: 22969.55s
                               ETA: 998363.4s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.525s, learning 0.180s)
               Value function loss: 59203.5762
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 694.85
               Mean episode length: 61.81
                  Mean reward/step: 13.79
       Mean episode length/episode: 7.13
            Mean episode successes: 1.1460
Mean episode consecutive_successes: 2.9274
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 8.70s
                        Total time: 22978.25s
                               ETA: 998287.6s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.827s, learning 0.193s)
               Value function loss: 47675.5903
                    Surrogate loss: -0.0033
             Mean action noise std: 0.74
                       Mean reward: 921.40
               Mean episode length: 59.63
                  Mean reward/step: 13.46
       Mean episode length/episode: 7.07
            Mean episode successes: 1.0117
Mean episode consecutive_successes: 2.9710
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 9.02s
                        Total time: 22987.27s
                               ETA: 998225.6s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.337s, learning 0.239s)
               Value function loss: 42577.0839
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 605.64
               Mean episode length: 59.41
                  Mean reward/step: 12.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.9849
Mean episode consecutive_successes: 2.9670
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 8.58s
                        Total time: 22995.85s
                               ETA: 998144.4s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.521s, learning 0.182s)
               Value function loss: 43499.1525
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 812.87
               Mean episode length: 58.39
                  Mean reward/step: 12.64
       Mean episode length/episode: 7.22
            Mean episode successes: 1.0439
Mean episode consecutive_successes: 2.9436
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 8.70s
                        Total time: 23004.55s
                               ETA: 998068.7s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.722s, learning 0.176s)
               Value function loss: 48699.6478
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 969.32
               Mean episode length: 59.85
                  Mean reward/step: 13.85
       Mean episode length/episode: 7.16
            Mean episode successes: 1.0459
Mean episode consecutive_successes: 2.9688
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 8.90s
                        Total time: 23013.45s
                               ETA: 998001.6s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.544s, learning 0.182s)
               Value function loss: 46577.5893
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 632.88
               Mean episode length: 57.79
                  Mean reward/step: 13.41
       Mean episode length/episode: 7.18
            Mean episode successes: 1.1289
Mean episode consecutive_successes: 2.9274
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 8.73s
                        Total time: 23022.17s
                               ETA: 997927.0s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.813s, learning 0.170s)
               Value function loss: 51128.1881
                    Surrogate loss: 0.0016
             Mean action noise std: 0.74
                       Mean reward: 1070.64
               Mean episode length: 59.10
                  Mean reward/step: 15.21
       Mean episode length/episode: 7.17
            Mean episode successes: 1.1904
Mean episode consecutive_successes: 2.9687
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 8.98s
                        Total time: 23031.16s
                               ETA: 997863.7s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.412s, learning 0.276s)
               Value function loss: 49637.4998
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 665.71
               Mean episode length: 58.22
                  Mean reward/step: 14.46
       Mean episode length/episode: 7.19
            Mean episode successes: 1.2104
Mean episode consecutive_successes: 2.9585
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 8.69s
                        Total time: 23039.85s
                               ETA: 997787.6s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.654s, learning 0.193s)
               Value function loss: 46864.0340
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 740.08
               Mean episode length: 57.92
                  Mean reward/step: 14.10
       Mean episode length/episode: 7.12
            Mean episode successes: 1.1470
Mean episode consecutive_successes: 3.0004
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 8.85s
                        Total time: 23048.69s
                               ETA: 997718.5s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.221s, learning 0.246s)
               Value function loss: 53509.9992
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 671.18
               Mean episode length: 60.59
                  Mean reward/step: 14.74
       Mean episode length/episode: 7.12
            Mean episode successes: 1.1938
Mean episode consecutive_successes: 2.9817
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 8.47s
                        Total time: 23057.16s
                               ETA: 997633.0s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.310s, learning 0.177s)
               Value function loss: 49232.7674
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 891.18
               Mean episode length: 59.83
                  Mean reward/step: 13.28
       Mean episode length/episode: 7.13
            Mean episode successes: 1.1084
Mean episode consecutive_successes: 3.0696
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 8.49s
                        Total time: 23065.65s
                               ETA: 997548.4s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.066s, learning 0.183s)
               Value function loss: 44462.3634
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 550.42
               Mean episode length: 58.51
                  Mean reward/step: 12.30
       Mean episode length/episode: 7.16
            Mean episode successes: 1.0239
Mean episode consecutive_successes: 3.0646
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 8.25s
                        Total time: 23073.90s
                               ETA: 997453.6s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.194s, learning 0.209s)
               Value function loss: 47275.9836
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 887.34
               Mean episode length: 58.17
                  Mean reward/step: 12.61
       Mean episode length/episode: 7.24
            Mean episode successes: 1.0864
Mean episode consecutive_successes: 3.0548
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 8.40s
                        Total time: 23082.30s
                               ETA: 997365.5s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.059s, learning 0.383s)
               Value function loss: 51685.1484
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 781.49
               Mean episode length: 61.13
                  Mean reward/step: 14.67
       Mean episode length/episode: 7.10
            Mean episode successes: 1.0737
Mean episode consecutive_successes: 3.0686
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 8.44s
                        Total time: 23090.74s
                               ETA: 997279.2s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.773s, learning 0.173s)
               Value function loss: 52353.4724
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 973.84
               Mean episode length: 58.96
                  Mean reward/step: 12.02
       Mean episode length/episode: 7.10
            Mean episode successes: 1.0244
Mean episode consecutive_successes: 3.0830
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 8.95s
                        Total time: 23099.69s
                               ETA: 997214.7s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.394s, learning 0.170s)
               Value function loss: 46331.6346
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 587.50
               Mean episode length: 57.94
                  Mean reward/step: 11.33
       Mean episode length/episode: 7.25
            Mean episode successes: 1.0098
Mean episode consecutive_successes: 3.0687
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 8.56s
                        Total time: 23108.25s
                               ETA: 997133.7s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.613s, learning 0.179s)
               Value function loss: 53497.7712
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 912.29
               Mean episode length: 59.02
                  Mean reward/step: 13.21
       Mean episode length/episode: 7.15
            Mean episode successes: 0.9619
Mean episode consecutive_successes: 3.1236
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 8.79s
                        Total time: 23117.04s
                               ETA: 997062.7s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.464s, learning 0.168s)
               Value function loss: 49086.6876
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 799.84
               Mean episode length: 60.93
                  Mean reward/step: 12.30
       Mean episode length/episode: 7.14
            Mean episode successes: 0.9717
Mean episode consecutive_successes: 3.0826
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 8.63s
                        Total time: 23125.67s
                               ETA: 996984.9s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.539s, learning 0.169s)
               Value function loss: 43192.4146
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 532.46
               Mean episode length: 58.22
                  Mean reward/step: 11.56
       Mean episode length/episode: 7.16
            Mean episode successes: 0.9600
Mean episode consecutive_successes: 3.0484
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 8.71s
                        Total time: 23134.38s
                               ETA: 996910.3s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.482s, learning 0.177s)
               Value function loss: 48877.3038
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 763.22
               Mean episode length: 60.75
                  Mean reward/step: 12.71
       Mean episode length/episode: 7.16
            Mean episode successes: 1.0425
Mean episode consecutive_successes: 2.9917
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 8.66s
                        Total time: 23143.04s
                               ETA: 996833.7s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.289s, learning 0.178s)
               Value function loss: 48420.0753
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: 665.74
               Mean episode length: 59.22
                  Mean reward/step: 14.45
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1216
Mean episode consecutive_successes: 2.9914
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 8.47s
                        Total time: 23151.51s
                               ETA: 996749.0s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.546s, learning 0.195s)
               Value function loss: 49803.9849
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 954.99
               Mean episode length: 60.33
                  Mean reward/step: 12.84
       Mean episode length/episode: 7.18
            Mean episode successes: 1.1064
Mean episode consecutive_successes: 3.0020
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 8.74s
                        Total time: 23160.25s
                               ETA: 996676.0s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.595s, learning 0.185s)
               Value function loss: 49038.7495
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 589.68
               Mean episode length: 59.62
                  Mean reward/step: 14.02
       Mean episode length/episode: 7.17
            Mean episode successes: 1.1792
Mean episode consecutive_successes: 2.9876
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 8.78s
                        Total time: 23169.03s
                               ETA: 996604.8s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.388s, learning 0.185s)
               Value function loss: 54676.1284
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 1463.58
               Mean episode length: 61.59
                  Mean reward/step: 14.78
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1860
Mean episode consecutive_successes: 3.0691
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 8.57s
                        Total time: 23177.60s
                               ETA: 996524.8s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.308s, learning 0.181s)
               Value function loss: 48254.0607
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 751.08
               Mean episode length: 61.07
                  Mean reward/step: 13.37
       Mean episode length/episode: 7.19
            Mean episode successes: 1.0220
Mean episode consecutive_successes: 3.1316
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 8.49s
                        Total time: 23186.09s
                               ETA: 996441.2s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.459s, learning 0.177s)
               Value function loss: 51305.0029
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 684.51
               Mean episode length: 56.64
                  Mean reward/step: 13.36
       Mean episode length/episode: 7.10
            Mean episode successes: 1.1240
Mean episode consecutive_successes: 3.0801
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 8.64s
                        Total time: 23194.73s
                               ETA: 996363.9s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.775s, learning 0.169s)
               Value function loss: 54857.3120
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 1059.33
               Mean episode length: 59.82
                  Mean reward/step: 12.12
       Mean episode length/episode: 7.14
            Mean episode successes: 0.9492
Mean episode consecutive_successes: 3.1523
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 8.94s
                        Total time: 23203.67s
                               ETA: 996300.0s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.177s, learning 0.166s)
               Value function loss: 51254.0004
                    Surrogate loss: 0.0074
             Mean action noise std: 0.74
                       Mean reward: 350.53
               Mean episode length: 61.09
                  Mean reward/step: 12.31
       Mean episode length/episode: 7.18
            Mean episode successes: 0.8994
Mean episode consecutive_successes: 3.1524
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 8.34s
                        Total time: 23212.01s
                               ETA: 996210.3s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.378s, learning 0.174s)
               Value function loss: 51588.2188
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 702.18
               Mean episode length: 58.01
                  Mean reward/step: 13.30
       Mean episode length/episode: 7.22
            Mean episode successes: 0.9961
Mean episode consecutive_successes: 3.1326
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 8.55s
                        Total time: 23220.57s
                               ETA: 996129.6s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.504s, learning 0.164s)
               Value function loss: 51412.1848
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 554.98
               Mean episode length: 58.79
                  Mean reward/step: 12.77
       Mean episode length/episode: 7.13
            Mean episode successes: 1.0171
Mean episode consecutive_successes: 3.0970
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 8.67s
                        Total time: 23229.23s
                               ETA: 996054.0s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.532s, learning 0.263s)
               Value function loss: 51369.4844
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 863.43
               Mean episode length: 58.91
                  Mean reward/step: 14.57
       Mean episode length/episode: 7.17
            Mean episode successes: 1.0952
Mean episode consecutive_successes: 3.0766
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 8.79s
                        Total time: 23238.03s
                               ETA: 995983.9s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.599s, learning 0.172s)
               Value function loss: 47919.9674
                    Surrogate loss: -0.0127
             Mean action noise std: 0.74
                       Mean reward: 792.87
               Mean episode length: 57.48
                  Mean reward/step: 15.73
       Mean episode length/episode: 7.08
            Mean episode successes: 1.1294
Mean episode consecutive_successes: 3.0579
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 8.77s
                        Total time: 23246.80s
                               ETA: 995912.8s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.694s, learning 0.258s)
               Value function loss: 53182.2150
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 994.80
               Mean episode length: 61.76
                  Mean reward/step: 14.89
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1929
Mean episode consecutive_successes: 3.0602
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 8.95s
                        Total time: 23255.75s
                               ETA: 995849.6s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.396s, learning 0.167s)
               Value function loss: 43541.9675
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 1167.06
               Mean episode length: 59.41
                  Mean reward/step: 13.47
       Mean episode length/episode: 7.12
            Mean episode successes: 1.1768
Mean episode consecutive_successes: 3.1091
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 8.56s
                        Total time: 23264.32s
                               ETA: 995769.8s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.561s, learning 0.175s)
               Value function loss: 52798.8978
                    Surrogate loss: -0.0148
             Mean action noise std: 0.74
                       Mean reward: 722.45
               Mean episode length: 57.16
                  Mean reward/step: 14.61
       Mean episode length/episode: 7.09
            Mean episode successes: 1.1113
Mean episode consecutive_successes: 3.1215
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 8.74s
                        Total time: 23273.05s
                               ETA: 995697.4s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.422s, learning 0.177s)
               Value function loss: 52056.5084
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 891.78
               Mean episode length: 59.16
                  Mean reward/step: 14.49
       Mean episode length/episode: 7.20
            Mean episode successes: 1.0649
Mean episode consecutive_successes: 3.1762
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 8.60s
                        Total time: 23281.65s
                               ETA: 995619.2s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.550s, learning 0.165s)
               Value function loss: 55857.1742
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 619.45
               Mean episode length: 57.77
                  Mean reward/step: 14.57
       Mean episode length/episode: 7.13
            Mean episode successes: 1.1104
Mean episode consecutive_successes: 3.1732
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 8.72s
                        Total time: 23290.37s
                               ETA: 995546.0s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.251s, learning 0.173s)
               Value function loss: 53942.3213
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 764.49
               Mean episode length: 56.47
                  Mean reward/step: 14.16
       Mean episode length/episode: 7.16
            Mean episode successes: 1.1548
Mean episode consecutive_successes: 3.1730
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 8.42s
                        Total time: 23298.79s
                               ETA: 995460.4s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.103s, learning 0.175s)
               Value function loss: 54607.4005
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: 1272.41
               Mean episode length: 56.06
                  Mean reward/step: 14.90
       Mean episode length/episode: 7.18
            Mean episode successes: 1.2173
Mean episode consecutive_successes: 3.1902
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 8.28s
                        Total time: 23307.07s
                               ETA: 995368.7s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.291s, learning 0.191s)
               Value function loss: 50301.0482
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 639.90
               Mean episode length: 58.11
                  Mean reward/step: 13.62
       Mean episode length/episode: 7.25
            Mean episode successes: 1.1851
Mean episode consecutive_successes: 3.1926
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 8.48s
                        Total time: 23315.55s
                               ETA: 995285.7s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.350s, learning 0.190s)
               Value function loss: 53536.0581
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 1125.48
               Mean episode length: 61.29
                  Mean reward/step: 13.72
       Mean episode length/episode: 7.11
            Mean episode successes: 1.1748
Mean episode consecutive_successes: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 8.54s
                        Total time: 23324.09s
                               ETA: 995205.3s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.126s, learning 0.180s)
               Value function loss: 61892.7975
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 1203.87
               Mean episode length: 63.43
                  Mean reward/step: 14.63
       Mean episode length/episode: 7.19
            Mean episode successes: 1.1548
Mean episode consecutive_successes: 3.2230
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 8.31s
                        Total time: 23332.40s
                               ETA: 995115.0s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.640s, learning 0.183s)
               Value function loss: 54811.5537
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 759.03
               Mean episode length: 59.98
                  Mean reward/step: 14.91
       Mean episode length/episode: 7.17
            Mean episode successes: 1.2788
Mean episode consecutive_successes: 3.1902
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 8.82s
                        Total time: 23341.22s
                               ETA: 995046.8s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.170s, learning 0.184s)
               Value function loss: 58158.1627
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 861.20
               Mean episode length: 61.51
                  Mean reward/step: 15.21
       Mean episode length/episode: 7.15
            Mean episode successes: 1.2441
Mean episode consecutive_successes: 3.2205
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 8.35s
                        Total time: 23349.57s
                               ETA: 994958.6s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.418s, learning 0.235s)
               Value function loss: 49242.4572
                    Surrogate loss: -0.0049
             Mean action noise std: 0.74
                       Mean reward: 762.32
               Mean episode length: 57.77
                  Mean reward/step: 14.52
       Mean episode length/episode: 7.09
            Mean episode successes: 1.1743
Mean episode consecutive_successes: 3.2389
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 8.65s
                        Total time: 23358.23s
                               ETA: 994883.3s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.318s, learning 0.201s)
               Value function loss: 42945.4764
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 765.15
               Mean episode length: 59.79
                  Mean reward/step: 12.98
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1528
Mean episode consecutive_successes: 3.2634
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 8.52s
                        Total time: 23366.74s
                               ETA: 994802.2s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.240s, learning 0.189s)
               Value function loss: 52434.5918
                    Surrogate loss: -0.0035
             Mean action noise std: 0.74
                       Mean reward: 760.50
               Mean episode length: 58.45
                  Mean reward/step: 11.92
       Mean episode length/episode: 7.14
            Mean episode successes: 0.9688
Mean episode consecutive_successes: 3.2986
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 8.43s
                        Total time: 23375.17s
                               ETA: 994717.5s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.374s, learning 0.166s)
               Value function loss: 47867.2381
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 570.40
               Mean episode length: 60.48
                  Mean reward/step: 11.88
       Mean episode length/episode: 7.10
            Mean episode successes: 0.9766
Mean episode consecutive_successes: 3.2424
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 8.54s
                        Total time: 23383.71s
                               ETA: 994637.5s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.280s, learning 0.291s)
               Value function loss: 58674.3598
                    Surrogate loss: 0.0045
             Mean action noise std: 0.74
                       Mean reward: 864.07
               Mean episode length: 57.49
                  Mean reward/step: 12.89
       Mean episode length/episode: 7.16
            Mean episode successes: 0.9155
Mean episode consecutive_successes: 3.2691
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 8.57s
                        Total time: 23392.28s
                               ETA: 994558.9s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.325s, learning 0.378s)
               Value function loss: 51699.9410
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 386.41
               Mean episode length: 57.01
                  Mean reward/step: 13.53
       Mean episode length/episode: 7.21
            Mean episode successes: 1.0908
Mean episode consecutive_successes: 3.1681
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 8.70s
                        Total time: 23400.99s
                               ETA: 994486.0s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.509s, learning 0.176s)
               Value function loss: 54163.6200
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 935.43
               Mean episode length: 59.00
                  Mean reward/step: 13.93
       Mean episode length/episode: 7.20
            Mean episode successes: 1.1548
Mean episode consecutive_successes: 3.1625
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 8.69s
                        Total time: 23409.67s
                               ETA: 994412.3s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.346s, learning 0.210s)
               Value function loss: 57672.2974
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 739.78
               Mean episode length: 60.74
                  Mean reward/step: 12.54
       Mean episode length/episode: 7.09
            Mean episode successes: 1.0747
Mean episode consecutive_successes: 3.1690
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 8.56s
                        Total time: 23418.23s
                               ETA: 994333.3s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.760s, learning 0.171s)
               Value function loss: 55311.3882
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 1011.54
               Mean episode length: 60.54
                  Mean reward/step: 12.48
       Mean episode length/episode: 7.22
            Mean episode successes: 1.0435
Mean episode consecutive_successes: 3.2012
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 8.93s
                        Total time: 23427.16s
                               ETA: 994270.2s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.376s, learning 0.173s)
               Value function loss: 52147.2202
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 512.89
               Mean episode length: 60.67
                  Mean reward/step: 13.63
       Mean episode length/episode: 7.09
            Mean episode successes: 1.0649
Mean episode consecutive_successes: 3.1461
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 8.55s
                        Total time: 23435.71s
                               ETA: 994191.1s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.424s, learning 0.229s)
               Value function loss: 62761.1801
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 804.18
               Mean episode length: 60.81
                  Mean reward/step: 15.05
       Mean episode length/episode: 7.15
            Mean episode successes: 1.1123
Mean episode consecutive_successes: 3.1416
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 8.65s
                        Total time: 23444.36s
                               ETA: 994116.3s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.448s, learning 0.168s)
               Value function loss: 56469.6723
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 1076.20
               Mean episode length: 64.31
                  Mean reward/step: 14.93
       Mean episode length/episode: 7.21
            Mean episode successes: 1.2485
Mean episode consecutive_successes: 3.1401
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 8.62s
                        Total time: 23452.98s
                               ETA: 994040.0s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.250s, learning 0.295s)
               Value function loss: 60591.6106
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 1146.01
               Mean episode length: 61.21
                  Mean reward/step: 16.77
       Mean episode length/episode: 7.22
            Mean episode successes: 1.2812
Mean episode consecutive_successes: 3.1476
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 8.54s
                        Total time: 23461.52s
                               ETA: 993960.8s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.368s, learning 0.178s)
               Value function loss: 64113.1932
                    Surrogate loss: -0.0045
             Mean action noise std: 0.74
                       Mean reward: 1226.85
               Mean episode length: 59.83
                  Mean reward/step: 15.49
       Mean episode length/episode: 7.14
            Mean episode successes: 1.3101
Mean episode consecutive_successes: 3.2191
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 8.55s
                        Total time: 23470.07s
                               ETA: 993881.7s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.175s, learning 0.297s)
               Value function loss: 60005.1101
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 1168.01
               Mean episode length: 60.68
                  Mean reward/step: 15.33
       Mean episode length/episode: 7.22
            Mean episode successes: 1.2417
Mean episode consecutive_successes: 3.2924
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 8.47s
                        Total time: 23478.54s
                               ETA: 993799.5s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.399s, learning 0.203s)
               Value function loss: 59058.0331
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 892.42
               Mean episode length: 61.36
                  Mean reward/step: 15.82
       Mean episode length/episode: 7.16
            Mean episode successes: 1.2832
Mean episode consecutive_successes: 3.2854
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 8.60s
                        Total time: 23487.14s
                               ETA: 993722.9s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.329s, learning 0.199s)
               Value function loss: 62207.3271
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 1025.42
               Mean episode length: 62.37
                  Mean reward/step: 14.95
       Mean episode length/episode: 7.15
            Mean episode successes: 1.2012
Mean episode consecutive_successes: 3.3575
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 8.53s
                        Total time: 23495.67s
                               ETA: 993643.1s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.292s, learning 0.221s)
               Value function loss: 66536.7991
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 697.29
               Mean episode length: 61.33
                  Mean reward/step: 14.73
       Mean episode length/episode: 7.14
            Mean episode successes: 1.2295
Mean episode consecutive_successes: 3.3100
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 8.51s
                        Total time: 23504.18s
                               ETA: 993562.9s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.337s, learning 0.181s)
               Value function loss: 72820.0314
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 1153.30
               Mean episode length: 62.68
                  Mean reward/step: 16.39
       Mean episode length/episode: 7.11
            Mean episode successes: 1.3730
Mean episode consecutive_successes: 3.3009
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 8.52s
                        Total time: 23512.70s
                               ETA: 993482.9s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.160s, learning 0.261s)
               Value function loss: 60261.7257
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 1021.76
               Mean episode length: 60.85
                  Mean reward/step: 14.81
       Mean episode length/episode: 7.15
            Mean episode successes: 1.2285
Mean episode consecutive_successes: 3.3770
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 8.42s
                        Total time: 23521.12s
                               ETA: 993398.8s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.184s, learning 0.198s)
               Value function loss: 65680.0022
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 585.61
               Mean episode length: 58.35
                  Mean reward/step: 15.15
       Mean episode length/episode: 7.16
            Mean episode successes: 1.1929
Mean episode consecutive_successes: 3.3958
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 8.38s
                        Total time: 23529.51s
                               ETA: 993313.2s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.660s, learning 0.212s)
               Value function loss: 65269.5669
                    Surrogate loss: -0.0168
             Mean action noise std: 0.74
                       Mean reward: 936.48
               Mean episode length: 59.56
                  Mean reward/step: 16.39
       Mean episode length/episode: 7.32
            Mean episode successes: 1.3457
Mean episode consecutive_successes: 3.4334
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 8.87s
                        Total time: 23538.38s
                               ETA: 993248.3s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.466s, learning 0.260s)
               Value function loss: 69153.6152
                    Surrogate loss: 0.0403
             Mean action noise std: 0.74
                       Mean reward: 1284.19
               Mean episode length: 59.55
                  Mean reward/step: 15.64
       Mean episode length/episode: 7.16
            Mean episode successes: 1.2358
Mean episode consecutive_successes: 3.4904
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 8.73s
                        Total time: 23547.10s
                               ETA: 993177.4s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.225s, learning 0.190s)
               Value function loss: 61092.2891
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 740.18
               Mean episode length: 57.50
                  Mean reward/step: 14.55
       Mean episode length/episode: 7.07
            Mean episode successes: 1.1855
Mean episode consecutive_successes: 3.4632
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 8.42s
                        Total time: 23555.52s
                               ETA: 993093.4s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.388s, learning 0.172s)
               Value function loss: 56864.2973
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 979.79
               Mean episode length: 59.49
                  Mean reward/step: 15.70
       Mean episode length/episode: 7.13
            Mean episode successes: 1.3276
Mean episode consecutive_successes: 3.3963
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 8.56s
                        Total time: 23564.08s
                               ETA: 993015.5s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.526s, learning 0.231s)
               Value function loss: 57070.8378
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 890.98
               Mean episode length: 63.09
                  Mean reward/step: 15.20
       Mean episode length/episode: 7.18
            Mean episode successes: 1.3364
Mean episode consecutive_successes: 3.4343
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 8.76s
                        Total time: 23572.84s
                               ETA: 992946.0s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.474s, learning 0.205s)
               Value function loss: 68481.5392
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 1294.50
               Mean episode length: 60.20
                  Mean reward/step: 15.17
       Mean episode length/episode: 7.21
            Mean episode successes: 1.2158
Mean episode consecutive_successes: 3.5254
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 8.68s
                        Total time: 23581.52s
                               ETA: 992873.3s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.553s, learning 0.181s)
               Value function loss: 62676.5075
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 1222.44
               Mean episode length: 62.66
                  Mean reward/step: 16.18
       Mean episode length/episode: 7.12
            Mean episode successes: 1.2065
Mean episode consecutive_successes: 3.5469
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 8.73s
                        Total time: 23590.25s
                               ETA: 992802.9s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.364s, learning 0.181s)
               Value function loss: 56706.2095
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 992.45
               Mean episode length: 62.34
                  Mean reward/step: 16.52
       Mean episode length/episode: 7.21
            Mean episode successes: 1.2915
Mean episode consecutive_successes: 3.5311
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 8.55s
                        Total time: 23598.79s
                               ETA: 992724.6s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.446s, learning 0.181s)
               Value function loss: 56605.7510
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 900.77
               Mean episode length: 59.41
                  Mean reward/step: 16.56
       Mean episode length/episode: 7.17
            Mean episode successes: 1.3467
Mean episode consecutive_successes: 3.5100
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 8.63s
                        Total time: 23607.42s
                               ETA: 992649.9s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.600s, learning 0.300s)
               Value function loss: 60416.8515
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 943.38
               Mean episode length: 60.28
                  Mean reward/step: 15.99
       Mean episode length/episode: 7.21
            Mean episode successes: 1.4614
Mean episode consecutive_successes: 3.4782
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 8.90s
                        Total time: 23616.32s
                               ETA: 992586.7s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.517s, learning 0.191s)
               Value function loss: 66278.9537
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 1314.87
               Mean episode length: 60.21
                  Mean reward/step: 15.41
       Mean episode length/episode: 7.19
            Mean episode successes: 1.3340
Mean episode consecutive_successes: 3.5943
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 8.71s
                        Total time: 23625.03s
                               ETA: 992515.5s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.401s, learning 0.401s)
               Value function loss: 62931.3648
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 1286.62
               Mean episode length: 63.45
                  Mean reward/step: 15.75
       Mean episode length/episode: 7.15
            Mean episode successes: 1.2192
Mean episode consecutive_successes: 3.6402
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 8.80s
                        Total time: 23633.83s
                               ETA: 992448.2s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.547s, learning 0.200s)
               Value function loss: 68990.8209
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 713.24
               Mean episode length: 59.75
                  Mean reward/step: 15.38
       Mean episode length/episode: 7.14
            Mean episode successes: 1.2324
Mean episode consecutive_successes: 3.6394
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 8.75s
                        Total time: 23642.58s
                               ETA: 992378.7s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.115s, learning 0.266s)
               Value function loss: 71849.1975
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: 813.74
               Mean episode length: 59.49
                  Mean reward/step: 16.74
       Mean episode length/episode: 7.23
            Mean episode successes: 1.2959
Mean episode consecutive_successes: 3.6275
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 8.38s
                        Total time: 23650.96s
                               ETA: 992293.9s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.537s, learning 0.190s)
               Value function loss: 55171.7564
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: 767.27
               Mean episode length: 60.40
                  Mean reward/step: 17.59
       Mean episode length/episode: 7.16
            Mean episode successes: 1.3887
Mean episode consecutive_successes: 3.6256
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 8.73s
                        Total time: 23659.69s
                               ETA: 992223.7s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.730s, learning 0.343s)
               Value function loss: 61659.7788
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 1152.28
               Mean episode length: 59.26
                  Mean reward/step: 17.59
       Mean episode length/episode: 7.17
            Mean episode successes: 1.4180
Mean episode consecutive_successes: 3.6513
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 9.07s
                        Total time: 23668.76s
                               ETA: 992168.0s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.296s, learning 0.175s)
               Value function loss: 61186.1402
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 861.43
               Mean episode length: 58.35
                  Mean reward/step: 15.20
       Mean episode length/episode: 7.15
            Mean episode successes: 1.2915
Mean episode consecutive_successes: 3.7116
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 8.47s
                        Total time: 23677.23s
                               ETA: 992087.1s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.486s, learning 0.195s)
               Value function loss: 53758.2518
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 769.92
               Mean episode length: 61.20
                  Mean reward/step: 14.09
       Mean episode length/episode: 7.15
            Mean episode successes: 1.2334
Mean episode consecutive_successes: 3.7054
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 8.68s
                        Total time: 23685.91s
                               ETA: 992015.1s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.491s, learning 0.191s)
               Value function loss: 50640.3138
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 1243.99
               Mean episode length: 59.32
                  Mean reward/step: 14.96
       Mean episode length/episode: 7.14
            Mean episode successes: 1.1533
Mean episode consecutive_successes: 3.7413
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 8.68s
                        Total time: 23694.59s
                               ETA: 991943.2s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.548s, learning 0.192s)
               Value function loss: 56279.5061
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 930.69
               Mean episode length: 62.52
                  Mean reward/step: 16.91
       Mean episode length/episode: 7.17
            Mean episode successes: 1.2173
Mean episode consecutive_successes: 3.7010
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 8.74s
                        Total time: 23703.33s
                               ETA: 991873.8s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.590s, learning 0.195s)
               Value function loss: 60346.5354
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 712.32
               Mean episode length: 56.58
                  Mean reward/step: 17.63
       Mean episode length/episode: 7.16
            Mean episode successes: 1.3545
Mean episode consecutive_successes: 3.6624
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 8.78s
                        Total time: 23712.12s
                               ETA: 991806.3s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.525s, learning 0.168s)
               Value function loss: 64749.4599
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 909.32
               Mean episode length: 57.97
                  Mean reward/step: 18.09
       Mean episode length/episode: 7.15
            Mean episode successes: 1.4902
Mean episode consecutive_successes: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 8.69s
                        Total time: 23720.81s
                               ETA: 991735.0s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.389s, learning 0.203s)
               Value function loss: 63680.9544
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 994.91
               Mean episode length: 59.10
                  Mean reward/step: 19.62
       Mean episode length/episode: 7.23
            Mean episode successes: 1.5391
Mean episode consecutive_successes: 3.6822
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 8.59s
                        Total time: 23729.40s
                               ETA: 991659.6s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.115s, learning 0.313s)
               Value function loss: 66733.1800
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 1328.14
               Mean episode length: 62.08
                  Mean reward/step: 18.74
       Mean episode length/episode: 7.21
            Mean episode successes: 1.5889
Mean episode consecutive_successes: 3.7797
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 8.43s
                        Total time: 23737.83s
                               ETA: 991577.3s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.479s, learning 0.169s)
               Value function loss: 69700.4655
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 990.48
               Mean episode length: 58.70
                  Mean reward/step: 17.33
       Mean episode length/episode: 7.14
            Mean episode successes: 1.4775
Mean episode consecutive_successes: 3.8157
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 8.65s
                        Total time: 23746.48s
                               ETA: 991504.3s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.565s, learning 0.172s)
               Value function loss: 67451.7896
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 1043.90
               Mean episode length: 58.60
                  Mean reward/step: 16.43
       Mean episode length/episode: 7.13
            Mean episode successes: 1.4697
Mean episode consecutive_successes: 3.8191
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 8.74s
                        Total time: 23755.22s
                               ETA: 991435.1s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.587s, learning 0.179s)
               Value function loss: 62586.2627
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 1497.06
               Mean episode length: 63.05
                  Mean reward/step: 16.32
       Mean episode length/episode: 7.18
            Mean episode successes: 1.2480
Mean episode consecutive_successes: 3.9408
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 8.77s
                        Total time: 23763.98s
                               ETA: 991367.1s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.373s, learning 0.336s)
               Value function loss: 58525.2113
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 916.88
               Mean episode length: 60.02
                  Mean reward/step: 15.99
       Mean episode length/episode: 7.21
            Mean episode successes: 1.3115
Mean episode consecutive_successes: 3.8930
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 8.71s
                        Total time: 23772.69s
                               ETA: 991296.8s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.057s, learning 0.204s)
               Value function loss: 66213.0354
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 1113.04
               Mean episode length: 60.73
                  Mean reward/step: 17.08
       Mean episode length/episode: 7.19
            Mean episode successes: 1.2939
Mean episode consecutive_successes: 3.9026
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 8.26s
                        Total time: 23780.95s
                               ETA: 991207.9s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.028s, learning 0.178s)
               Value function loss: 78114.2953
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 831.96
               Mean episode length: 60.06
                  Mean reward/step: 18.06
       Mean episode length/episode: 7.21
            Mean episode successes: 1.3555
Mean episode consecutive_successes: 3.9165
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 8.21s
                        Total time: 23789.16s
                               ETA: 991116.8s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.461s, learning 0.200s)
               Value function loss: 76652.4260
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 1001.95
               Mean episode length: 60.87
                  Mean reward/step: 18.70
       Mean episode length/episode: 7.09
            Mean episode successes: 1.4507
Mean episode consecutive_successes: 3.8956
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 8.66s
                        Total time: 23797.82s
                               ETA: 991044.7s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.363s, learning 0.243s)
               Value function loss: 83124.7104
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 1083.09
               Mean episode length: 60.99
                  Mean reward/step: 19.35
       Mean episode length/episode: 7.15
            Mean episode successes: 1.4639
Mean episode consecutive_successes: 3.9448
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 8.61s
                        Total time: 23806.42s
                               ETA: 990970.3s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.506s, learning 0.355s)
               Value function loss: 78941.9963
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 1172.61
               Mean episode length: 61.70
                  Mean reward/step: 18.60
       Mean episode length/episode: 7.26
            Mean episode successes: 1.4082
Mean episode consecutive_successes: 4.0482
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 8.86s
                        Total time: 23815.28s
                               ETA: 990906.6s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.542s, learning 0.220s)
               Value function loss: 76946.4990
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 951.50
               Mean episode length: 60.87
                  Mean reward/step: 18.16
       Mean episode length/episode: 7.14
            Mean episode successes: 1.5259
Mean episode consecutive_successes: 4.0316
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 8.76s
                        Total time: 23824.05s
                               ETA: 990838.9s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.573s, learning 0.319s)
               Value function loss: 74382.9826
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 987.68
               Mean episode length: 62.16
                  Mean reward/step: 19.18
       Mean episode length/episode: 7.24
            Mean episode successes: 1.6592
Mean episode consecutive_successes: 3.9921
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 8.89s
                        Total time: 23832.94s
                               ETA: 990776.6s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.261s, learning 0.200s)
               Value function loss: 79544.4510
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 1189.40
               Mean episode length: 60.64
                  Mean reward/step: 20.12
       Mean episode length/episode: 7.14
            Mean episode successes: 1.5586
Mean episode consecutive_successes: 4.0798
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 8.46s
                        Total time: 23841.40s
                               ETA: 990696.4s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.663s, learning 0.202s)
               Value function loss: 68970.5679
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 1548.60
               Mean episode length: 63.35
                  Mean reward/step: 19.49
       Mean episode length/episode: 7.24
            Mean episode successes: 1.5210
Mean episode consecutive_successes: 4.1794
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 8.86s
                        Total time: 23850.26s
                               ETA: 990633.1s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.449s, learning 0.211s)
               Value function loss: 71210.8448
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 1084.84
               Mean episode length: 60.84
                  Mean reward/step: 17.53
       Mean episode length/episode: 7.17
            Mean episode successes: 1.4897
Mean episode consecutive_successes: 4.2161
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 8.66s
                        Total time: 23858.92s
                               ETA: 990561.3s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.168s, learning 0.272s)
               Value function loss: 72176.8857
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 1008.16
               Mean episode length: 61.69
                  Mean reward/step: 18.15
       Mean episode length/episode: 7.27
            Mean episode successes: 1.6040
Mean episode consecutive_successes: 4.1936
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 8.44s
                        Total time: 23867.36s
                               ETA: 990480.4s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.545s, learning 0.175s)
               Value function loss: 89489.7121
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 1617.47
               Mean episode length: 63.94
                  Mean reward/step: 19.55
       Mean episode length/episode: 7.12
            Mean episode successes: 1.5947
Mean episode consecutive_successes: 4.2111
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 8.72s
                        Total time: 23876.08s
                               ETA: 990411.2s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.450s, learning 0.181s)
               Value function loss: 95555.9166
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 1398.05
               Mean episode length: 62.77
                  Mean reward/step: 18.73
       Mean episode length/episode: 7.10
            Mean episode successes: 1.4155
Mean episode consecutive_successes: 4.3216
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 8.63s
                        Total time: 23884.72s
                               ETA: 990338.4s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.192s, learning 0.172s)
               Value function loss: 85861.1146
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 662.78
               Mean episode length: 58.55
                  Mean reward/step: 15.15
       Mean episode length/episode: 7.14
            Mean episode successes: 1.3726
Mean episode consecutive_successes: 4.2102
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 8.36s
                        Total time: 23893.08s
                               ETA: 990254.6s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.891s, learning 0.307s)
               Value function loss: 79495.8330
                    Surrogate loss: 0.0042
             Mean action noise std: 0.74
                       Mean reward: 1205.29
               Mean episode length: 60.66
                  Mean reward/step: 16.58
       Mean episode length/episode: 7.19
            Mean episode successes: 1.4180
Mean episode consecutive_successes: 4.2090
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 17.20s
                        Total time: 23910.28s
                               ETA: 990536.8s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.534s, learning 0.194s)
               Value function loss: 73491.4357
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 1340.09
               Mean episode length: 62.00
                  Mean reward/step: 17.52
       Mean episode length/episode: 7.22
            Mean episode successes: 1.3398
Mean episode consecutive_successes: 4.2652
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 16.73s
                        Total time: 23927.01s
                               ETA: 990799.2s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.399s, learning 0.270s)
               Value function loss: 68104.3006
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 1278.21
               Mean episode length: 60.74
                  Mean reward/step: 16.93
       Mean episode length/episode: 7.18
            Mean episode successes: 1.3262
Mean episode consecutive_successes: 4.2705
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 16.67s
                        Total time: 23943.67s
                               ETA: 991059.0s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.370s, learning 0.190s)
               Value function loss: 65781.5390
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 643.09
               Mean episode length: 60.53
                  Mean reward/step: 16.12
       Mean episode length/episode: 7.11
            Mean episode successes: 1.3477
Mean episode consecutive_successes: 4.1522
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 16.56s
                        Total time: 23960.23s
                               ETA: 991314.1s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.242s, learning 0.175s)
               Value function loss: 62000.0689
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 1125.94
               Mean episode length: 58.93
                  Mean reward/step: 15.97
       Mean episode length/episode: 7.27
            Mean episode successes: 1.4434
Mean episode consecutive_successes: 4.1294
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 16.42s
                        Total time: 23976.65s
                               ETA: 991563.0s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.094s, learning 0.174s)
               Value function loss: 67977.7752
                    Surrogate loss: -0.0154
             Mean action noise std: 0.74
                       Mean reward: 1051.21
               Mean episode length: 59.19
                  Mean reward/step: 17.53
       Mean episode length/episode: 7.21
            Mean episode successes: 1.5044
Mean episode consecutive_successes: 4.0970
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 16.27s
                        Total time: 23992.92s
                               ETA: 991805.5s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.524s, learning 0.196s)
               Value function loss: 75295.2771
                    Surrogate loss: 0.0069
             Mean action noise std: 0.74
                       Mean reward: 1101.83
               Mean episode length: 63.63
                  Mean reward/step: 17.90
       Mean episode length/episode: 7.14
            Mean episode successes: 1.4287
Mean episode consecutive_successes: 4.1336
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 16.72s
                        Total time: 24009.64s
                               ETA: 992066.5s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.666s, learning 0.192s)
               Value function loss: 65486.2421
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 604.75
               Mean episode length: 58.38
                  Mean reward/step: 18.61
       Mean episode length/episode: 7.17
            Mean episode successes: 1.5474
Mean episode consecutive_successes: 4.0885
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 16.86s
                        Total time: 24026.50s
                               ETA: 992332.9s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.617s, learning 0.348s)
               Value function loss: 64403.8920
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 1132.89
               Mean episode length: 61.58
                  Mean reward/step: 19.16
       Mean episode length/episode: 7.15
            Mean episode successes: 1.5581
Mean episode consecutive_successes: 4.1137
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 16.96s
                        Total time: 24043.46s
                               ETA: 992603.5s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.159s, learning 0.181s)
               Value function loss: 64881.3049
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 1268.07
               Mean episode length: 63.08
                  Mean reward/step: 17.57
       Mean episode length/episode: 7.19
            Mean episode successes: 1.5518
Mean episode consecutive_successes: 4.1136
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 16.34s
                        Total time: 24059.80s
                               ETA: 992848.1s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.880s, learning 0.191s)
               Value function loss: 61639.8030
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 1031.14
               Mean episode length: 58.92
                  Mean reward/step: 16.49
       Mean episode length/episode: 7.13
            Mean episode successes: 1.3999
Mean episode consecutive_successes: 4.1722
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 17.07s
                        Total time: 24076.87s
                               ETA: 993122.6s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.546s, learning 0.202s)
               Value function loss: 64462.3084
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 1189.53
               Mean episode length: 60.97
                  Mean reward/step: 16.20
       Mean episode length/episode: 7.23
            Mean episode successes: 1.3198
Mean episode consecutive_successes: 4.2369
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 16.75s
                        Total time: 24093.62s
                               ETA: 993383.6s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.677s, learning 0.216s)
               Value function loss: 73574.7494
                    Surrogate loss: -0.0012
             Mean action noise std: 0.74
                       Mean reward: 924.72
               Mean episode length: 60.53
                  Mean reward/step: 16.25
       Mean episode length/episode: 7.13
            Mean episode successes: 1.2749
Mean episode consecutive_successes: 4.1947
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 16.89s
                        Total time: 24110.51s
                               ETA: 993650.2s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.554s, learning 0.205s)
               Value function loss: 67293.3686
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 556.94
               Mean episode length: 55.91
                  Mean reward/step: 18.95
       Mean episode length/episode: 7.22
            Mean episode successes: 1.4883
Mean episode consecutive_successes: 4.1011
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 16.76s
                        Total time: 24127.27s
                               ETA: 993911.2s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.616s, learning 0.306s)
               Value function loss: 65530.0671
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 1358.23
               Mean episode length: 62.13
                  Mean reward/step: 18.13
       Mean episode length/episode: 7.17
            Mean episode successes: 1.5649
Mean episode consecutive_successes: 4.1166
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 16.92s
                        Total time: 24144.19s
                               ETA: 994178.6s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.686s, learning 0.193s)
               Value function loss: 66335.6750
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 1225.91
               Mean episode length: 62.21
                  Mean reward/step: 17.17
       Mean episode length/episode: 7.18
            Mean episode successes: 1.5425
Mean episode consecutive_successes: 4.1165
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 16.88s
                        Total time: 24161.07s
                               ETA: 994444.0s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.477s, learning 0.177s)
               Value function loss: 72510.5627
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 1236.13
               Mean episode length: 59.01
                  Mean reward/step: 18.28
       Mean episode length/episode: 7.19
            Mean episode successes: 1.5488
Mean episode consecutive_successes: 4.1360
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 16.65s
                        Total time: 24177.72s
                               ETA: 994699.9s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.458s, learning 0.185s)
               Value function loss: 82434.5973
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 1353.20
               Mean episode length: 61.37
                  Mean reward/step: 17.70
       Mean episode length/episode: 7.09
            Mean episode successes: 1.3823
Mean episode consecutive_successes: 4.2126
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 16.64s
                        Total time: 24194.37s
                               ETA: 994955.2s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.536s, learning 0.171s)
               Value function loss: 92100.1484
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 687.90
               Mean episode length: 58.74
                  Mean reward/step: 17.57
       Mean episode length/episode: 7.11
            Mean episode successes: 1.3501
Mean episode consecutive_successes: 4.1748
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 16.71s
                        Total time: 24211.07s
                               ETA: 995212.8s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.329s, learning 0.277s)
               Value function loss: 79165.6715
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: 1113.47
               Mean episode length: 61.13
                  Mean reward/step: 18.12
       Mean episode length/episode: 7.18
            Mean episode successes: 1.5117
Mean episode consecutive_successes: 4.1354
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 16.61s
                        Total time: 24227.68s
                               ETA: 995466.0s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.907s, learning 0.264s)
               Value function loss: 66504.8768
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 837.26
               Mean episode length: 60.96
                  Mean reward/step: 16.96
       Mean episode length/episode: 7.21
            Mean episode successes: 1.5107
Mean episode consecutive_successes: 4.1096
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 17.17s
                        Total time: 24244.85s
                               ETA: 995742.3s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.649s, learning 0.180s)
               Value function loss: 65758.9615
                    Surrogate loss: -0.0008
             Mean action noise std: 0.74
                       Mean reward: 1092.16
               Mean episode length: 60.71
                  Mean reward/step: 16.94
       Mean episode length/episode: 7.15
            Mean episode successes: 1.4658
Mean episode consecutive_successes: 4.1352
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 16.83s
                        Total time: 24261.68s
                               ETA: 996004.2s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.528s, learning 0.311s)
               Value function loss: 56671.7081
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 1294.42
               Mean episode length: 60.09
                  Mean reward/step: 17.02
       Mean episode length/episode: 7.16
            Mean episode successes: 1.3706
Mean episode consecutive_successes: 4.1863
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 16.84s
                        Total time: 24278.52s
                               ETA: 996266.3s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.321s, learning 0.254s)
               Value function loss: 62317.9571
                    Surrogate loss: -0.0146
             Mean action noise std: 0.74
                       Mean reward: 843.43
               Mean episode length: 58.56
                  Mean reward/step: 17.07
       Mean episode length/episode: 7.17
            Mean episode successes: 1.4365
Mean episode consecutive_successes: 4.1396
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 16.58s
                        Total time: 24295.09s
                               ETA: 996517.4s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.416s, learning 0.187s)
               Value function loss: 65052.3270
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 850.93
               Mean episode length: 59.05
                  Mean reward/step: 16.82
       Mean episode length/episode: 7.08
            Mean episode successes: 1.3877
Mean episode consecutive_successes: 4.1057
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 16.60s
                        Total time: 24311.70s
                               ETA: 996769.4s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.821s, learning 0.210s)
               Value function loss: 65576.5742
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 730.77
               Mean episode length: 59.77
                  Mean reward/step: 16.72
       Mean episode length/episode: 7.20
            Mean episode successes: 1.4224
Mean episode consecutive_successes: 4.1015
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 17.03s
                        Total time: 24328.73s
                               ETA: 997038.6s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.518s, learning 0.221s)
               Value function loss: 70401.6280
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 621.08
               Mean episode length: 59.84
                  Mean reward/step: 17.70
       Mean episode length/episode: 7.20
            Mean episode successes: 1.5396
Mean episode consecutive_successes: 4.0457
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 16.74s
                        Total time: 24345.47s
                               ETA: 997295.7s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.688s, learning 0.332s)
               Value function loss: 82523.5379
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 1648.37
               Mean episode length: 60.84
                  Mean reward/step: 18.63
       Mean episode length/episode: 7.20
            Mean episode successes: 1.5068
Mean episode consecutive_successes: 4.1295
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 17.02s
                        Total time: 24362.49s
                               ETA: 997564.1s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.670s, learning 0.183s)
               Value function loss: 68995.3988
                    Surrogate loss: -0.0142
             Mean action noise std: 0.74
                       Mean reward: 1269.00
               Mean episode length: 63.53
                  Mean reward/step: 18.75
       Mean episode length/episode: 7.15
            Mean episode successes: 1.3833
Mean episode consecutive_successes: 4.1897
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 16.85s
                        Total time: 24379.34s
                               ETA: 997825.5s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.476s, learning 0.252s)
               Value function loss: 64429.4371
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: 570.18
               Mean episode length: 58.83
                  Mean reward/step: 17.56
       Mean episode length/episode: 7.21
            Mean episode successes: 1.5093
Mean episode consecutive_successes: 4.1087
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 16.73s
                        Total time: 24396.07s
                               ETA: 998081.4s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.922s, learning 0.225s)
               Value function loss: 63297.4187
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 766.76
               Mean episode length: 59.53
                  Mean reward/step: 19.49
       Mean episode length/episode: 7.14
            Mean episode successes: 1.5806
Mean episode consecutive_successes: 4.0797
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 17.15s
                        Total time: 24413.22s
                               ETA: 998354.2s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.820s, learning 0.210s)
               Value function loss: 70426.9389
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 1216.38
               Mean episode length: 61.86
                  Mean reward/step: 19.09
       Mean episode length/episode: 7.20
            Mean episode successes: 1.6631
Mean episode consecutive_successes: 4.0966
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 17.03s
                        Total time: 24430.24s
                               ETA: 998622.1s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.467s, learning 0.263s)
               Value function loss: 74978.3952
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 980.40
               Mean episode length: 58.08
                  Mean reward/step: 19.53
       Mean episode length/episode: 7.16
            Mean episode successes: 1.5537
Mean episode consecutive_successes: 4.1864
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 16.73s
                        Total time: 24446.97s
                               ETA: 998877.4s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.439s, learning 0.182s)
               Value function loss: 74719.8178
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 1217.95
               Mean episode length: 61.62
                  Mean reward/step: 19.38
       Mean episode length/episode: 7.13
            Mean episode successes: 1.5132
Mean episode consecutive_successes: 4.2285
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 16.62s
                        Total time: 24463.60s
                               ETA: 999128.0s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.445s, learning 0.203s)
               Value function loss: 68271.6135
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 849.81
               Mean episode length: 61.15
                  Mean reward/step: 18.49
       Mean episode length/episode: 7.15
            Mean episode successes: 1.5044
Mean episode consecutive_successes: 4.2450
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 16.65s
                        Total time: 24480.24s
                               ETA: 999379.6s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.843s, learning 0.181s)
               Value function loss: 67909.3269
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 1209.88
               Mean episode length: 60.92
                  Mean reward/step: 20.22
       Mean episode length/episode: 7.24
            Mean episode successes: 1.5962
Mean episode consecutive_successes: 4.2859
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 17.02s
                        Total time: 24497.27s
                               ETA: 999646.2s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.790s, learning 0.175s)
               Value function loss: 73301.2858
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 789.50
               Mean episode length: 61.44
                  Mean reward/step: 21.11
       Mean episode length/episode: 7.22
            Mean episode successes: 1.7437
Mean episode consecutive_successes: 4.2699
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 16.97s
                        Total time: 24514.23s
                               ETA: 999910.2s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1194 steps/s (collection: 13.515s, learning 0.196s)
               Value function loss: 77879.0432
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 1382.08
               Mean episode length: 62.06
                  Mean reward/step: 21.55
       Mean episode length/episode: 7.17
            Mean episode successes: 1.7310
Mean episode consecutive_successes: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 13.71s
                        Total time: 24527.94s
                               ETA: 1000041.3s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.330s, learning 0.292s)
               Value function loss: 74346.0217
                    Surrogate loss: 0.0008
             Mean action noise std: 0.74
                       Mean reward: 1187.76
               Mean episode length: 62.42
                  Mean reward/step: 18.98
       Mean episode length/episode: 7.18
            Mean episode successes: 1.5518
Mean episode consecutive_successes: 4.4687
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 8.62s
                        Total time: 24536.56s
                               ETA: 999964.9s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.182s, learning 0.240s)
               Value function loss: 72507.7563
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 1675.84
               Mean episode length: 62.43
                  Mean reward/step: 21.09
       Mean episode length/episode: 7.08
            Mean episode successes: 1.5913
Mean episode consecutive_successes: 4.5005
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 8.42s
                        Total time: 24544.99s
                               ETA: 999880.4s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.410s, learning 0.182s)
               Value function loss: 76029.4066
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: 1039.28
               Mean episode length: 60.60
                  Mean reward/step: 19.91
       Mean episode length/episode: 7.16
            Mean episode successes: 1.5928
Mean episode consecutive_successes: 4.4794
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 8.59s
                        Total time: 24553.58s
                               ETA: 999802.8s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.754s, learning 0.216s)
               Value function loss: 74055.9141
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 1148.26
               Mean episode length: 63.77
                  Mean reward/step: 18.66
       Mean episode length/episode: 7.23
            Mean episode successes: 1.4819
Mean episode consecutive_successes: 4.5606
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 8.97s
                        Total time: 24562.55s
                               ETA: 999740.8s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.325s, learning 0.199s)
               Value function loss: 71227.9393
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 1527.62
               Mean episode length: 59.95
                  Mean reward/step: 18.53
       Mean episode length/episode: 7.19
            Mean episode successes: 1.5337
Mean episode consecutive_successes: 4.5610
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 8.52s
                        Total time: 24571.07s
                               ETA: 999660.6s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.246s, learning 0.188s)
               Value function loss: 78931.7617
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 1187.93
               Mean episode length: 61.54
                  Mean reward/step: 18.02
       Mean episode length/episode: 7.18
            Mean episode successes: 1.3999
Mean episode consecutive_successes: 4.6012
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 8.43s
                        Total time: 24579.51s
                               ETA: 999576.8s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.359s, learning 0.245s)
               Value function loss: 69555.5834
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 1033.24
               Mean episode length: 61.55
                  Mean reward/step: 18.94
       Mean episode length/episode: 7.23
            Mean episode successes: 1.4517
Mean episode consecutive_successes: 4.5936
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 8.60s
                        Total time: 24588.11s
                               ETA: 999500.0s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.464s, learning 0.278s)
               Value function loss: 81681.3965
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 1249.81
               Mean episode length: 60.77
                  Mean reward/step: 19.73
       Mean episode length/episode: 7.18
            Mean episode successes: 1.5669
Mean episode consecutive_successes: 4.5617
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 8.74s
                        Total time: 24596.85s
                               ETA: 999428.8s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.350s, learning 0.198s)
               Value function loss: 78592.5035
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 714.74
               Mean episode length: 61.24
                  Mean reward/step: 18.80
       Mean episode length/episode: 7.15
            Mean episode successes: 1.6270
Mean episode consecutive_successes: 4.4560
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 8.55s
                        Total time: 24605.40s
                               ETA: 999349.9s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.278s, learning 0.173s)
               Value function loss: 75839.9650
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 1177.96
               Mean episode length: 61.54
                  Mean reward/step: 18.85
       Mean episode length/episode: 7.20
            Mean episode successes: 1.6167
Mean episode consecutive_successes: 4.5115
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 8.45s
                        Total time: 24613.85s
                               ETA: 999267.0s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.333s, learning 0.346s)
               Value function loss: 92823.4389
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 1206.67
               Mean episode length: 61.39
                  Mean reward/step: 17.59
       Mean episode length/episode: 7.16
            Mean episode successes: 1.4419
Mean episode consecutive_successes: 4.6024
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 8.68s
                        Total time: 24622.53s
                               ETA: 999193.5s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.450s, learning 0.179s)
               Value function loss: 90234.8186
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 793.14
               Mean episode length: 60.10
                  Mean reward/step: 18.70
       Mean episode length/episode: 7.09
            Mean episode successes: 1.4941
Mean episode consecutive_successes: 4.5132
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 8.63s
                        Total time: 24631.16s
                               ETA: 999118.0s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.374s, learning 0.171s)
               Value function loss: 64148.0144
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 859.09
               Mean episode length: 60.19
                  Mean reward/step: 18.60
       Mean episode length/episode: 7.21
            Mean episode successes: 1.5063
Mean episode consecutive_successes: 4.5348
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 8.54s
                        Total time: 24639.70s
                               ETA: 999039.1s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.495s, learning 0.369s)
               Value function loss: 67829.8740
                    Surrogate loss: 0.0011
             Mean action noise std: 0.74
                       Mean reward: 1046.80
               Mean episode length: 59.54
                  Mean reward/step: 18.26
       Mean episode length/episode: 7.16
            Mean episode successes: 1.4761
Mean episode consecutive_successes: 4.5447
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 8.86s
                        Total time: 24648.57s
                               ETA: 998973.3s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.341s, learning 0.195s)
               Value function loss: 65846.4896
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 1177.09
               Mean episode length: 61.75
                  Mean reward/step: 17.61
       Mean episode length/episode: 7.21
            Mean episode successes: 1.4941
Mean episode consecutive_successes: 4.5144
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 8.54s
                        Total time: 24657.10s
                               ETA: 998894.1s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.464s, learning 0.274s)
               Value function loss: 77070.4793
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 1461.54
               Mean episode length: 62.79
                  Mean reward/step: 18.96
       Mean episode length/episode: 7.19
            Mean episode successes: 1.5503
Mean episode consecutive_successes: 4.5255
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 8.74s
                        Total time: 24665.84s
                               ETA: 998823.3s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.245s, learning 0.207s)
               Value function loss: 69028.1106
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 978.28
               Mean episode length: 60.14
                  Mean reward/step: 19.43
       Mean episode length/episode: 7.09
            Mean episode successes: 1.4453
Mean episode consecutive_successes: 4.5276
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 8.45s
                        Total time: 24674.29s
                               ETA: 998740.9s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.429s, learning 0.275s)
               Value function loss: 68052.8470
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 815.66
               Mean episode length: 58.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 7.26
            Mean episode successes: 1.5566
Mean episode consecutive_successes: 4.4860
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 8.70s
                        Total time: 24683.00s
                               ETA: 998668.7s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.235s, learning 0.201s)
               Value function loss: 74149.2537
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 1315.45
               Mean episode length: 61.25
                  Mean reward/step: 19.47
       Mean episode length/episode: 7.19
            Mean episode successes: 1.6729
Mean episode consecutive_successes: 4.4608
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 8.44s
                        Total time: 24691.43s
                               ETA: 998585.9s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.739s, learning 0.249s)
               Value function loss: 72570.2385
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 1491.55
               Mean episode length: 62.06
                  Mean reward/step: 18.31
       Mean episode length/episode: 7.18
            Mean episode successes: 1.6479
Mean episode consecutive_successes: 4.4734
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 8.99s
                        Total time: 24700.42s
                               ETA: 998525.3s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.357s, learning 0.165s)
               Value function loss: 72108.4137
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 1141.69
               Mean episode length: 59.55
                  Mean reward/step: 17.88
       Mean episode length/episode: 7.10
            Mean episode successes: 1.3892
Mean episode consecutive_successes: 4.5405
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 8.52s
                        Total time: 24708.94s
                               ETA: 998446.0s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.670s, learning 0.213s)
               Value function loss: 71151.3370
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 1052.94
               Mean episode length: 59.09
                  Mean reward/step: 18.52
       Mean episode length/episode: 7.21
            Mean episode successes: 1.5479
Mean episode consecutive_successes: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 8.88s
                        Total time: 24717.83s
                               ETA: 998381.2s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.384s, learning 0.187s)
               Value function loss: 75748.1645
                    Surrogate loss: -0.0120
             Mean action noise std: 0.74
                       Mean reward: 942.91
               Mean episode length: 58.56
                  Mean reward/step: 19.79
       Mean episode length/episode: 7.19
            Mean episode successes: 1.6021
Mean episode consecutive_successes: 4.4418
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 8.57s
                        Total time: 24726.40s
                               ETA: 998304.0s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.657s, learning 0.193s)
               Value function loss: 75263.9535
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 1005.81
               Mean episode length: 59.37
                  Mean reward/step: 18.81
       Mean episode length/episode: 7.23
            Mean episode successes: 1.5820
Mean episode consecutive_successes: 4.4801
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 8.85s
                        Total time: 24735.25s
                               ETA: 998238.0s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.348s, learning 0.246s)
               Value function loss: 86303.4887
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 1142.61
               Mean episode length: 60.03
                  Mean reward/step: 19.84
       Mean episode length/episode: 7.18
            Mean episode successes: 1.6255
Mean episode consecutive_successes: 4.4673
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 8.59s
                        Total time: 24743.84s
                               ETA: 998161.9s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.471s, learning 0.187s)
               Value function loss: 86607.6709
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 1362.76
               Mean episode length: 60.91
                  Mean reward/step: 20.53
       Mean episode length/episode: 7.16
            Mean episode successes: 1.6367
Mean episode consecutive_successes: 4.5014
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 8.66s
                        Total time: 24752.50s
                               ETA: 998088.3s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.247s, learning 0.243s)
               Value function loss: 86323.1375
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 1434.65
               Mean episode length: 59.71
                  Mean reward/step: 19.99
       Mean episode length/episode: 7.19
            Mean episode successes: 1.7173
Mean episode consecutive_successes: 4.5160
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 8.49s
                        Total time: 24760.99s
                               ETA: 998008.0s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.191s, learning 0.179s)
               Value function loss: 83689.2217
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 1125.34
               Mean episode length: 62.35
                  Mean reward/step: 18.20
       Mean episode length/episode: 7.17
            Mean episode successes: 1.5605
Mean episode consecutive_successes: 4.5501
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 8.37s
                        Total time: 24769.36s
                               ETA: 997922.9s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.522s, learning 0.246s)
               Value function loss: 88739.4428
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 1301.68
               Mean episode length: 63.56
                  Mean reward/step: 18.72
       Mean episode length/episode: 7.12
            Mean episode successes: 1.5757
Mean episode consecutive_successes: 4.5168
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 8.77s
                        Total time: 24778.13s
                               ETA: 997853.9s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.740s, learning 0.181s)
               Value function loss: 83915.4758
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 798.77
               Mean episode length: 60.44
                  Mean reward/step: 18.90
       Mean episode length/episode: 7.19
            Mean episode successes: 1.4658
Mean episode consecutive_successes: 4.5401
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 8.92s
                        Total time: 24787.05s
                               ETA: 997791.2s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.355s, learning 0.213s)
               Value function loss: 90654.9402
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 1069.48
               Mean episode length: 61.22
                  Mean reward/step: 19.92
       Mean episode length/episode: 7.26
            Mean episode successes: 1.6045
Mean episode consecutive_successes: 4.5428
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 8.57s
                        Total time: 24795.62s
                               ETA: 997714.2s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.343s, learning 0.194s)
               Value function loss: 101376.1381
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 1242.26
               Mean episode length: 60.58
                  Mean reward/step: 21.38
       Mean episode length/episode: 7.24
            Mean episode successes: 1.7231
Mean episode consecutive_successes: 4.5438
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 8.54s
                        Total time: 24804.15s
                               ETA: 997636.1s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.418s, learning 0.313s)
               Value function loss: 127199.9330
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 954.00
               Mean episode length: 58.67
                  Mean reward/step: 22.19
       Mean episode length/episode: 7.15
            Mean episode successes: 1.8286
Mean episode consecutive_successes: 4.5219
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 8.73s
                        Total time: 24812.88s
                               ETA: 997565.8s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.347s, learning 0.186s)
               Value function loss: 132882.5152
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 1370.85
               Mean episode length: 59.08
                  Mean reward/step: 21.44
       Mean episode length/episode: 7.10
            Mean episode successes: 1.8408
Mean episode consecutive_successes: 4.5452
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 8.53s
                        Total time: 24821.42s
                               ETA: 997487.7s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.519s, learning 0.174s)
               Value function loss: 165272.8359
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 1432.73
               Mean episode length: 62.15
                  Mean reward/step: 21.60
       Mean episode length/episode: 7.20
            Mean episode successes: 1.8838
Mean episode consecutive_successes: 4.5856
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 8.69s
                        Total time: 24830.11s
                               ETA: 997416.0s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.050s, learning 0.187s)
               Value function loss: 127967.8309
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 1562.51
               Mean episode length: 60.60
                  Mean reward/step: 20.07
       Mean episode length/episode: 7.20
            Mean episode successes: 1.7026
Mean episode consecutive_successes: 4.7425
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 8.24s
                        Total time: 24838.35s
                               ETA: 997326.0s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.596s, learning 0.191s)
               Value function loss: 98715.1932
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: 1343.35
               Mean episode length: 61.65
                  Mean reward/step: 19.97
       Mean episode length/episode: 7.18
            Mean episode successes: 1.6499
Mean episode consecutive_successes: 4.7447
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 8.79s
                        Total time: 24847.13s
                               ETA: 997258.2s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.674s, learning 0.311s)
               Value function loss: 95670.3055
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 1195.23
               Mean episode length: 61.40
                  Mean reward/step: 20.34
       Mean episode length/episode: 7.18
            Mean episode successes: 1.6816
Mean episode consecutive_successes: 4.7751
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 8.99s
                        Total time: 24856.12s
                               ETA: 997198.4s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.585s, learning 0.209s)
               Value function loss: 95773.9785
                    Surrogate loss: -0.0005
             Mean action noise std: 0.74
                       Mean reward: 1061.16
               Mean episode length: 59.97
                  Mean reward/step: 20.83
       Mean episode length/episode: 7.17
            Mean episode successes: 1.6313
Mean episode consecutive_successes: 4.7789
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 8.79s
                        Total time: 24864.91s
                               ETA: 997131.0s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.623s, learning 0.192s)
               Value function loss: 93641.9881
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 850.90
               Mean episode length: 60.34
                  Mean reward/step: 20.81
       Mean episode length/episode: 7.14
            Mean episode successes: 1.7100
Mean episode consecutive_successes: 4.7256
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 8.82s
                        Total time: 24873.73s
                               ETA: 997064.5s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.649s, learning 0.411s)
               Value function loss: 87745.8557
                    Surrogate loss: 0.0045
             Mean action noise std: 0.74
                       Mean reward: 1060.84
               Mean episode length: 59.25
                  Mean reward/step: 19.62
       Mean episode length/episode: 7.21
            Mean episode successes: 1.8228
Mean episode consecutive_successes: 4.7049
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 9.06s
                        Total time: 24882.79s
                               ETA: 997007.8s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.368s, learning 0.170s)
               Value function loss: 97970.4166
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 1657.99
               Mean episode length: 61.14
                  Mean reward/step: 18.85
       Mean episode length/episode: 7.19
            Mean episode successes: 1.6274
Mean episode consecutive_successes: 4.7899
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 8.54s
                        Total time: 24891.33s
                               ETA: 996930.3s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.270s, learning 0.362s)
               Value function loss: 97902.7785
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 1169.07
               Mean episode length: 64.56
                  Mean reward/step: 18.07
       Mean episode length/episode: 7.19
            Mean episode successes: 1.5137
Mean episode consecutive_successes: 4.8325
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 8.63s
                        Total time: 24899.96s
                               ETA: 996856.6s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.797s, learning 0.189s)
               Value function loss: 83416.5324
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 872.59
               Mean episode length: 61.82
                  Mean reward/step: 19.09
       Mean episode length/episode: 7.18
            Mean episode successes: 1.5596
Mean episode consecutive_successes: 4.7847
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 8.99s
                        Total time: 24908.94s
                               ETA: 996797.1s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.542s, learning 0.180s)
               Value function loss: 87169.9420
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 1151.75
               Mean episode length: 61.30
                  Mean reward/step: 19.30
       Mean episode length/episode: 7.22
            Mean episode successes: 1.6016
Mean episode consecutive_successes: 4.7907
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 8.72s
                        Total time: 24917.66s
                               ETA: 996727.0s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.308s, learning 0.184s)
               Value function loss: 85824.4420
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 1222.26
               Mean episode length: 61.24
                  Mean reward/step: 19.12
       Mean episode length/episode: 7.16
            Mean episode successes: 1.5894
Mean episode consecutive_successes: 4.7749
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 8.49s
                        Total time: 24926.16s
                               ETA: 996647.9s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.311s, learning 0.202s)
               Value function loss: 78385.3836
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 848.28
               Mean episode length: 59.60
                  Mean reward/step: 19.73
       Mean episode length/episode: 7.20
            Mean episode successes: 1.6782
Mean episode consecutive_successes: 4.7122
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 8.51s
                        Total time: 24934.67s
                               ETA: 996569.6s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.410s, learning 0.189s)
               Value function loss: 79564.6781
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 1092.27
               Mean episode length: 62.44
                  Mean reward/step: 20.22
       Mean episode length/episode: 7.14
            Mean episode successes: 1.6587
Mean episode consecutive_successes: 4.7077
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 8.60s
                        Total time: 24943.27s
                               ETA: 996494.8s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.306s, learning 0.206s)
               Value function loss: 81600.4182
                    Surrogate loss: -0.0117
             Mean action noise std: 0.74
                       Mean reward: 1292.94
               Mean episode length: 61.96
                  Mean reward/step: 20.63
       Mean episode length/episode: 7.13
            Mean episode successes: 1.7388
Mean episode consecutive_successes: 4.7004
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 8.51s
                        Total time: 24951.78s
                               ETA: 996416.7s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.627s, learning 0.169s)
               Value function loss: 76686.4385
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 1280.19
               Mean episode length: 61.89
                  Mean reward/step: 19.48
       Mean episode length/episode: 7.20
            Mean episode successes: 1.6978
Mean episode consecutive_successes: 4.7184
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 8.80s
                        Total time: 24960.58s
                               ETA: 996349.9s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.623s, learning 0.195s)
               Value function loss: 79817.1270
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 1406.91
               Mean episode length: 59.47
                  Mean reward/step: 18.99
       Mean episode length/episode: 7.17
            Mean episode successes: 1.6504
Mean episode consecutive_successes: 4.7693
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 8.82s
                        Total time: 24969.40s
                               ETA: 996284.0s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.191s, learning 0.179s)
               Value function loss: 80733.4150
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: 1226.74
               Mean episode length: 60.16
                  Mean reward/step: 19.42
       Mean episode length/episode: 7.23
            Mean episode successes: 1.6040
Mean episode consecutive_successes: 4.7732
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 8.37s
                        Total time: 24977.76s
                               ETA: 996200.3s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.350s, learning 0.176s)
               Value function loss: 78114.7119
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 1317.48
               Mean episode length: 60.54
                  Mean reward/step: 20.85
       Mean episode length/episode: 7.16
            Mean episode successes: 1.6846
Mean episode consecutive_successes: 4.7721
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 8.53s
                        Total time: 24986.29s
                               ETA: 996122.8s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.665s, learning 0.290s)
               Value function loss: 83118.2051
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 800.21
               Mean episode length: 62.25
                  Mean reward/step: 19.14
       Mean episode length/episode: 7.21
            Mean episode successes: 1.6909
Mean episode consecutive_successes: 4.7308
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 8.96s
                        Total time: 24995.25s
                               ETA: 996062.6s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.201s, learning 0.254s)
               Value function loss: 85464.5023
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 1220.59
               Mean episode length: 62.12
                  Mean reward/step: 21.07
       Mean episode length/episode: 7.19
            Mean episode successes: 1.7803
Mean episode consecutive_successes: 4.7148
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 8.45s
                        Total time: 25003.70s
                               ETA: 995982.4s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.277s, learning 0.180s)
               Value function loss: 86309.3605
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 1488.57
               Mean episode length: 62.40
                  Mean reward/step: 21.98
       Mean episode length/episode: 7.23
            Mean episode successes: 1.9395
Mean episode consecutive_successes: 4.7265
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 8.46s
                        Total time: 25012.16s
                               ETA: 995902.5s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.392s, learning 0.336s)
               Value function loss: 92484.6977
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 1510.90
               Mean episode length: 62.61
                  Mean reward/step: 21.40
       Mean episode length/episode: 7.21
            Mean episode successes: 1.7437
Mean episode consecutive_successes: 4.8548
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 8.73s
                        Total time: 25020.89s
                               ETA: 995833.3s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.788s, learning 0.189s)
               Value function loss: 81736.2303
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 1173.49
               Mean episode length: 62.58
                  Mean reward/step: 21.17
       Mean episode length/episode: 7.12
            Mean episode successes: 1.7866
Mean episode consecutive_successes: 4.8338
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 8.98s
                        Total time: 25029.86s
                               ETA: 995774.1s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.163s, learning 0.168s)
               Value function loss: 83758.1816
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 1343.14
               Mean episode length: 63.10
                  Mean reward/step: 20.62
       Mean episode length/episode: 7.20
            Mean episode successes: 1.7529
Mean episode consecutive_successes: 4.8832
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 8.33s
                        Total time: 25038.19s
                               ETA: 995689.2s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.532s, learning 0.177s)
               Value function loss: 84580.3414
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 1177.66
               Mean episode length: 62.13
                  Mean reward/step: 22.82
       Mean episode length/episode: 7.20
            Mean episode successes: 1.8315
Mean episode consecutive_successes: 4.8669
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 8.71s
                        Total time: 25046.90s
                               ETA: 995619.4s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.584s, learning 0.365s)
               Value function loss: 84553.1986
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 1539.69
               Mean episode length: 60.97
                  Mean reward/step: 22.71
       Mean episode length/episode: 7.18
            Mean episode successes: 1.8672
Mean episode consecutive_successes: 4.9336
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 8.95s
                        Total time: 25055.85s
                               ETA: 995559.2s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.347s, learning 0.207s)
               Value function loss: 86851.6516
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 1774.91
               Mean episode length: 62.77
                  Mean reward/step: 22.48
       Mean episode length/episode: 7.25
            Mean episode successes: 1.8618
Mean episode consecutive_successes: 4.9691
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 8.55s
                        Total time: 25064.40s
                               ETA: 995483.4s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.425s, learning 0.246s)
               Value function loss: 85645.9244
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 1735.85
               Mean episode length: 61.52
                  Mean reward/step: 21.54
       Mean episode length/episode: 7.16
            Mean episode successes: 1.8398
Mean episode consecutive_successes: 5.0500
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 8.67s
                        Total time: 25073.07s
                               ETA: 995412.3s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.728s, learning 0.190s)
               Value function loss: 76979.8561
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 1986.59
               Mean episode length: 61.94
                  Mean reward/step: 19.73
       Mean episode length/episode: 7.15
            Mean episode successes: 1.6758
Mean episode consecutive_successes: 5.1255
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 8.92s
                        Total time: 25081.99s
                               ETA: 995351.0s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.318s, learning 0.215s)
               Value function loss: 79643.0141
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 1197.41
               Mean episode length: 60.75
                  Mean reward/step: 19.66
       Mean episode length/episode: 7.19
            Mean episode successes: 1.6143
Mean episode consecutive_successes: 5.0785
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 8.53s
                        Total time: 25090.53s
                               ETA: 995274.5s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.103s, learning 0.187s)
               Value function loss: 75514.2814
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 1074.58
               Mean episode length: 61.14
                  Mean reward/step: 21.51
       Mean episode length/episode: 7.22
            Mean episode successes: 1.7119
Mean episode consecutive_successes: 5.0279
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 8.29s
                        Total time: 25098.82s
                               ETA: 995188.4s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.448s, learning 0.182s)
               Value function loss: 81996.4582
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 1259.09
               Mean episode length: 63.58
                  Mean reward/step: 22.02
       Mean episode length/episode: 7.16
            Mean episode successes: 1.8398
Mean episode consecutive_successes: 4.9376
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 8.63s
                        Total time: 25107.45s
                               ETA: 995115.9s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.520s, learning 0.342s)
               Value function loss: 90782.3289
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 1393.29
               Mean episode length: 62.31
                  Mean reward/step: 23.98
       Mean episode length/episode: 7.25
            Mean episode successes: 1.9604
Mean episode consecutive_successes: 4.9887
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 8.86s
                        Total time: 25116.31s
                               ETA: 995052.6s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.448s, learning 0.164s)
               Value function loss: 88805.6057
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 1609.55
               Mean episode length: 62.37
                  Mean reward/step: 24.52
       Mean episode length/episode: 7.23
            Mean episode successes: 2.1064
Mean episode consecutive_successes: 5.0329
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 8.61s
                        Total time: 25124.92s
                               ETA: 994979.5s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.365s, learning 0.187s)
               Value function loss: 87670.6557
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 1141.63
               Mean episode length: 58.24
                  Mean reward/step: 23.10
       Mean episode length/episode: 7.15
            Mean episode successes: 1.9370
Mean episode consecutive_successes: 5.0903
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 8.55s
                        Total time: 25133.47s
                               ETA: 994904.0s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.365s, learning 0.185s)
               Value function loss: 81366.5609
                    Surrogate loss: -0.0144
             Mean action noise std: 0.74
                       Mean reward: 1175.83
               Mean episode length: 61.32
                  Mean reward/step: 22.18
       Mean episode length/episode: 7.18
            Mean episode successes: 1.8687
Mean episode consecutive_successes: 5.1252
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 8.55s
                        Total time: 25142.02s
                               ETA: 994828.5s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.556s, learning 0.192s)
               Value function loss: 89126.6777
                    Surrogate loss: 0.0072
             Mean action noise std: 0.74
                       Mean reward: 2029.94
               Mean episode length: 63.49
                  Mean reward/step: 23.07
       Mean episode length/episode: 7.24
            Mean episode successes: 1.8662
Mean episode consecutive_successes: 5.2273
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 8.75s
                        Total time: 25150.77s
                               ETA: 994760.9s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.491s, learning 0.168s)
               Value function loss: 85091.5816
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 1401.22
               Mean episode length: 62.78
                  Mean reward/step: 22.29
       Mean episode length/episode: 7.15
            Mean episode successes: 1.8608
Mean episode consecutive_successes: 5.1987
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 8.66s
                        Total time: 25159.43s
                               ETA: 994689.8s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.488s, learning 0.177s)
               Value function loss: 88474.9719
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 1768.05
               Mean episode length: 64.66
                  Mean reward/step: 23.48
       Mean episode length/episode: 7.18
            Mean episode successes: 1.8071
Mean episode consecutive_successes: 5.2865
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 8.67s
                        Total time: 25168.09s
                               ETA: 994619.0s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.316s, learning 0.289s)
               Value function loss: 82750.4670
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 995.09
               Mean episode length: 56.98
                  Mean reward/step: 23.14
       Mean episode length/episode: 7.19
            Mean episode successes: 1.9341
Mean episode consecutive_successes: 5.2261
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 8.60s
                        Total time: 25176.70s
                               ETA: 994545.9s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.079s, learning 0.181s)
               Value function loss: 93746.3561
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 2086.68
               Mean episode length: 63.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 7.23
            Mean episode successes: 1.9473
Mean episode consecutive_successes: 5.3704
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 8.26s
                        Total time: 25184.96s
                               ETA: 994459.2s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.285s, learning 0.170s)
               Value function loss: 92726.3922
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 1084.36
               Mean episode length: 59.26
                  Mean reward/step: 23.17
       Mean episode length/episode: 7.22
            Mean episode successes: 1.9409
Mean episode consecutive_successes: 5.2984
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 8.45s
                        Total time: 25193.41s
                               ETA: 994380.3s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.646s, learning 0.169s)
               Value function loss: 101417.4244
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 1558.71
               Mean episode length: 62.74
                  Mean reward/step: 24.19
       Mean episode length/episode: 7.08
            Mean episode successes: 1.9351
Mean episode consecutive_successes: 5.3475
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 8.81s
                        Total time: 25202.23s
                               ETA: 994315.6s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.132s, learning 0.179s)
               Value function loss: 109348.7000
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 1658.49
               Mean episode length: 61.36
                  Mean reward/step: 23.09
       Mean episode length/episode: 7.13
            Mean episode successes: 1.8359
Mean episode consecutive_successes: 5.3904
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 8.31s
                        Total time: 25210.54s
                               ETA: 994231.1s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.458s, learning 0.246s)
               Value function loss: 110530.0838
                    Surrogate loss: -0.0043
             Mean action noise std: 0.74
                       Mean reward: 1534.85
               Mean episode length: 60.85
                  Mean reward/step: 23.12
       Mean episode length/episode: 7.19
            Mean episode successes: 1.8101
Mean episode consecutive_successes: 5.4639
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 8.70s
                        Total time: 25219.24s
                               ETA: 994162.1s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.091s, learning 0.174s)
               Value function loss: 90526.3785
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 1074.53
               Mean episode length: 60.55
                  Mean reward/step: 22.78
       Mean episode length/episode: 7.20
            Mean episode successes: 1.8643
Mean episode consecutive_successes: 5.4067
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 8.27s
                        Total time: 25227.51s
                               ETA: 994075.9s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.121s, learning 0.174s)
               Value function loss: 90427.3055
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 1279.57
               Mean episode length: 61.34
                  Mean reward/step: 23.06
       Mean episode length/episode: 7.22
            Mean episode successes: 1.9268
Mean episode consecutive_successes: 5.4070
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 8.29s
                        Total time: 25235.80s
                               ETA: 993991.0s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.133s, learning 0.175s)
               Value function loss: 86593.4783
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 1853.55
               Mean episode length: 62.39
                  Mean reward/step: 22.88
       Mean episode length/episode: 7.24
            Mean episode successes: 1.8276
Mean episode consecutive_successes: 5.5375
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 8.31s
                        Total time: 25244.11s
                               ETA: 993906.6s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.655s, learning 0.169s)
               Value function loss: 95266.2920
                    Surrogate loss: -0.0157
             Mean action noise std: 0.74
                       Mean reward: 1100.09
               Mean episode length: 61.71
                  Mean reward/step: 23.36
       Mean episode length/episode: 7.27
            Mean episode successes: 2.0737
Mean episode consecutive_successes: 5.4413
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 8.82s
                        Total time: 25252.94s
                               ETA: 993842.6s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.736s, learning 0.163s)
               Value function loss: 90335.4145
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 1442.82
               Mean episode length: 61.52
                  Mean reward/step: 23.53
       Mean episode length/episode: 7.20
            Mean episode successes: 1.9761
Mean episode consecutive_successes: 5.5187
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 8.90s
                        Total time: 25261.83s
                               ETA: 993781.6s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.318s, learning 0.184s)
               Value function loss: 92077.4779
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 1102.55
               Mean episode length: 64.96
                  Mean reward/step: 22.66
       Mean episode length/episode: 7.21
            Mean episode successes: 1.9736
Mean episode consecutive_successes: 5.4931
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 8.50s
                        Total time: 25270.34s
                               ETA: 993705.0s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.270s, learning 0.195s)
               Value function loss: 82337.0580
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 1269.10
               Mean episode length: 63.15
                  Mean reward/step: 22.43
       Mean episode length/episode: 7.20
            Mean episode successes: 2.0068
Mean episode consecutive_successes: 5.4613
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 8.46s
                        Total time: 25278.80s
                               ETA: 993627.0s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.726s, learning 0.264s)
               Value function loss: 86590.2627
                    Surrogate loss: -0.0145
             Mean action noise std: 0.74
                       Mean reward: 1426.52
               Mean episode length: 62.95
                  Mean reward/step: 22.97
       Mean episode length/episode: 7.16
            Mean episode successes: 1.9526
Mean episode consecutive_successes: 5.5160
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 8.99s
                        Total time: 25287.79s
                               ETA: 993569.7s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.582s, learning 0.205s)
               Value function loss: 92040.5119
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 1379.06
               Mean episode length: 60.72
                  Mean reward/step: 23.80
       Mean episode length/episode: 7.08
            Mean episode successes: 1.8638
Mean episode consecutive_successes: 5.5602
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 8.79s
                        Total time: 25296.58s
                               ETA: 993504.5s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.197s, learning 0.178s)
               Value function loss: 90519.1701
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 1503.98
               Mean episode length: 63.73
                  Mean reward/step: 23.71
       Mean episode length/episode: 7.28
            Mean episode successes: 1.9512
Mean episode consecutive_successes: 5.5492
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 8.38s
                        Total time: 25304.95s
                               ETA: 993423.1s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.546s, learning 0.203s)
               Value function loss: 93170.9457
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 1277.93
               Mean episode length: 62.23
                  Mean reward/step: 24.89
       Mean episode length/episode: 7.29
            Mean episode successes: 2.0127
Mean episode consecutive_successes: 5.5822
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 8.75s
                        Total time: 25313.70s
                               ETA: 993356.5s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.604s, learning 0.232s)
               Value function loss: 94637.9182
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 1223.26
               Mean episode length: 61.38
                  Mean reward/step: 25.33
       Mean episode length/episode: 7.19
            Mean episode successes: 2.1655
Mean episode consecutive_successes: 5.5797
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 8.84s
                        Total time: 25322.54s
                               ETA: 993293.3s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.623s, learning 0.193s)
               Value function loss: 107885.1098
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 1670.16
               Mean episode length: 61.30
                  Mean reward/step: 26.43
       Mean episode length/episode: 7.19
            Mean episode successes: 2.0728
Mean episode consecutive_successes: 5.7172
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 8.82s
                        Total time: 25331.35s
                               ETA: 993229.4s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.300s, learning 0.165s)
               Value function loss: 96841.0430
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 1453.19
               Mean episode length: 61.99
                  Mean reward/step: 25.55
       Mean episode length/episode: 7.18
            Mean episode successes: 2.1553
Mean episode consecutive_successes: 5.7252
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 8.47s
                        Total time: 25339.82s
                               ETA: 993151.8s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.421s, learning 0.229s)
               Value function loss: 96582.6961
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 1548.82
               Mean episode length: 63.48
                  Mean reward/step: 24.58
       Mean episode length/episode: 7.17
            Mean episode successes: 2.0991
Mean episode consecutive_successes: 5.7618
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 8.65s
                        Total time: 25348.47s
                               ETA: 993081.5s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.206s, learning 0.179s)
               Value function loss: 92043.8221
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 1904.90
               Mean episode length: 63.96
                  Mean reward/step: 24.23
       Mean episode length/episode: 7.21
            Mean episode successes: 2.1387
Mean episode consecutive_successes: 5.7816
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 8.39s
                        Total time: 25356.85s
                               ETA: 993000.9s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.621s, learning 0.197s)
               Value function loss: 94636.4662
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 2058.05
               Mean episode length: 64.15
                  Mean reward/step: 22.77
       Mean episode length/episode: 7.23
            Mean episode successes: 1.9668
Mean episode consecutive_successes: 5.8811
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 8.82s
                        Total time: 25365.67s
                               ETA: 992937.2s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.142s, learning 0.247s)
               Value function loss: 86437.1160
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 1758.31
               Mean episode length: 65.51
                  Mean reward/step: 20.79
       Mean episode length/episode: 7.12
            Mean episode successes: 1.8252
Mean episode consecutive_successes: 5.8965
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 8.39s
                        Total time: 25374.06s
                               ETA: 992856.9s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.311s, learning 0.194s)
               Value function loss: 89159.6314
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 1097.82
               Mean episode length: 62.64
                  Mean reward/step: 23.43
       Mean episode length/episode: 7.19
            Mean episode successes: 1.8774
Mean episode consecutive_successes: 5.8121
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 8.50s
                        Total time: 25382.57s
                               ETA: 992781.1s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.324s, learning 0.199s)
               Value function loss: 93147.3215
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 2012.94
               Mean episode length: 64.22
                  Mean reward/step: 24.81
       Mean episode length/episode: 7.22
            Mean episode successes: 2.0205
Mean episode consecutive_successes: 5.8299
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 8.52s
                        Total time: 25391.09s
                               ETA: 992706.0s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.389s, learning 0.281s)
               Value function loss: 91805.5344
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 1456.33
               Mean episode length: 62.68
                  Mean reward/step: 24.83
       Mean episode length/episode: 7.27
            Mean episode successes: 2.0933
Mean episode consecutive_successes: 5.8160
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 8.67s
                        Total time: 25399.76s
                               ETA: 992636.8s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.524s, learning 0.191s)
               Value function loss: 93960.4768
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 1278.16
               Mean episode length: 62.98
                  Mean reward/step: 26.39
       Mean episode length/episode: 7.20
            Mean episode successes: 2.1602
Mean episode consecutive_successes: 5.8262
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 8.71s
                        Total time: 25408.47s
                               ETA: 992569.4s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.344s, learning 0.269s)
               Value function loss: 108992.3219
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 1800.97
               Mean episode length: 61.02
                  Mean reward/step: 26.27
       Mean episode length/episode: 7.16
            Mean episode successes: 2.1978
Mean episode consecutive_successes: 5.8316
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 8.61s
                        Total time: 25417.09s
                               ETA: 992498.0s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.424s, learning 0.179s)
               Value function loss: 90584.8086
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 1535.48
               Mean episode length: 61.21
                  Mean reward/step: 24.92
       Mean episode length/episode: 7.16
            Mean episode successes: 2.0630
Mean episode consecutive_successes: 5.9293
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 8.60s
                        Total time: 25425.69s
                               ETA: 992426.3s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.493s, learning 0.207s)
               Value function loss: 101883.6131
                    Surrogate loss: 0.0017
             Mean action noise std: 0.74
                       Mean reward: 1654.11
               Mean episode length: 62.37
                  Mean reward/step: 24.94
       Mean episode length/episode: 7.19
            Mean episode successes: 2.1499
Mean episode consecutive_successes: 5.9835
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 8.70s
                        Total time: 25434.39s
                               ETA: 992358.4s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.497s, learning 0.194s)
               Value function loss: 96570.8852
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 1334.49
               Mean episode length: 64.72
                  Mean reward/step: 24.82
       Mean episode length/episode: 7.12
            Mean episode successes: 2.1396
Mean episode consecutive_successes: 5.9699
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 8.69s
                        Total time: 25443.08s
                               ETA: 992290.3s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.116s, learning 0.171s)
               Value function loss: 102640.8043
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 1981.81
               Mean episode length: 65.20
                  Mean reward/step: 25.70
       Mean episode length/episode: 7.27
            Mean episode successes: 2.0171
Mean episode consecutive_successes: 6.1263
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 8.29s
                        Total time: 25451.37s
                               ETA: 992206.4s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.167s, learning 0.168s)
               Value function loss: 95649.2014
                    Surrogate loss: -0.0130
             Mean action noise std: 0.74
                       Mean reward: 1606.88
               Mean episode length: 64.21
                  Mean reward/step: 24.86
       Mean episode length/episode: 7.20
            Mean episode successes: 2.0063
Mean episode consecutive_successes: 6.1285
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 8.34s
                        Total time: 25459.70s
                               ETA: 992124.5s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.370s, learning 0.173s)
               Value function loss: 103126.4258
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 1303.18
               Mean episode length: 62.30
                  Mean reward/step: 25.67
       Mean episode length/episode: 7.25
            Mean episode successes: 2.1792
Mean episode consecutive_successes: 6.0634
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 8.54s
                        Total time: 25468.25s
                               ETA: 992050.7s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.383s, learning 0.178s)
               Value function loss: 124559.2633
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 2161.53
               Mean episode length: 66.36
                  Mean reward/step: 26.75
       Mean episode length/episode: 7.24
            Mean episode successes: 2.2227
Mean episode consecutive_successes: 6.0877
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 8.56s
                        Total time: 25476.81s
                               ETA: 991977.7s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.159s, learning 0.245s)
               Value function loss: 121650.5326
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 1580.90
               Mean episode length: 62.78
                  Mean reward/step: 28.44
       Mean episode length/episode: 7.19
            Mean episode successes: 2.2178
Mean episode consecutive_successes: 6.1803
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 8.40s
                        Total time: 25485.21s
                               ETA: 991898.6s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.190s, learning 0.354s)
               Value function loss: 109946.0398
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 1331.02
               Mean episode length: 61.83
                  Mean reward/step: 27.03
       Mean episode length/episode: 7.14
            Mean episode successes: 2.2461
Mean episode consecutive_successes: 6.1574
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 8.54s
                        Total time: 25493.75s
                               ETA: 991825.1s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.403s, learning 0.257s)
               Value function loss: 96502.2666
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 1472.09
               Mean episode length: 63.31
                  Mean reward/step: 27.44
       Mean episode length/episode: 7.18
            Mean episode successes: 2.3354
Mean episode consecutive_successes: 6.1436
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 8.66s
                        Total time: 25502.41s
                               ETA: 991756.0s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.319s, learning 0.192s)
               Value function loss: 111449.4570
                    Surrogate loss: 0.0011
             Mean action noise std: 0.74
                       Mean reward: 2148.99
               Mean episode length: 65.10
                  Mean reward/step: 27.15
       Mean episode length/episode: 7.22
            Mean episode successes: 2.3081
Mean episode consecutive_successes: 6.2614
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 8.51s
                        Total time: 25510.93s
                               ETA: 991681.3s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.214s, learning 0.192s)
               Value function loss: 112925.8785
                    Surrogate loss: -0.0136
             Mean action noise std: 0.74
                       Mean reward: 1699.04
               Mean episode length: 62.23
                  Mean reward/step: 27.36
       Mean episode length/episode: 7.10
            Mean episode successes: 2.1846
Mean episode consecutive_successes: 6.3091
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 8.41s
                        Total time: 25519.33s
                               ETA: 991602.5s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.370s, learning 0.177s)
               Value function loss: 107517.9348
                    Surrogate loss: -0.0161
             Mean action noise std: 0.74
                       Mean reward: 1534.15
               Mean episode length: 62.89
                  Mean reward/step: 27.18
       Mean episode length/episode: 7.27
            Mean episode successes: 2.2480
Mean episode consecutive_successes: 6.3241
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 8.55s
                        Total time: 25527.88s
                               ETA: 991529.3s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.543s, learning 0.249s)
               Value function loss: 110740.4859
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 1787.19
               Mean episode length: 63.56
                  Mean reward/step: 25.95
       Mean episode length/episode: 7.21
            Mean episode successes: 2.1738
Mean episode consecutive_successes: 6.3966
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 8.79s
                        Total time: 25536.67s
                               ETA: 991465.6s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.661s, learning 0.193s)
               Value function loss: 110241.8074
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 2172.71
               Mean episode length: 64.10
                  Mean reward/step: 25.97
       Mean episode length/episode: 7.22
            Mean episode successes: 2.1421
Mean episode consecutive_successes: 6.4556
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 8.85s
                        Total time: 25545.52s
                               ETA: 991404.3s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.183s, learning 0.181s)
               Value function loss: 106654.1062
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 1666.20
               Mean episode length: 63.16
                  Mean reward/step: 28.38
       Mean episode length/episode: 7.25
            Mean episode successes: 2.2324
Mean episode consecutive_successes: 6.4681
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 8.36s
                        Total time: 25553.89s
                               ETA: 991324.1s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.543s, learning 0.197s)
               Value function loss: 116059.0199
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 1599.60
               Mean episode length: 63.74
                  Mean reward/step: 27.97
       Mean episode length/episode: 7.14
            Mean episode successes: 2.2158
Mean episode consecutive_successes: 6.4867
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 8.74s
                        Total time: 25562.63s
                               ETA: 991258.5s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.478s, learning 0.243s)
               Value function loss: 112229.7047
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 2046.87
               Mean episode length: 64.68
                  Mean reward/step: 27.36
       Mean episode length/episode: 7.21
            Mean episode successes: 2.1963
Mean episode consecutive_successes: 6.5635
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 8.72s
                        Total time: 25571.35s
                               ETA: 991192.3s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.662s, learning 0.305s)
               Value function loss: 127083.5777
                    Surrogate loss: 0.0013
             Mean action noise std: 0.74
                       Mean reward: 1823.34
               Mean episode length: 65.18
                  Mean reward/step: 26.59
       Mean episode length/episode: 7.21
            Mean episode successes: 2.2495
Mean episode consecutive_successes: 6.5465
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 8.97s
                        Total time: 25580.32s
                               ETA: 991135.6s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.303s, learning 0.200s)
               Value function loss: 109914.4301
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 2142.67
               Mean episode length: 64.31
                  Mean reward/step: 27.74
       Mean episode length/episode: 7.26
            Mean episode successes: 2.1724
Mean episode consecutive_successes: 6.6231
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 8.50s
                        Total time: 25588.82s
                               ETA: 991060.9s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.536s, learning 0.194s)
               Value function loss: 113227.6609
                    Surrogate loss: 0.0156
             Mean action noise std: 0.74
                       Mean reward: 1803.93
               Mean episode length: 65.65
                  Mean reward/step: 26.20
       Mean episode length/episode: 7.22
            Mean episode successes: 2.1802
Mean episode consecutive_successes: 6.6257
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 8.73s
                        Total time: 25597.55s
                               ETA: 990995.2s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.372s, learning 0.179s)
               Value function loss: 106170.3184
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 1759.69
               Mean episode length: 63.65
                  Mean reward/step: 26.59
       Mean episode length/episode: 7.12
            Mean episode successes: 2.0737
Mean episode consecutive_successes: 6.6555
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 8.55s
                        Total time: 25606.10s
                               ETA: 990922.5s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.213s, learning 0.195s)
               Value function loss: 101573.9322
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 1987.60
               Mean episode length: 62.82
                  Mean reward/step: 26.60
       Mean episode length/episode: 7.17
            Mean episode successes: 2.1318
Mean episode consecutive_successes: 6.6497
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 8.41s
                        Total time: 25614.51s
                               ETA: 990844.4s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.590s, learning 0.211s)
               Value function loss: 118725.8166
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 1923.40
               Mean episode length: 63.44
                  Mean reward/step: 27.09
       Mean episode length/episode: 7.18
            Mean episode successes: 2.2075
Mean episode consecutive_successes: 6.5787
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 8.80s
                        Total time: 25623.31s
                               ETA: 990781.5s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.393s, learning 0.184s)
               Value function loss: 118760.9307
                    Surrogate loss: 0.0002
             Mean action noise std: 0.74
                       Mean reward: 1766.00
               Mean episode length: 65.16
                  Mean reward/step: 26.73
       Mean episode length/episode: 7.28
            Mean episode successes: 2.1636
Mean episode consecutive_successes: 6.6211
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 8.58s
                        Total time: 25631.89s
                               ETA: 990710.0s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.406s, learning 0.174s)
               Value function loss: 106779.7420
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 1681.71
               Mean episode length: 61.59
                  Mean reward/step: 27.80
       Mean episode length/episode: 7.20
            Mean episode successes: 2.2705
Mean episode consecutive_successes: 6.6037
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 8.58s
                        Total time: 25640.47s
                               ETA: 990638.7s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.537s, learning 0.217s)
               Value function loss: 114460.7881
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 2199.60
               Mean episode length: 62.43
                  Mean reward/step: 27.58
       Mean episode length/episode: 7.21
            Mean episode successes: 2.2153
Mean episode consecutive_successes: 6.6954
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 8.75s
                        Total time: 25649.22s
                               ETA: 990574.1s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.606s, learning 0.197s)
               Value function loss: 107555.0156
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: 1318.91
               Mean episode length: 60.44
                  Mean reward/step: 26.92
       Mean episode length/episode: 7.21
            Mean episode successes: 2.2314
Mean episode consecutive_successes: 6.6786
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 8.80s
                        Total time: 25658.02s
                               ETA: 990511.5s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.313s, learning 0.198s)
               Value function loss: 109306.1896
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 2166.30
               Mean episode length: 64.62
                  Mean reward/step: 28.31
       Mean episode length/episode: 7.32
            Mean episode successes: 2.2031
Mean episode consecutive_successes: 6.8072
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 8.51s
                        Total time: 25666.53s
                               ETA: 990437.6s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.367s, learning 0.230s)
               Value function loss: 107379.5340
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 1653.01
               Mean episode length: 63.97
                  Mean reward/step: 28.43
       Mean episode length/episode: 7.21
            Mean episode successes: 2.2988
Mean episode consecutive_successes: 6.7542
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 8.60s
                        Total time: 25675.13s
                               ETA: 990367.2s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.226s, learning 0.177s)
               Value function loss: 127643.3520
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 1794.68
               Mean episode length: 62.66
                  Mean reward/step: 31.57
       Mean episode length/episode: 7.17
            Mean episode successes: 2.4717
Mean episode consecutive_successes: 6.7286
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 8.40s
                        Total time: 25683.54s
                               ETA: 990289.3s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.407s, learning 0.176s)
               Value function loss: 158543.6543
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 2401.46
               Mean episode length: 64.70
                  Mean reward/step: 30.52
       Mean episode length/episode: 7.22
            Mean episode successes: 2.5132
Mean episode consecutive_successes: 6.8057
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 8.58s
                        Total time: 25692.12s
                               ETA: 990218.3s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.462s, learning 0.183s)
               Value function loss: 126123.5932
                    Surrogate loss: 0.0041
             Mean action noise std: 0.73
                       Mean reward: 1809.84
               Mean episode length: 64.46
                  Mean reward/step: 27.84
       Mean episode length/episode: 7.17
            Mean episode successes: 2.3130
Mean episode consecutive_successes: 6.8820
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 8.64s
                        Total time: 25700.76s
                               ETA: 990149.8s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.344s, learning 0.184s)
               Value function loss: 110490.7762
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 1960.76
               Mean episode length: 64.06
                  Mean reward/step: 26.47
       Mean episode length/episode: 7.26
            Mean episode successes: 2.2637
Mean episode consecutive_successes: 6.9319
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 8.53s
                        Total time: 25709.29s
                               ETA: 990076.8s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.506s, learning 0.200s)
               Value function loss: 113094.8639
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 1802.47
               Mean episode length: 62.83
                  Mean reward/step: 28.02
       Mean episode length/episode: 7.21
            Mean episode successes: 2.4180
Mean episode consecutive_successes: 6.8746
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 8.71s
                        Total time: 25718.00s
                               ETA: 990010.8s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.224s, learning 0.206s)
               Value function loss: 108407.2379
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 1833.54
               Mean episode length: 61.60
                  Mean reward/step: 29.07
       Mean episode length/episode: 7.27
            Mean episode successes: 2.5083
Mean episode consecutive_successes: 6.8882
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 8.43s
                        Total time: 25726.43s
                               ETA: 989934.2s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.671s, learning 0.177s)
               Value function loss: 114326.2264
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 2086.37
               Mean episode length: 64.73
                  Mean reward/step: 29.56
       Mean episode length/episode: 7.25
            Mean episode successes: 2.4619
Mean episode consecutive_successes: 6.9641
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 8.85s
                        Total time: 25735.27s
                               ETA: 989873.7s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.616s, learning 0.257s)
               Value function loss: 113125.2586
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: 1783.70
               Mean episode length: 62.25
                  Mean reward/step: 27.47
       Mean episode length/episode: 7.15
            Mean episode successes: 2.1982
Mean episode consecutive_successes: 7.0039
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 8.87s
                        Total time: 25744.15s
                               ETA: 989814.2s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.611s, learning 0.201s)
               Value function loss: 118186.4896
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 1773.59
               Mean episode length: 64.78
                  Mean reward/step: 29.80
       Mean episode length/episode: 7.23
            Mean episode successes: 2.3589
Mean episode consecutive_successes: 7.0065
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 8.81s
                        Total time: 25752.96s
                               ETA: 989752.4s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.608s, learning 0.192s)
               Value function loss: 118513.8578
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 2346.71
               Mean episode length: 65.03
                  Mean reward/step: 28.49
       Mean episode length/episode: 7.25
            Mean episode successes: 2.2935
Mean episode consecutive_successes: 7.1087
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 8.80s
                        Total time: 25761.76s
                               ETA: 989690.2s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.489s, learning 0.213s)
               Value function loss: 104716.8447
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 1980.20
               Mean episode length: 63.79
                  Mean reward/step: 26.61
       Mean episode length/episode: 7.18
            Mean episode successes: 2.1646
Mean episode consecutive_successes: 7.1051
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 8.70s
                        Total time: 25770.46s
                               ETA: 989624.3s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.398s, learning 0.193s)
               Value function loss: 98109.5006
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 1569.28
               Mean episode length: 63.82
                  Mean reward/step: 27.43
       Mean episode length/episode: 7.15
            Mean episode successes: 2.2612
Mean episode consecutive_successes: 6.9869
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 8.59s
                        Total time: 25779.05s
                               ETA: 989554.2s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.546s, learning 0.240s)
               Value function loss: 109782.3049
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 1740.48
               Mean episode length: 63.48
                  Mean reward/step: 29.79
       Mean episode length/episode: 7.28
            Mean episode successes: 2.3813
Mean episode consecutive_successes: 6.9965
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 8.79s
                        Total time: 25787.84s
                               ETA: 989491.5s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.588s, learning 0.204s)
               Value function loss: 118087.0156
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 1458.98
               Mean episode length: 62.74
                  Mean reward/step: 29.57
       Mean episode length/episode: 7.17
            Mean episode successes: 2.3159
Mean episode consecutive_successes: 7.0328
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 8.79s
                        Total time: 25796.63s
                               ETA: 989429.2s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.675s, learning 0.193s)
               Value function loss: 107696.7504
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 1740.77
               Mean episode length: 62.59
                  Mean reward/step: 27.97
       Mean episode length/episode: 7.21
            Mean episode successes: 2.2368
Mean episode consecutive_successes: 7.0945
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 8.87s
                        Total time: 25805.50s
                               ETA: 989369.8s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.509s, learning 0.170s)
               Value function loss: 104906.3508
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: 1398.55
               Mean episode length: 63.26
                  Mean reward/step: 27.58
       Mean episode length/episode: 7.29
            Mean episode successes: 2.4194
Mean episode consecutive_successes: 7.0141
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 8.68s
                        Total time: 25814.18s
                               ETA: 989303.2s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.403s, learning 0.238s)
               Value function loss: 117060.2029
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 1972.77
               Mean episode length: 63.59
                  Mean reward/step: 27.70
       Mean episode length/episode: 7.15
            Mean episode successes: 2.2046
Mean episode consecutive_successes: 7.1227
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 8.64s
                        Total time: 25822.82s
                               ETA: 989235.2s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.460s, learning 0.195s)
               Value function loss: 100403.5439
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 2038.73
               Mean episode length: 65.19
                  Mean reward/step: 26.09
       Mean episode length/episode: 7.20
            Mean episode successes: 2.1602
Mean episode consecutive_successes: 7.1247
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 8.65s
                        Total time: 25831.47s
                               ETA: 989167.8s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.634s, learning 0.172s)
               Value function loss: 110368.3506
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 1266.82
               Mean episode length: 61.89
                  Mean reward/step: 27.32
       Mean episode length/episode: 7.27
            Mean episode successes: 2.2334
Mean episode consecutive_successes: 7.0123
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 8.81s
                        Total time: 25840.28s
                               ETA: 989106.2s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.875s, learning 0.204s)
               Value function loss: 111010.3547
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: 1588.41
               Mean episode length: 65.63
                  Mean reward/step: 30.26
       Mean episode length/episode: 7.17
            Mean episode successes: 2.4077
Mean episode consecutive_successes: 6.9448
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 9.08s
                        Total time: 25849.36s
                               ETA: 989055.0s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.575s, learning 0.188s)
               Value function loss: 118019.6348
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 2011.71
               Mean episode length: 65.77
                  Mean reward/step: 30.89
       Mean episode length/episode: 7.10
            Mean episode successes: 2.4702
Mean episode consecutive_successes: 6.9543
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 8.76s
                        Total time: 25858.12s
                               ETA: 988991.9s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.585s, learning 0.189s)
               Value function loss: 129709.7750
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 2197.09
               Mean episode length: 65.47
                  Mean reward/step: 30.49
       Mean episode length/episode: 7.24
            Mean episode successes: 2.3818
Mean episode consecutive_successes: 7.0717
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 8.77s
                        Total time: 25866.89s
                               ETA: 988929.2s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.374s, learning 0.174s)
               Value function loss: 122968.6662
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 1919.57
               Mean episode length: 65.67
                  Mean reward/step: 30.58
       Mean episode length/episode: 7.24
            Mean episode successes: 2.4619
Mean episode consecutive_successes: 7.0871
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 8.55s
                        Total time: 25875.44s
                               ETA: 988857.9s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.378s, learning 0.173s)
               Value function loss: 122141.8027
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 2054.44
               Mean episode length: 63.26
                  Mean reward/step: 30.60
       Mean episode length/episode: 7.24
            Mean episode successes: 2.5601
Mean episode consecutive_successes: 7.0947
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 8.55s
                        Total time: 25883.99s
                               ETA: 988786.8s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.337s, learning 0.250s)
               Value function loss: 128346.6234
                    Surrogate loss: 0.0010
             Mean action noise std: 0.73
                       Mean reward: 1544.52
               Mean episode length: 62.15
                  Mean reward/step: 32.47
       Mean episode length/episode: 7.21
            Mean episode successes: 2.5601
Mean episode consecutive_successes: 7.1464
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 8.59s
                        Total time: 25892.58s
                               ETA: 988717.1s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.515s, learning 0.204s)
               Value function loss: 125316.5986
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 1973.88
               Mean episode length: 65.51
                  Mean reward/step: 33.59
       Mean episode length/episode: 7.29
            Mean episode successes: 2.6650
Mean episode consecutive_successes: 7.2925
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 8.72s
                        Total time: 25901.30s
                               ETA: 988652.5s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.260s, learning 0.177s)
               Value function loss: 128668.5449
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 1939.00
               Mean episode length: 62.08
                  Mean reward/step: 34.44
       Mean episode length/episode: 7.23
            Mean episode successes: 2.7061
Mean episode consecutive_successes: 7.3718
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 8.44s
                        Total time: 25909.74s
                               ETA: 988577.2s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.476s, learning 0.177s)
               Value function loss: 126173.9875
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 1913.58
               Mean episode length: 66.94
                  Mean reward/step: 32.73
       Mean episode length/episode: 7.25
            Mean episode successes: 2.6919
Mean episode consecutive_successes: 7.4525
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 8.65s
                        Total time: 25918.39s
                               ETA: 988510.2s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.636s, learning 0.276s)
               Value function loss: 128137.9125
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 2219.39
               Mean episode length: 65.43
                  Mean reward/step: 34.35
       Mean episode length/episode: 7.18
            Mean episode successes: 2.6865
Mean episode consecutive_successes: 7.5206
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 8.91s
                        Total time: 25927.30s
                               ETA: 988453.0s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.825s, learning 0.206s)
               Value function loss: 123560.5154
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 2252.90
               Mean episode length: 67.48
                  Mean reward/step: 32.98
       Mean episode length/episode: 7.21
            Mean episode successes: 2.7749
Mean episode consecutive_successes: 7.6249
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 9.03s
                        Total time: 25936.33s
                               ETA: 988400.5s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.375s, learning 0.162s)
               Value function loss: 130680.5338
                    Surrogate loss: 0.0040
             Mean action noise std: 0.73
                       Mean reward: 2087.70
               Mean episode length: 63.45
                  Mean reward/step: 30.00
       Mean episode length/episode: 7.17
            Mean episode successes: 2.4780
Mean episode consecutive_successes: 7.7421
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 8.54s
                        Total time: 25944.87s
                               ETA: 988329.2s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.476s, learning 0.220s)
               Value function loss: 118224.4852
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 1726.39
               Mean episode length: 66.28
                  Mean reward/step: 29.42
       Mean episode length/episode: 7.16
            Mean episode successes: 2.4546
Mean episode consecutive_successes: 7.7076
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 8.70s
                        Total time: 25953.57s
                               ETA: 988263.9s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.496s, learning 0.215s)
               Value function loss: 113574.2207
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 1503.54
               Mean episode length: 61.74
                  Mean reward/step: 27.82
       Mean episode length/episode: 7.20
            Mean episode successes: 2.3701
Mean episode consecutive_successes: 7.6589
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 8.71s
                        Total time: 25962.28s
                               ETA: 988199.3s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.539s, learning 0.309s)
               Value function loss: 116462.4867
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 1809.06
               Mean episode length: 62.50
                  Mean reward/step: 26.57
       Mean episode length/episode: 7.15
            Mean episode successes: 2.1948
Mean episode consecutive_successes: 7.6581
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 8.85s
                        Total time: 25971.12s
                               ETA: 988139.9s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.537s, learning 0.209s)
               Value function loss: 112166.1092
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 1579.46
               Mean episode length: 62.93
                  Mean reward/step: 26.59
       Mean episode length/episode: 7.27
            Mean episode successes: 2.1602
Mean episode consecutive_successes: 7.6391
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 8.75s
                        Total time: 25979.87s
                               ETA: 988076.7s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.150s, learning 0.168s)
               Value function loss: 130994.5605
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 2210.33
               Mean episode length: 67.08
                  Mean reward/step: 28.83
       Mean episode length/episode: 7.24
            Mean episode successes: 2.1636
Mean episode consecutive_successes: 7.6553
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 8.32s
                        Total time: 25988.19s
                               ETA: 987997.3s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.737s, learning 0.200s)
               Value function loss: 130730.9043
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 1551.43
               Mean episode length: 62.89
                  Mean reward/step: 30.88
       Mean episode length/episode: 7.28
            Mean episode successes: 2.4038
Mean episode consecutive_successes: 7.5715
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 8.94s
                        Total time: 25997.13s
                               ETA: 987941.5s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.583s, learning 0.202s)
               Value function loss: 135279.8502
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 1472.00
               Mean episode length: 63.73
                  Mean reward/step: 31.39
       Mean episode length/episode: 7.22
            Mean episode successes: 2.5835
Mean episode consecutive_successes: 7.4991
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 8.79s
                        Total time: 26005.91s
                               ETA: 987879.9s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.364s, learning 0.173s)
               Value function loss: 128076.4383
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 1694.86
               Mean episode length: 63.24
                  Mean reward/step: 32.03
       Mean episode length/episode: 7.20
            Mean episode successes: 2.6826
Mean episode consecutive_successes: 7.4460
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 8.54s
                        Total time: 26014.45s
                               ETA: 987808.9s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.710s, learning 0.196s)
               Value function loss: 128051.2461
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 2150.93
               Mean episode length: 63.74
                  Mean reward/step: 32.17
       Mean episode length/episode: 7.18
            Mean episode successes: 2.6255
Mean episode consecutive_successes: 7.4852
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 8.91s
                        Total time: 26023.35s
                               ETA: 987752.0s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.159s, learning 0.167s)
               Value function loss: 131610.3949
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 2329.72
               Mean episode length: 66.22
                  Mean reward/step: 31.44
       Mean episode length/episode: 7.21
            Mean episode successes: 2.6294
Mean episode consecutive_successes: 7.6032
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 8.33s
                        Total time: 26031.68s
                               ETA: 987673.1s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.488s, learning 0.351s)
               Value function loss: 120252.6738
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 2088.36
               Mean episode length: 64.27
                  Mean reward/step: 30.11
       Mean episode length/episode: 7.20
            Mean episode successes: 2.4409
Mean episode consecutive_successes: 7.6857
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 8.84s
                        Total time: 26040.52s
                               ETA: 987613.8s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.673s, learning 0.217s)
               Value function loss: 131173.9715
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 2020.78
               Mean episode length: 64.40
                  Mean reward/step: 31.15
       Mean episode length/episode: 7.27
            Mean episode successes: 2.3975
Mean episode consecutive_successes: 7.7623
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 8.89s
                        Total time: 26049.41s
                               ETA: 987556.4s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.411s, learning 0.170s)
               Value function loss: 134107.7834
                    Surrogate loss: 0.0138
             Mean action noise std: 0.73
                       Mean reward: 1937.22
               Mean episode length: 63.56
                  Mean reward/step: 31.02
       Mean episode length/episode: 7.26
            Mean episode successes: 2.3916
Mean episode consecutive_successes: 7.7798
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 8.58s
                        Total time: 26057.99s
                               ETA: 987487.3s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.402s, learning 0.190s)
               Value function loss: 128917.7607
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 2120.51
               Mean episode length: 63.58
                  Mean reward/step: 33.10
       Mean episode length/episode: 7.27
            Mean episode successes: 2.6758
Mean episode consecutive_successes: 7.7232
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 8.59s
                        Total time: 26066.58s
                               ETA: 987418.7s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.419s, learning 0.170s)
               Value function loss: 127898.2520
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 1967.63
               Mean episode length: 64.22
                  Mean reward/step: 31.78
       Mean episode length/episode: 7.28
            Mean episode successes: 2.7456
Mean episode consecutive_successes: 7.7271
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 8.59s
                        Total time: 26075.17s
                               ETA: 987350.1s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.084s, learning 0.169s)
               Value function loss: 135000.3809
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 2667.99
               Mean episode length: 67.03
                  Mean reward/step: 33.46
       Mean episode length/episode: 7.23
            Mean episode successes: 2.6743
Mean episode consecutive_successes: 7.8261
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 8.25s
                        Total time: 26083.42s
                               ETA: 987268.7s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.426s, learning 0.242s)
               Value function loss: 122748.2928
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 1797.30
               Mean episode length: 63.68
                  Mean reward/step: 32.05
       Mean episode length/episode: 7.15
            Mean episode successes: 2.7490
Mean episode consecutive_successes: 7.7798
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 8.67s
                        Total time: 26092.09s
                               ETA: 987203.1s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.271s, learning 0.194s)
               Value function loss: 121162.6730
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 1980.20
               Mean episode length: 64.09
                  Mean reward/step: 31.68
       Mean episode length/episode: 7.11
            Mean episode successes: 2.6226
Mean episode consecutive_successes: 7.7695
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 8.46s
                        Total time: 26100.55s
                               ETA: 987129.9s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.803s, learning 0.229s)
               Value function loss: 126713.3945
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 2231.81
               Mean episode length: 65.14
                  Mean reward/step: 30.85
       Mean episode length/episode: 7.20
            Mean episode successes: 2.4331
Mean episode consecutive_successes: 7.9030
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 9.03s
                        Total time: 26109.59s
                               ETA: 987078.1s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.467s, learning 0.287s)
               Value function loss: 121922.7494
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 1764.69
               Mean episode length: 63.40
                  Mean reward/step: 29.45
       Mean episode length/episode: 7.25
            Mean episode successes: 2.4092
Mean episode consecutive_successes: 7.9037
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 8.75s
                        Total time: 26118.34s
                               ETA: 987015.9s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.710s, learning 0.233s)
               Value function loss: 135671.8770
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 1859.61
               Mean episode length: 62.92
                  Mean reward/step: 29.45
       Mean episode length/episode: 7.28
            Mean episode successes: 2.4692
Mean episode consecutive_successes: 7.9046
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 8.94s
                        Total time: 26127.28s
                               ETA: 986960.9s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.447s, learning 0.188s)
               Value function loss: 125576.9937
                    Surrogate loss: 0.0190
             Mean action noise std: 0.73
                       Mean reward: 2044.65
               Mean episode length: 66.62
                  Mean reward/step: 30.26
       Mean episode length/episode: 7.23
            Mean episode successes: 2.4209
Mean episode consecutive_successes: 7.9101
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 8.64s
                        Total time: 26135.92s
                               ETA: 986894.3s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1755 steps/s (collection: 9.095s, learning 0.236s)
               Value function loss: 121129.7570
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 1972.74
               Mean episode length: 64.56
                  Mean reward/step: 31.53
       Mean episode length/episode: 7.17
            Mean episode successes: 2.4390
Mean episode consecutive_successes: 7.9081
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 9.33s
                        Total time: 26145.25s
                               ETA: 986854.0s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.621s, learning 0.169s)
               Value function loss: 109542.4760
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 1912.60
               Mean episode length: 64.74
                  Mean reward/step: 32.83
       Mean episode length/episode: 7.25
            Mean episode successes: 2.6113
Mean episode consecutive_successes: 7.8500
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 8.79s
                        Total time: 26154.04s
                               ETA: 986793.4s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.588s, learning 0.234s)
               Value function loss: 124796.8553
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 2095.09
               Mean episode length: 64.18
                  Mean reward/step: 33.59
       Mean episode length/episode: 7.26
            Mean episode successes: 2.6577
Mean episode consecutive_successes: 7.8890
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 8.82s
                        Total time: 26162.86s
                               ETA: 986733.9s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.458s, learning 0.265s)
               Value function loss: 132303.0977
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 2106.83
               Mean episode length: 65.80
                  Mean reward/step: 34.83
       Mean episode length/episode: 7.16
            Mean episode successes: 2.7593
Mean episode consecutive_successes: 7.8799
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 8.72s
                        Total time: 26171.59s
                               ETA: 986670.8s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.757s, learning 0.170s)
               Value function loss: 145885.2402
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 2602.64
               Mean episode length: 66.21
                  Mean reward/step: 33.16
       Mean episode length/episode: 7.21
            Mean episode successes: 2.6504
Mean episode consecutive_successes: 7.9394
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 8.93s
                        Total time: 26180.51s
                               ETA: 986615.4s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.720s, learning 0.165s)
               Value function loss: 134448.0242
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 2136.75
               Mean episode length: 66.51
                  Mean reward/step: 33.59
       Mean episode length/episode: 7.18
            Mean episode successes: 2.5352
Mean episode consecutive_successes: 8.0320
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 8.88s
                        Total time: 26189.40s
                               ETA: 986558.4s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.256s, learning 0.173s)
               Value function loss: 125182.4043
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 2127.22
               Mean episode length: 63.10
                  Mean reward/step: 34.00
       Mean episode length/episode: 7.19
            Mean episode successes: 2.5391
Mean episode consecutive_successes: 8.0208
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 8.43s
                        Total time: 26197.83s
                               ETA: 986484.3s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.208s, learning 0.170s)
               Value function loss: 130651.8771
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 2356.38
               Mean episode length: 65.79
                  Mean reward/step: 35.16
       Mean episode length/episode: 7.27
            Mean episode successes: 2.7876
Mean episode consecutive_successes: 8.0299
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 8.38s
                        Total time: 26206.20s
                               ETA: 986408.4s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.360s, learning 0.183s)
               Value function loss: 135950.0844
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 2357.98
               Mean episode length: 64.09
                  Mean reward/step: 35.58
       Mean episode length/episode: 7.14
            Mean episode successes: 2.7793
Mean episode consecutive_successes: 8.1720
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 8.54s
                        Total time: 26214.75s
                               ETA: 986338.7s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.875s, learning 0.173s)
               Value function loss: 149297.5488
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 2741.73
               Mean episode length: 65.42
                  Mean reward/step: 35.37
       Mean episode length/episode: 7.36
            Mean episode successes: 2.8940
Mean episode consecutive_successes: 8.3056
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 9.05s
                        Total time: 26223.79s
                               ETA: 986288.0s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.772s, learning 0.165s)
               Value function loss: 182121.9523
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 2340.68
               Mean episode length: 64.13
                  Mean reward/step: 35.51
       Mean episode length/episode: 7.21
            Mean episode successes: 2.7720
Mean episode consecutive_successes: 8.4188
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 8.94s
                        Total time: 26232.73s
                               ETA: 986233.2s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.843s, learning 0.190s)
               Value function loss: 177285.7055
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 1763.37
               Mean episode length: 64.35
                  Mean reward/step: 35.77
       Mean episode length/episode: 7.16
            Mean episode successes: 2.7842
Mean episode consecutive_successes: 8.4217
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 9.03s
                        Total time: 26241.76s
                               ETA: 986182.1s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.441s, learning 0.210s)
               Value function loss: 186941.6805
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 1701.00
               Mean episode length: 61.95
                  Mean reward/step: 34.45
       Mean episode length/episode: 7.22
            Mean episode successes: 2.8037
Mean episode consecutive_successes: 8.4202
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 8.65s
                        Total time: 26250.41s
                               ETA: 986116.6s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.419s, learning 0.189s)
               Value function loss: 192383.2547
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 2388.33
               Mean episode length: 67.32
                  Mean reward/step: 35.08
       Mean episode length/episode: 7.15
            Mean episode successes: 2.8315
Mean episode consecutive_successes: 8.4698
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 8.61s
                        Total time: 26259.02s
                               ETA: 986049.6s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.485s, learning 0.176s)
               Value function loss: 153468.5387
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 2756.83
               Mean episode length: 65.45
                  Mean reward/step: 32.26
       Mean episode length/episode: 7.20
            Mean episode successes: 2.5947
Mean episode consecutive_successes: 8.5909
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 8.66s
                        Total time: 26267.68s
                               ETA: 985984.6s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.487s, learning 0.165s)
               Value function loss: 168190.9246
                    Surrogate loss: -0.0140
             Mean action noise std: 0.73
                       Mean reward: 1980.86
               Mean episode length: 64.59
                  Mean reward/step: 33.42
       Mean episode length/episode: 7.17
            Mean episode successes: 2.5688
Mean episode consecutive_successes: 8.5898
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 8.65s
                        Total time: 26276.34s
                               ETA: 985919.3s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.852s, learning 0.163s)
               Value function loss: 151308.6555
                    Surrogate loss: 0.0010
             Mean action noise std: 0.73
                       Mean reward: 2071.36
               Mean episode length: 65.55
                  Mean reward/step: 32.86
       Mean episode length/episode: 7.22
            Mean episode successes: 2.6226
Mean episode consecutive_successes: 8.5866
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 9.02s
                        Total time: 26285.35s
                               ETA: 985867.7s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.359s, learning 0.229s)
               Value function loss: 123120.0748
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 2274.01
               Mean episode length: 65.05
                  Mean reward/step: 33.59
       Mean episode length/episode: 7.25
            Mean episode successes: 2.5527
Mean episode consecutive_successes: 8.6635
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 8.59s
                        Total time: 26293.94s
                               ETA: 985800.1s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.679s, learning 0.284s)
               Value function loss: 123029.3268
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 1827.70
               Mean episode length: 64.65
                  Mean reward/step: 32.41
       Mean episode length/episode: 7.32
            Mean episode successes: 2.5938
Mean episode consecutive_successes: 8.6682
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 8.96s
                        Total time: 26302.90s
                               ETA: 985746.5s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.784s, learning 0.231s)
               Value function loss: 125592.1064
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 2266.40
               Mean episode length: 66.48
                  Mean reward/step: 34.33
       Mean episode length/episode: 7.09
            Mean episode successes: 2.5854
Mean episode consecutive_successes: 8.6363
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 9.02s
                        Total time: 26311.92s
                               ETA: 985695.0s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.542s, learning 0.294s)
               Value function loss: 136370.4893
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2173.05
               Mean episode length: 66.00
                  Mean reward/step: 33.66
       Mean episode length/episode: 7.28
            Mean episode successes: 2.5903
Mean episode consecutive_successes: 8.6864
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 8.84s
                        Total time: 26320.75s
                               ETA: 985636.8s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.527s, learning 0.169s)
               Value function loss: 140233.8922
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 2183.69
               Mean episode length: 65.66
                  Mean reward/step: 33.55
       Mean episode length/episode: 7.26
            Mean episode successes: 2.7090
Mean episode consecutive_successes: 8.6280
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 8.70s
                        Total time: 26329.45s
                               ETA: 985573.4s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1150 steps/s (collection: 14.065s, learning 0.173s)
               Value function loss: 153475.4137
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 2388.86
               Mean episode length: 65.59
                  Mean reward/step: 33.62
       Mean episode length/episode: 7.22
            Mean episode successes: 2.6694
Mean episode consecutive_successes: 8.6812
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 14.24s
                        Total time: 26343.69s
                               ETA: 985717.4s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.324s, learning 0.210s)
               Value function loss: 152393.4562
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 2455.07
               Mean episode length: 67.55
                  Mean reward/step: 33.11
       Mean episode length/episode: 7.25
            Mean episode successes: 2.5625
Mean episode consecutive_successes: 8.7314
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 16.53s
                        Total time: 26360.22s
                               ETA: 985947.2s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.707s, learning 0.190s)
               Value function loss: 141788.4879
                    Surrogate loss: 0.0404
             Mean action noise std: 0.73
                       Mean reward: 2088.65
               Mean episode length: 65.14
                  Mean reward/step: 34.26
       Mean episode length/episode: 7.32
            Mean episode successes: 2.6333
Mean episode consecutive_successes: 8.7484
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 16.90s
                        Total time: 26377.12s
                               ETA: 986190.3s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.628s, learning 0.310s)
               Value function loss: 124327.5668
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 2574.14
               Mean episode length: 65.48
                  Mean reward/step: 34.99
       Mean episode length/episode: 7.15
            Mean episode successes: 2.6753
Mean episode consecutive_successes: 8.7425
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 16.94s
                        Total time: 26394.06s
                               ETA: 986434.8s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.207s, learning 0.162s)
               Value function loss: 118803.6340
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 1448.96
               Mean episode length: 64.00
                  Mean reward/step: 34.81
       Mean episode length/episode: 7.21
            Mean episode successes: 2.8428
Mean episode consecutive_successes: 8.5887
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 16.37s
                        Total time: 26410.43s
                               ETA: 986657.8s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.565s, learning 0.280s)
               Value function loss: 138521.0684
                    Surrogate loss: 0.0019
             Mean action noise std: 0.73
                       Mean reward: 2191.24
               Mean episode length: 64.54
                  Mean reward/step: 35.31
       Mean episode length/episode: 7.19
            Mean episode successes: 2.8057
Mean episode consecutive_successes: 8.6300
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 16.84s
                        Total time: 26427.27s
                               ETA: 986898.4s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.195s, learning 0.165s)
               Value function loss: 139026.5086
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 2233.81
               Mean episode length: 64.40
                  Mean reward/step: 35.44
       Mean episode length/episode: 7.19
            Mean episode successes: 2.7739
Mean episode consecutive_successes: 8.6862
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 16.36s
                        Total time: 26443.63s
                               ETA: 987120.7s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.334s, learning 0.269s)
               Value function loss: 125729.9885
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 2102.81
               Mean episode length: 62.84
                  Mean reward/step: 34.48
       Mean episode length/episode: 7.23
            Mean episode successes: 2.7690
Mean episode consecutive_successes: 8.6942
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 16.60s
                        Total time: 26460.23s
                               ETA: 987351.9s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.437s, learning 0.371s)
               Value function loss: 140090.7148
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 1597.00
               Mean episode length: 62.83
                  Mean reward/step: 33.83
       Mean episode length/episode: 7.18
            Mean episode successes: 2.6938
Mean episode consecutive_successes: 8.6891
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 16.81s
                        Total time: 26477.04s
                               ETA: 987590.6s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.419s, learning 0.171s)
               Value function loss: 138003.4789
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 2275.10
               Mean episode length: 63.90
                  Mean reward/step: 34.07
       Mean episode length/episode: 7.26
            Mean episode successes: 2.5913
Mean episode consecutive_successes: 8.8086
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 16.59s
                        Total time: 26493.63s
                               ETA: 987820.9s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.535s, learning 0.257s)
               Value function loss: 137940.4832
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 2360.24
               Mean episode length: 66.70
                  Mean reward/step: 36.19
       Mean episode length/episode: 7.21
            Mean episode successes: 2.6382
Mean episode consecutive_successes: 8.8134
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 16.79s
                        Total time: 26510.42s
                               ETA: 988058.5s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.922s, learning 0.186s)
               Value function loss: 132672.0049
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 2086.99
               Mean episode length: 62.97
                  Mean reward/step: 36.24
       Mean episode length/episode: 7.30
            Mean episode successes: 2.8198
Mean episode consecutive_successes: 8.8189
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 17.11s
                        Total time: 26527.53s
                               ETA: 988307.8s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.464s, learning 0.249s)
               Value function loss: 137794.9930
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2748.78
               Mean episode length: 67.38
                  Mean reward/step: 36.25
       Mean episode length/episode: 7.26
            Mean episode successes: 2.9131
Mean episode consecutive_successes: 8.7828
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 16.71s
                        Total time: 26544.24s
                               ETA: 988542.2s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.696s, learning 0.213s)
               Value function loss: 150479.0898
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 2289.27
               Mean episode length: 65.16
                  Mean reward/step: 38.12
       Mean episode length/episode: 7.25
            Mean episode successes: 2.8931
Mean episode consecutive_successes: 8.9134
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 16.91s
                        Total time: 26561.15s
                               ETA: 988783.6s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.219s, learning 0.169s)
               Value function loss: 143336.2723
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 2652.12
               Mean episode length: 65.79
                  Mean reward/step: 37.80
       Mean episode length/episode: 7.16
            Mean episode successes: 2.9185
Mean episode consecutive_successes: 8.9301
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 16.39s
                        Total time: 26577.54s
                               ETA: 989005.5s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.163s, learning 0.173s)
               Value function loss: 143011.0883
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 2708.52
               Mean episode length: 68.01
                  Mean reward/step: 36.90
       Mean episode length/episode: 7.30
            Mean episode successes: 2.9717
Mean episode consecutive_successes: 8.9931
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 16.34s
                        Total time: 26593.88s
                               ETA: 989225.2s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.324s, learning 0.160s)
               Value function loss: 129932.8270
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 2289.98
               Mean episode length: 67.19
                  Mean reward/step: 36.71
       Mean episode length/episode: 7.21
            Mean episode successes: 2.8828
Mean episode consecutive_successes: 9.0369
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 16.48s
                        Total time: 26610.36s
                               ETA: 989450.3s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.347s, learning 0.176s)
               Value function loss: 135534.1855
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 2016.86
               Mean episode length: 66.62
                  Mean reward/step: 38.33
       Mean episode length/episode: 7.25
            Mean episode successes: 2.9688
Mean episode consecutive_successes: 9.0540
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 16.52s
                        Total time: 26626.88s
                               ETA: 989676.6s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.908s, learning 0.173s)
               Value function loss: 146353.0570
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 2014.19
               Mean episode length: 62.73
                  Mean reward/step: 37.26
       Mean episode length/episode: 7.26
            Mean episode successes: 2.9326
Mean episode consecutive_successes: 9.1401
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 17.08s
                        Total time: 26643.97s
                               ETA: 989923.5s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.582s, learning 0.238s)
               Value function loss: 161174.0438
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 2841.81
               Mean episode length: 65.52
                  Mean reward/step: 38.16
       Mean episode length/episode: 7.22
            Mean episode successes: 2.8672
Mean episode consecutive_successes: 9.3104
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 16.82s
                        Total time: 26660.78s
                               ETA: 990160.4s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.254s, learning 0.170s)
               Value function loss: 138905.1195
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 2455.97
               Mean episode length: 65.56
                  Mean reward/step: 36.16
       Mean episode length/episode: 7.24
            Mean episode successes: 2.9014
Mean episode consecutive_successes: 9.3074
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 16.42s
                        Total time: 26677.21s
                               ETA: 990382.5s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.535s, learning 0.262s)
               Value function loss: 146816.2586
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 2663.85
               Mean episode length: 65.01
                  Mean reward/step: 33.90
       Mean episode length/episode: 7.17
            Mean episode successes: 2.6450
Mean episode consecutive_successes: 9.3655
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 16.80s
                        Total time: 26694.01s
                               ETA: 990618.2s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.691s, learning 0.173s)
               Value function loss: 178187.1859
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 1978.19
               Mean episode length: 64.53
                  Mean reward/step: 34.28
       Mean episode length/episode: 7.25
            Mean episode successes: 2.7764
Mean episode consecutive_successes: 9.2067
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 16.86s
                        Total time: 26710.87s
                               ETA: 990856.3s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.223s, learning 0.178s)
               Value function loss: 127688.7875
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 2409.72
               Mean episode length: 66.97
                  Mean reward/step: 32.57
       Mean episode length/episode: 7.19
            Mean episode successes: 2.4741
Mean episode consecutive_successes: 9.2892
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 16.40s
                        Total time: 26727.27s
                               ETA: 991076.9s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.501s, learning 0.156s)
               Value function loss: 131567.4891
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 2348.90
               Mean episode length: 65.96
                  Mean reward/step: 34.45
       Mean episode length/episode: 7.27
            Mean episode successes: 2.6753
Mean episode consecutive_successes: 9.2483
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 16.66s
                        Total time: 26743.93s
                               ETA: 991306.9s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.987s, learning 0.158s)
               Value function loss: 125600.6516
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 2745.38
               Mean episode length: 67.17
                  Mean reward/step: 36.55
       Mean episode length/episode: 7.19
            Mean episode successes: 2.6855
Mean episode consecutive_successes: 9.2832
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 16.15s
                        Total time: 26760.07s
                               ETA: 991517.7s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.613s, learning 0.180s)
               Value function loss: 135667.1910
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 2496.39
               Mean episode length: 64.98
                  Mean reward/step: 36.90
       Mean episode length/episode: 7.28
            Mean episode successes: 2.8247
Mean episode consecutive_successes: 9.2577
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 16.79s
                        Total time: 26776.87s
                               ETA: 991752.4s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.628s, learning 0.295s)
               Value function loss: 164067.0941
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 2293.14
               Mean episode length: 64.49
                  Mean reward/step: 36.95
       Mean episode length/episode: 7.21
            Mean episode successes: 2.8745
Mean episode consecutive_successes: 9.2438
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 16.92s
                        Total time: 26793.79s
                               ETA: 991991.6s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.532s, learning 0.155s)
               Value function loss: 198265.5434
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 2534.72
               Mean episode length: 65.89
                  Mean reward/step: 37.01
       Mean episode length/episode: 7.21
            Mean episode successes: 2.7988
Mean episode consecutive_successes: 9.2869
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 16.69s
                        Total time: 26810.48s
                               ETA: 992222.0s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.081s, learning 0.168s)
               Value function loss: 146494.4090
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 2458.44
               Mean episode length: 69.58
                  Mean reward/step: 36.82
       Mean episode length/episode: 7.19
            Mean episode successes: 2.8052
Mean episode consecutive_successes: 9.3237
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 16.25s
                        Total time: 26826.72s
                               ETA: 992435.9s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.536s, learning 0.165s)
               Value function loss: 190058.8828
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 2321.75
               Mean episode length: 65.48
                  Mean reward/step: 37.48
       Mean episode length/episode: 7.20
            Mean episode successes: 2.8379
Mean episode consecutive_successes: 9.2798
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 16.70s
                        Total time: 26843.43s
                               ETA: 992666.4s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.405s, learning 0.171s)
               Value function loss: 126790.3510
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 2713.36
               Mean episode length: 67.47
                  Mean reward/step: 36.78
       Mean episode length/episode: 7.21
            Mean episode successes: 2.7856
Mean episode consecutive_successes: 9.3225
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 16.58s
                        Total time: 26860.00s
                               ETA: 992892.1s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.337s, learning 0.167s)
               Value function loss: 122782.6615
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 2035.94
               Mean episode length: 64.51
                  Mean reward/step: 36.93
       Mean episode length/episode: 7.27
            Mean episode successes: 2.8057
Mean episode consecutive_successes: 9.2966
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 16.50s
                        Total time: 26876.51s
                               ETA: 993114.9s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.601s, learning 0.303s)
               Value function loss: 119939.0064
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 3066.11
               Mean episode length: 66.54
                  Mean reward/step: 36.63
       Mean episode length/episode: 7.28
            Mean episode successes: 2.7334
Mean episode consecutive_successes: 9.4693
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 16.90s
                        Total time: 26893.41s
                               ETA: 993352.4s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.614s, learning 0.172s)
               Value function loss: 125305.3787
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 2016.30
               Mean episode length: 65.32
                  Mean reward/step: 36.72
       Mean episode length/episode: 7.26
            Mean episode successes: 2.7183
Mean episode consecutive_successes: 9.4725
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 16.79s
                        Total time: 26910.20s
                               ETA: 993585.3s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 946 steps/s (collection: 17.144s, learning 0.163s)
               Value function loss: 136007.7488
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 2146.20
               Mean episode length: 67.53
                  Mean reward/step: 39.35
       Mean episode length/episode: 7.27
            Mean episode successes: 2.9297
Mean episode consecutive_successes: 9.4154
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 17.31s
                        Total time: 26927.50s
                               ETA: 993837.2s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.893s, learning 0.235s)
               Value function loss: 137785.6801
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 2913.18
               Mean episode length: 67.05
                  Mean reward/step: 39.24
       Mean episode length/episode: 7.26
            Mean episode successes: 3.0498
Mean episode consecutive_successes: 9.5142
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 17.13s
                        Total time: 26944.63s
                               ETA: 994082.3s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.795s, learning 0.171s)
               Value function loss: 153297.0234
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 2349.20
               Mean episode length: 65.84
                  Mean reward/step: 38.83
       Mean episode length/episode: 7.19
            Mean episode successes: 2.9712
Mean episode consecutive_successes: 9.5128
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 16.97s
                        Total time: 26961.60s
                               ETA: 994321.3s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.485s, learning 0.165s)
               Value function loss: 137702.0180
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 2643.12
               Mean episode length: 67.19
                  Mean reward/step: 38.08
       Mean episode length/episode: 7.20
            Mean episode successes: 2.9092
Mean episode consecutive_successes: 9.5593
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 8.65s
                        Total time: 26970.25s
                               ETA: 994253.5s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.197s, learning 0.174s)
               Value function loss: 145637.2328
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 2254.92
               Mean episode length: 66.53
                  Mean reward/step: 35.92
       Mean episode length/episode: 7.25
            Mean episode successes: 2.8301
Mean episode consecutive_successes: 9.5814
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 8.37s
                        Total time: 26978.62s
                               ETA: 994175.4s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.369s, learning 0.166s)
               Value function loss: 139937.4156
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 2262.88
               Mean episode length: 66.64
                  Mean reward/step: 37.11
       Mean episode length/episode: 7.23
            Mean episode successes: 2.8911
Mean episode consecutive_successes: 9.5595
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 8.53s
                        Total time: 26987.16s
                               ETA: 994103.5s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.239s, learning 0.163s)
               Value function loss: 145704.9551
                    Surrogate loss: 0.0120
             Mean action noise std: 0.73
                       Mean reward: 2896.93
               Mean episode length: 64.35
                  Mean reward/step: 40.23
       Mean episode length/episode: 7.27
            Mean episode successes: 2.9956
Mean episode consecutive_successes: 9.6440
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 8.40s
                        Total time: 26995.56s
                               ETA: 994026.6s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.407s, learning 0.163s)
               Value function loss: 145441.7676
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2622.26
               Mean episode length: 66.18
                  Mean reward/step: 40.03
       Mean episode length/episode: 7.28
            Mean episode successes: 3.0576
Mean episode consecutive_successes: 9.6848
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 8.57s
                        Total time: 27004.13s
                               ETA: 993956.1s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.517s, learning 0.276s)
               Value function loss: 159859.6445
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 2265.54
               Mean episode length: 63.92
                  Mean reward/step: 39.46
       Mean episode length/episode: 7.25
            Mean episode successes: 3.1431
Mean episode consecutive_successes: 9.6951
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 8.79s
                        Total time: 27012.92s
                               ETA: 993893.8s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.355s, learning 0.162s)
               Value function loss: 152162.6105
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 2742.70
               Mean episode length: 65.56
                  Mean reward/step: 38.28
       Mean episode length/episode: 7.22
            Mean episode successes: 3.0532
Mean episode consecutive_successes: 9.7723
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 8.52s
                        Total time: 27021.44s
                               ETA: 993821.3s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.551s, learning 0.321s)
               Value function loss: 147736.0004
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 2220.54
               Mean episode length: 65.26
                  Mean reward/step: 36.94
       Mean episode length/episode: 7.28
            Mean episode successes: 3.0093
Mean episode consecutive_successes: 9.7778
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 8.87s
                        Total time: 27030.31s
                               ETA: 993762.0s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.726s, learning 0.171s)
               Value function loss: 173834.2809
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 2374.87
               Mean episode length: 66.23
                  Mean reward/step: 37.14
       Mean episode length/episode: 7.16
            Mean episode successes: 2.8950
Mean episode consecutive_successes: 9.7912
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 8.90s
                        Total time: 27039.21s
                               ETA: 993703.6s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.852s, learning 0.162s)
               Value function loss: 159860.0832
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 2676.05
               Mean episode length: 67.44
                  Mean reward/step: 38.38
       Mean episode length/episode: 7.21
            Mean episode successes: 2.8989
Mean episode consecutive_successes: 9.8039
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 9.01s
                        Total time: 27048.22s
                               ETA: 993649.5s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.189s)
               Value function loss: 140251.6203
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 2713.47
               Mean episode length: 67.28
                  Mean reward/step: 38.93
       Mean episode length/episode: 7.25
            Mean episode successes: 2.9648
Mean episode consecutive_successes: 9.7961
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 8.79s
                        Total time: 27057.01s
                               ETA: 993587.3s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.054s, learning 0.177s)
               Value function loss: 137954.7691
                    Surrogate loss: 0.0025
             Mean action noise std: 0.73
                       Mean reward: 2585.49
               Mean episode length: 67.51
                  Mean reward/step: 38.28
       Mean episode length/episode: 7.29
            Mean episode successes: 3.0762
Mean episode consecutive_successes: 9.7921
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 8.23s
                        Total time: 27065.24s
                               ETA: 993504.6s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.452s, learning 0.183s)
               Value function loss: 131033.7295
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 2533.03
               Mean episode length: 68.06
                  Mean reward/step: 38.03
       Mean episode length/episode: 7.29
            Mean episode successes: 3.1211
Mean episode consecutive_successes: 9.8268
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 8.63s
                        Total time: 27073.88s
                               ETA: 993436.8s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.735s, learning 0.170s)
               Value function loss: 148549.9617
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 2597.01
               Mean episode length: 65.56
                  Mean reward/step: 38.70
       Mean episode length/episode: 7.28
            Mean episode successes: 2.9741
Mean episode consecutive_successes: 9.9494
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 8.90s
                        Total time: 27082.78s
                               ETA: 993378.9s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.925s, learning 0.173s)
               Value function loss: 152921.3969
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 2784.78
               Mean episode length: 66.93
                  Mean reward/step: 39.01
       Mean episode length/episode: 7.17
            Mean episode successes: 2.8921
Mean episode consecutive_successes: 9.9795
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 8.10s
                        Total time: 27090.88s
                               ETA: 993291.4s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.358s, learning 0.175s)
               Value function loss: 168665.8102
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 2545.72
               Mean episode length: 67.78
                  Mean reward/step: 38.61
       Mean episode length/episode: 7.17
            Mean episode successes: 2.9302
Mean episode consecutive_successes: 9.9630
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 8.53s
                        Total time: 27099.41s
                               ETA: 993220.0s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.357s, learning 0.162s)
               Value function loss: 173508.4629
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 2497.87
               Mean episode length: 66.46
                  Mean reward/step: 38.44
       Mean episode length/episode: 7.26
            Mean episode successes: 3.0190
Mean episode consecutive_successes: 9.9054
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 8.52s
                        Total time: 27107.93s
                               ETA: 993148.1s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.362s, learning 0.326s)
               Value function loss: 168173.4152
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 2418.36
               Mean episode length: 67.46
                  Mean reward/step: 38.00
       Mean episode length/episode: 7.20
            Mean episode successes: 2.9761
Mean episode consecutive_successes: 9.9340
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 8.69s
                        Total time: 27116.62s
                               ETA: 993082.4s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.167s)
               Value function loss: 129573.2482
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 2632.13
               Mean episode length: 66.47
                  Mean reward/step: 36.39
       Mean episode length/episode: 7.29
            Mean episode successes: 2.8970
Mean episode consecutive_successes: 9.9979
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 8.47s
                        Total time: 27125.09s
                               ETA: 993008.7s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.234s, learning 0.165s)
               Value function loss: 121523.0896
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 2064.37
               Mean episode length: 66.41
                  Mean reward/step: 35.82
       Mean episode length/episode: 7.17
            Mean episode successes: 2.7437
Mean episode consecutive_successes: 9.9420
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 8.40s
                        Total time: 27133.48s
                               ETA: 992932.5s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.298s, learning 0.242s)
               Value function loss: 128459.7283
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 2275.11
               Mean episode length: 67.91
                  Mean reward/step: 38.02
       Mean episode length/episode: 7.26
            Mean episode successes: 2.9209
Mean episode consecutive_successes: 9.9164
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 8.54s
                        Total time: 27142.02s
                               ETA: 992861.6s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.477s, learning 0.186s)
               Value function loss: 145234.4707
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 2286.30
               Mean episode length: 65.36
                  Mean reward/step: 39.77
       Mean episode length/episode: 7.25
            Mean episode successes: 3.0469
Mean episode consecutive_successes: 9.9179
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 8.66s
                        Total time: 27150.69s
                               ETA: 992795.2s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.495s, learning 0.173s)
               Value function loss: 137437.4920
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 2008.03
               Mean episode length: 64.61
                  Mean reward/step: 40.03
       Mean episode length/episode: 7.23
            Mean episode successes: 3.1230
Mean episode consecutive_successes: 9.8698
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 8.67s
                        Total time: 27159.35s
                               ETA: 992729.0s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.431s, learning 0.225s)
               Value function loss: 154251.4605
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 3498.41
               Mean episode length: 68.08
                  Mean reward/step: 40.46
       Mean episode length/episode: 7.25
            Mean episode successes: 3.1177
Mean episode consecutive_successes: 10.0209
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 8.66s
                        Total time: 27168.01s
                               ETA: 992662.4s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.594s, learning 0.315s)
               Value function loss: 162414.0988
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 2550.76
               Mean episode length: 67.55
                  Mean reward/step: 41.09
       Mean episode length/episode: 7.17
            Mean episode successes: 3.1157
Mean episode consecutive_successes: 10.0441
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 8.91s
                        Total time: 27176.92s
                               ETA: 992605.1s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.558s, learning 0.179s)
               Value function loss: 141731.6195
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 2378.22
               Mean episode length: 65.08
                  Mean reward/step: 40.68
       Mean episode length/episode: 7.26
            Mean episode successes: 3.1816
Mean episode consecutive_successes: 10.0527
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 8.74s
                        Total time: 27185.66s
                               ETA: 992541.6s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.205s, learning 0.167s)
               Value function loss: 155275.7516
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 2876.01
               Mean episode length: 67.48
                  Mean reward/step: 39.44
       Mean episode length/episode: 7.25
            Mean episode successes: 3.1948
Mean episode consecutive_successes: 10.1215
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 8.37s
                        Total time: 27194.03s
                               ETA: 992464.8s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.321s, learning 0.170s)
               Value function loss: 151097.1672
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2835.20
               Mean episode length: 66.74
                  Mean reward/step: 39.35
       Mean episode length/episode: 7.23
            Mean episode successes: 3.0527
Mean episode consecutive_successes: 10.1769
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 8.49s
                        Total time: 27202.52s
                               ETA: 992392.3s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.537s, learning 0.181s)
               Value function loss: 153698.8062
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 2455.37
               Mean episode length: 67.34
                  Mean reward/step: 39.85
       Mean episode length/episode: 7.21
            Mean episode successes: 3.0059
Mean episode consecutive_successes: 10.2075
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 8.72s
                        Total time: 27211.24s
                               ETA: 992328.3s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.233s, learning 0.188s)
               Value function loss: 148786.6863
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 2103.68
               Mean episode length: 65.78
                  Mean reward/step: 37.71
       Mean episode length/episode: 7.25
            Mean episode successes: 2.9570
Mean episode consecutive_successes: 10.2370
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 8.42s
                        Total time: 27219.66s
                               ETA: 992253.4s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.549s, learning 0.162s)
               Value function loss: 149079.4266
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2355.27
               Mean episode length: 67.66
                  Mean reward/step: 39.13
       Mean episode length/episode: 7.26
            Mean episode successes: 2.9902
Mean episode consecutive_successes: 10.2212
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 8.71s
                        Total time: 27228.37s
                               ETA: 992189.1s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.353s, learning 0.351s)
               Value function loss: 147271.7324
                    Surrogate loss: 0.0034
             Mean action noise std: 0.73
                       Mean reward: 2530.31
               Mean episode length: 66.50
                  Mean reward/step: 40.75
       Mean episode length/episode: 7.29
            Mean episode successes: 3.1719
Mean episode consecutive_successes: 10.2439
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 8.70s
                        Total time: 27237.07s
                               ETA: 992124.6s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.592s, learning 0.174s)
               Value function loss: 145876.4545
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 2009.18
               Mean episode length: 66.46
                  Mean reward/step: 40.66
       Mean episode length/episode: 7.26
            Mean episode successes: 3.2021
Mean episode consecutive_successes: 10.1757
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 8.77s
                        Total time: 27245.84s
                               ETA: 992062.5s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.283s, learning 0.274s)
               Value function loss: 148784.9566
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 3386.55
               Mean episode length: 68.61
                  Mean reward/step: 41.76
       Mean episode length/episode: 7.27
            Mean episode successes: 3.2393
Mean episode consecutive_successes: 10.3228
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 8.56s
                        Total time: 27254.39s
                               ETA: 991992.7s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.192s)
               Value function loss: 145427.4748
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 2236.34
               Mean episode length: 65.90
                  Mean reward/step: 40.20
       Mean episode length/episode: 7.25
            Mean episode successes: 3.2803
Mean episode consecutive_successes: 10.3223
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 8.79s
                        Total time: 27263.19s
                               ETA: 991931.6s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.623s, learning 0.287s)
               Value function loss: 155919.9566
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 2829.51
               Mean episode length: 67.27
                  Mean reward/step: 41.28
       Mean episode length/episode: 7.24
            Mean episode successes: 3.2593
Mean episode consecutive_successes: 10.4022
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 8.91s
                        Total time: 27272.10s
                               ETA: 991874.8s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.556s, learning 0.202s)
               Value function loss: 150593.2703
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 3081.83
               Mean episode length: 68.75
                  Mean reward/step: 38.67
       Mean episode length/episode: 7.20
            Mean episode successes: 2.9463
Mean episode consecutive_successes: 10.5155
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 8.76s
                        Total time: 27280.86s
                               ETA: 991812.5s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.628s, learning 0.170s)
               Value function loss: 141769.1195
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 2811.20
               Mean episode length: 65.90
                  Mean reward/step: 37.33
       Mean episode length/episode: 7.29
            Mean episode successes: 2.9648
Mean episode consecutive_successes: 10.4843
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 8.80s
                        Total time: 27289.65s
                               ETA: 991751.7s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.230s, learning 0.301s)
               Value function loss: 144413.9395
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 2242.13
               Mean episode length: 67.65
                  Mean reward/step: 37.27
       Mean episode length/episode: 7.24
            Mean episode successes: 3.0044
Mean episode consecutive_successes: 10.3809
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 8.53s
                        Total time: 27298.18s
                               ETA: 991681.2s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.474s, learning 0.160s)
               Value function loss: 157072.5484
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 2554.29
               Mean episode length: 66.19
                  Mean reward/step: 39.22
       Mean episode length/episode: 7.21
            Mean episode successes: 2.9287
Mean episode consecutive_successes: 10.3999
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 8.63s
                        Total time: 27306.82s
                               ETA: 991614.5s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.464s, learning 0.169s)
               Value function loss: 157901.0648
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 2968.93
               Mean episode length: 68.41
                  Mean reward/step: 40.02
       Mean episode length/episode: 7.31
            Mean episode successes: 3.0635
Mean episode consecutive_successes: 10.4455
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 8.63s
                        Total time: 27315.45s
                               ETA: 991547.9s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.558s, learning 0.175s)
               Value function loss: 154280.2254
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 2642.43
               Mean episode length: 66.52
                  Mean reward/step: 39.15
       Mean episode length/episode: 7.21
            Mean episode successes: 3.1611
Mean episode consecutive_successes: 10.3456
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 8.73s
                        Total time: 27324.18s
                               ETA: 991484.8s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.189s, learning 0.168s)
               Value function loss: 151235.9770
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 3099.10
               Mean episode length: 68.50
                  Mean reward/step: 40.22
       Mean episode length/episode: 7.22
            Mean episode successes: 3.0806
Mean episode consecutive_successes: 10.4058
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 8.36s
                        Total time: 27332.54s
                               ETA: 991408.2s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.666s, learning 0.271s)
               Value function loss: 144837.5523
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 2927.82
               Mean episode length: 66.59
                  Mean reward/step: 40.86
       Mean episode length/episode: 7.18
            Mean episode successes: 2.9849
Mean episode consecutive_successes: 10.4224
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 8.94s
                        Total time: 27341.48s
                               ETA: 991352.7s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.294s, learning 0.170s)
               Value function loss: 152612.9750
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 2925.94
               Mean episode length: 67.78
                  Mean reward/step: 40.96
       Mean episode length/episode: 7.24
            Mean episode successes: 3.1660
Mean episode consecutive_successes: 10.4200
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 8.46s
                        Total time: 27349.94s
                               ETA: 991280.1s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1792 steps/s (collection: 8.806s, learning 0.335s)
               Value function loss: 151706.1383
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 2579.68
               Mean episode length: 66.95
                  Mean reward/step: 40.83
       Mean episode length/episode: 7.26
            Mean episode successes: 3.2456
Mean episode consecutive_successes: 10.4920
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 9.14s
                        Total time: 27359.08s
                               ETA: 991232.0s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.759s, learning 0.167s)
               Value function loss: 156497.5828
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 3022.41
               Mean episode length: 66.27
                  Mean reward/step: 42.42
       Mean episode length/episode: 7.29
            Mean episode successes: 3.3027
Mean episode consecutive_successes: 10.5972
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 8.93s
                        Total time: 27368.01s
                               ETA: 991176.2s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.409s, learning 0.178s)
               Value function loss: 158868.3855
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 2624.49
               Mean episode length: 66.89
                  Mean reward/step: 42.82
       Mean episode length/episode: 7.21
            Mean episode successes: 3.1787
Mean episode consecutive_successes: 10.6312
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 8.59s
                        Total time: 27376.60s
                               ETA: 991108.1s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 2033 steps/s (collection: 7.896s, learning 0.163s)
               Value function loss: 157475.1914
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 2285.05
               Mean episode length: 67.82
                  Mean reward/step: 44.28
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3828
Mean episode consecutive_successes: 10.5766
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 8.06s
                        Total time: 27384.65s
                               ETA: 991021.0s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.358s, learning 0.182s)
               Value function loss: 161789.1812
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 3070.64
               Mean episode length: 67.98
                  Mean reward/step: 43.76
       Mean episode length/episode: 7.28
            Mean episode successes: 3.4302
Mean episode consecutive_successes: 10.6899
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 8.54s
                        Total time: 27393.19s
                               ETA: 990951.3s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.475s, learning 0.157s)
               Value function loss: 170826.3656
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 2840.52
               Mean episode length: 66.25
                  Mean reward/step: 42.26
       Mean episode length/episode: 7.16
            Mean episode successes: 3.3071
Mean episode consecutive_successes: 10.7643
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 8.63s
                        Total time: 27401.83s
                               ETA: 990885.1s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.360s, learning 0.167s)
               Value function loss: 150407.6422
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 3483.93
               Mean episode length: 67.25
                  Mean reward/step: 41.85
       Mean episode length/episode: 7.28
            Mean episode successes: 3.3105
Mean episode consecutive_successes: 10.8972
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 8.53s
                        Total time: 27410.35s
                               ETA: 990815.1s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.312s, learning 0.245s)
               Value function loss: 164323.2285
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 2647.49
               Mean episode length: 67.63
                  Mean reward/step: 39.17
       Mean episode length/episode: 7.23
            Mean episode successes: 3.0044
Mean episode consecutive_successes: 10.9375
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 8.56s
                        Total time: 27418.91s
                               ETA: 990746.2s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.405s, learning 0.175s)
               Value function loss: 151477.3621
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 2664.12
               Mean episode length: 68.94
                  Mean reward/step: 38.69
       Mean episode length/episode: 7.27
            Mean episode successes: 2.8628
Mean episode consecutive_successes: 11.0038
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 8.58s
                        Total time: 27427.49s
                               ETA: 990678.1s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.281s, learning 0.264s)
               Value function loss: 154282.1078
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 2664.74
               Mean episode length: 66.78
                  Mean reward/step: 40.51
       Mean episode length/episode: 7.31
            Mean episode successes: 3.1367
Mean episode consecutive_successes: 10.9154
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 8.55s
                        Total time: 27436.04s
                               ETA: 990608.9s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.373s, learning 0.180s)
               Value function loss: 158114.2875
                    Surrogate loss: 0.0053
             Mean action noise std: 0.73
                       Mean reward: 2915.15
               Mean episode length: 66.76
                  Mean reward/step: 41.99
       Mean episode length/episode: 7.24
            Mean episode successes: 3.2573
Mean episode consecutive_successes: 10.8823
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 8.55s
                        Total time: 27444.59s
                               ETA: 990540.0s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.324s, learning 0.294s)
               Value function loss: 155816.7477
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 2749.85
               Mean episode length: 67.37
                  Mean reward/step: 42.62
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3262
Mean episode consecutive_successes: 10.8526
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 8.62s
                        Total time: 27453.21s
                               ETA: 990473.5s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.410s, learning 0.234s)
               Value function loss: 167895.0934
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 3033.02
               Mean episode length: 67.82
                  Mean reward/step: 42.47
       Mean episode length/episode: 7.23
            Mean episode successes: 3.2549
Mean episode consecutive_successes: 10.9001
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 8.64s
                        Total time: 27461.85s
                               ETA: 990407.9s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.667s, learning 0.166s)
               Value function loss: 161890.7895
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 2294.83
               Mean episode length: 66.01
                  Mean reward/step: 40.93
       Mean episode length/episode: 7.23
            Mean episode successes: 3.1924
Mean episode consecutive_successes: 10.8990
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 8.83s
                        Total time: 27470.68s
                               ETA: 990349.2s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.581s, learning 0.196s)
               Value function loss: 143800.8734
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 3124.08
               Mean episode length: 68.14
                  Mean reward/step: 41.37
       Mean episode length/episode: 7.24
            Mean episode successes: 3.2793
Mean episode consecutive_successes: 10.8888
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 8.78s
                        Total time: 27479.46s
                               ETA: 990288.6s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.369s, learning 0.211s)
               Value function loss: 162858.4020
                    Surrogate loss: 0.0032
             Mean action noise std: 0.73
                       Mean reward: 2565.68
               Mean episode length: 68.16
                  Mean reward/step: 42.38
       Mean episode length/episode: 7.29
            Mean episode successes: 3.2729
Mean episode consecutive_successes: 10.9438
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 8.58s
                        Total time: 27488.04s
                               ETA: 990220.8s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.288s, learning 0.162s)
               Value function loss: 144285.5068
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 2657.55
               Mean episode length: 68.76
                  Mean reward/step: 40.60
       Mean episode length/episode: 7.28
            Mean episode successes: 3.2812
Mean episode consecutive_successes: 11.0064
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 8.45s
                        Total time: 27496.49s
                               ETA: 990148.5s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.850s, learning 0.203s)
               Value function loss: 159509.4934
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 2508.61
               Mean episode length: 65.36
                  Mean reward/step: 39.91
       Mean episode length/episode: 7.17
            Mean episode successes: 2.9766
Mean episode consecutive_successes: 11.0759
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 9.05s
                        Total time: 27505.54s
                               ETA: 990097.8s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.096s, learning 0.240s)
               Value function loss: 149867.8684
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 2290.63
               Mean episode length: 64.48
                  Mean reward/step: 40.94
       Mean episode length/episode: 7.24
            Mean episode successes: 3.0972
Mean episode consecutive_successes: 10.9962
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 8.34s
                        Total time: 27513.88s
                               ETA: 990021.5s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.600s, learning 0.191s)
               Value function loss: 160090.7910
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 2890.97
               Mean episode length: 68.32
                  Mean reward/step: 42.74
       Mean episode length/episode: 7.27
            Mean episode successes: 3.1816
Mean episode consecutive_successes: 10.9514
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 8.79s
                        Total time: 27522.67s
                               ETA: 989961.5s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.526s, learning 0.184s)
               Value function loss: 182617.9074
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 2577.66
               Mean episode length: 66.75
                  Mean reward/step: 45.29
       Mean episode length/episode: 7.28
            Mean episode successes: 3.5889
Mean episode consecutive_successes: 10.8909
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 8.71s
                        Total time: 27531.38s
                               ETA: 989898.7s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.475s, learning 0.168s)
               Value function loss: 201564.5234
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: 3239.11
               Mean episode length: 68.99
                  Mean reward/step: 44.21
       Mean episode length/episode: 7.25
            Mean episode successes: 3.5151
Mean episode consecutive_successes: 11.0095
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 8.64s
                        Total time: 27540.03s
                               ETA: 989833.5s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.169s)
               Value function loss: 188548.8277
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 3145.71
               Mean episode length: 67.72
                  Mean reward/step: 42.63
       Mean episode length/episode: 7.21
            Mean episode successes: 3.2202
Mean episode consecutive_successes: 11.1214
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 8.45s
                        Total time: 27548.48s
                               ETA: 989761.5s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.563s, learning 0.168s)
               Value function loss: 188817.0395
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 2725.21
               Mean episode length: 67.09
                  Mean reward/step: 40.38
       Mean episode length/episode: 7.23
            Mean episode successes: 3.0620
Mean episode consecutive_successes: 11.2151
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 8.73s
                        Total time: 27557.21s
                               ETA: 989699.5s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.439s, learning 0.181s)
               Value function loss: 185459.2594
                    Surrogate loss: 0.0017
             Mean action noise std: 0.73
                       Mean reward: 2604.67
               Mean episode length: 66.29
                  Mean reward/step: 39.59
       Mean episode length/episode: 7.20
            Mean episode successes: 2.9829
Mean episode consecutive_successes: 11.1707
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 8.62s
                        Total time: 27565.83s
                               ETA: 989633.6s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.265s, learning 0.228s)
               Value function loss: 159515.6316
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 2761.41
               Mean episode length: 68.63
                  Mean reward/step: 40.60
       Mean episode length/episode: 7.31
            Mean episode successes: 3.0864
Mean episode consecutive_successes: 11.1198
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 8.49s
                        Total time: 27574.32s
                               ETA: 989563.2s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.319s, learning 0.225s)
               Value function loss: 163646.1809
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 3127.14
               Mean episode length: 68.71
                  Mean reward/step: 42.41
       Mean episode length/episode: 7.34
            Mean episode successes: 3.1221
Mean episode consecutive_successes: 11.2216
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 8.54s
                        Total time: 27582.87s
                               ETA: 989494.6s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.615s, learning 0.321s)
               Value function loss: 156440.7930
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 2542.22
               Mean episode length: 66.46
                  Mean reward/step: 42.04
       Mean episode length/episode: 7.26
            Mean episode successes: 3.1953
Mean episode consecutive_successes: 11.1252
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 8.94s
                        Total time: 27591.80s
                               ETA: 989440.2s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.431s, learning 0.363s)
               Value function loss: 175251.9828
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 2670.26
               Mean episode length: 67.45
                  Mean reward/step: 42.63
       Mean episode length/episode: 7.23
            Mean episode successes: 3.0938
Mean episode consecutive_successes: 11.1143
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 8.79s
                        Total time: 27600.60s
                               ETA: 989380.7s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.166s)
               Value function loss: 215859.3473
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 3028.37
               Mean episode length: 68.83
                  Mean reward/step: 43.47
       Mean episode length/episode: 7.27
            Mean episode successes: 3.3901
Mean episode consecutive_successes: 10.9924
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 8.37s
                        Total time: 27608.96s
                               ETA: 989305.9s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1803 steps/s (collection: 8.764s, learning 0.320s)
               Value function loss: 184015.1105
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 2818.63
               Mean episode length: 68.42
                  Mean reward/step: 42.36
       Mean episode length/episode: 7.25
            Mean episode successes: 3.3643
Mean episode consecutive_successes: 11.0785
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 9.08s
                        Total time: 27618.05s
                               ETA: 989256.9s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.531s, learning 0.196s)
               Value function loss: 175884.3719
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 2765.06
               Mean episode length: 66.69
                  Mean reward/step: 39.54
       Mean episode length/episode: 7.22
            Mean episode successes: 3.1445
Mean episode consecutive_successes: 11.1130
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 8.73s
                        Total time: 27626.77s
                               ETA: 989195.1s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.372s, learning 0.278s)
               Value function loss: 156225.1746
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 2495.50
               Mean episode length: 68.24
                  Mean reward/step: 38.98
       Mean episode length/episode: 7.24
            Mean episode successes: 3.0352
Mean episode consecutive_successes: 11.1082
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 8.65s
                        Total time: 27635.42s
                               ETA: 989130.6s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.594s, learning 0.254s)
               Value function loss: 149329.2156
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 3275.57
               Mean episode length: 66.66
                  Mean reward/step: 41.50
       Mean episode length/episode: 7.22
            Mean episode successes: 2.8677
Mean episode consecutive_successes: 11.1446
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 8.85s
                        Total time: 27644.27s
                               ETA: 989073.2s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.677s, learning 0.269s)
               Value function loss: 157148.6164
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 2373.03
               Mean episode length: 68.97
                  Mean reward/step: 42.42
       Mean episode length/episode: 7.29
            Mean episode successes: 3.1812
Mean episode consecutive_successes: 11.0612
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 8.95s
                        Total time: 27653.22s
                               ETA: 989019.4s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.599s, learning 0.243s)
               Value function loss: 163533.7254
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 2704.76
               Mean episode length: 67.99
                  Mean reward/step: 43.26
       Mean episode length/episode: 7.31
            Mean episode successes: 3.3652
Mean episode consecutive_successes: 11.0954
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 8.84s
                        Total time: 27662.06s
                               ETA: 988961.9s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.676s, learning 0.362s)
               Value function loss: 209437.5020
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 2908.79
               Mean episode length: 65.20
                  Mean reward/step: 43.96
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3691
Mean episode consecutive_successes: 11.0345
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 9.04s
                        Total time: 27671.10s
                               ETA: 988911.4s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.340s, learning 0.215s)
               Value function loss: 216909.5129
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 3112.83
               Mean episode length: 66.69
                  Mean reward/step: 41.70
       Mean episode length/episode: 7.18
            Mean episode successes: 3.3579
Mean episode consecutive_successes: 11.0872
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 8.55s
                        Total time: 27679.65s
                               ETA: 988843.6s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.810s, learning 0.240s)
               Value function loss: 197113.3785
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 2593.80
               Mean episode length: 69.06
                  Mean reward/step: 40.07
       Mean episode length/episode: 7.30
            Mean episode successes: 3.3252
Mean episode consecutive_successes: 11.1318
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 9.05s
                        Total time: 27688.70s
                               ETA: 988793.7s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.403s, learning 0.258s)
               Value function loss: 121877.7816
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 2980.97
               Mean episode length: 68.74
                  Mean reward/step: 39.67
       Mean episode length/episode: 7.24
            Mean episode successes: 3.1865
Mean episode consecutive_successes: 11.1423
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 8.66s
                        Total time: 27697.36s
                               ETA: 988729.8s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.283s, learning 0.159s)
               Value function loss: 110619.3914
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 2545.92
               Mean episode length: 68.66
                  Mean reward/step: 38.79
       Mean episode length/episode: 7.29
            Mean episode successes: 2.9478
Mean episode consecutive_successes: 11.2171
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 8.44s
                        Total time: 27705.81s
                               ETA: 988658.2s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.408s, learning 0.173s)
               Value function loss: 117129.2365
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 2727.81
               Mean episode length: 67.69
                  Mean reward/step: 38.86
       Mean episode length/episode: 7.30
            Mean episode successes: 3.0684
Mean episode consecutive_successes: 11.1791
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 8.58s
                        Total time: 27714.39s
                               ETA: 988591.6s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.109s, learning 0.184s)
               Value function loss: 140298.1602
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 2791.01
               Mean episode length: 67.97
                  Mean reward/step: 41.28
       Mean episode length/episode: 7.26
            Mean episode successes: 3.2319
Mean episode consecutive_successes: 11.0382
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 8.29s
                        Total time: 27722.68s
                               ETA: 988514.8s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.342s, learning 0.215s)
               Value function loss: 152662.9637
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 2148.39
               Mean episode length: 66.22
                  Mean reward/step: 41.23
       Mean episode length/episode: 7.20
            Mean episode successes: 3.1611
Mean episode consecutive_successes: 10.9991
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 8.56s
                        Total time: 27731.24s
                               ETA: 988447.4s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.059s, learning 0.155s)
               Value function loss: 156783.6141
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 2679.26
               Mean episode length: 65.86
                  Mean reward/step: 41.73
       Mean episode length/episode: 7.31
            Mean episode successes: 3.2300
Mean episode consecutive_successes: 10.9722
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 8.21s
                        Total time: 27739.45s
                               ETA: 988367.8s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.588s, learning 0.166s)
               Value function loss: 161383.6719
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 3258.85
               Mean episode length: 67.90
                  Mean reward/step: 43.38
       Mean episode length/episode: 7.19
            Mean episode successes: 3.2900
Mean episode consecutive_successes: 11.0001
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 8.75s
                        Total time: 27748.20s
                               ETA: 988307.5s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.459s, learning 0.160s)
               Value function loss: 161374.5176
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 3230.41
               Mean episode length: 67.55
                  Mean reward/step: 42.77
       Mean episode length/episode: 7.21
            Mean episode successes: 3.1411
Mean episode consecutive_successes: 10.9924
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 8.62s
                        Total time: 27756.82s
                               ETA: 988242.5s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.689s, learning 0.236s)
               Value function loss: 149586.0996
                    Surrogate loss: -0.0152
             Mean action noise std: 0.73
                       Mean reward: 2895.06
               Mean episode length: 66.37
                  Mean reward/step: 40.65
       Mean episode length/episode: 7.29
            Mean episode successes: 3.1040
Mean episode consecutive_successes: 11.0794
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 8.92s
                        Total time: 27765.75s
                               ETA: 988188.4s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.769s, learning 0.174s)
               Value function loss: 163007.2637
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 2878.47
               Mean episode length: 67.82
                  Mean reward/step: 42.47
       Mean episode length/episode: 7.28
            Mean episode successes: 3.1592
Mean episode consecutive_successes: 11.0767
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 8.94s
                        Total time: 27774.69s
                               ETA: 988134.9s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.551s, learning 0.166s)
               Value function loss: 156445.4680
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 2382.06
               Mean episode length: 66.31
                  Mean reward/step: 42.18
       Mean episode length/episode: 7.29
            Mean episode successes: 3.3335
Mean episode consecutive_successes: 11.0131
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 8.72s
                        Total time: 27783.41s
                               ETA: 988073.5s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.559s, learning 0.177s)
               Value function loss: 177263.6555
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 2325.98
               Mean episode length: 68.10
                  Mean reward/step: 45.94
       Mean episode length/episode: 7.24
            Mean episode successes: 3.4951
Mean episode consecutive_successes: 11.0391
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 8.74s
                        Total time: 27792.14s
                               ETA: 988012.7s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.602s, learning 0.169s)
               Value function loss: 182495.2141
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 2825.92
               Mean episode length: 67.51
                  Mean reward/step: 48.31
       Mean episode length/episode: 7.26
            Mean episode successes: 3.6821
Mean episode consecutive_successes: 11.1131
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 8.77s
                        Total time: 27800.92s
                               ETA: 987953.3s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.587s, learning 0.245s)
               Value function loss: 186532.2941
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 3406.32
               Mean episode length: 67.05
                  Mean reward/step: 48.01
       Mean episode length/episode: 7.13
            Mean episode successes: 3.5586
Mean episode consecutive_successes: 11.1957
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 8.83s
                        Total time: 27809.75s
                               ETA: 987896.0s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.721s, learning 0.184s)
               Value function loss: 192570.4465
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 3333.08
               Mean episode length: 67.05
                  Mean reward/step: 45.85
       Mean episode length/episode: 7.29
            Mean episode successes: 3.5752
Mean episode consecutive_successes: 11.3758
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 8.90s
                        Total time: 27818.65s
                               ETA: 987841.4s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.500s, learning 0.270s)
               Value function loss: 158803.7914
                    Surrogate loss: 0.0015
             Mean action noise std: 0.73
                       Mean reward: 3700.24
               Mean episode length: 69.89
                  Mean reward/step: 45.79
       Mean episode length/episode: 7.27
            Mean episode successes: 3.5879
Mean episode consecutive_successes: 11.4376
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 8.77s
                        Total time: 27827.42s
                               ETA: 987782.0s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.271s, learning 0.193s)
               Value function loss: 149915.4004
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 3388.76
               Mean episode length: 68.65
                  Mean reward/step: 43.73
       Mean episode length/episode: 7.20
            Mean episode successes: 3.3730
Mean episode consecutive_successes: 11.4817
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 8.46s
                        Total time: 27835.89s
                               ETA: 987711.9s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.172s)
               Value function loss: 150754.5012
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 2743.33
               Mean episode length: 64.30
                  Mean reward/step: 41.33
       Mean episode length/episode: 7.21
            Mean episode successes: 3.2603
Mean episode consecutive_successes: 11.5189
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 8.86s
                        Total time: 27844.75s
                               ETA: 987655.8s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.962s, learning 0.194s)
               Value function loss: 157902.6969
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 2953.77
               Mean episode length: 68.76
                  Mean reward/step: 41.50
       Mean episode length/episode: 7.27
            Mean episode successes: 3.1567
Mean episode consecutive_successes: 11.6035
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 8.16s
                        Total time: 27852.90s
                               ETA: 987574.8s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.455s, learning 0.160s)
               Value function loss: 152975.7930
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 3158.03
               Mean episode length: 67.31
                  Mean reward/step: 43.29
       Mean episode length/episode: 7.32
            Mean episode successes: 3.3140
Mean episode consecutive_successes: 11.6068
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 8.61s
                        Total time: 27861.52s
                               ETA: 987510.1s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.418s, learning 0.317s)
               Value function loss: 154856.4746
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 3566.67
               Mean episode length: 68.40
                  Mean reward/step: 43.42
       Mean episode length/episode: 7.23
            Mean episode successes: 3.1177
Mean episode consecutive_successes: 11.6481
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 8.73s
                        Total time: 27870.25s
                               ETA: 987449.7s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.276s, learning 0.174s)
               Value function loss: 156034.8180
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 2771.29
               Mean episode length: 68.49
                  Mean reward/step: 45.73
       Mean episode length/episode: 7.26
            Mean episode successes: 3.4780
Mean episode consecutive_successes: 11.5537
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 8.45s
                        Total time: 27878.70s
                               ETA: 987379.2s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.436s, learning 0.285s)
               Value function loss: 168854.2352
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 3735.87
               Mean episode length: 70.86
                  Mean reward/step: 46.34
       Mean episode length/episode: 7.18
            Mean episode successes: 3.6118
Mean episode consecutive_successes: 11.5042
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 8.72s
                        Total time: 27887.42s
                               ETA: 987318.4s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.306s, learning 0.183s)
               Value function loss: 159912.0418
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 3387.37
               Mean episode length: 69.60
                  Mean reward/step: 43.10
       Mean episode length/episode: 7.30
            Mean episode successes: 3.3804
Mean episode consecutive_successes: 11.6741
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 8.49s
                        Total time: 27895.91s
                               ETA: 987249.4s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.820s, learning 0.212s)
               Value function loss: 164230.2867
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 2389.72
               Mean episode length: 67.99
                  Mean reward/step: 41.94
       Mean episode length/episode: 7.26
            Mean episode successes: 3.3760
Mean episode consecutive_successes: 11.6084
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 9.03s
                        Total time: 27904.95s
                               ETA: 987199.6s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.785s, learning 0.180s)
               Value function loss: 150141.9645
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 2969.91
               Mean episode length: 66.07
                  Mean reward/step: 42.73
       Mean episode length/episode: 7.21
            Mean episode successes: 3.3384
Mean episode consecutive_successes: 11.5815
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 8.96s
                        Total time: 27913.91s
                               ETA: 987147.5s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.461s, learning 0.193s)
               Value function loss: 149074.3410
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 3053.27
               Mean episode length: 68.17
                  Mean reward/step: 41.76
       Mean episode length/episode: 7.22
            Mean episode successes: 3.1558
Mean episode consecutive_successes: 11.6018
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 8.65s
                        Total time: 27922.56s
                               ETA: 987084.5s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.674s, learning 0.172s)
               Value function loss: 144411.4055
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 2000.63
               Mean episode length: 66.31
                  Mean reward/step: 41.89
       Mean episode length/episode: 7.27
            Mean episode successes: 3.2598
Mean episode consecutive_successes: 11.4847
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 8.85s
                        Total time: 27931.41s
                               ETA: 987028.3s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.437s, learning 0.267s)
               Value function loss: 155903.1098
                    Surrogate loss: -0.0170
             Mean action noise std: 0.73
                       Mean reward: 2768.57
               Mean episode length: 68.57
                  Mean reward/step: 42.72
       Mean episode length/episode: 7.25
            Mean episode successes: 3.2671
Mean episode consecutive_successes: 11.5040
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 8.70s
                        Total time: 27940.12s
                               ETA: 986967.1s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.415s, learning 0.174s)
               Value function loss: 151160.7461
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 2651.77
               Mean episode length: 66.00
                  Mean reward/step: 43.31
       Mean episode length/episode: 7.29
            Mean episode successes: 3.4048
Mean episode consecutive_successes: 11.5153
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 8.59s
                        Total time: 27948.70s
                               ETA: 986901.8s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.561s, learning 0.169s)
               Value function loss: 157443.3215
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 2815.78
               Mean episode length: 68.71
                  Mean reward/step: 42.53
       Mean episode length/episode: 7.30
            Mean episode successes: 3.4697
Mean episode consecutive_successes: 11.5556
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 8.73s
                        Total time: 27957.43s
                               ETA: 986841.6s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.217s, learning 0.165s)
               Value function loss: 176397.5113
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 2590.72
               Mean episode length: 68.69
                  Mean reward/step: 43.00
       Mean episode length/episode: 7.23
            Mean episode successes: 3.3623
Mean episode consecutive_successes: 11.5970
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 8.38s
                        Total time: 27965.82s
                               ETA: 986769.2s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.671s, learning 0.171s)
               Value function loss: 186013.9535
                    Surrogate loss: 0.0076
             Mean action noise std: 0.73
                       Mean reward: 2847.82
               Mean episode length: 67.45
                  Mean reward/step: 43.21
       Mean episode length/episode: 7.22
            Mean episode successes: 3.2363
Mean episode consecutive_successes: 11.6414
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 8.84s
                        Total time: 27974.66s
                               ETA: 986713.0s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.275s, learning 0.266s)
               Value function loss: 166051.1723
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 2808.34
               Mean episode length: 69.40
                  Mean reward/step: 44.65
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3325
Mean episode consecutive_successes: 11.5789
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 8.54s
                        Total time: 27983.20s
                               ETA: 986646.2s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.426s, learning 0.408s)
               Value function loss: 178157.7816
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 2961.23
               Mean episode length: 69.13
                  Mean reward/step: 43.50
       Mean episode length/episode: 7.24
            Mean episode successes: 3.4351
Mean episode consecutive_successes: 11.5256
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 8.83s
                        Total time: 27992.03s
                               ETA: 986589.8s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1996 steps/s (collection: 7.933s, learning 0.275s)
               Value function loss: 173529.6387
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 2911.42
               Mean episode length: 68.00
                  Mean reward/step: 42.55
       Mean episode length/episode: 7.26
            Mean episode successes: 3.3306
Mean episode consecutive_successes: 11.5485
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 8.21s
                        Total time: 28000.24s
                               ETA: 986511.4s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.694s, learning 0.178s)
               Value function loss: 212547.8398
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 3096.97
               Mean episode length: 66.92
                  Mean reward/step: 41.10
       Mean episode length/episode: 7.27
            Mean episode successes: 3.2144
Mean episode consecutive_successes: 11.6663
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 8.87s
                        Total time: 28009.11s
                               ETA: 986456.4s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.215s, learning 0.261s)
               Value function loss: 178222.9891
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 2284.50
               Mean episode length: 66.04
                  Mean reward/step: 41.87
       Mean episode length/episode: 7.20
            Mean episode successes: 3.1562
Mean episode consecutive_successes: 11.5386
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 8.48s
                        Total time: 28017.59s
                               ETA: 986387.6s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1791 steps/s (collection: 8.897s, learning 0.251s)
               Value function loss: 126786.9934
                    Surrogate loss: 0.0122
             Mean action noise std: 0.73
                       Mean reward: 2944.58
               Mean episode length: 66.50
                  Mean reward/step: 43.90
       Mean episode length/episode: 7.30
            Mean episode successes: 3.3315
Mean episode consecutive_successes: 11.5266
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 9.15s
                        Total time: 28026.74s
                               ETA: 986342.4s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.640s, learning 0.180s)
               Value function loss: 117634.0840
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 3180.64
               Mean episode length: 66.94
                  Mean reward/step: 43.32
       Mean episode length/episode: 7.23
            Mean episode successes: 3.2588
Mean episode consecutive_successes: 11.5651
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 8.82s
                        Total time: 28035.56s
                               ETA: 986285.7s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.556s, learning 0.253s)
               Value function loss: 118171.2789
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 2898.35
               Mean episode length: 66.98
                  Mean reward/step: 45.77
       Mean episode length/episode: 7.35
            Mean episode successes: 3.3774
Mean episode consecutive_successes: 11.6763
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 8.81s
                        Total time: 28044.37s
                               ETA: 986228.6s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.685s, learning 0.316s)
               Value function loss: 133964.0133
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 2535.62
               Mean episode length: 68.93
                  Mean reward/step: 44.87
       Mean episode length/episode: 7.18
            Mean episode successes: 3.4448
Mean episode consecutive_successes: 11.5395
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 9.00s
                        Total time: 28053.37s
                               ETA: 986178.3s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.644s, learning 0.161s)
               Value function loss: 132996.8320
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 3365.82
               Mean episode length: 66.25
                  Mean reward/step: 44.13
       Mean episode length/episode: 7.26
            Mean episode successes: 3.4546
Mean episode consecutive_successes: 11.6249
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 8.80s
                        Total time: 28062.17s
                               ETA: 986121.2s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.577s, learning 0.169s)
               Value function loss: 137595.4105
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 2674.05
               Mean episode length: 69.41
                  Mean reward/step: 43.00
       Mean episode length/episode: 7.25
            Mean episode successes: 3.4697
Mean episode consecutive_successes: 11.5778
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 8.75s
                        Total time: 28070.92s
                               ETA: 986062.0s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.247s, learning 0.376s)
               Value function loss: 147987.1031
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 3538.80
               Mean episode length: 67.85
                  Mean reward/step: 43.17
       Mean episode length/episode: 7.23
            Mean episode successes: 3.2559
Mean episode consecutive_successes: 11.7325
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 8.62s
                        Total time: 28079.54s
                               ETA: 985998.5s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.701s, learning 0.170s)
               Value function loss: 148564.6812
                    Surrogate loss: 0.0118
             Mean action noise std: 0.73
                       Mean reward: 3527.41
               Mean episode length: 69.22
                  Mean reward/step: 43.05
       Mean episode length/episode: 7.26
            Mean episode successes: 3.2812
Mean episode consecutive_successes: 11.7532
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 8.87s
                        Total time: 28088.41s
                               ETA: 985943.8s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.268s, learning 0.202s)
               Value function loss: 142451.1031
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 3163.66
               Mean episode length: 67.29
                  Mean reward/step: 41.89
       Mean episode length/episode: 7.29
            Mean episode successes: 3.3384
Mean episode consecutive_successes: 11.6954
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 8.47s
                        Total time: 28096.88s
                               ETA: 985875.0s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.579s, learning 0.174s)
               Value function loss: 141737.4527
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 2382.86
               Mean episode length: 68.39
                  Mean reward/step: 40.42
       Mean episode length/episode: 7.27
            Mean episode successes: 3.1836
Mean episode consecutive_successes: 11.6334
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 8.75s
                        Total time: 28105.63s
                               ETA: 985816.3s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.261s, learning 0.253s)
               Value function loss: 151864.6898
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 3095.66
               Mean episode length: 68.59
                  Mean reward/step: 41.93
       Mean episode length/episode: 7.24
            Mean episode successes: 3.2114
Mean episode consecutive_successes: 11.6560
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 8.51s
                        Total time: 28114.15s
                               ETA: 985749.1s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.681s, learning 0.169s)
               Value function loss: 159065.2023
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 2832.45
               Mean episode length: 66.50
                  Mean reward/step: 43.89
       Mean episode length/episode: 7.36
            Mean episode successes: 3.3755
Mean episode consecutive_successes: 11.6184
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 8.85s
                        Total time: 28123.00s
                               ETA: 985693.8s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.452s, learning 0.295s)
               Value function loss: 180395.4715
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 3270.40
               Mean episode length: 70.61
                  Mean reward/step: 46.41
       Mean episode length/episode: 7.23
            Mean episode successes: 3.5688
Mean episode consecutive_successes: 11.5628
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 8.75s
                        Total time: 28131.74s
                               ETA: 985635.0s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.426s, learning 0.165s)
               Value function loss: 202498.3906
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 3291.30
               Mean episode length: 68.79
                  Mean reward/step: 46.71
       Mean episode length/episode: 7.25
            Mean episode successes: 3.6328
Mean episode consecutive_successes: 11.5676
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 8.59s
                        Total time: 28140.34s
                               ETA: 985570.6s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1796 steps/s (collection: 8.715s, learning 0.405s)
               Value function loss: 187616.2293
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 2790.69
               Mean episode length: 68.59
                  Mean reward/step: 46.56
       Mean episode length/episode: 7.29
            Mean episode successes: 3.6572
Mean episode consecutive_successes: 11.6313
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 9.12s
                        Total time: 28149.45s
                               ETA: 985524.9s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.636s, learning 0.255s)
               Value function loss: 220811.8375
                    Surrogate loss: 0.0003
             Mean action noise std: 0.73
                       Mean reward: 2703.43
               Mean episode length: 69.20
                  Mean reward/step: 47.17
       Mean episode length/episode: 7.21
            Mean episode successes: 3.6079
Mean episode consecutive_successes: 11.6928
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 8.89s
                        Total time: 28158.35s
                               ETA: 985471.1s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.417s, learning 0.251s)
               Value function loss: 208045.5113
                    Surrogate loss: 0.0025
             Mean action noise std: 0.73
                       Mean reward: 3767.81
               Mean episode length: 69.84
                  Mean reward/step: 44.70
       Mean episode length/episode: 7.30
            Mean episode successes: 3.5972
Mean episode consecutive_successes: 11.8489
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 8.67s
                        Total time: 28167.01s
                               ETA: 985409.6s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.354s, learning 0.176s)
               Value function loss: 163786.5930
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 3576.76
               Mean episode length: 69.07
                  Mean reward/step: 42.28
       Mean episode length/episode: 7.21
            Mean episode successes: 3.2715
Mean episode consecutive_successes: 11.9140
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 8.53s
                        Total time: 28175.54s
                               ETA: 985343.4s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.261s, learning 0.252s)
               Value function loss: 147417.7766
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2962.31
               Mean episode length: 66.77
                  Mean reward/step: 42.45
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3652
Mean episode consecutive_successes: 11.8296
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 8.51s
                        Total time: 28184.06s
                               ETA: 985276.5s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.231s, learning 0.176s)
               Value function loss: 158487.0809
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 2835.51
               Mean episode length: 68.66
                  Mean reward/step: 43.61
       Mean episode length/episode: 7.32
            Mean episode successes: 3.4258
Mean episode consecutive_successes: 11.8529
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 8.41s
                        Total time: 28192.46s
                               ETA: 985206.0s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.207s, learning 0.215s)
               Value function loss: 176299.5559
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: 2652.29
               Mean episode length: 70.34
                  Mean reward/step: 42.63
       Mean episode length/episode: 7.22
            Mean episode successes: 3.1812
Mean episode consecutive_successes: 11.8762
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 8.42s
                        Total time: 28200.89s
                               ETA: 985136.1s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.643s, learning 0.165s)
               Value function loss: 162236.6371
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 2781.61
               Mean episode length: 66.61
                  Mean reward/step: 43.57
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3599
Mean episode consecutive_successes: 11.7755
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 8.81s
                        Total time: 28209.69s
                               ETA: 985079.7s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.548s, learning 0.254s)
               Value function loss: 170578.7586
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 2908.38
               Mean episode length: 67.94
                  Mean reward/step: 42.44
       Mean episode length/episode: 7.23
            Mean episode successes: 3.3164
Mean episode consecutive_successes: 11.7502
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 8.80s
                        Total time: 28218.50s
                               ETA: 985023.1s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.626s, learning 0.185s)
               Value function loss: 171939.2215
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 3234.16
               Mean episode length: 70.02
                  Mean reward/step: 44.07
       Mean episode length/episode: 7.34
            Mean episode successes: 3.4648
Mean episode consecutive_successes: 11.8083
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 8.81s
                        Total time: 28227.31s
                               ETA: 984966.9s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.618s, learning 0.300s)
               Value function loss: 189566.2719
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 2779.20
               Mean episode length: 66.49
                  Mean reward/step: 43.08
       Mean episode length/episode: 7.25
            Mean episode successes: 3.4248
Mean episode consecutive_successes: 11.7861
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 8.92s
                        Total time: 28236.22s
                               ETA: 984914.4s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.416s, learning 0.175s)
               Value function loss: 164700.8824
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 2594.03
               Mean episode length: 69.23
                  Mean reward/step: 44.41
       Mean episode length/episode: 7.21
            Mean episode successes: 3.4482
Mean episode consecutive_successes: 11.7065
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 8.59s
                        Total time: 28244.82s
                               ETA: 984850.5s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.218s, learning 0.245s)
               Value function loss: 167179.0305
                    Surrogate loss: 0.0168
             Mean action noise std: 0.73
                       Mean reward: 3264.07
               Mean episode length: 68.93
                  Mean reward/step: 45.40
       Mean episode length/episode: 7.20
            Mean episode successes: 3.4146
Mean episode consecutive_successes: 11.7503
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 8.46s
                        Total time: 28253.28s
                               ETA: 984782.3s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.364s, learning 0.169s)
               Value function loss: 164142.6691
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 3338.26
               Mean episode length: 68.17
                  Mean reward/step: 44.02
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3022
Mean episode consecutive_successes: 11.7810
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 8.53s
                        Total time: 28261.81s
                               ETA: 984716.5s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.279s, learning 0.190s)
               Value function loss: 142657.7621
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 2663.09
               Mean episode length: 68.21
                  Mean reward/step: 43.01
       Mean episode length/episode: 7.35
            Mean episode successes: 3.4893
Mean episode consecutive_successes: 11.7362
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 8.47s
                        Total time: 28270.28s
                               ETA: 984648.5s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.628s, learning 0.163s)
               Value function loss: 168193.4555
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 3168.76
               Mean episode length: 68.32
                  Mean reward/step: 45.11
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3374
Mean episode consecutive_successes: 11.8237
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 8.79s
                        Total time: 28279.07s
                               ETA: 984591.8s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.763s, learning 0.163s)
               Value function loss: 166349.8992
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 3057.41
               Mean episode length: 69.62
                  Mean reward/step: 44.84
       Mean episode length/episode: 7.34
            Mean episode successes: 3.4727
Mean episode consecutive_successes: 11.8858
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 8.93s
                        Total time: 28288.00s
                               ETA: 984539.9s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.597s, learning 0.175s)
               Value function loss: 165069.0055
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 3302.31
               Mean episode length: 66.70
                  Mean reward/step: 44.28
       Mean episode length/episode: 7.21
            Mean episode successes: 3.3569
Mean episode consecutive_successes: 11.8866
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 8.77s
                        Total time: 28296.77s
                               ETA: 984482.6s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.608s, learning 0.168s)
               Value function loss: 163884.8309
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 3237.76
               Mean episode length: 66.26
                  Mean reward/step: 44.11
       Mean episode length/episode: 7.29
            Mean episode successes: 3.3057
Mean episode consecutive_successes: 11.9564
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 8.78s
                        Total time: 28305.55s
                               ETA: 984425.4s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.318s, learning 0.222s)
               Value function loss: 153117.4188
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 2742.63
               Mean episode length: 68.09
                  Mean reward/step: 44.43
       Mean episode length/episode: 7.25
            Mean episode successes: 3.4731
Mean episode consecutive_successes: 11.8642
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 8.54s
                        Total time: 28314.09s
                               ETA: 984360.1s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.601s, learning 0.178s)
               Value function loss: 158049.2977
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: 2939.68
               Mean episode length: 69.54
                  Mean reward/step: 46.44
       Mean episode length/episode: 7.27
            Mean episode successes: 3.5786
Mean episode consecutive_successes: 11.8742
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 8.78s
                        Total time: 28322.87s
                               ETA: 984303.2s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.588s, learning 0.190s)
               Value function loss: 150173.5387
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 3441.50
               Mean episode length: 70.21
                  Mean reward/step: 46.27
       Mean episode length/episode: 7.27
            Mean episode successes: 3.6855
Mean episode consecutive_successes: 11.8696
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 8.78s
                        Total time: 28331.65s
                               ETA: 984246.2s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.754s, learning 0.160s)
               Value function loss: 150780.7781
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 3153.86
               Mean episode length: 68.46
                  Mean reward/step: 45.74
       Mean episode length/episode: 7.28
            Mean episode successes: 3.5718
Mean episode consecutive_successes: 12.0143
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 8.91s
                        Total time: 28340.56s
                               ETA: 984194.0s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.477s, learning 0.170s)
               Value function loss: 164761.2102
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 3060.24
               Mean episode length: 68.32
                  Mean reward/step: 46.30
       Mean episode length/episode: 7.27
            Mean episode successes: 3.5391
Mean episode consecutive_successes: 12.0735
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 8.65s
                        Total time: 28349.21s
                               ETA: 984132.6s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.461s, learning 0.264s)
               Value function loss: 162293.3246
                    Surrogate loss: 0.0037
             Mean action noise std: 0.73
                       Mean reward: 3362.04
               Mean episode length: 69.24
                  Mean reward/step: 47.45
       Mean episode length/episode: 7.28
            Mean episode successes: 3.6694
Mean episode consecutive_successes: 12.0780
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 8.72s
                        Total time: 28357.93s
                               ETA: 984073.9s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.634s, learning 0.171s)
               Value function loss: 146682.8012
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 2844.96
               Mean episode length: 67.67
                  Mean reward/step: 45.48
       Mean episode length/episode: 7.21
            Mean episode successes: 3.5513
Mean episode consecutive_successes: 12.0725
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 8.81s
                        Total time: 28366.74s
                               ETA: 984018.0s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.482s, learning 0.169s)
               Value function loss: 156121.1266
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 2948.04
               Mean episode length: 68.09
                  Mean reward/step: 44.13
       Mean episode length/episode: 7.29
            Mean episode successes: 3.5923
Mean episode consecutive_successes: 12.1188
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 8.65s
                        Total time: 28375.39s
                               ETA: 983956.8s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.490s, learning 0.275s)
               Value function loss: 143167.9477
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 3211.26
               Mean episode length: 68.19
                  Mean reward/step: 43.58
       Mean episode length/episode: 7.22
            Mean episode successes: 3.3984
Mean episode consecutive_successes: 12.1455
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 8.77s
                        Total time: 28384.15s
                               ETA: 983899.6s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.164s)
               Value function loss: 146789.8523
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 3012.87
               Mean episode length: 67.17
                  Mean reward/step: 45.61
       Mean episode length/episode: 7.29
            Mean episode successes: 3.4502
Mean episode consecutive_successes: 12.1292
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 8.49s
                        Total time: 28392.64s
                               ETA: 983832.7s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.736s, learning 0.170s)
               Value function loss: 155537.7434
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 3018.70
               Mean episode length: 68.82
                  Mean reward/step: 44.76
       Mean episode length/episode: 7.29
            Mean episode successes: 3.4414
Mean episode consecutive_successes: 12.1639
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 8.91s
                        Total time: 28401.54s
                               ETA: 983780.5s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.712s, learning 0.278s)
               Value function loss: 154099.2074
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 2960.09
               Mean episode length: 67.48
                  Mean reward/step: 46.24
       Mean episode length/episode: 7.29
            Mean episode successes: 3.5771
Mean episode consecutive_successes: 12.1386
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 8.99s
                        Total time: 28410.53s
                               ETA: 983731.2s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.431s, learning 0.221s)
               Value function loss: 165089.7910
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 3237.90
               Mean episode length: 69.49
                  Mean reward/step: 47.90
       Mean episode length/episode: 7.24
            Mean episode successes: 3.6289
Mean episode consecutive_successes: 12.2016
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 8.65s
                        Total time: 28419.19s
                               ETA: 983670.2s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.655s, learning 0.171s)
               Value function loss: 163095.0508
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 2429.44
               Mean episode length: 67.25
                  Mean reward/step: 45.46
       Mean episode length/episode: 7.29
            Mean episode successes: 3.5190
Mean episode consecutive_successes: 12.2404
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 8.83s
                        Total time: 28428.01s
                               ETA: 983615.3s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.709s, learning 0.179s)
               Value function loss: 157608.5215
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 2856.02
               Mean episode length: 67.00
                  Mean reward/step: 46.24
       Mean episode length/episode: 7.26
            Mean episode successes: 3.5254
Mean episode consecutive_successes: 12.2408
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 8.89s
                        Total time: 28436.90s
                               ETA: 983562.6s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.271s, learning 0.182s)
               Value function loss: 168837.3285
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 3507.38
               Mean episode length: 69.03
                  Mean reward/step: 46.85
       Mean episode length/episode: 7.22
            Mean episode successes: 3.5688
Mean episode consecutive_successes: 12.2638
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 8.45s
                        Total time: 28445.35s
                               ETA: 983494.8s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.586s, learning 0.297s)
               Value function loss: 179903.3473
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 3636.56
               Mean episode length: 68.32
                  Mean reward/step: 46.68
       Mean episode length/episode: 7.33
            Mean episode successes: 3.5986
Mean episode consecutive_successes: 12.4260
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 8.88s
                        Total time: 28454.24s
                               ETA: 983441.9s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.459s, learning 0.177s)
               Value function loss: 153781.5566
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 2475.76
               Mean episode length: 69.91
                  Mean reward/step: 44.88
       Mean episode length/episode: 7.26
            Mean episode successes: 3.5396
Mean episode consecutive_successes: 12.4211
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 8.64s
                        Total time: 28462.87s
                               ETA: 983380.6s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.415s, learning 0.329s)
               Value function loss: 161545.8656
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 2897.09
               Mean episode length: 66.02
                  Mean reward/step: 46.18
       Mean episode length/episode: 7.26
            Mean episode successes: 3.5669
Mean episode consecutive_successes: 12.3971
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 8.74s
                        Total time: 28471.62s
                               ETA: 983323.0s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.573s, learning 0.167s)
               Value function loss: 196901.9156
                    Surrogate loss: 0.0048
             Mean action noise std: 0.73
                       Mean reward: 3318.56
               Mean episode length: 68.75
                  Mean reward/step: 46.56
       Mean episode length/episode: 7.24
            Mean episode successes: 3.4702
Mean episode consecutive_successes: 12.4499
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 8.74s
                        Total time: 28480.36s
                               ETA: 983265.4s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.399s, learning 0.167s)
               Value function loss: 175955.3754
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 3126.41
               Mean episode length: 68.85
                  Mean reward/step: 45.75
       Mean episode length/episode: 7.21
            Mean episode successes: 3.4883
Mean episode consecutive_successes: 12.4369
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 8.57s
                        Total time: 28488.92s
                               ETA: 983201.7s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.603s, learning 0.189s)
               Value function loss: 171229.6930
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2727.14
               Mean episode length: 69.05
                  Mean reward/step: 44.42
       Mean episode length/episode: 7.28
            Mean episode successes: 3.5171
Mean episode consecutive_successes: 12.3718
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 8.79s
                        Total time: 28497.71s
                               ETA: 983145.8s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.715s, learning 0.175s)
               Value function loss: 174401.8758
                    Surrogate loss: 0.0051
             Mean action noise std: 0.73
                       Mean reward: 3794.89
               Mean episode length: 68.93
                  Mean reward/step: 43.15
       Mean episode length/episode: 7.28
            Mean episode successes: 3.3667
Mean episode consecutive_successes: 12.5215
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 8.89s
                        Total time: 28506.60s
                               ETA: 983093.4s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.767s, learning 0.160s)
               Value function loss: 141004.3746
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 2550.36
               Mean episode length: 67.95
                  Mean reward/step: 42.65
       Mean episode length/episode: 7.22
            Mean episode successes: 3.1846
Mean episode consecutive_successes: 12.4261
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 8.93s
                        Total time: 28515.53s
                               ETA: 983042.3s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.187s, learning 0.217s)
               Value function loss: 148856.0953
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 3476.73
               Mean episode length: 68.39
                  Mean reward/step: 44.72
       Mean episode length/episode: 7.27
            Mean episode successes: 3.2998
Mean episode consecutive_successes: 12.4349
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 8.40s
                        Total time: 28523.93s
                               ETA: 982973.2s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.125s, learning 0.182s)
               Value function loss: 153463.3223
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 2005.71
               Mean episode length: 65.98
                  Mean reward/step: 45.90
       Mean episode length/episode: 7.27
            Mean episode successes: 3.5127
Mean episode consecutive_successes: 12.2189
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 8.31s
                        Total time: 28532.24s
                               ETA: 982900.8s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.179s, learning 0.309s)
               Value function loss: 173982.6969
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 3199.70
               Mean episode length: 70.13
                  Mean reward/step: 47.78
       Mean episode length/episode: 7.24
            Mean episode successes: 3.6729
Mean episode consecutive_successes: 12.1698
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 8.49s
                        Total time: 28540.73s
                               ETA: 982834.7s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.787s, learning 0.184s)
               Value function loss: 156324.9254
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 3133.16
               Mean episode length: 68.92
                  Mean reward/step: 48.26
       Mean episode length/episode: 7.32
            Mean episode successes: 3.8413
Mean episode consecutive_successes: 12.2380
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 8.97s
                        Total time: 28549.70s
                               ETA: 982785.2s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.443s, learning 0.188s)
               Value function loss: 167164.0047
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 3166.16
               Mean episode length: 69.71
                  Mean reward/step: 47.41
       Mean episode length/episode: 7.25
            Mean episode successes: 3.8164
Mean episode consecutive_successes: 12.3507
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 8.63s
                        Total time: 28558.33s
                               ETA: 982724.1s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.456s, learning 0.167s)
               Value function loss: 173833.8469
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 3648.04
               Mean episode length: 70.54
                  Mean reward/step: 45.42
       Mean episode length/episode: 7.28
            Mean episode successes: 3.5205
Mean episode consecutive_successes: 12.5623
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 8.62s
                        Total time: 28566.95s
                               ETA: 982662.7s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.395s, learning 0.177s)
               Value function loss: 149903.9445
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 3203.05
               Mean episode length: 68.21
                  Mean reward/step: 45.02
       Mean episode length/episode: 7.22
            Mean episode successes: 3.4634
Mean episode consecutive_successes: 12.5908
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 8.57s
                        Total time: 28575.53s
                               ETA: 982599.7s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.336s, learning 0.253s)
               Value function loss: 168371.0168
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 3399.88
               Mean episode length: 69.75
                  Mean reward/step: 45.49
       Mean episode length/episode: 7.26
            Mean episode successes: 3.3740
Mean episode consecutive_successes: 12.6145
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 8.59s
                        Total time: 28584.11s
                               ETA: 982537.2s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.601s, learning 0.172s)
               Value function loss: 162300.1957
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 2740.93
               Mean episode length: 69.74
                  Mean reward/step: 46.99
       Mean episode length/episode: 7.21
            Mean episode successes: 3.4604
Mean episode consecutive_successes: 12.5167
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 8.77s
                        Total time: 28592.89s
                               ETA: 982481.2s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.181s, learning 0.196s)
               Value function loss: 179027.1180
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 2374.25
               Mean episode length: 67.09
                  Mean reward/step: 46.42
       Mean episode length/episode: 7.27
            Mean episode successes: 3.6953
Mean episode consecutive_successes: 12.4158
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 8.38s
                        Total time: 28601.26s
                               ETA: 982411.5s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.745s, learning 0.165s)
               Value function loss: 186412.6891
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: 3123.17
               Mean episode length: 67.31
                  Mean reward/step: 45.28
       Mean episode length/episode: 7.24
            Mean episode successes: 3.6582
Mean episode consecutive_successes: 12.4567
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 8.91s
                        Total time: 28610.17s
                               ETA: 982360.2s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.484s, learning 0.183s)
               Value function loss: 183007.2199
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 3293.79
               Mean episode length: 69.08
                  Mean reward/step: 44.70
       Mean episode length/episode: 7.26
            Mean episode successes: 3.6382
Mean episode consecutive_successes: 12.4529
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 8.67s
                        Total time: 28618.84s
                               ETA: 982300.6s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.652s, learning 0.170s)
               Value function loss: 173695.5340
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: 3809.59
               Mean episode length: 68.73
                  Mean reward/step: 44.14
       Mean episode length/episode: 7.25
            Mean episode successes: 3.3921
Mean episode consecutive_successes: 12.5830
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 8.82s
                        Total time: 28627.66s
                               ETA: 982246.3s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.372s, learning 0.173s)
               Value function loss: 181839.2996
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 3521.58
               Mean episode length: 69.00
                  Mean reward/step: 43.76
       Mean episode length/episode: 7.29
            Mean episode successes: 3.3188
Mean episode consecutive_successes: 12.6546
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 8.54s
                        Total time: 28636.21s
                               ETA: 982182.5s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.539s, learning 0.293s)
               Value function loss: 153015.3000
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 3440.25
               Mean episode length: 70.49
                  Mean reward/step: 42.83
       Mean episode length/episode: 7.31
            Mean episode successes: 3.3726
Mean episode consecutive_successes: 12.5677
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 8.83s
                        Total time: 28645.04s
                               ETA: 982128.7s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.811s, learning 0.196s)
               Value function loss: 164700.9840
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 3044.75
               Mean episode length: 71.07
                  Mean reward/step: 46.14
       Mean episode length/episode: 7.24
            Mean episode successes: 3.3672
Mean episode consecutive_successes: 12.4805
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 9.01s
                        Total time: 28654.05s
                               ETA: 982080.8s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.267s, learning 0.171s)
               Value function loss: 167167.9477
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 2535.60
               Mean episode length: 68.49
                  Mean reward/step: 47.69
       Mean episode length/episode: 7.29
            Mean episode successes: 3.6587
Mean episode consecutive_successes: 12.3648
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 8.44s
                        Total time: 28662.49s
                               ETA: 982013.5s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.533s, learning 0.164s)
               Value function loss: 168400.9289
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 2914.47
               Mean episode length: 69.00
                  Mean reward/step: 49.87
       Mean episode length/episode: 7.27
            Mean episode successes: 3.7842
Mean episode consecutive_successes: 12.3815
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 8.70s
                        Total time: 28671.18s
                               ETA: 981955.1s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.540s, learning 0.170s)
               Value function loss: 162586.7449
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 3315.43
               Mean episode length: 67.87
                  Mean reward/step: 47.67
       Mean episode length/episode: 7.24
            Mean episode successes: 3.7134
Mean episode consecutive_successes: 12.4201
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 8.71s
                        Total time: 28679.89s
                               ETA: 981897.2s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.521s, learning 0.175s)
               Value function loss: 183198.0371
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 3184.09
               Mean episode length: 68.62
                  Mean reward/step: 46.97
       Mean episode length/episode: 7.24
            Mean episode successes: 3.5889
Mean episode consecutive_successes: 12.4656
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 8.70s
                        Total time: 28688.59s
                               ETA: 981838.9s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.550s, learning 0.165s)
               Value function loss: 163824.6227
                    Surrogate loss: 0.0227
             Mean action noise std: 0.72
                       Mean reward: 2901.41
               Mean episode length: 69.23
                  Mean reward/step: 47.58
       Mean episode length/episode: 7.28
            Mean episode successes: 3.7075
Mean episode consecutive_successes: 12.4726
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 8.71s
                        Total time: 28697.30s
                               ETA: 981781.2s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.229s, learning 0.168s)
               Value function loss: 162395.0008
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: 4034.03
               Mean episode length: 70.98
                  Mean reward/step: 48.20
       Mean episode length/episode: 7.31
            Mean episode successes: 3.7041
Mean episode consecutive_successes: 12.6862
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 8.40s
                        Total time: 28705.70s
                               ETA: 981712.7s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.347s, learning 0.175s)
               Value function loss: 161832.0918
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 2976.07
               Mean episode length: 69.00
                  Mean reward/step: 46.27
       Mean episode length/episode: 7.27
            Mean episode successes: 3.5981
Mean episode consecutive_successes: 12.6881
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 8.52s
                        Total time: 28714.22s
                               ETA: 981648.5s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.521s, learning 0.168s)
               Value function loss: 171687.6543
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 2812.04
               Mean episode length: 66.35
                  Mean reward/step: 46.91
       Mean episode length/episode: 7.20
            Mean episode successes: 3.6030
Mean episode consecutive_successes: 12.6328
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 8.69s
                        Total time: 28722.91s
                               ETA: 981590.1s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.396s, learning 0.197s)
               Value function loss: 184197.9504
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 2721.47
               Mean episode length: 68.45
                  Mean reward/step: 48.44
       Mean episode length/episode: 7.28
            Mean episode successes: 3.7559
Mean episode consecutive_successes: 12.6446
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 8.59s
                        Total time: 28731.50s
                               ETA: 981528.4s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.422s, learning 0.163s)
               Value function loss: 203802.0301
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 3217.12
               Mean episode length: 70.40
                  Mean reward/step: 47.07
       Mean episode length/episode: 7.31
            Mean episode successes: 3.8311
Mean episode consecutive_successes: 12.6654
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 8.58s
                        Total time: 28740.09s
                               ETA: 981466.5s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.365s, learning 0.268s)
               Value function loss: 186099.1586
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 3141.22
               Mean episode length: 69.77
                  Mean reward/step: 45.72
       Mean episode length/episode: 7.22
            Mean episode successes: 3.5967
Mean episode consecutive_successes: 12.7018
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 8.63s
                        Total time: 28748.72s
                               ETA: 981406.2s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.405s, learning 0.168s)
               Value function loss: 185189.5113
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: 3199.54
               Mean episode length: 69.95
                  Mean reward/step: 43.48
       Mean episode length/episode: 7.20
            Mean episode successes: 3.5864
Mean episode consecutive_successes: 12.6599
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 8.57s
                        Total time: 28757.29s
                               ETA: 981343.9s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.387s, learning 0.269s)
               Value function loss: 213188.2391
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 3073.16
               Mean episode length: 66.80
                  Mean reward/step: 44.01
       Mean episode length/episode: 7.30
            Mean episode successes: 3.5415
Mean episode consecutive_successes: 12.6460
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 8.66s
                        Total time: 28765.95s
                               ETA: 981284.6s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.400s, learning 0.163s)
               Value function loss: 157009.0738
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 3503.41
               Mean episode length: 71.21
                  Mean reward/step: 44.73
       Mean episode length/episode: 7.25
            Mean episode successes: 3.4434
Mean episode consecutive_successes: 12.6852
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 8.56s
                        Total time: 28774.51s
                               ETA: 981222.0s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.615s, learning 0.186s)
               Value function loss: 154034.4953
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3212.85
               Mean episode length: 69.58
                  Mean reward/step: 46.63
       Mean episode length/episode: 7.35
            Mean episode successes: 3.6733
Mean episode consecutive_successes: 12.5953
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 8.80s
                        Total time: 28783.31s
                               ETA: 981167.7s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.420s, learning 0.195s)
               Value function loss: 176731.8691
                    Surrogate loss: 0.0131
             Mean action noise std: 0.72
                       Mean reward: 3636.63
               Mean episode length: 68.60
                  Mean reward/step: 47.75
       Mean episode length/episode: 7.25
            Mean episode successes: 3.6333
Mean episode consecutive_successes: 12.6474
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 8.61s
                        Total time: 28791.93s
                               ETA: 981107.0s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.691s, learning 0.218s)
               Value function loss: 162049.3355
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: 3454.14
               Mean episode length: 72.43
                  Mean reward/step: 49.07
       Mean episode length/episode: 7.32
            Mean episode successes: 3.6597
Mean episode consecutive_successes: 12.6680
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 8.91s
                        Total time: 28800.84s
                               ETA: 981056.3s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.558s, learning 0.172s)
               Value function loss: 166524.0887
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 2942.41
               Mean episode length: 68.05
                  Mean reward/step: 51.96
       Mean episode length/episode: 7.29
            Mean episode successes: 3.8682
Mean episode consecutive_successes: 12.7173
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 8.73s
                        Total time: 28809.57s
                               ETA: 980999.6s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1179 steps/s (collection: 13.697s, learning 0.196s)
               Value function loss: 177007.7406
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 2913.05
               Mean episode length: 68.32
                  Mean reward/step: 51.99
       Mean episode length/episode: 7.20
            Mean episode successes: 3.9248
Mean episode consecutive_successes: 12.7317
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 13.89s
                        Total time: 28823.46s
                               ETA: 981118.7s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.947s, learning 0.223s)
               Value function loss: 185211.8316
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 3449.70
               Mean episode length: 70.53
                  Mean reward/step: 50.61
       Mean episode length/episode: 7.19
            Mean episode successes: 3.9434
Mean episode consecutive_successes: 12.7474
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 17.17s
                        Total time: 28840.63s
                               ETA: 981349.2s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.816s, learning 0.219s)
               Value function loss: 168228.7727
                    Surrogate loss: 0.0019
             Mean action noise std: 0.72
                       Mean reward: 3139.58
               Mean episode length: 69.16
                  Mean reward/step: 47.95
       Mean episode length/episode: 7.27
            Mean episode successes: 3.8438
Mean episode consecutive_successes: 12.8795
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 17.04s
                        Total time: 28857.67s
                               ETA: 981574.9s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.752s, learning 0.267s)
               Value function loss: 165894.8082
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 3376.72
               Mean episode length: 68.28
                  Mean reward/step: 48.23
       Mean episode length/episode: 7.21
            Mean episode successes: 3.6748
Mean episode consecutive_successes: 12.8933
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 17.02s
                        Total time: 28874.69s
                               ETA: 981800.0s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.825s, learning 0.199s)
               Value function loss: 174074.8801
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 3292.08
               Mean episode length: 67.19
                  Mean reward/step: 47.17
       Mean episode length/episode: 7.27
            Mean episode successes: 3.5581
Mean episode consecutive_successes: 12.9703
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 17.02s
                        Total time: 28891.71s
                               ETA: 982025.0s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.275s, learning 0.162s)
               Value function loss: 159758.0930
                    Surrogate loss: -0.0149
             Mean action noise std: 0.72
                       Mean reward: 3014.57
               Mean episode length: 66.78
                  Mean reward/step: 46.96
       Mean episode length/episode: 7.36
            Mean episode successes: 3.5786
Mean episode consecutive_successes: 13.0886
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 16.44s
                        Total time: 28908.15s
                               ETA: 982229.9s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.718s, learning 0.204s)
               Value function loss: 179813.7289
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 3509.49
               Mean episode length: 69.33
                  Mean reward/step: 48.99
       Mean episode length/episode: 7.29
            Mean episode successes: 3.7896
Mean episode consecutive_successes: 13.0810
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 16.92s
                        Total time: 28925.07s
                               ETA: 982451.1s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.301s, learning 0.187s)
               Value function loss: 203067.7270
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 3165.11
               Mean episode length: 67.82
                  Mean reward/step: 50.00
       Mean episode length/episode: 7.25
            Mean episode successes: 3.9409
Mean episode consecutive_successes: 12.9784
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 16.49s
                        Total time: 28941.56s
                               ETA: 982657.4s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.505s, learning 0.254s)
               Value function loss: 265359.3523
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 3932.26
               Mean episode length: 69.80
                  Mean reward/step: 50.38
       Mean episode length/episode: 7.34
            Mean episode successes: 3.9448
Mean episode consecutive_successes: 13.0831
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 16.76s
                        Total time: 28958.32s
                               ETA: 982872.7s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.987s, learning 0.207s)
               Value function loss: 184963.6387
                    Surrogate loss: 0.0057
             Mean action noise std: 0.72
                       Mean reward: 3854.80
               Mean episode length: 69.54
                  Mean reward/step: 51.12
       Mean episode length/episode: 7.23
            Mean episode successes: 3.7627
Mean episode consecutive_successes: 13.2147
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 17.19s
                        Total time: 28975.51s
                               ETA: 983102.7s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.809s, learning 0.286s)
               Value function loss: 170291.1461
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 3988.27
               Mean episode length: 69.79
                  Mean reward/step: 49.05
       Mean episode length/episode: 7.27
            Mean episode successes: 3.8442
Mean episode consecutive_successes: 13.1910
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 17.10s
                        Total time: 28992.60s
                               ETA: 983329.1s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.368s, learning 0.161s)
               Value function loss: 165897.1129
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: 2883.38
               Mean episode length: 68.42
                  Mean reward/step: 49.98
       Mean episode length/episode: 7.25
            Mean episode successes: 3.8335
Mean episode consecutive_successes: 13.1506
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 16.53s
                        Total time: 29009.13s
                               ETA: 983536.2s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.551s, learning 0.189s)
               Value function loss: 172730.5066
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 3165.16
               Mean episode length: 69.75
                  Mean reward/step: 49.03
       Mean episode length/episode: 7.22
            Mean episode successes: 3.8076
Mean episode consecutive_successes: 13.1532
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 16.74s
                        Total time: 29025.87s
                               ETA: 983750.3s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.433s, learning 0.258s)
               Value function loss: 172153.6672
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: 3017.90
               Mean episode length: 67.85
                  Mean reward/step: 47.32
       Mean episode length/episode: 7.25
            Mean episode successes: 3.6729
Mean episode consecutive_successes: 13.2406
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 16.69s
                        Total time: 29042.57s
                               ETA: 983962.5s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.606s, learning 0.324s)
               Value function loss: 150801.8965
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 2982.89
               Mean episode length: 67.46
                  Mean reward/step: 47.64
       Mean episode length/episode: 7.33
            Mean episode successes: 3.6592
Mean episode consecutive_successes: 13.2575
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 16.93s
                        Total time: 29059.50s
                               ETA: 984182.7s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.849s, learning 0.160s)
               Value function loss: 167654.4457
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 3629.24
               Mean episode length: 69.67
                  Mean reward/step: 50.93
       Mean episode length/episode: 7.32
            Mean episode successes: 3.8398
Mean episode consecutive_successes: 13.3146
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 17.01s
                        Total time: 29076.50s
                               ETA: 984405.4s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.506s, learning 0.252s)
               Value function loss: 176218.8250
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 3237.56
               Mean episode length: 67.88
                  Mean reward/step: 51.37
       Mean episode length/episode: 7.22
            Mean episode successes: 3.8984
Mean episode consecutive_successes: 13.3179
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 16.76s
                        Total time: 29093.26s
                               ETA: 984619.4s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.642s, learning 0.192s)
               Value function loss: 178474.6785
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 3364.19
               Mean episode length: 68.67
                  Mean reward/step: 50.99
       Mean episode length/episode: 7.27
            Mean episode successes: 4.0728
Mean episode consecutive_successes: 13.3153
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 16.83s
                        Total time: 29110.10s
                               ETA: 984835.8s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.222s, learning 0.174s)
               Value function loss: 197230.5512
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 4024.35
               Mean episode length: 69.93
                  Mean reward/step: 52.28
       Mean episode length/episode: 7.25
            Mean episode successes: 4.0649
Mean episode consecutive_successes: 13.3902
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 16.40s
                        Total time: 29126.49s
                               ETA: 985037.3s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.283s, learning 0.178s)
               Value function loss: 259621.7648
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 3484.61
               Mean episode length: 70.07
                  Mean reward/step: 53.05
       Mean episode length/episode: 7.27
            Mean episode successes: 4.0259
Mean episode consecutive_successes: 13.4245
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 16.46s
                        Total time: 29142.95s
                               ETA: 985240.8s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.689s, learning 0.188s)
               Value function loss: 274352.9336
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 2978.57
               Mean episode length: 68.53
                  Mean reward/step: 52.84
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1841
Mean episode consecutive_successes: 13.4086
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 16.88s
                        Total time: 29159.83s
                               ETA: 985458.2s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 953 steps/s (collection: 17.011s, learning 0.166s)
               Value function loss: 178357.6180
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 4017.61
               Mean episode length: 69.43
                  Mean reward/step: 51.10
       Mean episode length/episode: 7.28
            Mean episode successes: 4.1309
Mean episode consecutive_successes: 13.5609
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 17.18s
                        Total time: 29177.01s
                               ETA: 985685.6s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 958 steps/s (collection: 16.930s, learning 0.172s)
               Value function loss: 164201.8336
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 3546.35
               Mean episode length: 68.24
                  Mean reward/step: 49.91
       Mean episode length/episode: 7.26
            Mean episode successes: 3.9614
Mean episode consecutive_successes: 13.6025
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 17.10s
                        Total time: 29194.11s
                               ETA: 985910.2s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.797s, learning 0.200s)
               Value function loss: 157566.0445
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 3259.04
               Mean episode length: 68.98
                  Mean reward/step: 51.60
       Mean episode length/episode: 7.20
            Mean episode successes: 3.7827
Mean episode consecutive_successes: 13.6310
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 17.00s
                        Total time: 29211.11s
                               ETA: 986131.2s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.895s, learning 0.309s)
               Value function loss: 177361.7750
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 3948.10
               Mean episode length: 69.94
                  Mean reward/step: 50.96
       Mean episode length/episode: 7.29
            Mean episode successes: 3.8110
Mean episode consecutive_successes: 13.7030
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 17.20s
                        Total time: 29228.31s
                               ETA: 986359.0s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.769s, learning 0.269s)
               Value function loss: 167032.4879
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: 3625.65
               Mean episode length: 68.38
                  Mean reward/step: 50.28
       Mean episode length/episode: 7.27
            Mean episode successes: 3.9805
Mean episode consecutive_successes: 13.6833
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 17.04s
                        Total time: 29245.35s
                               ETA: 986581.0s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.453s, learning 0.197s)
               Value function loss: 172603.2875
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 4028.34
               Mean episode length: 70.37
                  Mean reward/step: 48.89
       Mean episode length/episode: 7.28
            Mean episode successes: 3.8418
Mean episode consecutive_successes: 13.8219
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 16.65s
                        Total time: 29262.00s
                               ETA: 986789.7s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.408s, learning 0.162s)
               Value function loss: 177972.2227
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 4131.28
               Mean episode length: 70.68
                  Mean reward/step: 49.92
       Mean episode length/episode: 7.26
            Mean episode successes: 3.7798
Mean episode consecutive_successes: 13.8942
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 16.57s
                        Total time: 29278.57s
                               ETA: 986995.6s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1030 steps/s (collection: 15.733s, learning 0.170s)
               Value function loss: 173090.8152
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 2835.46
               Mean episode length: 69.01
                  Mean reward/step: 50.20
       Mean episode length/episode: 7.21
            Mean episode successes: 3.7280
Mean episode consecutive_successes: 13.7949
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 15.90s
                        Total time: 29294.47s
                               ETA: 987178.9s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.471s, learning 0.164s)
               Value function loss: 186764.7789
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 3670.52
               Mean episode length: 70.26
                  Mean reward/step: 50.00
       Mean episode length/episode: 7.30
            Mean episode successes: 3.9756
Mean episode consecutive_successes: 13.7024
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 16.64s
                        Total time: 29311.11s
                               ETA: 987386.7s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.564s, learning 0.164s)
               Value function loss: 203318.7066
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 3329.42
               Mean episode length: 69.59
                  Mean reward/step: 49.30
       Mean episode length/episode: 7.20
            Mean episode successes: 3.8770
Mean episode consecutive_successes: 13.7085
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 16.73s
                        Total time: 29327.83s
                               ETA: 987597.5s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 963 steps/s (collection: 16.831s, learning 0.172s)
               Value function loss: 175404.9832
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 3317.06
               Mean episode length: 70.47
                  Mean reward/step: 48.96
       Mean episode length/episode: 7.28
            Mean episode successes: 3.7700
Mean episode consecutive_successes: 13.6941
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 17.00s
                        Total time: 29344.84s
                               ETA: 987817.4s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.041s, learning 0.169s)
               Value function loss: 183905.3992
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3251.48
               Mean episode length: 69.16
                  Mean reward/step: 50.84
       Mean episode length/episode: 7.23
            Mean episode successes: 3.7847
Mean episode consecutive_successes: 13.6916
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 17.21s
                        Total time: 29362.05s
                               ETA: 988044.1s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.435s, learning 0.207s)
               Value function loss: 178842.8078
                    Surrogate loss: -0.0058
             Mean action noise std: 0.72
                       Mean reward: 3426.00
               Mean episode length: 69.24
                  Mean reward/step: 49.72
       Mean episode length/episode: 7.29
            Mean episode successes: 3.7842
Mean episode consecutive_successes: 13.7536
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 16.64s
                        Total time: 29378.69s
                               ETA: 988251.5s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.565s, learning 0.166s)
               Value function loss: 188763.2590
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 3491.51
               Mean episode length: 68.27
                  Mean reward/step: 49.42
       Mean episode length/episode: 7.32
            Mean episode successes: 3.9966
Mean episode consecutive_successes: 13.7005
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 16.73s
                        Total time: 29395.42s
                               ETA: 988461.8s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.859s, learning 0.169s)
               Value function loss: 215277.4047
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 4011.00
               Mean episode length: 69.86
                  Mean reward/step: 51.16
       Mean episode length/episode: 7.20
            Mean episode successes: 3.9028
Mean episode consecutive_successes: 13.7334
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 17.03s
                        Total time: 29412.45s
                               ETA: 988681.8s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.492s, learning 0.274s)
               Value function loss: 201607.7051
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 4425.99
               Mean episode length: 69.62
                  Mean reward/step: 52.29
       Mean episode length/episode: 7.35
            Mean episode successes: 3.9053
Mean episode consecutive_successes: 13.8103
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 16.77s
                        Total time: 29429.22s
                               ETA: 988892.9s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.670s, learning 0.169s)
               Value function loss: 205501.7680
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 3499.52
               Mean episode length: 69.99
                  Mean reward/step: 50.99
       Mean episode length/episode: 7.26
            Mean episode successes: 4.0190
Mean episode consecutive_successes: 13.7187
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 16.84s
                        Total time: 29446.05s
                               ETA: 989106.3s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.343s, learning 0.295s)
               Value function loss: 192009.3430
                    Surrogate loss: 0.0003
             Mean action noise std: 0.72
                       Mean reward: 3215.47
               Mean episode length: 68.08
                  Mean reward/step: 49.25
       Mean episode length/episode: 7.35
            Mean episode successes: 3.9375
Mean episode consecutive_successes: 13.7247
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 8.64s
                        Total time: 29454.69s
                               ETA: 989044.1s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.168s)
               Value function loss: 212621.3508
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 3499.36
               Mean episode length: 68.36
                  Mean reward/step: 50.08
       Mean episode length/episode: 7.22
            Mean episode successes: 4.0283
Mean episode consecutive_successes: 13.6721
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 8.46s
                        Total time: 29463.15s
                               ETA: 988976.1s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.755s, learning 0.207s)
               Value function loss: 229550.9883
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 3281.64
               Mean episode length: 69.45
                  Mean reward/step: 50.38
       Mean episode length/episode: 7.27
            Mean episode successes: 3.9883
Mean episode consecutive_successes: 13.7233
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 8.96s
                        Total time: 29472.12s
                               ETA: 988924.9s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.483s, learning 0.187s)
               Value function loss: 233522.9031
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 3462.07
               Mean episode length: 69.90
                  Mean reward/step: 49.83
       Mean episode length/episode: 7.25
            Mean episode successes: 3.8276
Mean episode consecutive_successes: 13.8154
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 8.67s
                        Total time: 29480.79s
                               ETA: 988864.0s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.563s, learning 0.182s)
               Value function loss: 197631.8770
                    Surrogate loss: 0.0065
             Mean action noise std: 0.72
                       Mean reward: 3113.76
               Mean episode length: 68.58
                  Mean reward/step: 47.77
       Mean episode length/episode: 7.28
            Mean episode successes: 3.5942
Mean episode consecutive_successes: 13.8140
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 8.75s
                        Total time: 29489.53s
                               ETA: 988805.6s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.549s, learning 0.160s)
               Value function loss: 195239.6246
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 3467.92
               Mean episode length: 66.34
                  Mean reward/step: 50.87
       Mean episode length/episode: 7.33
            Mean episode successes: 3.8438
Mean episode consecutive_successes: 13.7796
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 8.71s
                        Total time: 29498.24s
                               ETA: 988746.0s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.341s, learning 0.197s)
               Value function loss: 207983.0645
                    Surrogate loss: 0.0152
             Mean action noise std: 0.72
                       Mean reward: 3466.69
               Mean episode length: 69.77
                  Mean reward/step: 51.27
       Mean episode length/episode: 7.25
            Mean episode successes: 3.8628
Mean episode consecutive_successes: 13.7278
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 8.54s
                        Total time: 29506.78s
                               ETA: 988680.7s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.387s, learning 0.177s)
               Value function loss: 188859.8598
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 3122.92
               Mean episode length: 66.43
                  Mean reward/step: 52.84
       Mean episode length/episode: 7.24
            Mean episode successes: 3.9849
Mean episode consecutive_successes: 13.6693
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 8.56s
                        Total time: 29515.34s
                               ETA: 988616.4s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.586s, learning 0.377s)
               Value function loss: 170028.4777
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 4096.69
               Mean episode length: 68.09
                  Mean reward/step: 53.56
       Mean episode length/episode: 7.27
            Mean episode successes: 4.0977
Mean episode consecutive_successes: 13.7373
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 8.96s
                        Total time: 29524.31s
                               ETA: 988565.4s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.594s, learning 0.173s)
               Value function loss: 194488.9723
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 3866.63
               Mean episode length: 69.18
                  Mean reward/step: 52.70
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0659
Mean episode consecutive_successes: 13.8651
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 8.77s
                        Total time: 29533.07s
                               ETA: 988507.9s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.223s, learning 0.255s)
               Value function loss: 220217.7465
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: 3754.07
               Mean episode length: 70.35
                  Mean reward/step: 52.14
       Mean episode length/episode: 7.28
            Mean episode successes: 3.9478
Mean episode consecutive_successes: 13.9652
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 8.48s
                        Total time: 29541.55s
                               ETA: 988440.7s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.490s, learning 0.161s)
               Value function loss: 258408.5348
                    Surrogate loss: -0.0056
             Mean action noise std: 0.72
                       Mean reward: 3582.12
               Mean episode length: 68.66
                  Mean reward/step: 50.22
       Mean episode length/episode: 7.20
            Mean episode successes: 3.7266
Mean episode consecutive_successes: 13.9578
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 8.65s
                        Total time: 29550.20s
                               ETA: 988379.4s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.190s, learning 0.324s)
               Value function loss: 182688.8695
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 3240.49
               Mean episode length: 70.64
                  Mean reward/step: 48.14
       Mean episode length/episode: 7.27
            Mean episode successes: 3.6592
Mean episode consecutive_successes: 13.9767
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 8.51s
                        Total time: 29558.72s
                               ETA: 988313.6s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.645s, learning 0.323s)
               Value function loss: 195384.8582
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 4385.91
               Mean episode length: 70.22
                  Mean reward/step: 47.68
       Mean episode length/episode: 7.29
            Mean episode successes: 3.6685
Mean episode consecutive_successes: 14.0339
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 8.97s
                        Total time: 29567.68s
                               ETA: 988262.9s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.173s, learning 0.226s)
               Value function loss: 186695.9477
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: 2359.86
               Mean episode length: 67.24
                  Mean reward/step: 50.43
       Mean episode length/episode: 7.35
            Mean episode successes: 3.9233
Mean episode consecutive_successes: 13.8825
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 8.40s
                        Total time: 29576.08s
                               ETA: 988193.3s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.768s, learning 0.172s)
               Value function loss: 160390.1848
                    Surrogate loss: 0.0044
             Mean action noise std: 0.72
                       Mean reward: 3615.30
               Mean episode length: 68.71
                  Mean reward/step: 49.80
       Mean episode length/episode: 7.23
            Mean episode successes: 3.7988
Mean episode consecutive_successes: 13.9875
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 8.94s
                        Total time: 29585.02s
                               ETA: 988141.7s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.433s, learning 0.173s)
               Value function loss: 164601.0801
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 3673.17
               Mean episode length: 70.30
                  Mean reward/step: 51.85
       Mean episode length/episode: 7.21
            Mean episode successes: 3.8530
Mean episode consecutive_successes: 13.8921
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 8.61s
                        Total time: 29593.63s
                               ETA: 988079.1s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.257s, learning 0.163s)
               Value function loss: 159793.4641
                    Surrogate loss: 0.0105
             Mean action noise std: 0.72
                       Mean reward: 3850.60
               Mean episode length: 70.12
                  Mean reward/step: 53.03
       Mean episode length/episode: 7.22
            Mean episode successes: 3.9419
Mean episode consecutive_successes: 13.8898
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 8.42s
                        Total time: 29602.05s
                               ETA: 988010.3s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.391s, learning 0.285s)
               Value function loss: 151247.1027
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 3775.35
               Mean episode length: 69.16
                  Mean reward/step: 52.96
       Mean episode length/episode: 7.26
            Mean episode successes: 3.8379
Mean episode consecutive_successes: 13.9444
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 8.68s
                        Total time: 29610.72s
                               ETA: 987950.1s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.787s, learning 0.310s)
               Value function loss: 147989.8863
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 2772.77
               Mean episode length: 67.69
                  Mean reward/step: 52.63
       Mean episode length/episode: 7.31
            Mean episode successes: 4.1650
Mean episode consecutive_successes: 13.8792
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 9.10s
                        Total time: 29619.82s
                               ETA: 987904.0s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.230s, learning 0.175s)
               Value function loss: 191969.2512
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: 4391.16
               Mean episode length: 70.48
                  Mean reward/step: 54.61
       Mean episode length/episode: 7.23
            Mean episode successes: 3.9922
Mean episode consecutive_successes: 14.0307
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 8.41s
                        Total time: 29628.23s
                               ETA: 987834.8s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.687s, learning 0.206s)
               Value function loss: 188043.6355
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3735.46
               Mean episode length: 70.44
                  Mean reward/step: 51.47
       Mean episode length/episode: 7.30
            Mean episode successes: 3.9873
Mean episode consecutive_successes: 14.0518
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 8.89s
                        Total time: 29637.12s
                               ETA: 987781.9s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.569s, learning 0.164s)
               Value function loss: 192276.9023
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: 3862.74
               Mean episode length: 69.21
                  Mean reward/step: 52.61
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0146
Mean episode consecutive_successes: 14.0982
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 8.73s
                        Total time: 29645.85s
                               ETA: 987723.7s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.712s, learning 0.312s)
               Value function loss: 167215.3305
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 3898.17
               Mean episode length: 71.10
                  Mean reward/step: 52.92
       Mean episode length/episode: 7.36
            Mean episode successes: 4.1816
Mean episode consecutive_successes: 14.1916
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 9.02s
                        Total time: 29654.88s
                               ETA: 987675.2s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.547s, learning 0.186s)
               Value function loss: 182860.6859
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 3674.04
               Mean episode length: 67.77
                  Mean reward/step: 53.67
       Mean episode length/episode: 7.24
            Mean episode successes: 4.0405
Mean episode consecutive_successes: 14.3171
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 8.73s
                        Total time: 29663.61s
                               ETA: 987617.1s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.602s, learning 0.170s)
               Value function loss: 158731.5121
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 3255.22
               Mean episode length: 70.92
                  Mean reward/step: 52.20
       Mean episode length/episode: 7.21
            Mean episode successes: 4.0591
Mean episode consecutive_successes: 14.2173
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 8.77s
                        Total time: 29672.38s
                               ETA: 987560.3s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.533s, learning 0.162s)
               Value function loss: 168521.7336
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: 3373.65
               Mean episode length: 68.72
                  Mean reward/step: 52.49
       Mean episode length/episode: 7.21
            Mean episode successes: 4.0630
Mean episode consecutive_successes: 14.2079
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 8.70s
                        Total time: 29681.08s
                               ETA: 987501.0s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.202s, learning 0.161s)
               Value function loss: 165047.0410
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: 3413.79
               Mean episode length: 67.89
                  Mean reward/step: 52.90
       Mean episode length/episode: 7.26
            Mean episode successes: 4.0713
Mean episode consecutive_successes: 14.1863
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 8.36s
                        Total time: 29689.44s
                               ETA: 987430.7s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.339s, learning 0.247s)
               Value function loss: 170468.9625
                    Surrogate loss: -0.0108
             Mean action noise std: 0.72
                       Mean reward: 4518.72
               Mean episode length: 70.93
                  Mean reward/step: 51.99
       Mean episode length/episode: 7.33
            Mean episode successes: 4.0044
Mean episode consecutive_successes: 14.3891
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 8.59s
                        Total time: 29698.02s
                               ETA: 987367.8s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.425s, learning 0.160s)
               Value function loss: 176321.5973
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 3972.17
               Mean episode length: 69.23
                  Mean reward/step: 51.33
       Mean episode length/episode: 7.19
            Mean episode successes: 3.8770
Mean episode consecutive_successes: 14.3914
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 8.58s
                        Total time: 29706.61s
                               ETA: 987304.9s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.293s, learning 0.300s)
               Value function loss: 192198.2777
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 3840.05
               Mean episode length: 69.58
                  Mean reward/step: 50.55
       Mean episode length/episode: 7.28
            Mean episode successes: 3.8042
Mean episode consecutive_successes: 14.4052
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 8.59s
                        Total time: 29715.20s
                               ETA: 987242.3s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.557s, learning 0.194s)
               Value function loss: 193623.1102
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 3724.63
               Mean episode length: 68.88
                  Mean reward/step: 52.84
       Mean episode length/episode: 7.27
            Mean episode successes: 3.9517
Mean episode consecutive_successes: 14.3519
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 8.75s
                        Total time: 29723.95s
                               ETA: 987185.1s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.525s, learning 0.346s)
               Value function loss: 190721.7277
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 3654.66
               Mean episode length: 69.23
                  Mean reward/step: 53.15
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0557
Mean episode consecutive_successes: 14.2841
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 8.87s
                        Total time: 29732.82s
                               ETA: 987131.8s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.527s, learning 0.175s)
               Value function loss: 186740.3445
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 3181.29
               Mean episode length: 67.51
                  Mean reward/step: 54.70
       Mean episode length/episode: 7.34
            Mean episode successes: 4.4902
Mean episode consecutive_successes: 14.1749
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 8.70s
                        Total time: 29741.53s
                               ETA: 987072.9s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1816 steps/s (collection: 8.833s, learning 0.189s)
               Value function loss: 221759.0543
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 4053.18
               Mean episode length: 70.09
                  Mean reward/step: 52.24
       Mean episode length/episode: 7.19
            Mean episode successes: 4.0405
Mean episode consecutive_successes: 14.3156
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 9.02s
                        Total time: 29750.55s
                               ETA: 987024.7s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.415s, learning 0.402s)
               Value function loss: 225414.6168
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 3253.69
               Mean episode length: 67.34
                  Mean reward/step: 53.53
       Mean episode length/episode: 7.27
            Mean episode successes: 4.0640
Mean episode consecutive_successes: 14.3275
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 8.82s
                        Total time: 29759.36s
                               ETA: 986969.8s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.298s, learning 0.189s)
               Value function loss: 217261.4711
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 3449.18
               Mean episode length: 68.82
                  Mean reward/step: 53.95
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0879
Mean episode consecutive_successes: 14.3524
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 8.49s
                        Total time: 29767.85s
                               ETA: 986903.9s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.507s, learning 0.193s)
               Value function loss: 141338.4990
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3877.86
               Mean episode length: 70.28
                  Mean reward/step: 55.50
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2031
Mean episode consecutive_successes: 14.3394
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 8.70s
                        Total time: 29776.55s
                               ETA: 986845.1s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.636s, learning 0.194s)
               Value function loss: 153076.0332
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 3846.13
               Mean episode length: 71.25
                  Mean reward/step: 56.82
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4248
Mean episode consecutive_successes: 14.3878
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 8.83s
                        Total time: 29785.38s
                               ETA: 986790.7s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1773 steps/s (collection: 8.906s, learning 0.333s)
               Value function loss: 168747.5000
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 4005.00
               Mean episode length: 68.55
                  Mean reward/step: 57.55
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4077
Mean episode consecutive_successes: 14.4710
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 9.24s
                        Total time: 29794.62s
                               ETA: 986749.8s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.489s, learning 0.170s)
               Value function loss: 161026.5113
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: 4432.39
               Mean episode length: 69.07
                  Mean reward/step: 58.42
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5015
Mean episode consecutive_successes: 14.6637
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 8.66s
                        Total time: 29803.28s
                               ETA: 986689.8s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.602s, learning 0.159s)
               Value function loss: 164754.6137
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 3812.82
               Mean episode length: 70.37
                  Mean reward/step: 54.89
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3613
Mean episode consecutive_successes: 14.7422
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 8.76s
                        Total time: 29812.04s
                               ETA: 986633.2s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.170s)
               Value function loss: 163961.4742
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3279.27
               Mean episode length: 69.26
                  Mean reward/step: 53.24
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3975
Mean episode consecutive_successes: 14.6813
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 8.62s
                        Total time: 29820.66s
                               ETA: 986571.9s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1795 steps/s (collection: 8.941s, learning 0.184s)
               Value function loss: 187239.7613
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 4297.85
               Mean episode length: 69.11
                  Mean reward/step: 52.13
       Mean episode length/episode: 7.23
            Mean episode successes: 4.1377
Mean episode consecutive_successes: 14.8143
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 9.12s
                        Total time: 29829.78s
                               ETA: 986527.4s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.526s, learning 0.169s)
               Value function loss: 172985.0656
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 3960.97
               Mean episode length: 70.19
                  Mean reward/step: 51.04
       Mean episode length/episode: 7.22
            Mean episode successes: 3.9238
Mean episode consecutive_successes: 14.8283
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 8.70s
                        Total time: 29838.48s
                               ETA: 986468.7s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.498s, learning 0.165s)
               Value function loss: 173981.7578
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 3074.44
               Mean episode length: 70.11
                  Mean reward/step: 51.16
       Mean episode length/episode: 7.26
            Mean episode successes: 3.8887
Mean episode consecutive_successes: 14.7336
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 8.66s
                        Total time: 29847.14s
                               ETA: 986408.9s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.511s, learning 0.171s)
               Value function loss: 171870.0355
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 4468.51
               Mean episode length: 71.90
                  Mean reward/step: 52.04
       Mean episode length/episode: 7.32
            Mean episode successes: 4.0107
Mean episode consecutive_successes: 14.8021
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 8.68s
                        Total time: 29855.82s
                               ETA: 986349.9s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.465s, learning 0.164s)
               Value function loss: 173425.5031
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 3873.34
               Mean episode length: 70.49
                  Mean reward/step: 53.99
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1714
Mean episode consecutive_successes: 14.7344
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 8.63s
                        Total time: 29864.45s
                               ETA: 986289.1s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.333s, learning 0.220s)
               Value function loss: 185106.4574
                    Surrogate loss: -0.0027
             Mean action noise std: 0.72
                       Mean reward: 3711.79
               Mean episode length: 69.58
                  Mean reward/step: 55.10
       Mean episode length/episode: 7.23
            Mean episode successes: 4.2622
Mean episode consecutive_successes: 14.6488
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 8.55s
                        Total time: 29873.01s
                               ETA: 986225.8s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.381s, learning 0.172s)
               Value function loss: 185473.2348
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 3309.12
               Mean episode length: 69.33
                  Mean reward/step: 52.74
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2803
Mean episode consecutive_successes: 14.6714
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 8.55s
                        Total time: 29881.56s
                               ETA: 986162.6s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.383s, learning 0.166s)
               Value function loss: 200918.9914
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 3933.12
               Mean episode length: 69.92
                  Mean reward/step: 52.80
       Mean episode length/episode: 7.23
            Mean episode successes: 4.1401
Mean episode consecutive_successes: 14.7885
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 8.55s
                        Total time: 29890.11s
                               ETA: 986099.3s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.841s, learning 0.195s)
               Value function loss: 161673.3258
                    Surrogate loss: -0.0045
             Mean action noise std: 0.72
                       Mean reward: 3519.01
               Mean episode length: 68.95
                  Mean reward/step: 51.36
       Mean episode length/episode: 7.31
            Mean episode successes: 4.1484
Mean episode consecutive_successes: 14.7265
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 9.04s
                        Total time: 29899.14s
                               ETA: 986052.0s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.675s, learning 0.261s)
               Value function loss: 155346.9648
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 3836.44
               Mean episode length: 71.00
                  Mean reward/step: 51.18
       Mean episode length/episode: 7.26
            Mean episode successes: 3.9897
Mean episode consecutive_successes: 14.8199
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 8.94s
                        Total time: 29908.08s
                               ETA: 986001.5s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.167s, learning 0.205s)
               Value function loss: 166380.2895
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: 3117.00
               Mean episode length: 69.84
                  Mean reward/step: 50.18
       Mean episode length/episode: 7.16
            Mean episode successes: 3.8447
Mean episode consecutive_successes: 14.6701
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 8.37s
                        Total time: 29916.45s
                               ETA: 985932.5s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.596s, learning 0.169s)
               Value function loss: 176815.6387
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 3282.54
               Mean episode length: 70.99
                  Mean reward/step: 49.17
       Mean episode length/episode: 7.25
            Mean episode successes: 3.7705
Mean episode consecutive_successes: 14.6206
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 8.77s
                        Total time: 29925.22s
                               ETA: 985876.4s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.220s, learning 0.166s)
               Value function loss: 182239.7914
                    Surrogate loss: -0.0122
             Mean action noise std: 0.72
                       Mean reward: 3461.96
               Mean episode length: 69.40
                  Mean reward/step: 48.85
       Mean episode length/episode: 7.27
            Mean episode successes: 3.6880
Mean episode consecutive_successes: 14.5620
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 8.39s
                        Total time: 29933.60s
                               ETA: 985807.9s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.216s, learning 0.246s)
               Value function loss: 226605.9113
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 3648.90
               Mean episode length: 69.97
                  Mean reward/step: 48.54
       Mean episode length/episode: 7.32
            Mean episode successes: 3.7988
Mean episode consecutive_successes: 14.5019
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 8.46s
                        Total time: 29942.07s
                               ETA: 985742.0s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.496s, learning 0.265s)
               Value function loss: 192531.6668
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 3903.37
               Mean episode length: 70.53
                  Mean reward/step: 51.24
       Mean episode length/episode: 7.31
            Mean episode successes: 3.9043
Mean episode consecutive_successes: 14.4852
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 8.76s
                        Total time: 29950.83s
                               ETA: 985685.9s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.764s, learning 0.271s)
               Value function loss: 199267.9316
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: 3886.07
               Mean episode length: 71.53
                  Mean reward/step: 52.50
       Mean episode length/episode: 7.23
            Mean episode successes: 3.8760
Mean episode consecutive_successes: 14.5235
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 9.03s
                        Total time: 29959.86s
                               ETA: 985638.8s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.291s, learning 0.284s)
               Value function loss: 188223.7871
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 3284.17
               Mean episode length: 69.21
                  Mean reward/step: 54.04
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1260
Mean episode consecutive_successes: 14.4060
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 8.57s
                        Total time: 29968.44s
                               ETA: 985576.6s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.624s, learning 0.200s)
               Value function loss: 180556.6859
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 3149.27
               Mean episode length: 69.37
                  Mean reward/step: 55.57
       Mean episode length/episode: 7.28
            Mean episode successes: 4.1675
Mean episode consecutive_successes: 14.4145
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 8.82s
                        Total time: 29977.26s
                               ETA: 985522.7s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.127s, learning 0.340s)
               Value function loss: 175848.9832
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 3960.81
               Mean episode length: 71.18
                  Mean reward/step: 55.09
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3257
Mean episode consecutive_successes: 14.4157
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 8.47s
                        Total time: 29985.73s
                               ETA: 985457.1s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.532s, learning 0.172s)
               Value function loss: 168695.5941
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 4601.80
               Mean episode length: 70.95
                  Mean reward/step: 53.30
       Mean episode length/episode: 7.23
            Mean episode successes: 4.1611
Mean episode consecutive_successes: 14.5176
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 8.70s
                        Total time: 29994.43s
                               ETA: 985399.3s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.577s, learning 0.223s)
               Value function loss: 155760.4586
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: 3396.32
               Mean episode length: 69.14
                  Mean reward/step: 51.82
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1777
Mean episode consecutive_successes: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 8.80s
                        Total time: 30003.23s
                               ETA: 985344.7s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.641s, learning 0.419s)
               Value function loss: 170604.0434
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: 4084.10
               Mean episode length: 70.74
                  Mean reward/step: 51.82
       Mean episode length/episode: 7.23
            Mean episode successes: 4.0127
Mean episode consecutive_successes: 14.4698
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 9.06s
                        Total time: 30012.29s
                               ETA: 985298.6s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.794s, learning 0.243s)
               Value function loss: 189935.3504
                    Surrogate loss: 0.0043
             Mean action noise std: 0.72
                       Mean reward: 3788.94
               Mean episode length: 70.14
                  Mean reward/step: 52.17
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1172
Mean episode consecutive_successes: 14.4641
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 9.04s
                        Total time: 30021.33s
                               ETA: 985251.8s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.703s, learning 0.253s)
               Value function loss: 184324.1785
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: 4250.05
               Mean episode length: 68.38
                  Mean reward/step: 52.99
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1914
Mean episode consecutive_successes: 14.5507
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 8.96s
                        Total time: 30030.28s
                               ETA: 985202.4s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.434s, learning 0.298s)
               Value function loss: 198848.4566
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 3698.46
               Mean episode length: 69.65
                  Mean reward/step: 55.74
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2031
Mean episode consecutive_successes: 14.6341
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 8.73s
                        Total time: 30039.01s
                               ETA: 985145.7s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.592s, learning 0.215s)
               Value function loss: 198682.0672
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 3472.51
               Mean episode length: 69.01
                  Mean reward/step: 56.17
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3457
Mean episode consecutive_successes: 14.6245
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 8.81s
                        Total time: 30047.82s
                               ETA: 985091.5s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.331s, learning 0.173s)
               Value function loss: 211140.7191
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 3012.20
               Mean episode length: 70.01
                  Mean reward/step: 56.30
       Mean episode length/episode: 7.21
            Mean episode successes: 4.2998
Mean episode consecutive_successes: 14.6188
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 8.50s
                        Total time: 30056.33s
                               ETA: 985027.3s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.205s, learning 0.163s)
               Value function loss: 243873.7102
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 3711.35
               Mean episode length: 68.91
                  Mean reward/step: 57.01
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4976
Mean episode consecutive_successes: 14.6787
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 8.37s
                        Total time: 30064.69s
                               ETA: 984958.8s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.387s, learning 0.158s)
               Value function loss: 232912.6203
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 4026.45
               Mean episode length: 71.32
                  Mean reward/step: 55.25
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5723
Mean episode consecutive_successes: 14.7032
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 8.54s
                        Total time: 30073.24s
                               ETA: 984896.0s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.535s, learning 0.184s)
               Value function loss: 196284.6828
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 3474.44
               Mean episode length: 70.42
                  Mean reward/step: 52.72
       Mean episode length/episode: 7.21
            Mean episode successes: 4.2778
Mean episode consecutive_successes: 14.7592
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 8.72s
                        Total time: 30081.96s
                               ETA: 984839.0s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.373s, learning 0.164s)
               Value function loss: 176735.6941
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 4497.50
               Mean episode length: 72.21
                  Mean reward/step: 51.32
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1128
Mean episode consecutive_successes: 14.8589
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 8.54s
                        Total time: 30090.49s
                               ETA: 984776.1s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.407s, learning 0.200s)
               Value function loss: 190288.5660
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: 3802.60
               Mean episode length: 70.35
                  Mean reward/step: 51.52
       Mean episode length/episode: 7.32
            Mean episode successes: 4.0562
Mean episode consecutive_successes: 14.9560
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 8.61s
                        Total time: 30099.10s
                               ETA: 984715.5s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.736s, learning 0.397s)
               Value function loss: 220951.1496
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 3980.92
               Mean episode length: 70.55
                  Mean reward/step: 53.62
       Mean episode length/episode: 7.22
            Mean episode successes: 3.9897
Mean episode consecutive_successes: 14.9246
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 9.13s
                        Total time: 30108.23s
                               ETA: 984672.2s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.439s, learning 0.185s)
               Value function loss: 192977.1852
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 3938.71
               Mean episode length: 71.26
                  Mean reward/step: 52.33
       Mean episode length/episode: 7.30
            Mean episode successes: 4.0771
Mean episode consecutive_successes: 14.9515
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 8.62s
                        Total time: 30116.86s
                               ETA: 984612.2s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.477s, learning 0.224s)
               Value function loss: 166200.0195
                    Surrogate loss: 0.0000
             Mean action noise std: 0.72
                       Mean reward: 3274.75
               Mean episode length: 68.97
                  Mean reward/step: 51.78
       Mean episode length/episode: 7.28
            Mean episode successes: 4.0444
Mean episode consecutive_successes: 14.8495
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 8.70s
                        Total time: 30125.56s
                               ETA: 984554.8s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.539s, learning 0.224s)
               Value function loss: 162665.8766
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 4037.20
               Mean episode length: 71.33
                  Mean reward/step: 52.48
       Mean episode length/episode: 7.30
            Mean episode successes: 4.0830
Mean episode consecutive_successes: 14.9003
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 8.76s
                        Total time: 30134.32s
                               ETA: 984499.5s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1807 steps/s (collection: 8.854s, learning 0.211s)
               Value function loss: 174885.1906
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 4080.82
               Mean episode length: 70.29
                  Mean reward/step: 51.71
       Mean episode length/episode: 7.23
            Mean episode successes: 3.8872
Mean episode consecutive_successes: 14.8631
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 9.06s
                        Total time: 30143.39s
                               ETA: 984454.0s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1777 steps/s (collection: 9.034s, learning 0.183s)
               Value function loss: 169396.9051
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 3314.28
               Mean episode length: 68.06
                  Mean reward/step: 52.46
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0322
Mean episode consecutive_successes: 14.7704
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 9.22s
                        Total time: 30152.60s
                               ETA: 984413.5s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.489s, learning 0.192s)
               Value function loss: 197179.7141
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 3715.52
               Mean episode length: 70.01
                  Mean reward/step: 51.86
       Mean episode length/episode: 7.24
            Mean episode successes: 3.9790
Mean episode consecutive_successes: 14.7893
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 8.68s
                        Total time: 30161.28s
                               ETA: 984355.6s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.398s, learning 0.413s)
               Value function loss: 174633.2422
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 3456.00
               Mean episode length: 71.81
                  Mean reward/step: 51.95
       Mean episode length/episode: 7.25
            Mean episode successes: 3.8530
Mean episode consecutive_successes: 14.7853
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 8.81s
                        Total time: 30170.10s
                               ETA: 984301.9s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.410s, learning 0.170s)
               Value function loss: 236785.2094
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 4086.37
               Mean episode length: 71.40
                  Mean reward/step: 54.74
       Mean episode length/episode: 7.36
            Mean episode successes: 3.9580
Mean episode consecutive_successes: 14.8992
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 8.58s
                        Total time: 30178.68s
                               ETA: 984240.7s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.443s, learning 0.217s)
               Value function loss: 174229.6715
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 2928.53
               Mean episode length: 69.25
                  Mean reward/step: 54.72
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3931
Mean episode consecutive_successes: 14.7485
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 8.66s
                        Total time: 30187.34s
                               ETA: 984182.2s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.585s, learning 0.191s)
               Value function loss: 217132.4090
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: 3583.24
               Mean episode length: 69.95
                  Mean reward/step: 54.26
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3457
Mean episode consecutive_successes: 14.7490
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 8.78s
                        Total time: 30196.11s
                               ETA: 984127.5s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.608s, learning 0.226s)
               Value function loss: 168700.1664
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 4186.21
               Mean episode length: 70.81
                  Mean reward/step: 54.14
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3071
Mean episode consecutive_successes: 14.8143
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 8.83s
                        Total time: 30204.95s
                               ETA: 984074.7s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.525s, learning 0.161s)
               Value function loss: 145340.1805
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 3710.53
               Mean episode length: 70.66
                  Mean reward/step: 53.43
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1167
Mean episode consecutive_successes: 14.8655
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 8.69s
                        Total time: 30213.63s
                               ETA: 984017.1s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.752s, learning 0.228s)
               Value function loss: 149928.5779
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: 3536.03
               Mean episode length: 70.86
                  Mean reward/step: 51.94
       Mean episode length/episode: 7.23
            Mean episode successes: 3.9106
Mean episode consecutive_successes: 14.8823
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 8.98s
                        Total time: 30222.61s
                               ETA: 983969.1s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.404s, learning 0.182s)
               Value function loss: 148430.3500
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 3690.89
               Mean episode length: 70.00
                  Mean reward/step: 50.83
       Mean episode length/episode: 7.30
            Mean episode successes: 3.8530
Mean episode consecutive_successes: 14.9311
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 8.59s
                        Total time: 30231.20s
                               ETA: 983908.4s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.482s, learning 0.345s)
               Value function loss: 160457.9344
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 3357.51
               Mean episode length: 70.88
                  Mean reward/step: 52.27
       Mean episode length/episode: 7.28
            Mean episode successes: 3.9341
Mean episode consecutive_successes: 14.8601
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 8.83s
                        Total time: 30240.02s
                               ETA: 983855.4s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.469s, learning 0.191s)
               Value function loss: 181272.5480
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 3548.13
               Mean episode length: 70.62
                  Mean reward/step: 52.94
       Mean episode length/episode: 7.24
            Mean episode successes: 3.8403
Mean episode consecutive_successes: 14.8985
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 8.66s
                        Total time: 30248.68s
                               ETA: 983797.1s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.817s, learning 0.169s)
               Value function loss: 173657.1258
                    Surrogate loss: -0.0070
             Mean action noise std: 0.72
                       Mean reward: 3055.42
               Mean episode length: 67.35
                  Mean reward/step: 49.70
       Mean episode length/episode: 7.31
            Mean episode successes: 3.8203
Mean episode consecutive_successes: 14.8329
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 8.99s
                        Total time: 30257.67s
                               ETA: 983749.5s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.611s, learning 0.192s)
               Value function loss: 182779.0297
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 3339.69
               Mean episode length: 68.97
                  Mean reward/step: 52.90
       Mean episode length/episode: 7.32
            Mean episode successes: 4.0742
Mean episode consecutive_successes: 14.7525
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 8.80s
                        Total time: 30266.47s
                               ETA: 983695.9s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.710s, learning 0.184s)
               Value function loss: 181076.1801
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 3362.57
               Mean episode length: 69.74
                  Mean reward/step: 53.69
       Mean episode length/episode: 7.28
            Mean episode successes: 4.1523
Mean episode consecutive_successes: 14.6779
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 8.89s
                        Total time: 30275.37s
                               ETA: 983645.3s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.720s, learning 0.158s)
               Value function loss: 183041.2625
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 2829.17
               Mean episode length: 70.05
                  Mean reward/step: 54.59
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1797
Mean episode consecutive_successes: 14.6361
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 8.88s
                        Total time: 30284.24s
                               ETA: 983594.1s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.764s, learning 0.159s)
               Value function loss: 211718.2352
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 3820.30
               Mean episode length: 69.03
                  Mean reward/step: 55.47
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1392
Mean episode consecutive_successes: 14.6869
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 8.92s
                        Total time: 30293.17s
                               ETA: 983544.5s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.246s, learning 0.248s)
               Value function loss: 188588.0195
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 4534.02
               Mean episode length: 70.60
                  Mean reward/step: 55.76
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2109
Mean episode consecutive_successes: 14.7823
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 8.49s
                        Total time: 30301.66s
                               ETA: 983481.1s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.390s, learning 0.207s)
               Value function loss: 167176.4473
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: 2992.46
               Mean episode length: 70.01
                  Mean reward/step: 52.77
       Mean episode length/episode: 7.34
            Mean episode successes: 4.1895
Mean episode consecutive_successes: 14.7663
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 8.60s
                        Total time: 30310.26s
                               ETA: 983420.9s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.659s, learning 0.201s)
               Value function loss: 180868.8555
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 3341.54
               Mean episode length: 69.42
                  Mean reward/step: 55.97
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3105
Mean episode consecutive_successes: 14.8106
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 8.86s
                        Total time: 30319.12s
                               ETA: 983369.4s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.362s, learning 0.283s)
               Value function loss: 210262.9820
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 3730.24
               Mean episode length: 69.82
                  Mean reward/step: 56.50
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4292
Mean episode consecutive_successes: 14.8264
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 8.65s
                        Total time: 30327.76s
                               ETA: 983310.9s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.746s, learning 0.172s)
               Value function loss: 180110.5996
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 4329.97
               Mean episode length: 71.43
                  Mean reward/step: 54.16
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2803
Mean episode consecutive_successes: 14.8801
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 8.92s
                        Total time: 30336.68s
                               ETA: 983261.2s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.430s, learning 0.176s)
               Value function loss: 193955.4660
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 3582.36
               Mean episode length: 70.07
                  Mean reward/step: 53.10
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2075
Mean episode consecutive_successes: 14.8068
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 8.61s
                        Total time: 30345.29s
                               ETA: 983201.5s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.772s, learning 0.166s)
               Value function loss: 176202.0797
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: 3306.59
               Mean episode length: 69.60
                  Mean reward/step: 53.33
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1924
Mean episode consecutive_successes: 14.9018
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 8.94s
                        Total time: 30354.23s
                               ETA: 983152.6s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.449s, learning 0.322s)
               Value function loss: 169266.0961
                    Surrogate loss: 0.0028
             Mean action noise std: 0.72
                       Mean reward: 4201.12
               Mean episode length: 70.06
                  Mean reward/step: 52.52
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1362
Mean episode consecutive_successes: 14.9282
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 8.77s
                        Total time: 30363.00s
                               ETA: 983098.3s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.528s, learning 0.178s)
               Value function loss: 169711.0258
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 3209.63
               Mean episode length: 67.74
                  Mean reward/step: 52.44
       Mean episode length/episode: 7.27
            Mean episode successes: 4.0186
Mean episode consecutive_successes: 14.9436
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 8.71s
                        Total time: 30371.70s
                               ETA: 983041.9s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.349s, learning 0.271s)
               Value function loss: 170638.9695
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 3476.05
               Mean episode length: 69.45
                  Mean reward/step: 50.37
       Mean episode length/episode: 7.27
            Mean episode successes: 3.9941
Mean episode consecutive_successes: 14.9187
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 8.62s
                        Total time: 30380.32s
                               ETA: 982982.8s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.584s, learning 0.183s)
               Value function loss: 178779.1516
                    Surrogate loss: -0.0155
             Mean action noise std: 0.72
                       Mean reward: 3316.63
               Mean episode length: 68.93
                  Mean reward/step: 53.09
       Mean episode length/episode: 7.25
            Mean episode successes: 4.0479
Mean episode consecutive_successes: 14.8539
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 8.77s
                        Total time: 30389.09s
                               ETA: 982928.5s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.316s, learning 0.162s)
               Value function loss: 193951.9180
                    Surrogate loss: 0.0013
             Mean action noise std: 0.72
                       Mean reward: 3356.25
               Mean episode length: 68.85
                  Mean reward/step: 55.58
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1265
Mean episode consecutive_successes: 14.9083
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 8.48s
                        Total time: 30397.57s
                               ETA: 982864.9s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.647s, learning 0.168s)
               Value function loss: 179407.8145
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 4011.60
               Mean episode length: 71.84
                  Mean reward/step: 54.54
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2383
Mean episode consecutive_successes: 14.9154
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 8.82s
                        Total time: 30406.38s
                               ETA: 982812.2s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.471s, learning 0.163s)
               Value function loss: 191771.4430
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 4313.57
               Mean episode length: 69.94
                  Mean reward/step: 53.39
       Mean episode length/episode: 7.36
            Mean episode successes: 4.2441
Mean episode consecutive_successes: 15.0966
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 8.63s
                        Total time: 30415.02s
                               ETA: 982753.6s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.465s, learning 0.178s)
               Value function loss: 182762.9281
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 4089.19
               Mean episode length: 71.03
                  Mean reward/step: 51.84
       Mean episode length/episode: 7.23
            Mean episode successes: 4.0854
Mean episode consecutive_successes: 15.0904
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 8.64s
                        Total time: 30423.66s
                               ETA: 982695.4s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.445s, learning 0.171s)
               Value function loss: 197186.1082
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: 3025.12
               Mean episode length: 70.47
                  Mean reward/step: 53.40
       Mean episode length/episode: 7.22
            Mean episode successes: 4.0391
Mean episode consecutive_successes: 15.0255
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 8.62s
                        Total time: 30432.28s
                               ETA: 982636.4s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.161s)
               Value function loss: 173848.6023
                    Surrogate loss: -0.0106
             Mean action noise std: 0.72
                       Mean reward: 3430.49
               Mean episode length: 70.58
                  Mean reward/step: 55.63
       Mean episode length/episode: 7.22
            Mean episode successes: 4.1641
Mean episode consecutive_successes: 14.9296
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 8.45s
                        Total time: 30440.73s
                               ETA: 982571.9s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.281s, learning 0.164s)
               Value function loss: 186830.3184
                    Surrogate loss: 0.0030
             Mean action noise std: 0.72
                       Mean reward: 3576.46
               Mean episode length: 70.81
                  Mean reward/step: 54.96
       Mean episode length/episode: 7.34
            Mean episode successes: 4.4141
Mean episode consecutive_successes: 14.8943
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 8.44s
                        Total time: 30449.17s
                               ETA: 982507.4s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.698s, learning 0.175s)
               Value function loss: 187734.4480
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 3011.83
               Mean episode length: 68.62
                  Mean reward/step: 52.98
       Mean episode length/episode: 7.21
            Mean episode successes: 4.2988
Mean episode consecutive_successes: 14.8032
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 8.87s
                        Total time: 30458.04s
                               ETA: 982456.7s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.608s, learning 0.155s)
               Value function loss: 183561.3668
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 4539.47
               Mean episode length: 70.86
                  Mean reward/step: 54.57
       Mean episode length/episode: 7.23
            Mean episode successes: 4.0781
Mean episode consecutive_successes: 14.9594
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 8.76s
                        Total time: 30466.81s
                               ETA: 982402.5s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.257s, learning 0.252s)
               Value function loss: 174535.0777
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 3706.27
               Mean episode length: 71.30
                  Mean reward/step: 54.50
       Mean episode length/episode: 7.32
            Mean episode successes: 4.1948
Mean episode consecutive_successes: 14.9998
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 8.51s
                        Total time: 30475.31s
                               ETA: 982340.2s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.328s, learning 0.157s)
               Value function loss: 177443.0895
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 3008.27
               Mean episode length: 67.85
                  Mean reward/step: 54.31
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1089
Mean episode consecutive_successes: 15.0322
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 8.49s
                        Total time: 30483.80s
                               ETA: 982277.2s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.370s, learning 0.168s)
               Value function loss: 180212.3227
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: 3292.51
               Mean episode length: 69.19
                  Mean reward/step: 54.65
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2500
Mean episode consecutive_successes: 15.0316
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 8.54s
                        Total time: 30492.34s
                               ETA: 982215.8s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.552s, learning 0.166s)
               Value function loss: 205252.3590
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 3918.87
               Mean episode length: 70.55
                  Mean reward/step: 55.31
       Mean episode length/episode: 7.31
            Mean episode successes: 4.5112
Mean episode consecutive_successes: 14.9883
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 8.72s
                        Total time: 30501.06s
                               ETA: 982160.3s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.256s, learning 0.161s)
               Value function loss: 162921.9711
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 3192.55
               Mean episode length: 70.79
                  Mean reward/step: 55.20
       Mean episode length/episode: 7.23
            Mean episode successes: 4.3638
Mean episode consecutive_successes: 14.9876
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 8.42s
                        Total time: 30509.47s
                               ETA: 982095.2s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.322s, learning 0.160s)
               Value function loss: 168714.0254
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 3896.22
               Mean episode length: 70.51
                  Mean reward/step: 57.79
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4663
Mean episode consecutive_successes: 15.0080
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 8.48s
                        Total time: 30517.96s
                               ETA: 982032.2s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.172s)
               Value function loss: 196545.0531
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 4002.82
               Mean episode length: 69.17
                  Mean reward/step: 57.30
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4604
Mean episode consecutive_successes: 15.1441
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 8.54s
                        Total time: 30526.50s
                               ETA: 981971.2s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.653s, learning 0.171s)
               Value function loss: 188841.1660
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 4344.14
               Mean episode length: 71.64
                  Mean reward/step: 54.28
       Mean episode length/episode: 7.19
            Mean episode successes: 4.1348
Mean episode consecutive_successes: 15.2105
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 8.82s
                        Total time: 30535.32s
                               ETA: 981919.2s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.206s, learning 0.183s)
               Value function loss: 163733.3926
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 3890.85
               Mean episode length: 69.85
                  Mean reward/step: 53.26
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1499
Mean episode consecutive_successes: 15.1790
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 8.39s
                        Total time: 30543.71s
                               ETA: 981853.3s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.229s, learning 0.159s)
               Value function loss: 161193.8531
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 3671.06
               Mean episode length: 71.81
                  Mean reward/step: 53.64
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1914
Mean episode consecutive_successes: 15.1866
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 8.39s
                        Total time: 30552.10s
                               ETA: 981787.4s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.531s, learning 0.321s)
               Value function loss: 173108.4672
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 3403.58
               Mean episode length: 69.31
                  Mean reward/step: 53.35
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2104
Mean episode consecutive_successes: 15.0818
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 8.85s
                        Total time: 30560.95s
                               ETA: 981736.4s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.453s, learning 0.163s)
               Value function loss: 183969.4488
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 3934.06
               Mean episode length: 71.10
                  Mean reward/step: 53.56
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1724
Mean episode consecutive_successes: 15.1178
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 8.62s
                        Total time: 30569.57s
                               ETA: 981677.9s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.697s, learning 0.286s)
               Value function loss: 190651.6332
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 3873.54
               Mean episode length: 71.26
                  Mean reward/step: 54.16
       Mean episode length/episode: 7.35
            Mean episode successes: 4.4209
Mean episode consecutive_successes: 15.1255
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 8.98s
                        Total time: 30578.55s
                               ETA: 981631.2s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.396s, learning 0.269s)
               Value function loss: 180493.9000
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 4889.90
               Mean episode length: 71.05
                  Mean reward/step: 52.59
       Mean episode length/episode: 7.24
            Mean episode successes: 4.1382
Mean episode consecutive_successes: 15.2730
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 8.67s
                        Total time: 30587.22s
                               ETA: 981574.3s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.696s, learning 0.231s)
               Value function loss: 164165.7684
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 3910.22
               Mean episode length: 70.53
                  Mean reward/step: 54.50
       Mean episode length/episode: 7.28
            Mean episode successes: 4.1270
Mean episode consecutive_successes: 15.2394
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 8.93s
                        Total time: 30596.14s
                               ETA: 981525.9s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.372s, learning 0.156s)
               Value function loss: 159088.1773
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 3018.87
               Mean episode length: 68.50
                  Mean reward/step: 55.35
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2559
Mean episode consecutive_successes: 15.0990
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 8.53s
                        Total time: 30604.67s
                               ETA: 981464.7s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.560s, learning 0.158s)
               Value function loss: 162162.9215
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 2859.24
               Mean episode length: 69.35
                  Mean reward/step: 56.74
       Mean episode length/episode: 7.24
            Mean episode successes: 4.4673
Mean episode consecutive_successes: 14.9902
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 8.72s
                        Total time: 30613.39s
                               ETA: 981409.6s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.169s, learning 0.156s)
               Value function loss: 171753.4773
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: 3935.35
               Mean episode length: 70.74
                  Mean reward/step: 54.54
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3555
Mean episode consecutive_successes: 15.0729
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 8.33s
                        Total time: 30621.71s
                               ETA: 981342.0s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.456s, learning 0.281s)
               Value function loss: 162550.9312
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 4188.52
               Mean episode length: 70.23
                  Mean reward/step: 54.34
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1284
Mean episode consecutive_successes: 15.2489
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 8.74s
                        Total time: 30630.45s
                               ETA: 981287.6s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.565s, learning 0.159s)
               Value function loss: 167365.1656
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 3015.64
               Mean episode length: 68.71
                  Mean reward/step: 53.86
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2412
Mean episode consecutive_successes: 15.2545
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 8.72s
                        Total time: 30639.18s
                               ETA: 981232.7s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.363s, learning 0.158s)
               Value function loss: 169459.0293
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 3621.08
               Mean episode length: 70.28
                  Mean reward/step: 54.29
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1836
Mean episode consecutive_successes: 15.2923
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 8.52s
                        Total time: 30647.70s
                               ETA: 981171.5s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.627s, learning 0.201s)
               Value function loss: 165553.8691
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: 3621.95
               Mean episode length: 70.11
                  Mean reward/step: 55.37
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2554
Mean episode consecutive_successes: 15.2781
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 8.83s
                        Total time: 30656.52s
                               ETA: 981120.1s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.511s, learning 0.206s)
               Value function loss: 188335.3652
                    Surrogate loss: 0.0037
             Mean action noise std: 0.72
                       Mean reward: 4456.82
               Mean episode length: 71.25
                  Mean reward/step: 58.13
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3872
Mean episode consecutive_successes: 15.3133
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 8.72s
                        Total time: 30665.24s
                               ETA: 981065.2s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.309s, learning 0.328s)
               Value function loss: 164155.8992
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 4029.71
               Mean episode length: 71.12
                  Mean reward/step: 54.15
       Mean episode length/episode: 7.31
            Mean episode successes: 4.1772
Mean episode consecutive_successes: 15.4251
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 8.64s
                        Total time: 30673.88s
                               ETA: 981007.7s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.149s, learning 0.158s)
               Value function loss: 167339.4434
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 4460.47
               Mean episode length: 72.92
                  Mean reward/step: 54.51
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2222
Mean episode consecutive_successes: 15.4198
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 8.31s
                        Total time: 30682.19s
                               ETA: 980939.7s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.512s, learning 0.328s)
               Value function loss: 177367.9613
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: 3599.96
               Mean episode length: 71.38
                  Mean reward/step: 55.11
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4585
Mean episode consecutive_successes: 15.2418
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 8.84s
                        Total time: 30691.03s
                               ETA: 980888.8s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.806s, learning 0.159s)
               Value function loss: 175759.4988
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 4459.45
               Mean episode length: 71.07
                  Mean reward/step: 54.36
       Mean episode length/episode: 7.20
            Mean episode successes: 4.1206
Mean episode consecutive_successes: 15.3633
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 8.96s
                        Total time: 30699.99s
                               ETA: 980841.9s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.662s, learning 0.158s)
               Value function loss: 165157.4305
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 3700.97
               Mean episode length: 71.80
                  Mean reward/step: 53.07
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1172
Mean episode consecutive_successes: 15.3113
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 8.82s
                        Total time: 30708.81s
                               ETA: 980790.4s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1804 steps/s (collection: 8.914s, learning 0.168s)
               Value function loss: 179364.6473
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 3637.67
               Mean episode length: 70.26
                  Mean reward/step: 54.05
       Mean episode length/episode: 7.37
            Mean episode successes: 4.3682
Mean episode consecutive_successes: 15.2538
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 9.08s
                        Total time: 30717.89s
                               ETA: 980747.3s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.577s, learning 0.158s)
               Value function loss: 177132.7070
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 3596.13
               Mean episode length: 71.36
                  Mean reward/step: 53.74
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1226
Mean episode consecutive_successes: 15.3844
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 8.74s
                        Total time: 30726.63s
                               ETA: 980693.2s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.446s, learning 0.289s)
               Value function loss: 192339.7098
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 4354.01
               Mean episode length: 70.72
                  Mean reward/step: 55.03
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1377
Mean episode consecutive_successes: 15.4172
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 8.73s
                        Total time: 30735.36s
                               ETA: 980639.0s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.427s, learning 0.159s)
               Value function loss: 193626.9738
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 3749.80
               Mean episode length: 70.88
                  Mean reward/step: 55.96
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3823
Mean episode consecutive_successes: 15.2476
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 8.59s
                        Total time: 30743.95s
                               ETA: 980580.2s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.455s, learning 0.169s)
               Value function loss: 208382.5832
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 3808.57
               Mean episode length: 70.84
                  Mean reward/step: 54.53
       Mean episode length/episode: 7.19
            Mean episode successes: 4.1318
Mean episode consecutive_successes: 15.2438
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 8.62s
                        Total time: 30752.57s
                               ETA: 980522.6s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.654s, learning 0.157s)
               Value function loss: 191753.1301
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3628.58
               Mean episode length: 70.68
                  Mean reward/step: 53.41
       Mean episode length/episode: 7.37
            Mean episode successes: 4.4019
Mean episode consecutive_successes: 15.2168
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 8.81s
                        Total time: 30761.38s
                               ETA: 980471.0s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.247s, learning 0.318s)
               Value function loss: 189390.2074
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 4232.54
               Mean episode length: 71.91
                  Mean reward/step: 55.13
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2783
Mean episode consecutive_successes: 15.3684
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 8.56s
                        Total time: 30769.95s
                               ETA: 980411.6s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.356s, learning 0.158s)
               Value function loss: 235949.9465
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 3465.62
               Mean episode length: 71.11
                  Mean reward/step: 53.92
       Mean episode length/episode: 7.20
            Mean episode successes: 3.9917
Mean episode consecutive_successes: 15.3869
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 8.51s
                        Total time: 30778.46s
                               ETA: 980350.6s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.439s, learning 0.343s)
               Value function loss: 234476.6117
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 3595.96
               Mean episode length: 70.99
                  Mean reward/step: 56.45
       Mean episode length/episode: 7.37
            Mean episode successes: 4.2017
Mean episode consecutive_successes: 15.3751
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 8.78s
                        Total time: 30787.24s
                               ETA: 980298.1s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.595s, learning 0.158s)
               Value function loss: 169524.8406
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 3527.56
               Mean episode length: 69.39
                  Mean reward/step: 57.34
       Mean episode length/episode: 7.28
            Mean episode successes: 4.3271
Mean episode consecutive_successes: 15.3399
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 8.75s
                        Total time: 30796.00s
                               ETA: 980244.8s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.394s, learning 0.316s)
               Value function loss: 199212.6398
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 3570.01
               Mean episode length: 70.21
                  Mean reward/step: 55.80
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4648
Mean episode consecutive_successes: 15.3063
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 8.71s
                        Total time: 30804.71s
                               ETA: 980190.1s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.660s, learning 0.159s)
               Value function loss: 213740.2387
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 3653.90
               Mean episode length: 70.98
                  Mean reward/step: 54.59
       Mean episode length/episode: 7.20
            Mean episode successes: 4.4580
Mean episode consecutive_successes: 15.2707
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 8.82s
                        Total time: 30813.52s
                               ETA: 980139.0s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.676s, learning 0.160s)
               Value function loss: 296573.0930
                    Surrogate loss: -0.0058
             Mean action noise std: 0.72
                       Mean reward: 3846.16
               Mean episode length: 69.87
                  Mean reward/step: 55.61
       Mean episode length/episode: 7.28
            Mean episode successes: 4.3452
Mean episode consecutive_successes: 15.3029
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 8.84s
                        Total time: 30822.36s
                               ETA: 980088.4s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.523s, learning 0.159s)
               Value function loss: 173306.5098
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 3489.08
               Mean episode length: 69.33
                  Mean reward/step: 51.39
       Mean episode length/episode: 7.23
            Mean episode successes: 4.0190
Mean episode consecutive_successes: 15.3726
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 8.68s
                        Total time: 30831.04s
                               ETA: 980032.9s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.395s, learning 0.161s)
               Value function loss: 143313.4887
                    Surrogate loss: 0.0080
             Mean action noise std: 0.72
                       Mean reward: 3626.05
               Mean episode length: 71.12
                  Mean reward/step: 52.30
       Mean episode length/episode: 7.35
            Mean episode successes: 4.1089
Mean episode consecutive_successes: 15.3724
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 8.56s
                        Total time: 30839.60s
                               ETA: 979973.5s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.366s, learning 0.278s)
               Value function loss: 156522.2871
                    Surrogate loss: 0.0071
             Mean action noise std: 0.72
                       Mean reward: 3447.81
               Mean episode length: 70.67
                  Mean reward/step: 56.43
       Mean episode length/episode: 7.37
            Mean episode successes: 4.3833
Mean episode consecutive_successes: 15.3165
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 8.64s
                        Total time: 30848.24s
                               ETA: 979916.9s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.447s, learning 0.159s)
               Value function loss: 188710.2848
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: 4371.62
               Mean episode length: 71.50
                  Mean reward/step: 58.09
       Mean episode length/episode: 7.23
            Mean episode successes: 4.3740
Mean episode consecutive_successes: 15.3834
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 8.61s
                        Total time: 30856.85s
                               ETA: 979859.1s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.404s, learning 0.184s)
               Value function loss: 173198.3563
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 3477.49
               Mean episode length: 69.81
                  Mean reward/step: 58.48
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4092
Mean episode consecutive_successes: 15.2814
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 8.59s
                        Total time: 30865.44s
                               ETA: 979800.8s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.375s, learning 0.160s)
               Value function loss: 161013.1219
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 4508.55
               Mean episode length: 70.85
                  Mean reward/step: 59.31
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5503
Mean episode consecutive_successes: 15.3826
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 8.54s
                        Total time: 30873.97s
                               ETA: 979740.8s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.314s, learning 0.234s)
               Value function loss: 159904.9016
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 4398.94
               Mean episode length: 70.06
                  Mean reward/step: 58.06
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6099
Mean episode consecutive_successes: 15.4321
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 8.55s
                        Total time: 30882.52s
                               ETA: 979681.3s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.426s, learning 0.159s)
               Value function loss: 151764.1398
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 4483.96
               Mean episode length: 70.44
                  Mean reward/step: 57.89
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4634
Mean episode consecutive_successes: 15.6063
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 8.59s
                        Total time: 30891.11s
                               ETA: 979623.0s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.621s, learning 0.317s)
               Value function loss: 168318.5965
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 3730.54
               Mean episode length: 71.08
                  Mean reward/step: 59.28
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6655
Mean episode consecutive_successes: 15.5290
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 8.94s
                        Total time: 30900.04s
                               ETA: 979575.9s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.842s, learning 0.164s)
               Value function loss: 168069.1063
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 4459.63
               Mean episode length: 71.67
                  Mean reward/step: 57.02
       Mean episode length/episode: 7.23
            Mean episode successes: 4.3159
Mean episode consecutive_successes: 15.6903
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 9.01s
                        Total time: 30909.05s
                               ETA: 979531.0s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.438s, learning 0.158s)
               Value function loss: 173098.0980
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 4163.75
               Mean episode length: 71.51
                  Mean reward/step: 56.63
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2930
Mean episode consecutive_successes: 15.7382
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 8.60s
                        Total time: 30917.65s
                               ETA: 979473.1s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.142s, learning 0.161s)
               Value function loss: 177684.7754
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 3790.00
               Mean episode length: 69.28
                  Mean reward/step: 57.11
       Mean episode length/episode: 7.33
            Mean episode successes: 4.5068
Mean episode consecutive_successes: 15.6424
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 8.30s
                        Total time: 30925.95s
                               ETA: 979405.9s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.507s, learning 0.162s)
               Value function loss: 180963.5301
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 4111.10
               Mean episode length: 70.91
                  Mean reward/step: 58.15
       Mean episode length/episode: 7.32
            Mean episode successes: 4.5771
Mean episode consecutive_successes: 15.7609
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 8.67s
                        Total time: 30934.62s
                               ETA: 979350.4s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.317s, learning 0.162s)
               Value function loss: 216091.7594
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 3613.08
               Mean episode length: 68.70
                  Mean reward/step: 56.42
       Mean episode length/episode: 7.23
            Mean episode successes: 4.3252
Mean episode consecutive_successes: 15.7752
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 8.48s
                        Total time: 30943.10s
                               ETA: 979289.0s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.652s, learning 0.197s)
               Value function loss: 218998.2875
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 4283.74
               Mean episode length: 71.57
                  Mean reward/step: 55.81
       Mean episode length/episode: 7.32
            Mean episode successes: 4.5020
Mean episode consecutive_successes: 15.7387
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 8.85s
                        Total time: 30951.95s
                               ETA: 979239.2s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.422s, learning 0.171s)
               Value function loss: 180718.5004
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 3744.84
               Mean episode length: 69.28
                  Mean reward/step: 55.47
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3076
Mean episode consecutive_successes: 15.7345
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 8.59s
                        Total time: 30960.54s
                               ETA: 979181.4s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.633s, learning 0.159s)
               Value function loss: 168200.8145
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: 3400.05
               Mean episode length: 70.08
                  Mean reward/step: 53.36
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2432
Mean episode consecutive_successes: 15.7153
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 8.79s
                        Total time: 30969.33s
                               ETA: 979129.9s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.210s, learning 0.158s)
               Value function loss: 170046.4695
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 4165.15
               Mean episode length: 70.70
                  Mean reward/step: 54.80
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2212
Mean episode consecutive_successes: 15.6857
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 8.37s
                        Total time: 30977.70s
                               ETA: 979065.0s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.379s, learning 0.306s)
               Value function loss: 177692.7590
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: 4249.48
               Mean episode length: 71.42
                  Mean reward/step: 55.99
       Mean episode length/episode: 7.33
            Mean episode successes: 4.3179
Mean episode consecutive_successes: 15.6236
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 8.68s
                        Total time: 30986.39s
                               ETA: 979010.2s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.327s, learning 0.179s)
               Value function loss: 173484.0566
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 3867.89
               Mean episode length: 70.26
                  Mean reward/step: 56.94
       Mean episode length/episode: 7.27
            Mean episode successes: 4.5708
Mean episode consecutive_successes: 15.5650
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 8.51s
                        Total time: 30994.89s
                               ETA: 978949.8s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.615s, learning 0.165s)
               Value function loss: 163541.9555
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 4130.68
               Mean episode length: 71.36
                  Mean reward/step: 58.37
       Mean episode length/episode: 7.38
            Mean episode successes: 4.7046
Mean episode consecutive_successes: 15.7294
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 8.78s
                        Total time: 31003.67s
                               ETA: 978898.0s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.551s, learning 0.158s)
               Value function loss: 178789.5949
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 3929.31
               Mean episode length: 68.99
                  Mean reward/step: 60.58
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6865
Mean episode consecutive_successes: 15.7787
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 8.71s
                        Total time: 31012.38s
                               ETA: 978844.1s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.520s, learning 0.208s)
               Value function loss: 197241.1168
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 4148.53
               Mean episode length: 70.10
                  Mean reward/step: 59.56
       Mean episode length/episode: 7.25
            Mean episode successes: 4.7026
Mean episode consecutive_successes: 15.7707
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 8.73s
                        Total time: 31021.11s
                               ETA: 978790.7s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.398s, learning 0.157s)
               Value function loss: 186212.9188
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 4882.82
               Mean episode length: 73.03
                  Mean reward/step: 58.23
       Mean episode length/episode: 7.32
            Mean episode successes: 4.7222
Mean episode consecutive_successes: 15.8971
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 8.56s
                        Total time: 31029.66s
                               ETA: 978732.0s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.756s, learning 0.161s)
               Value function loss: 183742.4105
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 4248.62
               Mean episode length: 70.26
                  Mean reward/step: 60.35
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6987
Mean episode consecutive_successes: 15.9516
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 8.92s
                        Total time: 31038.58s
                               ETA: 978684.6s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.500s, learning 0.157s)
               Value function loss: 174892.1434
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 3981.22
               Mean episode length: 70.86
                  Mean reward/step: 57.75
       Mean episode length/episode: 7.23
            Mean episode successes: 4.5332
Mean episode consecutive_successes: 15.9914
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 8.66s
                        Total time: 31047.24s
                               ETA: 978629.2s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.802s, learning 0.195s)
               Value function loss: 176333.3094
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 4141.56
               Mean episode length: 70.76
                  Mean reward/step: 57.78
       Mean episode length/episode: 7.24
            Mean episode successes: 4.4980
Mean episode consecutive_successes: 15.9776
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 9.00s
                        Total time: 31056.24s
                               ETA: 978584.4s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.407s, learning 0.219s)
               Value function loss: 171798.4879
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: 4113.24
               Mean episode length: 69.25
                  Mean reward/step: 55.42
       Mean episode length/episode: 7.33
            Mean episode successes: 4.3940
Mean episode consecutive_successes: 16.0892
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 8.63s
                        Total time: 31064.86s
                               ETA: 978528.0s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.644s, learning 0.164s)
               Value function loss: 178135.4020
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 4000.89
               Mean episode length: 70.34
                  Mean reward/step: 53.32
       Mean episode length/episode: 7.22
            Mean episode successes: 4.1548
Mean episode consecutive_successes: 16.0389
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 8.81s
                        Total time: 31073.67s
                               ETA: 978477.3s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.469s, learning 0.172s)
               Value function loss: 186777.3047
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: 4526.11
               Mean episode length: 69.10
                  Mean reward/step: 54.68
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1328
Mean episode consecutive_successes: 16.1562
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 8.64s
                        Total time: 31082.31s
                               ETA: 978421.5s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.177s, learning 0.168s)
               Value function loss: 181340.8406
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 3130.17
               Mean episode length: 70.61
                  Mean reward/step: 56.07
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3740
Mean episode consecutive_successes: 15.9401
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 8.35s
                        Total time: 31090.66s
                               ETA: 978356.3s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.434s, learning 0.174s)
               Value function loss: 182614.2973
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 3510.09
               Mean episode length: 71.24
                  Mean reward/step: 57.60
       Mean episode length/episode: 7.21
            Mean episode successes: 4.4204
Mean episode consecutive_successes: 15.8448
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 8.61s
                        Total time: 31099.26s
                               ETA: 978299.4s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.776s, learning 0.171s)
               Value function loss: 208037.6566
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 3669.73
               Mean episode length: 69.13
                  Mean reward/step: 54.88
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2598
Mean episode consecutive_successes: 15.9301
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 8.95s
                        Total time: 31108.21s
                               ETA: 978253.3s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.175s)
               Value function loss: 248871.3730
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 3439.63
               Mean episode length: 70.12
                  Mean reward/step: 56.38
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4634
Mean episode consecutive_successes: 15.8447
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 8.86s
                        Total time: 31117.07s
                               ETA: 978204.6s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.318s, learning 0.225s)
               Value function loss: 230265.1297
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 3445.83
               Mean episode length: 71.18
                  Mean reward/step: 57.40
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2139
Mean episode consecutive_successes: 15.8921
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 8.54s
                        Total time: 31125.62s
                               ETA: 978145.7s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.639s, learning 0.193s)
               Value function loss: 193214.2754
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: 4086.50
               Mean episode length: 71.38
                  Mean reward/step: 55.56
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2256
Mean episode consecutive_successes: 15.8586
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 8.83s
                        Total time: 31134.45s
                               ETA: 978096.1s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.363s, learning 0.310s)
               Value function loss: 174015.4832
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 3282.63
               Mean episode length: 66.96
                  Mean reward/step: 53.79
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3306
Mean episode consecutive_successes: 15.6711
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 8.67s
                        Total time: 31143.12s
                               ETA: 978041.4s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.576s, learning 0.163s)
               Value function loss: 167420.1523
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 3995.53
               Mean episode length: 70.03
                  Mean reward/step: 53.32
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3022
Mean episode consecutive_successes: 15.6352
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 8.74s
                        Total time: 31151.86s
                               ETA: 977988.8s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.500s, learning 0.173s)
               Value function loss: 154967.8629
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: 4091.67
               Mean episode length: 70.64
                  Mean reward/step: 51.73
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0723
Mean episode consecutive_successes: 15.6815
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 8.67s
                        Total time: 31160.53s
                               ETA: 977934.2s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.572s, learning 0.175s)
               Value function loss: 164413.2223
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 4406.12
               Mean episode length: 71.22
                  Mean reward/step: 55.00
       Mean episode length/episode: 7.34
            Mean episode successes: 4.1299
Mean episode consecutive_successes: 15.7598
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 8.75s
                        Total time: 31169.28s
                               ETA: 977882.0s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.641s, learning 0.167s)
               Value function loss: 170147.5805
                    Surrogate loss: 0.0036
             Mean action noise std: 0.72
                       Mean reward: 3941.54
               Mean episode length: 71.42
                  Mean reward/step: 56.55
       Mean episode length/episode: 7.31
            Mean episode successes: 4.1802
Mean episode consecutive_successes: 15.7668
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 8.81s
                        Total time: 31178.09s
                               ETA: 977831.7s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.515s, learning 0.327s)
               Value function loss: 174082.0980
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: 3091.12
               Mean episode length: 68.63
                  Mean reward/step: 57.19
       Mean episode length/episode: 7.25
            Mean episode successes: 4.5220
Mean episode consecutive_successes: 15.5342
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 8.84s
                        Total time: 31186.93s
                               ETA: 977782.5s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.578s, learning 0.220s)
               Value function loss: 186302.7066
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 3875.54
               Mean episode length: 70.40
                  Mean reward/step: 59.96
       Mean episode length/episode: 7.34
            Mean episode successes: 4.7661
Mean episode consecutive_successes: 15.5874
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 8.80s
                        Total time: 31195.73s
                               ETA: 977731.9s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.802s, learning 0.160s)
               Value function loss: 194219.4055
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 4061.41
               Mean episode length: 70.95
                  Mean reward/step: 60.75
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6357
Mean episode consecutive_successes: 15.7000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 8.96s
                        Total time: 31204.69s
                               ETA: 977686.5s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1763 steps/s (collection: 9.051s, learning 0.242s)
               Value function loss: 173294.5332
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 3226.31
               Mean episode length: 71.38
                  Mean reward/step: 56.55
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4502
Mean episode consecutive_successes: 15.7790
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 9.29s
                        Total time: 31213.98s
                               ETA: 977651.5s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.701s, learning 0.274s)
               Value function loss: 209691.5340
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 4489.81
               Mean episode length: 71.39
                  Mean reward/step: 56.46
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3257
Mean episode consecutive_successes: 15.8891
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 8.97s
                        Total time: 31222.96s
                               ETA: 977606.5s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.679s, learning 0.252s)
               Value function loss: 171134.3547
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 4510.28
               Mean episode length: 72.13
                  Mean reward/step: 53.69
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1860
Mean episode consecutive_successes: 15.9421
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 8.93s
                        Total time: 31231.89s
                               ETA: 977560.2s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.653s, learning 0.158s)
               Value function loss: 188625.3180
                    Surrogate loss: -0.0031
             Mean action noise std: 0.72
                       Mean reward: 3559.45
               Mean episode length: 69.03
                  Mean reward/step: 52.96
       Mean episode length/episode: 7.19
            Mean episode successes: 4.0220
Mean episode consecutive_successes: 15.8536
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 8.81s
                        Total time: 31240.70s
                               ETA: 977510.1s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.315s, learning 0.205s)
               Value function loss: 171305.3832
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 3761.24
               Mean episode length: 70.83
                  Mean reward/step: 54.38
       Mean episode length/episode: 7.35
            Mean episode successes: 4.1196
Mean episode consecutive_successes: 15.8235
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 8.52s
                        Total time: 31249.22s
                               ETA: 977451.0s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.667s, learning 0.174s)
               Value function loss: 179367.5477
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 4560.89
               Mean episode length: 70.51
                  Mean reward/step: 54.13
       Mean episode length/episode: 7.31
            Mean episode successes: 4.1401
Mean episode consecutive_successes: 15.8644
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 8.84s
                        Total time: 31258.06s
                               ETA: 977402.0s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1817 steps/s (collection: 8.778s, learning 0.235s)
               Value function loss: 162173.7730
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 3575.14
               Mean episode length: 69.33
                  Mean reward/step: 53.06
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2119
Mean episode consecutive_successes: 15.7310
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 9.01s
                        Total time: 31267.07s
                               ETA: 977358.3s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.309s, learning 0.165s)
               Value function loss: 178797.1668
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 3259.88
               Mean episode length: 69.60
                  Mean reward/step: 53.47
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2788
Mean episode consecutive_successes: 15.5877
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 8.47s
                        Total time: 31275.55s
                               ETA: 977297.9s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.315s, learning 0.183s)
               Value function loss: 170330.4703
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 3888.82
               Mean episode length: 71.24
                  Mean reward/step: 56.32
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4390
Mean episode consecutive_successes: 15.5307
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 8.50s
                        Total time: 31284.05s
                               ETA: 977238.2s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.424s, learning 0.159s)
               Value function loss: 176872.7527
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 4373.16
               Mean episode length: 70.18
                  Mean reward/step: 53.58
       Mean episode length/episode: 7.19
            Mean episode successes: 4.0073
Mean episode consecutive_successes: 15.5862
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 8.58s
                        Total time: 31292.63s
                               ETA: 977181.2s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.471s, learning 0.180s)
               Value function loss: 175048.7313
                    Surrogate loss: 0.0267
             Mean action noise std: 0.72
                       Mean reward: 3673.85
               Mean episode length: 71.21
                  Mean reward/step: 54.50
       Mean episode length/episode: 7.38
            Mean episode successes: 4.2612
Mean episode consecutive_successes: 15.5250
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 8.65s
                        Total time: 31301.28s
                               ETA: 977126.3s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.421s, learning 0.171s)
               Value function loss: 167705.2449
                    Surrogate loss: 0.0043
             Mean action noise std: 0.72
                       Mean reward: 3449.78
               Mean episode length: 69.58
                  Mean reward/step: 57.39
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5034
Mean episode consecutive_successes: 15.4390
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 8.59s
                        Total time: 31309.87s
                               ETA: 977069.7s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.469s, learning 0.293s)
               Value function loss: 181202.8023
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 3956.62
               Mean episode length: 70.72
                  Mean reward/step: 57.11
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2974
Mean episode consecutive_successes: 15.5352
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 8.76s
                        Total time: 31318.63s
                               ETA: 977018.4s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.454s, learning 0.162s)
               Value function loss: 181969.6672
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 3612.05
               Mean episode length: 69.51
                  Mean reward/step: 55.35
       Mean episode length/episode: 7.28
            Mean episode successes: 4.3853
Mean episode consecutive_successes: 15.4398
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 8.62s
                        Total time: 31327.25s
                               ETA: 976962.5s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.368s, learning 0.203s)
               Value function loss: 207195.5879
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 4162.83
               Mean episode length: 70.01
                  Mean reward/step: 57.11
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6235
Mean episode consecutive_successes: 15.4286
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 8.57s
                        Total time: 31335.82s
                               ETA: 976905.3s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.648s, learning 0.174s)
               Value function loss: 215189.6063
                    Surrogate loss: 0.0044
             Mean action noise std: 0.72
                       Mean reward: 3857.44
               Mean episode length: 69.12
                  Mean reward/step: 56.78
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4736
Mean episode consecutive_successes: 15.5210
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 8.82s
                        Total time: 31344.64s
                               ETA: 976855.9s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1814 steps/s (collection: 8.684s, learning 0.344s)
               Value function loss: 180682.6344
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 4101.28
               Mean episode length: 70.81
                  Mean reward/step: 56.43
       Mean episode length/episode: 7.33
            Mean episode successes: 4.4756
Mean episode consecutive_successes: 15.5945
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 9.03s
                        Total time: 31353.67s
                               ETA: 976813.0s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.529s, learning 0.179s)
               Value function loss: 173606.1668
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 3811.28
               Mean episode length: 71.02
                  Mean reward/step: 56.75
       Mean episode length/episode: 7.31
            Mean episode successes: 4.5107
Mean episode consecutive_successes: 15.6189
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 8.71s
                        Total time: 31362.38s
                               ETA: 976760.1s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.432s, learning 0.170s)
               Value function loss: 188598.4324
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3906.82
               Mean episode length: 71.81
                  Mean reward/step: 57.71
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3945
Mean episode consecutive_successes: 15.7033
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 8.60s
                        Total time: 31370.98s
                               ETA: 976704.0s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.757s, learning 0.182s)
               Value function loss: 176412.5281
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 3389.65
               Mean episode length: 68.74
                  Mean reward/step: 54.79
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3955
Mean episode consecutive_successes: 15.6239
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 8.94s
                        Total time: 31379.92s
                               ETA: 976658.4s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.644s, learning 0.259s)
               Value function loss: 199541.1766
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 4097.35
               Mean episode length: 71.92
                  Mean reward/step: 58.08
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4521
Mean episode consecutive_successes: 15.6383
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 8.90s
                        Total time: 31388.82s
                               ETA: 976611.7s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.553s, learning 0.165s)
               Value function loss: 209822.7027
                    Surrogate loss: -0.0146
             Mean action noise std: 0.72
                       Mean reward: 4151.36
               Mean episode length: 71.60
                  Mean reward/step: 56.84
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4868
Mean episode consecutive_successes: 15.7167
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 8.72s
                        Total time: 31397.54s
                               ETA: 976559.2s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1551 steps/s (collection: 10.388s, learning 0.170s)
               Value function loss: 244194.9859
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: 3982.93
               Mean episode length: 69.90
                  Mean reward/step: 56.26
       Mean episode length/episode: 7.23
            Mean episode successes: 4.3330
Mean episode consecutive_successes: 15.7579
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 10.56s
                        Total time: 31408.10s
                               ETA: 976564.0s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.257s, learning 0.219s)
               Value function loss: 215910.9070
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 3702.54
               Mean episode length: 69.23
                  Mean reward/step: 55.59
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2998
Mean episode consecutive_successes: 15.8157
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 16.48s
                        Total time: 31424.57s
                               ETA: 976752.8s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.160s, learning 0.165s)
               Value function loss: 213266.6367
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 4520.19
               Mean episode length: 72.37
                  Mean reward/step: 55.69
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3130
Mean episode consecutive_successes: 15.7668
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 16.33s
                        Total time: 31440.90s
                               ETA: 976936.7s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.860s, learning 0.187s)
               Value function loss: 190810.1910
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: 3896.30
               Mean episode length: 71.12
                  Mean reward/step: 53.61
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2153
Mean episode consecutive_successes: 15.7821
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 17.05s
                        Total time: 31457.95s
                               ETA: 977142.9s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.435s, learning 0.160s)
               Value function loss: 187579.5941
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 3867.18
               Mean episode length: 68.84
                  Mean reward/step: 55.04
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2847
Mean episode consecutive_successes: 15.7770
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 16.60s
                        Total time: 31474.54s
                               ETA: 977334.9s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.437s, learning 0.188s)
               Value function loss: 180115.8891
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 4110.84
               Mean episode length: 72.08
                  Mean reward/step: 56.18
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2637
Mean episode consecutive_successes: 15.7268
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 16.62s
                        Total time: 31491.17s
                               ETA: 977527.8s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.741s, learning 0.301s)
               Value function loss: 178500.5566
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: 4087.93
               Mean episode length: 71.06
                  Mean reward/step: 53.58
       Mean episode length/episode: 7.25
            Mean episode successes: 4.0054
Mean episode consecutive_successes: 15.8611
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 17.04s
                        Total time: 31508.21s
                               ETA: 977733.4s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.888s, learning 0.161s)
               Value function loss: 186245.1973
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3605.71
               Mean episode length: 70.57
                  Mean reward/step: 52.68
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2314
Mean episode consecutive_successes: 15.6833
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 17.05s
                        Total time: 31525.26s
                               ETA: 977939.1s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.291s, learning 0.162s)
               Value function loss: 180698.9711
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 3806.02
               Mean episode length: 70.98
                  Mean reward/step: 54.74
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2227
Mean episode consecutive_successes: 15.6958
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 16.45s
                        Total time: 31541.71s
                               ETA: 978126.2s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.372s, learning 0.303s)
               Value function loss: 191540.2047
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 3461.36
               Mean episode length: 69.65
                  Mean reward/step: 52.71
       Mean episode length/episode: 7.20
            Mean episode successes: 4.0024
Mean episode consecutive_successes: 15.5798
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 16.67s
                        Total time: 31558.38s
                               ETA: 978320.0s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.845s, learning 0.238s)
               Value function loss: 198241.3328
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 3669.93
               Mean episode length: 69.39
                  Mean reward/step: 52.13
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1113
Mean episode consecutive_successes: 15.5077
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 17.08s
                        Total time: 31575.47s
                               ETA: 978526.4s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.684s, learning 0.157s)
               Value function loss: 216802.5562
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 3802.07
               Mean episode length: 72.02
                  Mean reward/step: 54.47
       Mean episode length/episode: 7.32
            Mean episode successes: 4.1445
Mean episode consecutive_successes: 15.5277
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 16.84s
                        Total time: 31592.31s
                               ETA: 978725.1s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.138s, learning 0.155s)
               Value function loss: 202545.4352
                    Surrogate loss: 0.0324
             Mean action noise std: 0.72
                       Mean reward: 3385.15
               Mean episode length: 70.02
                  Mean reward/step: 56.56
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2236
Mean episode consecutive_successes: 15.4892
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 16.29s
                        Total time: 31608.60s
                               ETA: 978906.7s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.454s, learning 0.159s)
               Value function loss: 191186.5035
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 3470.92
               Mean episode length: 70.71
                  Mean reward/step: 56.13
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2842
Mean episode consecutive_successes: 15.4038
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 16.61s
                        Total time: 31625.21s
                               ETA: 979098.1s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.474s, learning 0.182s)
               Value function loss: 184462.4473
                    Surrogate loss: -0.0118
             Mean action noise std: 0.72
                       Mean reward: 3893.80
               Mean episode length: 70.75
                  Mean reward/step: 57.31
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5820
Mean episode consecutive_successes: 15.3116
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 16.66s
                        Total time: 31641.87s
                               ETA: 979290.6s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.548s, learning 0.160s)
               Value function loss: 198021.5078
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 4323.73
               Mean episode length: 70.75
                  Mean reward/step: 56.48
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4497
Mean episode consecutive_successes: 15.4381
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 16.71s
                        Total time: 31658.58s
                               ETA: 979484.7s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.721s, learning 0.162s)
               Value function loss: 177201.4066
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 3932.88
               Mean episode length: 69.64
                  Mean reward/step: 55.76
       Mean episode length/episode: 7.33
            Mean episode successes: 4.4927
Mean episode consecutive_successes: 15.4781
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 16.88s
                        Total time: 31675.46s
                               ETA: 979684.0s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.630s, learning 0.158s)
               Value function loss: 192261.9707
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 4913.14
               Mean episode length: 72.75
                  Mean reward/step: 53.81
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2949
Mean episode consecutive_successes: 15.5301
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 16.79s
                        Total time: 31692.25s
                               ETA: 979880.3s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.713s, learning 0.277s)
               Value function loss: 203865.8102
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 4299.34
               Mean episode length: 70.83
                  Mean reward/step: 55.75
       Mean episode length/episode: 7.28
            Mean episode successes: 4.3862
Mean episode consecutive_successes: 15.5391
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 16.99s
                        Total time: 31709.24s
                               ETA: 980082.6s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.414s, learning 0.158s)
               Value function loss: 182958.6137
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 3332.29
               Mean episode length: 69.47
                  Mean reward/step: 54.20
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2095
Mean episode consecutive_successes: 15.4758
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 16.57s
                        Total time: 31725.81s
                               ETA: 980271.9s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.682s, learning 0.196s)
               Value function loss: 184000.8031
                    Surrogate loss: -0.0141
             Mean action noise std: 0.72
                       Mean reward: 3778.60
               Mean episode length: 70.83
                  Mean reward/step: 56.01
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2036
Mean episode consecutive_successes: 15.4677
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 16.88s
                        Total time: 31742.69s
                               ETA: 980470.6s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.485s, learning 0.187s)
               Value function loss: 179228.0207
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 3289.88
               Mean episode length: 70.85
                  Mean reward/step: 57.53
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4458
Mean episode consecutive_successes: 15.4284
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 16.67s
                        Total time: 31759.36s
                               ETA: 980662.7s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.563s, learning 0.158s)
               Value function loss: 193820.7719
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 4360.91
               Mean episode length: 70.78
                  Mean reward/step: 58.96
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5547
Mean episode consecutive_successes: 15.5359
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 16.72s
                        Total time: 31776.08s
                               ETA: 980856.2s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.147s, learning 0.161s)
               Value function loss: 223907.0277
                    Surrogate loss: -0.0086
             Mean action noise std: 0.72
                       Mean reward: 3901.40
               Mean episode length: 70.58
                  Mean reward/step: 58.25
       Mean episode length/episode: 7.24
            Mean episode successes: 4.6562
Mean episode consecutive_successes: 15.4921
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 16.31s
                        Total time: 31792.39s
                               ETA: 981036.8s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.360s, learning 0.160s)
               Value function loss: 175942.5152
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 3966.70
               Mean episode length: 71.39
                  Mean reward/step: 57.89
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6455
Mean episode consecutive_successes: 15.5326
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 16.52s
                        Total time: 31808.91s
                               ETA: 981223.8s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.275s, learning 0.303s)
               Value function loss: 179054.4043
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 4095.44
               Mean episode length: 69.65
                  Mean reward/step: 56.68
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5376
Mean episode consecutive_successes: 15.6673
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 16.58s
                        Total time: 31825.49s
                               ETA: 981412.5s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.066s, learning 0.336s)
               Value function loss: 175633.4246
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 3742.84
               Mean episode length: 70.53
                  Mean reward/step: 55.95
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5410
Mean episode consecutive_successes: 15.6734
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 16.40s
                        Total time: 31841.89s
                               ETA: 981595.7s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.559s, learning 0.284s)
               Value function loss: 187253.2453
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 4464.56
               Mean episode length: 71.58
                  Mean reward/step: 55.65
       Mean episode length/episode: 7.33
            Mean episode successes: 4.5361
Mean episode consecutive_successes: 15.8771
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 16.84s
                        Total time: 31858.73s
                               ETA: 981792.3s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.567s, learning 0.154s)
               Value function loss: 177813.3898
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 4179.00
               Mean episode length: 71.64
                  Mean reward/step: 55.23
       Mean episode length/episode: 7.19
            Mean episode successes: 4.3218
Mean episode consecutive_successes: 15.8588
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 16.72s
                        Total time: 31875.45s
                               ETA: 981985.0s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.267s, learning 0.161s)
               Value function loss: 168713.7086
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 3818.92
               Mean episode length: 70.77
                  Mean reward/step: 54.36
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1929
Mean episode consecutive_successes: 15.8571
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 16.43s
                        Total time: 31891.88s
                               ETA: 982168.5s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.286s, learning 0.160s)
               Value function loss: 169043.5391
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 4348.21
               Mean episode length: 69.59
                  Mean reward/step: 55.67
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2656
Mean episode consecutive_successes: 15.8915
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 16.45s
                        Total time: 31908.33s
                               ETA: 982352.5s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.365s, learning 0.165s)
               Value function loss: 203486.1383
                    Surrogate loss: 0.0114
             Mean action noise std: 0.72
                       Mean reward: 3890.91
               Mean episode length: 71.02
                  Mean reward/step: 53.99
       Mean episode length/episode: 7.25
            Mean episode successes: 4.0298
Mean episode consecutive_successes: 15.9342
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 16.53s
                        Total time: 31924.86s
                               ETA: 982538.9s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.942s, learning 0.160s)
               Value function loss: 190359.8227
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 3822.58
               Mean episode length: 69.31
                  Mean reward/step: 55.64
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1196
Mean episode consecutive_successes: 15.8668
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 16.10s
                        Total time: 31940.96s
                               ETA: 982712.1s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.573s, learning 0.200s)
               Value function loss: 201769.3672
                    Surrogate loss: -0.0074
             Mean action noise std: 0.72
                       Mean reward: 4231.03
               Mean episode length: 71.26
                  Mean reward/step: 56.92
       Mean episode length/episode: 7.24
            Mean episode successes: 4.3027
Mean episode consecutive_successes: 15.7326
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 16.77s
                        Total time: 31957.73s
                               ETA: 982905.8s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.738s, learning 0.187s)
               Value function loss: 193263.5230
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 4045.85
               Mean episode length: 70.20
                  Mean reward/step: 55.00
       Mean episode length/episode: 7.23
            Mean episode successes: 4.4209
Mean episode consecutive_successes: 15.6443
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 16.93s
                        Total time: 31974.66s
                               ETA: 983104.0s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.864s, learning 0.229s)
               Value function loss: 200657.7891
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 3502.81
               Mean episode length: 70.16
                  Mean reward/step: 53.68
       Mean episode length/episode: 7.36
            Mean episode successes: 4.3379
Mean episode consecutive_successes: 15.6880
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 16.09s
                        Total time: 31990.75s
                               ETA: 983276.5s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.494s, learning 0.229s)
               Value function loss: 211726.8223
                    Surrogate loss: 0.0059
             Mean action noise std: 0.72
                       Mean reward: 3847.68
               Mean episode length: 68.86
                  Mean reward/step: 55.74
       Mean episode length/episode: 7.33
            Mean episode successes: 4.3857
Mean episode consecutive_successes: 15.7137
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 16.72s
                        Total time: 32007.47s
                               ETA: 983468.2s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 945 steps/s (collection: 17.153s, learning 0.183s)
               Value function loss: 191820.8883
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 3658.56
               Mean episode length: 71.23
                  Mean reward/step: 56.39
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4307
Mean episode consecutive_successes: 15.6819
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 17.34s
                        Total time: 32024.81s
                               ETA: 983678.6s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.706s, learning 0.156s)
               Value function loss: 202997.0645
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 4529.42
               Mean episode length: 70.11
                  Mean reward/step: 56.02
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2788
Mean episode consecutive_successes: 15.7489
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 11.86s
                        Total time: 32036.67s
                               ETA: 983720.8s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.412s, learning 0.224s)
               Value function loss: 186357.0988
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 4237.34
               Mean episode length: 71.10
                  Mean reward/step: 56.15
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4175
Mean episode consecutive_successes: 15.7301
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 8.64s
                        Total time: 32045.31s
                               ETA: 983663.9s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.498s, learning 0.350s)
               Value function loss: 236696.4273
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 4085.26
               Mean episode length: 69.00
                  Mean reward/step: 57.96
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3892
Mean episode consecutive_successes: 15.7377
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 8.85s
                        Total time: 32054.15s
                               ETA: 983613.6s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.416s, learning 0.160s)
               Value function loss: 220277.3852
                    Surrogate loss: 0.0126
             Mean action noise std: 0.72
                       Mean reward: 4189.13
               Mean episode length: 70.58
                  Mean reward/step: 54.67
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2329
Mean episode consecutive_successes: 15.7649
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 8.58s
                        Total time: 32062.73s
                               ETA: 983555.0s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.513s, learning 0.181s)
               Value function loss: 195971.6809
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 3668.65
               Mean episode length: 70.73
                  Mean reward/step: 53.74
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2061
Mean episode consecutive_successes: 15.7153
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 8.69s
                        Total time: 32071.42s
                               ETA: 983500.0s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.033s, learning 0.223s)
               Value function loss: 204184.0016
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 3123.23
               Mean episode length: 71.03
                  Mean reward/step: 55.69
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1968
Mean episode consecutive_successes: 15.7075
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 8.26s
                        Total time: 32079.68s
                               ETA: 983431.6s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.627s, learning 0.178s)
               Value function loss: 181285.6500
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 4022.51
               Mean episode length: 69.38
                  Mean reward/step: 57.16
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3999
Mean episode consecutive_successes: 15.6391
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 8.80s
                        Total time: 32088.49s
                               ETA: 983380.1s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.407s, learning 0.186s)
               Value function loss: 184003.6324
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 3579.39
               Mean episode length: 69.38
                  Mean reward/step: 57.93
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4766
Mean episode consecutive_successes: 15.6368
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 8.59s
                        Total time: 32097.08s
                               ETA: 983322.1s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.713s, learning 0.207s)
               Value function loss: 189844.3406
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3938.28
               Mean episode length: 69.34
                  Mean reward/step: 59.98
       Mean episode length/episode: 7.36
            Mean episode successes: 4.8213
Mean episode consecutive_successes: 15.7607
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 8.92s
                        Total time: 32106.00s
                               ETA: 983274.2s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.431s, learning 0.157s)
               Value function loss: 210349.4324
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 4377.10
               Mean episode length: 71.62
                  Mean reward/step: 59.78
       Mean episode length/episode: 7.22
            Mean episode successes: 4.5615
Mean episode consecutive_successes: 15.8427
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 8.59s
                        Total time: 32114.59s
                               ETA: 983216.0s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.363s, learning 0.158s)
               Value function loss: 186860.5867
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 4035.10
               Mean episode length: 69.87
                  Mean reward/step: 58.02
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4834
Mean episode consecutive_successes: 15.8778
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 8.52s
                        Total time: 32123.11s
                               ETA: 983155.9s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.144s, learning 0.160s)
               Value function loss: 197934.6066
                    Surrogate loss: -0.0095
             Mean action noise std: 0.72
                       Mean reward: 4366.81
               Mean episode length: 71.36
                  Mean reward/step: 56.68
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5112
Mean episode consecutive_successes: 15.9173
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 8.30s
                        Total time: 32131.41s
                               ETA: 983089.2s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.365s, learning 0.160s)
               Value function loss: 175379.8555
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 3995.02
               Mean episode length: 69.85
                  Mean reward/step: 53.81
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5024
Mean episode consecutive_successes: 15.8677
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 8.52s
                        Total time: 32139.94s
                               ETA: 983029.3s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.281s, learning 0.160s)
               Value function loss: 222245.3824
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 2716.93
               Mean episode length: 70.56
                  Mean reward/step: 55.52
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3877
Mean episode consecutive_successes: 15.7779
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 8.44s
                        Total time: 32148.38s
                               ETA: 982966.8s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.550s, learning 0.159s)
               Value function loss: 205840.4449
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 4360.50
               Mean episode length: 69.95
                  Mean reward/step: 57.60
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3359
Mean episode consecutive_successes: 15.9063
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 8.71s
                        Total time: 32157.09s
                               ETA: 982912.6s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.473s, learning 0.244s)
               Value function loss: 182205.8559
                    Surrogate loss: -0.0064
             Mean action noise std: 0.72
                       Mean reward: 4265.88
               Mean episode length: 70.03
                  Mean reward/step: 56.11
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2036
Mean episode consecutive_successes: 15.9061
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 8.72s
                        Total time: 32165.80s
                               ETA: 982858.6s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.322s, learning 0.281s)
               Value function loss: 196569.4234
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 3048.34
               Mean episode length: 68.77
                  Mean reward/step: 55.40
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4204
Mean episode consecutive_successes: 15.7698
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 8.60s
                        Total time: 32174.41s
                               ETA: 982801.2s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.443s, learning 0.223s)
               Value function loss: 258432.9480
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 3545.41
               Mean episode length: 68.84
                  Mean reward/step: 58.35
       Mean episode length/episode: 7.39
            Mean episode successes: 4.8857
Mean episode consecutive_successes: 15.7038
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 8.67s
                        Total time: 32183.07s
                               ETA: 982745.8s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.510s, learning 0.169s)
               Value function loss: 234863.4070
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 4504.21
               Mean episode length: 69.99
                  Mean reward/step: 59.94
       Mean episode length/episode: 7.19
            Mean episode successes: 4.5874
Mean episode consecutive_successes: 15.8461
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 8.68s
                        Total time: 32191.75s
                               ETA: 982690.7s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.738s, learning 0.164s)
               Value function loss: 185034.7910
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 3983.46
               Mean episode length: 70.45
                  Mean reward/step: 60.93
       Mean episode length/episode: 7.21
            Mean episode successes: 4.6133
Mean episode consecutive_successes: 15.8428
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 8.90s
                        Total time: 32200.65s
                               ETA: 982642.5s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.668s, learning 0.203s)
               Value function loss: 186415.6906
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 4187.27
               Mean episode length: 71.48
                  Mean reward/step: 59.33
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8164
Mean episode consecutive_successes: 15.8586
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 8.87s
                        Total time: 32209.52s
                               ETA: 982593.5s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.223s, learning 0.185s)
               Value function loss: 184672.0387
                    Surrogate loss: -0.0159
             Mean action noise std: 0.72
                       Mean reward: 4596.90
               Mean episode length: 71.50
                  Mean reward/step: 58.67
       Mean episode length/episode: 7.31
            Mean episode successes: 4.7803
Mean episode consecutive_successes: 16.0420
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 8.41s
                        Total time: 32217.93s
                               ETA: 982530.3s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.441s, learning 0.188s)
               Value function loss: 198510.5750
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 3750.95
               Mean episode length: 70.65
                  Mean reward/step: 59.13
       Mean episode length/episode: 7.22
            Mean episode successes: 4.7500
Mean episode consecutive_successes: 15.9997
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 8.63s
                        Total time: 32226.56s
                               ETA: 982473.8s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.601s, learning 0.196s)
               Value function loss: 201219.9086
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 4186.71
               Mean episode length: 69.97
                  Mean reward/step: 60.02
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6597
Mean episode consecutive_successes: 16.0881
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 8.80s
                        Total time: 32235.36s
                               ETA: 982422.5s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.453s, learning 0.163s)
               Value function loss: 213230.8297
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 4680.59
               Mean episode length: 72.32
                  Mean reward/step: 59.36
       Mean episode length/episode: 7.21
            Mean episode successes: 4.3369
Mean episode consecutive_successes: 16.2762
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 8.62s
                        Total time: 32243.97s
                               ETA: 982365.8s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.440s, learning 0.158s)
               Value function loss: 183921.0836
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 3372.21
               Mean episode length: 69.16
                  Mean reward/step: 56.41
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5171
Mean episode consecutive_successes: 16.1576
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 8.60s
                        Total time: 32252.57s
                               ETA: 982308.5s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.796s, learning 0.160s)
               Value function loss: 198354.7770
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 3643.36
               Mean episode length: 70.46
                  Mean reward/step: 57.10
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6489
Mean episode consecutive_successes: 16.1171
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 8.96s
                        Total time: 32261.53s
                               ETA: 982262.1s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.356s, learning 0.162s)
               Value function loss: 228012.2141
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 4012.56
               Mean episode length: 69.24
                  Mean reward/step: 57.37
       Mean episode length/episode: 7.24
            Mean episode successes: 4.7568
Mean episode consecutive_successes: 16.0750
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 8.52s
                        Total time: 32270.05s
                               ETA: 982202.4s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.200s, learning 0.165s)
               Value function loss: 203796.8125
                    Surrogate loss: -0.0142
             Mean action noise std: 0.72
                       Mean reward: 4548.77
               Mean episode length: 70.78
                  Mean reward/step: 56.61
       Mean episode length/episode: 7.21
            Mean episode successes: 4.4897
Mean episode consecutive_successes: 16.0904
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 8.37s
                        Total time: 32278.41s
                               ETA: 982138.1s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.386s, learning 0.172s)
               Value function loss: 212615.9418
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 4006.68
               Mean episode length: 70.36
                  Mean reward/step: 54.89
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2744
Mean episode consecutive_successes: 16.1429
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 8.56s
                        Total time: 32286.97s
                               ETA: 982079.7s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.367s, learning 0.156s)
               Value function loss: 274057.7656
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 3993.49
               Mean episode length: 70.76
                  Mean reward/step: 54.40
       Mean episode length/episode: 7.32
            Mean episode successes: 4.3125
Mean episode consecutive_successes: 16.1418
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 8.52s
                        Total time: 32295.49s
                               ETA: 982020.3s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1794 steps/s (collection: 8.905s, learning 0.225s)
               Value function loss: 243309.7023
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: 3510.31
               Mean episode length: 68.91
                  Mean reward/step: 54.42
       Mean episode length/episode: 7.23
            Mean episode successes: 4.3481
Mean episode consecutive_successes: 15.9947
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 9.13s
                        Total time: 32304.62s
                               ETA: 981979.3s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.607s, learning 0.162s)
               Value function loss: 228634.9574
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: 4480.42
               Mean episode length: 72.40
                  Mean reward/step: 53.86
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3149
Mean episode consecutive_successes: 15.9898
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 8.77s
                        Total time: 32313.39s
                               ETA: 981927.4s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.331s, learning 0.160s)
               Value function loss: 219519.5055
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 3847.48
               Mean episode length: 70.03
                  Mean reward/step: 53.58
       Mean episode length/episode: 7.24
            Mean episode successes: 4.3096
Mean episode consecutive_successes: 15.8974
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 8.49s
                        Total time: 32321.88s
                               ETA: 981867.1s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.622s, learning 0.198s)
               Value function loss: 225727.8813
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 3689.75
               Mean episode length: 69.06
                  Mean reward/step: 54.08
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2026
Mean episode consecutive_successes: 15.9267
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 8.82s
                        Total time: 32330.70s
                               ETA: 981816.8s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.511s, learning 0.234s)
               Value function loss: 187381.1184
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: 3635.60
               Mean episode length: 69.58
                  Mean reward/step: 56.93
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3335
Mean episode consecutive_successes: 15.8193
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 8.74s
                        Total time: 32339.44s
                               ETA: 981764.3s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.564s, learning 0.157s)
               Value function loss: 188131.3090
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3676.20
               Mean episode length: 71.11
                  Mean reward/step: 57.70
       Mean episode length/episode: 7.38
            Mean episode successes: 4.6304
Mean episode consecutive_successes: 15.7660
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 8.72s
                        Total time: 32348.17s
                               ETA: 981711.1s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.229s, learning 0.200s)
               Value function loss: 196577.4359
                    Surrogate loss: -0.0112
             Mean action noise std: 0.72
                       Mean reward: 4205.63
               Mean episode length: 69.98
                  Mean reward/step: 55.95
       Mean episode length/episode: 7.24
            Mean episode successes: 4.6382
Mean episode consecutive_successes: 15.7282
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 8.43s
                        Total time: 32356.59s
                               ETA: 981649.0s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1770 steps/s (collection: 8.999s, learning 0.256s)
               Value function loss: 214183.8715
                    Surrogate loss: 0.0051
             Mean action noise std: 0.72
                       Mean reward: 3932.60
               Mean episode length: 69.91
                  Mean reward/step: 58.13
       Mean episode length/episode: 7.25
            Mean episode successes: 4.5381
Mean episode consecutive_successes: 15.7431
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 9.25s
                        Total time: 32365.85s
                               ETA: 981612.0s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.374s, learning 0.161s)
               Value function loss: 199668.6293
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 3867.78
               Mean episode length: 68.34
                  Mean reward/step: 57.49
       Mean episode length/episode: 7.26
            Mean episode successes: 4.5171
Mean episode consecutive_successes: 15.7462
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 8.54s
                        Total time: 32374.39s
                               ETA: 981553.2s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.636s, learning 0.180s)
               Value function loss: 221759.8043
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 3695.91
               Mean episode length: 70.98
                  Mean reward/step: 55.99
       Mean episode length/episode: 7.34
            Mean episode successes: 4.7339
Mean episode consecutive_successes: 15.6815
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 8.82s
                        Total time: 32383.20s
                               ETA: 981503.0s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.320s, learning 0.207s)
               Value function loss: 236294.0320
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 4155.06
               Mean episode length: 69.52
                  Mean reward/step: 55.31
       Mean episode length/episode: 7.30
            Mean episode successes: 4.6182
Mean episode consecutive_successes: 15.7447
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 8.53s
                        Total time: 32391.73s
                               ETA: 981444.0s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.665s, learning 0.178s)
               Value function loss: 229929.1023
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 4828.82
               Mean episode length: 70.74
                  Mean reward/step: 55.78
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4131
Mean episode consecutive_successes: 15.9230
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 8.84s
                        Total time: 32400.57s
                               ETA: 981394.6s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.090s, learning 0.170s)
               Value function loss: 201737.3848
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 3742.60
               Mean episode length: 69.44
                  Mean reward/step: 53.55
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2397
Mean episode consecutive_successes: 15.8194
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 8.26s
                        Total time: 32408.83s
                               ETA: 981327.6s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.631s, learning 0.167s)
               Value function loss: 208682.1480
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 3580.86
               Mean episode length: 71.03
                  Mean reward/step: 54.68
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2407
Mean episode consecutive_successes: 15.7941
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 8.80s
                        Total time: 32417.63s
                               ETA: 981276.9s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.401s, learning 0.175s)
               Value function loss: 206204.9211
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 3849.38
               Mean episode length: 68.18
                  Mean reward/step: 55.80
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4565
Mean episode consecutive_successes: 15.6854
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 8.58s
                        Total time: 32426.20s
                               ETA: 981219.6s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.462s, learning 0.162s)
               Value function loss: 231012.0824
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 3856.14
               Mean episode length: 70.48
                  Mean reward/step: 56.65
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2373
Mean episode consecutive_successes: 15.7147
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 8.62s
                        Total time: 32434.83s
                               ETA: 981163.7s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.663s, learning 0.227s)
               Value function loss: 193882.2234
                    Surrogate loss: 0.0053
             Mean action noise std: 0.72
                       Mean reward: 3683.12
               Mean episode length: 70.84
                  Mean reward/step: 57.87
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4092
Mean episode consecutive_successes: 15.6027
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 8.89s
                        Total time: 32443.72s
                               ETA: 981115.9s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.363s, learning 0.163s)
               Value function loss: 202957.5023
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 3837.65
               Mean episode length: 69.27
                  Mean reward/step: 57.78
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5308
Mean episode consecutive_successes: 15.6036
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 8.53s
                        Total time: 32452.24s
                               ETA: 981057.1s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.399s, learning 0.158s)
               Value function loss: 203436.5266
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 3977.25
               Mean episode length: 68.87
                  Mean reward/step: 57.52
       Mean episode length/episode: 7.33
            Mean episode successes: 4.6641
Mean episode consecutive_successes: 15.5788
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 8.56s
                        Total time: 32460.80s
                               ETA: 980999.2s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.179s, learning 0.360s)
               Value function loss: 250459.4273
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 4546.31
               Mean episode length: 70.60
                  Mean reward/step: 57.57
       Mean episode length/episode: 7.22
            Mean episode successes: 4.3706
Mean episode consecutive_successes: 15.7688
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 8.54s
                        Total time: 32469.34s
                               ETA: 980940.9s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.446s, learning 0.201s)
               Value function loss: 256721.2418
                    Surrogate loss: -0.0133
             Mean action noise std: 0.72
                       Mean reward: 4048.23
               Mean episode length: 70.08
                  Mean reward/step: 57.93
       Mean episode length/episode: 7.33
            Mean episode successes: 4.6387
Mean episode consecutive_successes: 15.7889
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 8.65s
                        Total time: 32477.99s
                               ETA: 980885.9s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.547s, learning 0.258s)
               Value function loss: 223795.7930
                    Surrogate loss: -0.0020
             Mean action noise std: 0.72
                       Mean reward: 4576.78
               Mean episode length: 71.28
                  Mean reward/step: 59.83
       Mean episode length/episode: 7.25
            Mean episode successes: 4.5288
Mean episode consecutive_successes: 15.9345
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 8.81s
                        Total time: 32486.79s
                               ETA: 980835.7s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.481s, learning 0.165s)
               Value function loss: 213119.4773
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 3925.94
               Mean episode length: 70.29
                  Mean reward/step: 60.06
       Mean episode length/episode: 7.31
            Mean episode successes: 4.6763
Mean episode consecutive_successes: 15.8505
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 8.65s
                        Total time: 32495.44s
                               ETA: 980780.7s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.388s, learning 0.169s)
               Value function loss: 191561.3312
                    Surrogate loss: 0.0175
             Mean action noise std: 0.72
                       Mean reward: 3682.32
               Mean episode length: 70.02
                  Mean reward/step: 58.55
       Mean episode length/episode: 7.28
            Mean episode successes: 4.7368
Mean episode consecutive_successes: 15.8554
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 8.56s
                        Total time: 32504.00s
                               ETA: 980723.0s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.780s, learning 0.174s)
               Value function loss: 232120.6477
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 4350.05
               Mean episode length: 69.77
                  Mean reward/step: 58.55
       Mean episode length/episode: 7.19
            Mean episode successes: 4.5728
Mean episode consecutive_successes: 15.9090
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 8.95s
                        Total time: 32512.95s
                               ETA: 980677.3s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.420s, learning 0.173s)
               Value function loss: 214667.6527
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 4233.97
               Mean episode length: 70.60
                  Mean reward/step: 56.67
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5410
Mean episode consecutive_successes: 16.0144
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 8.59s
                        Total time: 32521.54s
                               ETA: 980620.7s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.263s, learning 0.169s)
               Value function loss: 206456.5898
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 4469.58
               Mean episode length: 70.87
                  Mean reward/step: 55.58
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4604
Mean episode consecutive_successes: 16.0675
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 8.43s
                        Total time: 32529.97s
                               ETA: 980559.4s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.648s, learning 0.188s)
               Value function loss: 224295.3770
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 3792.60
               Mean episode length: 68.94
                  Mean reward/step: 53.77
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2295
Mean episode consecutive_successes: 16.0599
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 8.84s
                        Total time: 32538.81s
                               ETA: 980510.2s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.367s, learning 0.224s)
               Value function loss: 190576.6164
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 3460.62
               Mean episode length: 70.25
                  Mean reward/step: 53.44
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2939
Mean episode consecutive_successes: 15.9873
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 8.59s
                        Total time: 32547.40s
                               ETA: 980453.7s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.465s, learning 0.167s)
               Value function loss: 206328.5934
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 3859.26
               Mean episode length: 67.32
                  Mean reward/step: 55.16
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4810
Mean episode consecutive_successes: 15.8651
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 8.63s
                        Total time: 32556.03s
                               ETA: 980398.5s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.534s, learning 0.216s)
               Value function loss: 207610.8512
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 3895.22
               Mean episode length: 69.90
                  Mean reward/step: 54.60
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2588
Mean episode consecutive_successes: 15.8843
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 8.75s
                        Total time: 32564.78s
                               ETA: 980346.8s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.674s, learning 0.195s)
               Value function loss: 195986.1723
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 3845.40
               Mean episode length: 69.37
                  Mean reward/step: 53.90
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3320
Mean episode consecutive_successes: 15.7194
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 8.87s
                        Total time: 32573.65s
                               ETA: 980298.8s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.375s, learning 0.160s)
               Value function loss: 219686.8953
                    Surrogate loss: -0.0063
             Mean action noise std: 0.72
                       Mean reward: 3610.01
               Mean episode length: 70.43
                  Mean reward/step: 51.81
       Mean episode length/episode: 7.34
            Mean episode successes: 4.2861
Mean episode consecutive_successes: 15.7353
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 8.53s
                        Total time: 32582.19s
                               ETA: 980240.7s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.212s, learning 0.256s)
               Value function loss: 206192.5645
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 3723.04
               Mean episode length: 70.91
                  Mean reward/step: 52.78
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3081
Mean episode consecutive_successes: 15.6533
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 8.47s
                        Total time: 32590.65s
                               ETA: 980180.6s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.375s, learning 0.157s)
               Value function loss: 203849.7824
                    Surrogate loss: -0.0161
             Mean action noise std: 0.72
                       Mean reward: 3770.69
               Mean episode length: 70.75
                  Mean reward/step: 52.44
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1128
Mean episode consecutive_successes: 15.6176
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 8.53s
                        Total time: 32599.19s
                               ETA: 980122.5s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.261s, learning 0.307s)
               Value function loss: 226453.8102
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 3053.72
               Mean episode length: 68.93
                  Mean reward/step: 52.94
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1426
Mean episode consecutive_successes: 15.5588
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 8.57s
                        Total time: 32607.75s
                               ETA: 980065.5s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.467s, learning 0.161s)
               Value function loss: 221758.2953
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: 3006.34
               Mean episode length: 69.51
                  Mean reward/step: 54.16
       Mean episode length/episode: 7.20
            Mean episode successes: 4.0581
Mean episode consecutive_successes: 15.3831
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 8.63s
                        Total time: 32616.38s
                               ETA: 980010.4s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.337s, learning 0.163s)
               Value function loss: 202479.3578
                    Surrogate loss: -0.0123
             Mean action noise std: 0.72
                       Mean reward: 3583.01
               Mean episode length: 70.62
                  Mean reward/step: 56.05
       Mean episode length/episode: 7.35
            Mean episode successes: 4.3999
Mean episode consecutive_successes: 15.3149
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 8.50s
                        Total time: 32624.88s
                               ETA: 979951.4s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.411s, learning 0.200s)
               Value function loss: 247546.4414
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: 4001.17
               Mean episode length: 71.20
                  Mean reward/step: 56.13
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4780
Mean episode consecutive_successes: 15.3358
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 8.61s
                        Total time: 32633.49s
                               ETA: 979895.8s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1819 steps/s (collection: 8.818s, learning 0.185s)
               Value function loss: 269089.3445
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 4243.78
               Mean episode length: 69.81
                  Mean reward/step: 56.56
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5156
Mean episode consecutive_successes: 15.4133
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 9.00s
                        Total time: 32642.50s
                               ETA: 979852.0s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.556s, learning 0.159s)
               Value function loss: 229959.3453
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 3639.95
               Mean episode length: 68.91
                  Mean reward/step: 55.73
       Mean episode length/episode: 7.20
            Mean episode successes: 4.4067
Mean episode consecutive_successes: 15.4093
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 8.71s
                        Total time: 32651.21s
                               ETA: 979799.6s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.383s, learning 0.160s)
               Value function loss: 213197.1953
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 3793.26
               Mean episode length: 70.77
                  Mean reward/step: 54.69
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3970
Mean episode consecutive_successes: 15.4255
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 8.54s
                        Total time: 32659.75s
                               ETA: 979742.0s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.253s, learning 0.178s)
               Value function loss: 210316.4254
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 4313.00
               Mean episode length: 68.47
                  Mean reward/step: 55.12
       Mean episode length/episode: 7.22
            Mean episode successes: 4.3472
Mean episode consecutive_successes: 15.4228
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 8.43s
                        Total time: 32668.19s
                               ETA: 979681.1s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.442s, learning 0.177s)
               Value function loss: 252331.6320
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 3925.44
               Mean episode length: 70.21
                  Mean reward/step: 55.46
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2881
Mean episode consecutive_successes: 15.4083
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 8.62s
                        Total time: 32676.80s
                               ETA: 979625.9s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.828s, learning 0.166s)
               Value function loss: 266027.6809
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: 3649.69
               Mean episode length: 69.30
                  Mean reward/step: 56.36
       Mean episode length/episode: 7.34
            Mean episode successes: 4.5317
Mean episode consecutive_successes: 15.4329
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 8.99s
                        Total time: 32685.80s
                               ETA: 979581.9s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.352s, learning 0.165s)
               Value function loss: 323279.9039
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 4199.62
               Mean episode length: 71.78
                  Mean reward/step: 56.55
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2759
Mean episode consecutive_successes: 15.5934
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 8.52s
                        Total time: 32694.32s
                               ETA: 979523.7s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.445s, learning 0.169s)
               Value function loss: 239950.2992
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 3797.29
               Mean episode length: 69.34
                  Mean reward/step: 55.17
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3882
Mean episode consecutive_successes: 15.5462
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 8.61s
                        Total time: 32702.93s
                               ETA: 979468.4s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.882s, learning 0.166s)
               Value function loss: 197604.6754
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 4098.29
               Mean episode length: 69.14
                  Mean reward/step: 57.70
       Mean episode length/episode: 7.23
            Mean episode successes: 4.3579
Mean episode consecutive_successes: 15.4912
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 9.05s
                        Total time: 32711.98s
                               ETA: 979426.1s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.647s, learning 0.182s)
               Value function loss: 194595.5141
                    Surrogate loss: -0.0126
             Mean action noise std: 0.72
                       Mean reward: 3499.28
               Mean episode length: 69.57
                  Mean reward/step: 55.49
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5176
Mean episode consecutive_successes: 15.4133
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 8.83s
                        Total time: 32720.81s
                               ETA: 979377.4s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.388s, learning 0.175s)
               Value function loss: 191157.0715
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 4550.33
               Mean episode length: 72.06
                  Mean reward/step: 53.20
       Mean episode length/episode: 7.21
            Mean episode successes: 4.3457
Mean episode consecutive_successes: 15.4347
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 8.56s
                        Total time: 32729.37s
                               ETA: 979320.6s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.497s, learning 0.172s)
               Value function loss: 190111.7430
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 4837.43
               Mean episode length: 72.02
                  Mean reward/step: 52.97
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1626
Mean episode consecutive_successes: 15.5986
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 8.67s
                        Total time: 32738.04s
                               ETA: 979267.1s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.170s)
               Value function loss: 192863.6871
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3192.01
               Mean episode length: 69.93
                  Mean reward/step: 54.10
       Mean episode length/episode: 7.24
            Mean episode successes: 4.1646
Mean episode consecutive_successes: 15.4602
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 8.53s
                        Total time: 32746.57s
                               ETA: 979209.4s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.596s, learning 0.170s)
               Value function loss: 239891.0945
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 4076.08
               Mean episode length: 70.61
                  Mean reward/step: 53.81
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1387
Mean episode consecutive_successes: 15.4886
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 8.77s
                        Total time: 32755.33s
                               ETA: 979158.8s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.212s, learning 0.173s)
               Value function loss: 297033.8148
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: 3755.39
               Mean episode length: 69.24
                  Mean reward/step: 57.32
       Mean episode length/episode: 7.36
            Mean episode successes: 4.4473
Mean episode consecutive_successes: 15.3749
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 8.38s
                        Total time: 32763.72s
                               ETA: 979096.9s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.452s, learning 0.184s)
               Value function loss: 227084.9621
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: 3966.19
               Mean episode length: 70.74
                  Mean reward/step: 57.61
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5796
Mean episode consecutive_successes: 15.3660
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 8.64s
                        Total time: 32772.35s
                               ETA: 979042.5s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.794s, learning 0.184s)
               Value function loss: 205332.3062
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 3916.40
               Mean episode length: 71.30
                  Mean reward/step: 55.49
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5859
Mean episode consecutive_successes: 15.3776
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 8.98s
                        Total time: 32781.33s
                               ETA: 978998.3s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.558s, learning 0.322s)
               Value function loss: 249793.1164
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: 4018.10
               Mean episode length: 69.75
                  Mean reward/step: 57.98
       Mean episode length/episode: 7.27
            Mean episode successes: 4.7119
Mean episode consecutive_successes: 15.3747
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 8.88s
                        Total time: 32790.21s
                               ETA: 978951.2s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.620s, learning 0.173s)
               Value function loss: 200981.8988
                    Surrogate loss: -0.0079
             Mean action noise std: 0.72
                       Mean reward: 4207.37
               Mean episode length: 68.56
                  Mean reward/step: 56.33
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4077
Mean episode consecutive_successes: 15.5264
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 8.79s
                        Total time: 32799.00s
                               ETA: 978901.6s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.683s, learning 0.235s)
               Value function loss: 165009.0316
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 4015.63
               Mean episode length: 70.85
                  Mean reward/step: 54.28
       Mean episode length/episode: 7.26
            Mean episode successes: 4.5259
Mean episode consecutive_successes: 15.4498
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 8.92s
                        Total time: 32807.92s
                               ETA: 978855.6s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.924s, learning 0.209s)
               Value function loss: 177012.7160
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 3745.92
               Mean episode length: 70.87
                  Mean reward/step: 55.09
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4658
Mean episode consecutive_successes: 15.4492
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 9.13s
                        Total time: 32817.05s
                               ETA: 978816.2s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.450s, learning 0.187s)
               Value function loss: 196963.4781
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: 4118.53
               Mean episode length: 70.90
                  Mean reward/step: 54.12
       Mean episode length/episode: 7.34
            Mean episode successes: 4.4614
Mean episode consecutive_successes: 15.5544
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 8.64s
                        Total time: 32825.69s
                               ETA: 978761.9s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.696s, learning 0.174s)
               Value function loss: 202662.7082
                    Surrogate loss: -0.0055
             Mean action noise std: 0.72
                       Mean reward: 3995.87
               Mean episode length: 69.89
                  Mean reward/step: 57.89
       Mean episode length/episode: 7.23
            Mean episode successes: 4.5215
Mean episode consecutive_successes: 15.5782
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 8.87s
                        Total time: 32834.56s
                               ETA: 978714.7s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.460s, learning 0.175s)
               Value function loss: 188634.5887
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 3644.38
               Mean episode length: 69.34
                  Mean reward/step: 56.60
       Mean episode length/episode: 7.24
            Mean episode successes: 4.4575
Mean episode consecutive_successes: 15.5163
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 8.63s
                        Total time: 32843.20s
                               ETA: 978660.5s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.444s, learning 0.188s)
               Value function loss: 206586.1359
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 3469.49
               Mean episode length: 70.73
                  Mean reward/step: 56.50
       Mean episode length/episode: 7.30
            Mean episode successes: 4.6045
Mean episode consecutive_successes: 15.5088
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 8.63s
                        Total time: 32851.83s
                               ETA: 978606.1s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.552s, learning 0.155s)
               Value function loss: 199023.1969
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 3970.72
               Mean episode length: 70.17
                  Mean reward/step: 54.24
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5522
Mean episode consecutive_successes: 15.5051
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 8.71s
                        Total time: 32860.53s
                               ETA: 978554.1s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.237s, learning 0.160s)
               Value function loss: 202898.2883
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 4509.85
               Mean episode length: 68.32
                  Mean reward/step: 54.40
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4121
Mean episode consecutive_successes: 15.5921
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 8.40s
                        Total time: 32868.93s
                               ETA: 978492.9s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.360s, learning 0.161s)
               Value function loss: 229705.0125
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 3444.76
               Mean episode length: 68.88
                  Mean reward/step: 52.26
       Mean episode length/episode: 7.25
            Mean episode successes: 4.0645
Mean episode consecutive_successes: 15.6567
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 8.52s
                        Total time: 32877.45s
                               ETA: 978435.4s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.699s, learning 0.334s)
               Value function loss: 207368.1984
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 2895.83
               Mean episode length: 68.72
                  Mean reward/step: 52.26
       Mean episode length/episode: 7.28
            Mean episode successes: 4.1475
Mean episode consecutive_successes: 15.5070
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 9.03s
                        Total time: 32886.48s
                               ETA: 978393.1s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.554s, learning 0.167s)
               Value function loss: 231819.8484
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 4088.17
               Mean episode length: 70.55
                  Mean reward/step: 52.77
       Mean episode length/episode: 7.28
            Mean episode successes: 4.1846
Mean episode consecutive_successes: 15.5188
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 8.72s
                        Total time: 32895.20s
                               ETA: 978341.6s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.609s, learning 0.209s)
               Value function loss: 196751.8871
                    Surrogate loss: 0.0020
             Mean action noise std: 0.72
                       Mean reward: 3800.12
               Mean episode length: 69.80
                  Mean reward/step: 52.83
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3281
Mean episode consecutive_successes: 15.3475
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 8.82s
                        Total time: 32904.02s
                               ETA: 978293.0s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.623s, learning 0.168s)
               Value function loss: 199285.3723
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 4220.50
               Mean episode length: 70.11
                  Mean reward/step: 53.56
       Mean episode length/episode: 7.23
            Mean episode successes: 4.1968
Mean episode consecutive_successes: 15.3739
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 8.79s
                        Total time: 32912.82s
                               ETA: 978243.7s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1798 steps/s (collection: 8.833s, learning 0.278s)
               Value function loss: 192033.5875
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 4053.84
               Mean episode length: 72.03
                  Mean reward/step: 54.98
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2290
Mean episode consecutive_successes: 15.2828
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 9.11s
                        Total time: 32921.93s
                               ETA: 978203.9s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.637s, learning 0.175s)
               Value function loss: 198367.6691
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 3666.08
               Mean episode length: 68.82
                  Mean reward/step: 56.72
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4966
Mean episode consecutive_successes: 15.2488
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 8.81s
                        Total time: 32930.74s
                               ETA: 978155.2s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.379s, learning 0.192s)
               Value function loss: 214050.7590
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 4093.67
               Mean episode length: 70.74
                  Mean reward/step: 58.48
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6333
Mean episode consecutive_successes: 15.2889
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 8.57s
                        Total time: 32939.31s
                               ETA: 978099.3s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.503s, learning 0.161s)
               Value function loss: 231731.6020
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 4697.09
               Mean episode length: 70.98
                  Mean reward/step: 57.95
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7256
Mean episode consecutive_successes: 15.3309
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 8.66s
                        Total time: 32947.97s
                               ETA: 978046.3s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.787s, learning 0.162s)
               Value function loss: 238085.2961
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 3637.68
               Mean episode length: 70.36
                  Mean reward/step: 56.92
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4893
Mean episode consecutive_successes: 15.4038
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 8.95s
                        Total time: 32956.92s
                               ETA: 978001.7s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.664s, learning 0.309s)
               Value function loss: 216300.4500
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 3852.71
               Mean episode length: 70.23
                  Mean reward/step: 55.12
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5879
Mean episode consecutive_successes: 15.3664
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 8.97s
                        Total time: 32965.89s
                               ETA: 977957.9s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.419s, learning 0.187s)
               Value function loss: 223419.9172
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 3577.89
               Mean episode length: 70.98
                  Mean reward/step: 54.06
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2842
Mean episode consecutive_successes: 15.4997
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 8.61s
                        Total time: 32974.50s
                               ETA: 977903.2s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.734s, learning 0.172s)
               Value function loss: 227024.8367
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: 3815.73
               Mean episode length: 70.98
                  Mean reward/step: 52.48
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1060
Mean episode consecutive_successes: 15.5552
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 8.91s
                        Total time: 32983.41s
                               ETA: 977857.4s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.473s, learning 0.302s)
               Value function loss: 196766.2637
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 3913.12
               Mean episode length: 70.93
                  Mean reward/step: 53.17
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2383
Mean episode consecutive_successes: 15.4885
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 8.78s
                        Total time: 32992.18s
                               ETA: 977807.8s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.584s, learning 0.163s)
               Value function loss: 219224.6066
                    Surrogate loss: -0.0104
             Mean action noise std: 0.72
                       Mean reward: 3500.18
               Mean episode length: 70.03
                  Mean reward/step: 51.00
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0684
Mean episode consecutive_successes: 15.4944
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 8.75s
                        Total time: 33000.93s
                               ETA: 977757.4s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.632s, learning 0.158s)
               Value function loss: 212442.3867
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 3534.14
               Mean episode length: 69.11
                  Mean reward/step: 50.95
       Mean episode length/episode: 7.28
            Mean episode successes: 4.1636
Mean episode consecutive_successes: 15.4022
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 8.79s
                        Total time: 33009.72s
                               ETA: 977708.3s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.451s, learning 0.186s)
               Value function loss: 216072.6754
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 4189.52
               Mean episode length: 71.39
                  Mean reward/step: 53.44
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1118
Mean episode consecutive_successes: 15.3787
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 8.64s
                        Total time: 33018.36s
                               ETA: 977654.6s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1808 steps/s (collection: 8.857s, learning 0.203s)
               Value function loss: 213392.5738
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 3269.98
               Mean episode length: 70.39
                  Mean reward/step: 54.13
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3174
Mean episode consecutive_successes: 15.1638
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 9.06s
                        Total time: 33027.42s
                               ETA: 977613.6s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.230s, learning 0.165s)
               Value function loss: 227806.7496
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 3397.79
               Mean episode length: 70.51
                  Mean reward/step: 54.76
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3320
Mean episode consecutive_successes: 15.1720
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 8.39s
                        Total time: 33035.81s
                               ETA: 977552.8s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.455s, learning 0.287s)
               Value function loss: 205394.8141
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 3552.28
               Mean episode length: 70.27
                  Mean reward/step: 57.02
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5679
Mean episode consecutive_successes: 15.1331
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 8.74s
                        Total time: 33044.55s
                               ETA: 977502.4s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.474s, learning 0.183s)
               Value function loss: 196368.8766
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3258.75
               Mean episode length: 69.60
                  Mean reward/step: 56.21
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4722
Mean episode consecutive_successes: 15.1164
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 8.66s
                        Total time: 33053.21s
                               ETA: 977449.4s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.860s, learning 0.165s)
               Value function loss: 213301.1047
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 3821.11
               Mean episode length: 71.17
                  Mean reward/step: 57.83
       Mean episode length/episode: 7.36
            Mean episode successes: 4.5684
Mean episode consecutive_successes: 15.3217
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 9.02s
                        Total time: 33062.24s
                               ETA: 977407.4s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.312s, learning 0.328s)
               Value function loss: 200025.9441
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 3739.62
               Mean episode length: 68.56
                  Mean reward/step: 58.30
       Mean episode length/episode: 7.34
            Mean episode successes: 4.8169
Mean episode consecutive_successes: 15.2801
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 8.64s
                        Total time: 33070.88s
                               ETA: 977354.0s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.330s, learning 0.204s)
               Value function loss: 235664.9844
                    Surrogate loss: -0.0099
             Mean action noise std: 0.72
                       Mean reward: 3850.21
               Mean episode length: 69.46
                  Mean reward/step: 58.05
       Mean episode length/episode: 7.22
            Mean episode successes: 4.5522
Mean episode consecutive_successes: 15.3872
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 8.53s
                        Total time: 33079.41s
                               ETA: 977297.5s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.718s, learning 0.199s)
               Value function loss: 208378.8793
                    Surrogate loss: -0.0066
             Mean action noise std: 0.72
                       Mean reward: 3336.46
               Mean episode length: 69.35
                  Mean reward/step: 56.13
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6304
Mean episode consecutive_successes: 15.3005
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 8.92s
                        Total time: 33088.33s
                               ETA: 977252.3s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.640s, learning 0.181s)
               Value function loss: 242569.0297
                    Surrogate loss: -0.0060
             Mean action noise std: 0.72
                       Mean reward: 3758.81
               Mean episode length: 71.13
                  Mean reward/step: 59.44
       Mean episode length/episode: 7.26
            Mean episode successes: 4.8164
Mean episode consecutive_successes: 15.2492
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 8.82s
                        Total time: 33097.15s
                               ETA: 977204.4s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.218s, learning 0.190s)
               Value function loss: 227048.5645
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 3432.25
               Mean episode length: 68.87
                  Mean reward/step: 58.81
       Mean episode length/episode: 7.25
            Mean episode successes: 4.7139
Mean episode consecutive_successes: 15.3728
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 8.41s
                        Total time: 33105.55s
                               ETA: 977144.2s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.405s, learning 0.280s)
               Value function loss: 239372.6500
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 4488.99
               Mean episode length: 69.84
                  Mean reward/step: 55.93
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5464
Mean episode consecutive_successes: 15.5675
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 8.68s
                        Total time: 33114.24s
                               ETA: 977092.3s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.118s, learning 0.179s)
               Value function loss: 232144.9883
                    Surrogate loss: -0.0084
             Mean action noise std: 0.72
                       Mean reward: 4158.89
               Mean episode length: 70.54
                  Mean reward/step: 53.82
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4351
Mean episode consecutive_successes: 15.6497
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 8.30s
                        Total time: 33122.54s
                               ETA: 977029.0s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.502s, learning 0.164s)
               Value function loss: 253672.4547
                    Surrogate loss: -0.0081
             Mean action noise std: 0.72
                       Mean reward: 4473.47
               Mean episode length: 70.54
                  Mean reward/step: 55.66
       Mean episode length/episode: 7.24
            Mean episode successes: 4.3350
Mean episode consecutive_successes: 15.6818
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 8.67s
                        Total time: 33131.20s
                               ETA: 976976.6s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.365s, learning 0.196s)
               Value function loss: 217183.1910
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 3298.67
               Mean episode length: 69.36
                  Mean reward/step: 55.41
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3745
Mean episode consecutive_successes: 15.5917
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 8.56s
                        Total time: 33139.76s
                               ETA: 976921.1s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.412s, learning 0.161s)
               Value function loss: 236304.4109
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 4370.57
               Mean episode length: 69.51
                  Mean reward/step: 55.98
       Mean episode length/episode: 7.32
            Mean episode successes: 4.5532
Mean episode consecutive_successes: 15.5553
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 8.57s
                        Total time: 33148.34s
                               ETA: 976866.0s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.185s, learning 0.309s)
               Value function loss: 245048.0043
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 3700.63
               Mean episode length: 69.87
                  Mean reward/step: 54.24
       Mean episode length/episode: 7.22
            Mean episode successes: 4.3506
Mean episode consecutive_successes: 15.5929
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 8.49s
                        Total time: 33156.83s
                               ETA: 976808.6s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.572s, learning 0.175s)
               Value function loss: 222783.7602
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 4186.96
               Mean episode length: 72.26
                  Mean reward/step: 52.94
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3511
Mean episode consecutive_successes: 15.6352
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 8.75s
                        Total time: 33165.58s
                               ETA: 976758.6s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1858 steps/s (collection: 8.562s, learning 0.254s)
               Value function loss: 222838.7938
                    Surrogate loss: -0.0116
             Mean action noise std: 0.72
                       Mean reward: 4105.33
               Mean episode length: 70.85
                  Mean reward/step: 53.59
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1519
Mean episode consecutive_successes: 15.6461
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 8.82s
                        Total time: 33174.40s
                               ETA: 976710.8s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 1805 steps/s (collection: 8.795s, learning 0.278s)
               Value function loss: 221533.0652
                    Surrogate loss: -0.0034
             Mean action noise std: 0.72
                       Mean reward: 3916.22
               Mean episode length: 70.82
                  Mean reward/step: 54.68
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1973
Mean episode consecutive_successes: 15.5755
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 9.07s
                        Total time: 33183.47s
                               ETA: 976670.5s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.517s, learning 0.171s)
               Value function loss: 181381.0309
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 2742.57
               Mean episode length: 66.81
                  Mean reward/step: 53.30
       Mean episode length/episode: 7.20
            Mean episode successes: 4.2285
Mean episode consecutive_successes: 15.3634
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 8.69s
                        Total time: 33192.16s
                               ETA: 976618.9s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.189s, learning 0.179s)
               Value function loss: 192670.6453
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 3867.70
               Mean episode length: 69.78
                  Mean reward/step: 53.25
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3110
Mean episode consecutive_successes: 15.3666
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 8.37s
                        Total time: 33200.53s
                               ETA: 976557.9s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.465s, learning 0.235s)
               Value function loss: 192197.3230
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 3964.82
               Mean episode length: 70.15
                  Mean reward/step: 52.31
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2065
Mean episode consecutive_successes: 15.3881
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 8.70s
                        Total time: 33209.23s
                               ETA: 976506.7s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.632s, learning 0.197s)
               Value function loss: 207606.1066
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 3814.48
               Mean episode length: 68.58
                  Mean reward/step: 54.34
       Mean episode length/episode: 7.26
            Mean episode successes: 3.8979
Mean episode consecutive_successes: 15.4779
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 8.83s
                        Total time: 33218.05s
                               ETA: 976459.3s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.983s, learning 0.168s)
               Value function loss: 200493.3016
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 3382.11
               Mean episode length: 69.98
                  Mean reward/step: 54.22
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1445
Mean episode consecutive_successes: 15.3697
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 9.15s
                        Total time: 33227.21s
                               ETA: 976421.5s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.442s, learning 0.231s)
               Value function loss: 203217.7148
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 3692.78
               Mean episode length: 71.21
                  Mean reward/step: 54.44
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2456
Mean episode consecutive_successes: 15.3259
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 8.67s
                        Total time: 33235.88s
                               ETA: 976369.6s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.576s, learning 0.164s)
               Value function loss: 247505.8656
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 3760.63
               Mean episode length: 70.95
                  Mean reward/step: 55.12
       Mean episode length/episode: 7.31
            Mean episode successes: 4.5005
Mean episode consecutive_successes: 15.2125
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 8.74s
                        Total time: 33244.62s
                               ETA: 976319.6s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.417s, learning 0.189s)
               Value function loss: 245217.2789
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 3143.95
               Mean episode length: 69.14
                  Mean reward/step: 56.66
       Mean episode length/episode: 7.24
            Mean episode successes: 4.5537
Mean episode consecutive_successes: 15.1624
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 8.61s
                        Total time: 33253.22s
                               ETA: 976265.8s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.694s, learning 0.207s)
               Value function loss: 245247.5152
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 3605.79
               Mean episode length: 67.90
                  Mean reward/step: 56.92
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4868
Mean episode consecutive_successes: 15.2823
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 8.90s
                        Total time: 33262.13s
                               ETA: 976220.7s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.101s, learning 0.166s)
               Value function loss: 207976.4844
                    Surrogate loss: -0.0181
             Mean action noise std: 0.72
                       Mean reward: 4299.58
               Mean episode length: 71.72
                  Mean reward/step: 55.05
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4268
Mean episode consecutive_successes: 15.3917
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 8.27s
                        Total time: 33270.39s
                               ETA: 976157.0s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.469s, learning 0.172s)
               Value function loss: 185867.8410
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 4659.74
               Mean episode length: 71.90
                  Mean reward/step: 54.80
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3452
Mean episode consecutive_successes: 15.4759
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 8.64s
                        Total time: 33279.03s
                               ETA: 976104.2s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.457s, learning 0.162s)
               Value function loss: 200434.1547
                    Surrogate loss: -0.0105
             Mean action noise std: 0.72
                       Mean reward: 4243.56
               Mean episode length: 70.77
                  Mean reward/step: 53.81
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2402
Mean episode consecutive_successes: 15.5326
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 8.62s
                        Total time: 33287.65s
                               ETA: 976050.9s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.168s)
               Value function loss: 197626.6910
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 3624.63
               Mean episode length: 68.47
                  Mean reward/step: 56.19
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3389
Mean episode consecutive_successes: 15.4349
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 8.47s
                        Total time: 33296.12s
                               ETA: 975993.1s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.264s, learning 0.299s)
               Value function loss: 195676.7801
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 3151.52
               Mean episode length: 69.19
                  Mean reward/step: 56.51
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4268
Mean episode consecutive_successes: 15.3061
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 8.56s
                        Total time: 33304.68s
                               ETA: 975938.2s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.819s, learning 0.169s)
               Value function loss: 198182.8094
                    Surrogate loss: -0.0176
             Mean action noise std: 0.72
                       Mean reward: 3366.50
               Mean episode length: 68.26
                  Mean reward/step: 55.87
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4160
Mean episode consecutive_successes: 15.2676
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 8.99s
                        Total time: 33313.67s
                               ETA: 975895.8s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.128s, learning 0.231s)
               Value function loss: 208573.7168
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 4385.93
               Mean episode length: 70.13
                  Mean reward/step: 56.47
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6104
Mean episode consecutive_successes: 15.2996
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 8.36s
                        Total time: 33322.03s
                               ETA: 975834.9s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.603s, learning 0.171s)
               Value function loss: 218915.5121
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 4254.32
               Mean episode length: 71.53
                  Mean reward/step: 58.26
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5728
Mean episode consecutive_successes: 15.4268
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 8.77s
                        Total time: 33330.80s
                               ETA: 975786.3s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.973s, learning 0.194s)
               Value function loss: 225770.4980
                    Surrogate loss: -0.0107
             Mean action noise std: 0.72
                       Mean reward: 4532.45
               Mean episode length: 72.00
                  Mean reward/step: 58.12
       Mean episode length/episode: 7.31
            Mean episode successes: 4.6030
Mean episode consecutive_successes: 15.5835
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 8.17s
                        Total time: 33338.97s
                               ETA: 975719.9s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.448s, learning 0.251s)
               Value function loss: 236153.0672
                    Surrogate loss: -0.0120
             Mean action noise std: 0.72
                       Mean reward: 3849.52
               Mean episode length: 72.03
                  Mean reward/step: 56.36
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4712
Mean episode consecutive_successes: 15.7005
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 8.70s
                        Total time: 33347.67s
                               ETA: 975669.1s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.686s, learning 0.194s)
               Value function loss: 214764.1582
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 4212.33
               Mean episode length: 68.43
                  Mean reward/step: 56.88
       Mean episode length/episode: 7.21
            Mean episode successes: 4.3740
Mean episode consecutive_successes: 15.6733
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 8.88s
                        Total time: 33356.55s
                               ETA: 975623.6s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.442s, learning 0.167s)
               Value function loss: 220566.7195
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 3997.81
               Mean episode length: 69.87
                  Mean reward/step: 54.16
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3110
Mean episode consecutive_successes: 15.6962
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 8.61s
                        Total time: 33365.16s
                               ETA: 975570.2s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.764s, learning 0.178s)
               Value function loss: 207554.1035
                    Surrogate loss: -0.0135
             Mean action noise std: 0.72
                       Mean reward: 3785.50
               Mean episode length: 69.80
                  Mean reward/step: 54.80
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4180
Mean episode consecutive_successes: 15.6727
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 8.94s
                        Total time: 33374.10s
                               ETA: 975526.5s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.633s, learning 0.194s)
               Value function loss: 226626.2578
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 3748.77
               Mean episode length: 71.27
                  Mean reward/step: 54.72
       Mean episode length/episode: 7.19
            Mean episode successes: 4.2671
Mean episode consecutive_successes: 15.6611
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 8.83s
                        Total time: 33382.93s
                               ETA: 975479.6s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.319s, learning 0.322s)
               Value function loss: 232455.6961
                    Surrogate loss: -0.0121
             Mean action noise std: 0.72
                       Mean reward: 3129.77
               Mean episode length: 70.04
                  Mean reward/step: 52.08
       Mean episode length/episode: 7.22
            Mean episode successes: 4.1108
Mean episode consecutive_successes: 15.5696
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 8.64s
                        Total time: 33391.57s
                               ETA: 975427.2s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.104s, learning 0.171s)
               Value function loss: 206653.8945
                    Surrogate loss: -0.0153
             Mean action noise std: 0.72
                       Mean reward: 3454.33
               Mean episode length: 69.87
                  Mean reward/step: 54.09
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3525
Mean episode consecutive_successes: 15.4440
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 8.27s
                        Total time: 33399.84s
                               ETA: 975364.1s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.787s, learning 0.170s)
               Value function loss: 216145.6152
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: 4842.62
               Mean episode length: 71.91
                  Mean reward/step: 56.02
       Mean episode length/episode: 7.33
            Mean episode successes: 4.3472
Mean episode consecutive_successes: 15.5640
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 8.96s
                        Total time: 33408.80s
                               ETA: 975321.0s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.699s, learning 0.158s)
               Value function loss: 209679.8965
                    Surrogate loss: 0.0026
             Mean action noise std: 0.72
                       Mean reward: 3286.30
               Mean episode length: 68.17
                  Mean reward/step: 53.88
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3848
Mean episode consecutive_successes: 15.4972
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 8.86s
                        Total time: 33417.65s
                               ETA: 975275.0s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.790s, learning 0.163s)
               Value function loss: 208095.6512
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 3918.10
               Mean episode length: 71.01
                  Mean reward/step: 55.18
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3652
Mean episode consecutive_successes: 15.5463
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 8.95s
                        Total time: 33426.61s
                               ETA: 975231.9s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.220s, learning 0.233s)
               Value function loss: 225433.1395
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: 4215.56
               Mean episode length: 70.09
                  Mean reward/step: 58.17
       Mean episode length/episode: 7.32
            Mean episode successes: 4.6509
Mean episode consecutive_successes: 15.5211
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 8.45s
                        Total time: 33435.06s
                               ETA: 975174.1s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.405s, learning 0.171s)
               Value function loss: 233967.0793
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 3662.46
               Mean episode length: 69.42
                  Mean reward/step: 55.99
       Mean episode length/episode: 7.21
            Mean episode successes: 4.4077
Mean episode consecutive_successes: 15.5431
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 8.58s
                        Total time: 33443.64s
                               ETA: 975120.0s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.570s, learning 0.281s)
               Value function loss: 235919.9914
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 4334.46
               Mean episode length: 71.55
                  Mean reward/step: 53.26
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3052
Mean episode consecutive_successes: 15.5358
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 8.85s
                        Total time: 33452.49s
                               ETA: 975073.9s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.836s, learning 0.187s)
               Value function loss: 240447.2961
                    Surrogate loss: 0.0567
             Mean action noise std: 0.72
                       Mean reward: 3682.45
               Mean episode length: 69.98
                  Mean reward/step: 52.65
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2012
Mean episode consecutive_successes: 15.4402
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 9.02s
                        Total time: 33461.51s
                               ETA: 975032.9s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.702s, learning 0.244s)
               Value function loss: 195406.8355
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 4284.54
               Mean episode length: 70.71
                  Mean reward/step: 52.25
       Mean episode length/episode: 7.25
            Mean episode successes: 3.9424
Mean episode consecutive_successes: 15.5137
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 8.95s
                        Total time: 33470.45s
                               ETA: 974989.6s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.329s, learning 0.175s)
               Value function loss: 199653.6973
                    Surrogate loss: -0.0115
             Mean action noise std: 0.72
                       Mean reward: 4049.89
               Mean episode length: 71.38
                  Mean reward/step: 51.96
       Mean episode length/episode: 7.27
            Mean episode successes: 3.8604
Mean episode consecutive_successes: 15.5290
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 8.50s
                        Total time: 33478.96s
                               ETA: 974933.5s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1783 steps/s (collection: 8.979s, learning 0.206s)
               Value function loss: 198659.8109
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 2671.24
               Mean episode length: 68.74
                  Mean reward/step: 53.63
       Mean episode length/episode: 7.35
            Mean episode successes: 4.1465
Mean episode consecutive_successes: 15.3179
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 9.19s
                        Total time: 33488.14s
                               ETA: 974897.2s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.424s, learning 0.290s)
               Value function loss: 244168.6820
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 3253.32
               Mean episode length: 71.13
                  Mean reward/step: 55.31
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2993
Mean episode consecutive_successes: 15.3572
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 8.71s
                        Total time: 33496.86s
                               ETA: 974847.3s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.722s, learning 0.167s)
               Value function loss: 321552.0508
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 3614.48
               Mean episode length: 68.96
                  Mean reward/step: 57.35
       Mean episode length/episode: 7.24
            Mean episode successes: 4.3726
Mean episode consecutive_successes: 15.3420
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 8.89s
                        Total time: 33505.75s
                               ETA: 974802.5s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.537s, learning 0.173s)
               Value function loss: 261542.3062
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 4045.33
               Mean episode length: 69.76
                  Mean reward/step: 58.66
       Mean episode length/episode: 7.34
            Mean episode successes: 4.6660
Mean episode consecutive_successes: 15.3610
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 8.71s
                        Total time: 33514.46s
                               ETA: 974752.5s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.747s, learning 0.166s)
               Value function loss: 209442.3305
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 3743.71
               Mean episode length: 70.68
                  Mean reward/step: 56.65
       Mean episode length/episode: 7.25
            Mean episode successes: 4.6108
Mean episode consecutive_successes: 15.3293
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 8.91s
                        Total time: 33523.37s
                               ETA: 974708.4s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.462s, learning 0.161s)
               Value function loss: 226107.1816
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 4111.55
               Mean episode length: 70.90
                  Mean reward/step: 53.78
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4814
Mean episode consecutive_successes: 15.3352
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 8.62s
                        Total time: 33531.99s
                               ETA: 974655.9s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.571s, learning 0.319s)
               Value function loss: 202040.4492
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 3943.05
               Mean episode length: 70.69
                  Mean reward/step: 53.49
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3262
Mean episode consecutive_successes: 15.3693
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 8.89s
                        Total time: 33540.88s
                               ETA: 974611.2s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.288s, learning 0.165s)
               Value function loss: 209294.0625
                    Surrogate loss: -0.0134
             Mean action noise std: 0.72
                       Mean reward: 3804.97
               Mean episode length: 71.87
                  Mean reward/step: 51.91
       Mean episode length/episode: 7.23
            Mean episode successes: 4.0088
Mean episode consecutive_successes: 15.4000
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 8.45s
                        Total time: 33549.34s
                               ETA: 974553.8s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1744 steps/s (collection: 9.094s, learning 0.295s)
               Value function loss: 198167.3086
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 3908.88
               Mean episode length: 71.11
                  Mean reward/step: 52.31
       Mean episode length/episode: 7.29
            Mean episode successes: 3.9941
Mean episode consecutive_successes: 15.4289
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 9.39s
                        Total time: 33558.73s
                               ETA: 974523.6s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.641s, learning 0.202s)
               Value function loss: 215008.7828
                    Surrogate loss: -0.0111
             Mean action noise std: 0.72
                       Mean reward: 3093.31
               Mean episode length: 68.80
                  Mean reward/step: 55.03
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2148
Mean episode consecutive_successes: 15.2707
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 8.84s
                        Total time: 33567.57s
                               ETA: 974477.6s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.613s, learning 0.177s)
               Value function loss: 204188.3398
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: 4806.47
               Mean episode length: 70.30
                  Mean reward/step: 55.56
       Mean episode length/episode: 7.35
            Mean episode successes: 4.4575
Mean episode consecutive_successes: 15.4084
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 8.79s
                        Total time: 33576.36s
                               ETA: 974430.1s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.429s, learning 0.182s)
               Value function loss: 221064.4848
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 3890.93
               Mean episode length: 70.58
                  Mean reward/step: 55.25
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3623
Mean episode consecutive_successes: 15.4033
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 8.61s
                        Total time: 33584.97s
                               ETA: 974377.4s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.401s, learning 0.291s)
               Value function loss: 223090.0012
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 3610.46
               Mean episode length: 71.16
                  Mean reward/step: 56.52
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5186
Mean episode consecutive_successes: 15.3608
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 8.69s
                        Total time: 33593.66s
                               ETA: 974327.1s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1757 steps/s (collection: 8.989s, learning 0.335s)
               Value function loss: 218227.6066
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 3791.90
               Mean episode length: 71.30
                  Mean reward/step: 57.63
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6021
Mean episode consecutive_successes: 15.3601
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 9.32s
                        Total time: 33602.99s
                               ETA: 974295.1s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.592s, learning 0.161s)
               Value function loss: 227542.8152
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 3857.16
               Mean episode length: 71.99
                  Mean reward/step: 55.29
       Mean episode length/episode: 7.24
            Mean episode successes: 4.4360
Mean episode consecutive_successes: 15.3659
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 8.75s
                        Total time: 33611.74s
                               ETA: 974246.6s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.573s, learning 0.163s)
               Value function loss: 213757.3129
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 4026.29
               Mean episode length: 70.86
                  Mean reward/step: 53.84
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3647
Mean episode consecutive_successes: 15.3523
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 8.74s
                        Total time: 33620.48s
                               ETA: 974197.6s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1790 steps/s (collection: 8.959s, learning 0.190s)
               Value function loss: 212288.5211
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 3750.16
               Mean episode length: 70.38
                  Mean reward/step: 53.22
       Mean episode length/episode: 7.18
            Mean episode successes: 4.0840
Mean episode consecutive_successes: 15.3298
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 9.15s
                        Total time: 33629.62s
                               ETA: 974160.6s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.675s, learning 0.320s)
               Value function loss: 203483.3988
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: 4188.92
               Mean episode length: 71.18
                  Mean reward/step: 50.34
       Mean episode length/episode: 7.31
            Mean episode successes: 3.9844
Mean episode consecutive_successes: 15.3930
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 9.00s
                        Total time: 33638.62s
                               ETA: 974119.2s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.552s, learning 0.182s)
               Value function loss: 230459.4977
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 3963.18
               Mean episode length: 70.09
                  Mean reward/step: 53.20
       Mean episode length/episode: 7.26
            Mean episode successes: 4.0005
Mean episode consecutive_successes: 15.3551
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 8.73s
                        Total time: 33647.35s
                               ETA: 974070.2s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.377s, learning 0.236s)
               Value function loss: 238208.4379
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 3720.30
               Mean episode length: 71.22
                  Mean reward/step: 54.47
       Mean episode length/episode: 7.32
            Mean episode successes: 4.1758
Mean episode consecutive_successes: 15.3500
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 8.61s
                        Total time: 33655.97s
                               ETA: 974017.7s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.664s, learning 0.167s)
               Value function loss: 206188.8180
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: 3076.32
               Mean episode length: 68.30
                  Mean reward/step: 56.26
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4478
Mean episode consecutive_successes: 15.2028
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.83s
                        Total time: 33664.80s
                               ETA: 973971.6s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.453s, learning 0.167s)
               Value function loss: 205272.1000
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 4022.97
               Mean episode length: 70.72
                  Mean reward/step: 58.73
       Mean episode length/episode: 7.27
            Mean episode successes: 4.7070
Mean episode consecutive_successes: 15.1772
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 8.62s
                        Total time: 33673.42s
                               ETA: 973919.4s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.512s, learning 0.236s)
               Value function loss: 236609.3941
                    Surrogate loss: -0.0098
             Mean action noise std: 0.72
                       Mean reward: 3622.49
               Mean episode length: 70.25
                  Mean reward/step: 58.66
       Mean episode length/episode: 7.24
            Mean episode successes: 4.6431
Mean episode consecutive_successes: 15.2817
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 8.75s
                        Total time: 33682.16s
                               ETA: 973871.0s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.611s, learning 0.156s)
               Value function loss: 238832.7234
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 4030.80
               Mean episode length: 70.47
                  Mean reward/step: 59.29
       Mean episode length/episode: 7.25
            Mean episode successes: 4.6807
Mean episode consecutive_successes: 15.3763
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 8.77s
                        Total time: 33690.93s
                               ETA: 973823.1s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.521s, learning 0.162s)
               Value function loss: 263988.6789
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 4067.32
               Mean episode length: 72.26
                  Mean reward/step: 55.87
       Mean episode length/episode: 7.23
            Mean episode successes: 4.5264
Mean episode consecutive_successes: 15.3950
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 8.68s
                        Total time: 33699.61s
                               ETA: 973772.8s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.456s, learning 0.160s)
               Value function loss: 196865.5664
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: 3798.64
               Mean episode length: 70.98
                  Mean reward/step: 54.65
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5684
Mean episode consecutive_successes: 15.4132
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 8.62s
                        Total time: 33708.23s
                               ETA: 973720.6s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.653s, learning 0.194s)
               Value function loss: 183463.5352
                    Surrogate loss: -0.0150
             Mean action noise std: 0.72
                       Mean reward: 3627.18
               Mean episode length: 69.84
                  Mean reward/step: 54.32
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4619
Mean episode consecutive_successes: 15.3789
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 8.85s
                        Total time: 33717.08s
                               ETA: 973675.0s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.335s, learning 0.232s)
               Value function loss: 203112.1223
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 3938.32
               Mean episode length: 71.62
                  Mean reward/step: 54.65
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2598
Mean episode consecutive_successes: 15.5094
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 8.57s
                        Total time: 33725.64s
                               ETA: 973621.5s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.786s, learning 0.159s)
               Value function loss: 202093.0695
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 3973.12
               Mean episode length: 69.51
                  Mean reward/step: 55.41
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2778
Mean episode consecutive_successes: 15.5219
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 8.95s
                        Total time: 33734.59s
                               ETA: 973578.8s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.679s, learning 0.255s)
               Value function loss: 217073.5734
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 3780.55
               Mean episode length: 70.74
                  Mean reward/step: 58.10
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4121
Mean episode consecutive_successes: 15.5057
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 8.93s
                        Total time: 33743.52s
                               ETA: 973535.9s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.782s, learning 0.163s)
               Value function loss: 206020.5672
                    Surrogate loss: -0.0139
             Mean action noise std: 0.72
                       Mean reward: 3587.49
               Mean episode length: 70.14
                  Mean reward/step: 56.91
       Mean episode length/episode: 7.30
            Mean episode successes: 4.6587
Mean episode consecutive_successes: 15.4715
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 8.94s
                        Total time: 33752.47s
                               ETA: 973493.3s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.770s, learning 0.198s)
               Value function loss: 217499.5016
                    Surrogate loss: -0.0137
             Mean action noise std: 0.72
                       Mean reward: 4043.64
               Mean episode length: 70.18
                  Mean reward/step: 58.50
       Mean episode length/episode: 7.30
            Mean episode successes: 4.7437
Mean episode consecutive_successes: 15.4812
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 8.97s
                        Total time: 33761.43s
                               ETA: 973451.3s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.845s, learning 0.201s)
               Value function loss: 236028.6219
                    Surrogate loss: -0.0124
             Mean action noise std: 0.72
                       Mean reward: 4384.58
               Mean episode length: 71.03
                  Mean reward/step: 57.68
       Mean episode length/episode: 7.20
            Mean episode successes: 4.5435
Mean episode consecutive_successes: 15.5533
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 9.05s
                        Total time: 33770.48s
                               ETA: 973411.7s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.186s, learning 0.157s)
               Value function loss: 223612.6699
                    Surrogate loss: 0.0073
             Mean action noise std: 0.72
                       Mean reward: 4066.06
               Mean episode length: 71.18
                  Mean reward/step: 56.42
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4951
Mean episode consecutive_successes: 15.6174
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 8.34s
                        Total time: 33778.82s
                               ETA: 973351.8s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.504s, learning 0.168s)
               Value function loss: 198301.7437
                    Surrogate loss: -0.0082
             Mean action noise std: 0.72
                       Mean reward: 3768.10
               Mean episode length: 71.92
                  Mean reward/step: 54.30
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4653
Mean episode consecutive_successes: 15.6188
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 8.67s
                        Total time: 33787.50s
                               ETA: 973301.5s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1801 steps/s (collection: 8.929s, learning 0.164s)
               Value function loss: 208786.9578
                    Surrogate loss: -0.0092
             Mean action noise std: 0.72
                       Mean reward: 4647.07
               Mean episode length: 71.78
                  Mean reward/step: 51.57
       Mean episode length/episode: 7.28
            Mean episode successes: 4.3091
Mean episode consecutive_successes: 15.6412
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 9.09s
                        Total time: 33796.59s
                               ETA: 973263.2s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.904s, learning 0.164s)
               Value function loss: 221324.3535
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 3192.11
               Mean episode length: 68.72
                  Mean reward/step: 49.77
       Mean episode length/episode: 7.27
            Mean episode successes: 4.0146
Mean episode consecutive_successes: 15.6137
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 9.07s
                        Total time: 33805.66s
                               ETA: 973224.3s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.680s, learning 0.192s)
               Value function loss: 212604.8961
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 4352.65
               Mean episode length: 71.87
                  Mean reward/step: 51.87
       Mean episode length/episode: 7.26
            Mean episode successes: 4.0718
Mean episode consecutive_successes: 15.5322
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 8.87s
                        Total time: 33814.53s
                               ETA: 973179.8s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.774s, learning 0.186s)
               Value function loss: 201224.3563
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 3973.38
               Mean episode length: 70.03
                  Mean reward/step: 53.14
       Mean episode length/episode: 7.31
            Mean episode successes: 4.0083
Mean episode consecutive_successes: 15.5192
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 8.96s
                        Total time: 33823.49s
                               ETA: 973137.8s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.023s, learning 0.256s)
               Value function loss: 202293.6719
                    Surrogate loss: -0.0053
             Mean action noise std: 0.71
                       Mean reward: 3302.85
               Mean episode length: 71.04
                  Mean reward/step: 53.00
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2188
Mean episode consecutive_successes: 15.3241
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 9.28s
                        Total time: 33832.77s
                               ETA: 973105.0s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.636s, learning 0.169s)
               Value function loss: 206456.6969
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 3450.25
               Mean episode length: 70.14
                  Mean reward/step: 52.61
       Mean episode length/episode: 7.35
            Mean episode successes: 4.4175
Mean episode consecutive_successes: 15.2607
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 8.80s
                        Total time: 33841.57s
                               ETA: 973058.5s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.602s, learning 0.262s)
               Value function loss: 216568.3086
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 3623.13
               Mean episode length: 71.09
                  Mean reward/step: 55.46
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4922
Mean episode consecutive_successes: 15.2482
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 8.86s
                        Total time: 33850.44s
                               ETA: 973013.8s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.448s, learning 0.164s)
               Value function loss: 206051.7148
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 3938.80
               Mean episode length: 70.21
                  Mean reward/step: 52.97
       Mean episode length/episode: 7.21
            Mean episode successes: 4.2227
Mean episode consecutive_successes: 15.2616
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 8.61s
                        Total time: 33859.05s
                               ETA: 972961.9s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 1789 steps/s (collection: 8.986s, learning 0.170s)
               Value function loss: 199065.8195
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 3619.13
               Mean episode length: 69.12
                  Mean reward/step: 52.60
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2964
Mean episode consecutive_successes: 15.1752
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 9.16s
                        Total time: 33868.21s
                               ETA: 972925.6s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.503s, learning 0.165s)
               Value function loss: 198574.0730
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 3485.66
               Mean episode length: 70.67
                  Mean reward/step: 55.09
       Mean episode length/episode: 7.21
            Mean episode successes: 4.2749
Mean episode consecutive_successes: 15.1315
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 8.67s
                        Total time: 33876.87s
                               ETA: 972875.4s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.745s, learning 0.267s)
               Value function loss: 192950.3586
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 3928.49
               Mean episode length: 70.51
                  Mean reward/step: 52.85
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2969
Mean episode consecutive_successes: 15.1085
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 9.01s
                        Total time: 33885.89s
                               ETA: 972835.0s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.685s, learning 0.172s)
               Value function loss: 205696.9215
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 3376.78
               Mean episode length: 69.90
                  Mean reward/step: 52.71
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2266
Mean episode consecutive_successes: 15.0238
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 8.86s
                        Total time: 33894.74s
                               ETA: 972790.2s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.717s, learning 0.177s)
               Value function loss: 200226.5324
                    Surrogate loss: -0.0060
             Mean action noise std: 0.71
                       Mean reward: 3608.72
               Mean episode length: 72.43
                  Mean reward/step: 53.58
       Mean episode length/episode: 7.32
            Mean episode successes: 4.3765
Mean episode consecutive_successes: 15.0273
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 8.89s
                        Total time: 33903.64s
                               ETA: 972746.5s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.459s, learning 0.170s)
               Value function loss: 213404.9316
                    Surrogate loss: -0.0143
             Mean action noise std: 0.71
                       Mean reward: 4694.02
               Mean episode length: 70.85
                  Mean reward/step: 52.30
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1465
Mean episode consecutive_successes: 15.2177
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 8.63s
                        Total time: 33912.27s
                               ETA: 972695.2s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.889s, learning 0.215s)
               Value function loss: 201709.6129
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 3643.04
               Mean episode length: 70.05
                  Mean reward/step: 51.08
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1265
Mean episode consecutive_successes: 15.1725
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 9.10s
                        Total time: 33921.37s
                               ETA: 972657.5s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.520s, learning 0.195s)
               Value function loss: 203749.3094
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 3304.29
               Mean episode length: 69.58
                  Mean reward/step: 54.17
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3940
Mean episode consecutive_successes: 15.0372
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 8.71s
                        Total time: 33930.08s
                               ETA: 972608.7s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.613s, learning 0.187s)
               Value function loss: 213095.0949
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 3748.88
               Mean episode length: 71.93
                  Mean reward/step: 55.44
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3320
Mean episode consecutive_successes: 15.0646
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 8.80s
                        Total time: 33938.88s
                               ETA: 972562.4s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.309s, learning 0.158s)
               Value function loss: 207991.5406
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 3784.23
               Mean episode length: 70.75
                  Mean reward/step: 53.09
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2231
Mean episode consecutive_successes: 15.1191
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 8.47s
                        Total time: 33947.35s
                               ETA: 972506.6s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.584s, learning 0.171s)
               Value function loss: 207211.2652
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 3223.15
               Mean episode length: 71.77
                  Mean reward/step: 52.97
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3369
Mean episode consecutive_successes: 15.0864
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 8.75s
                        Total time: 33956.11s
                               ETA: 972459.0s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1793 steps/s (collection: 8.871s, learning 0.265s)
               Value function loss: 222683.0957
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 3745.28
               Mean episode length: 69.99
                  Mean reward/step: 51.41
       Mean episode length/episode: 7.21
            Mean episode successes: 4.2715
Mean episode consecutive_successes: 15.0491
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 9.14s
                        Total time: 33965.24s
                               ETA: 972422.3s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.732s, learning 0.189s)
               Value function loss: 204521.7023
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 3535.15
               Mean episode length: 71.13
                  Mean reward/step: 51.07
       Mean episode length/episode: 7.22
            Mean episode successes: 4.0142
Mean episode consecutive_successes: 14.9993
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 8.92s
                        Total time: 33974.16s
                               ETA: 972379.6s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.604s, learning 0.256s)
               Value function loss: 207509.3195
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 3913.24
               Mean episode length: 69.77
                  Mean reward/step: 50.34
       Mean episode length/episode: 7.27
            Mean episode successes: 3.9199
Mean episode consecutive_successes: 14.9904
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 8.86s
                        Total time: 33983.02s
                               ETA: 972335.1s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.476s, learning 0.176s)
               Value function loss: 201861.6535
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 3577.30
               Mean episode length: 70.46
                  Mean reward/step: 52.40
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1685
Mean episode consecutive_successes: 14.9017
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 8.65s
                        Total time: 33991.67s
                               ETA: 972284.6s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.855s, learning 0.214s)
               Value function loss: 211526.7051
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 3472.65
               Mean episode length: 70.09
                  Mean reward/step: 50.64
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0854
Mean episode consecutive_successes: 14.9158
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 9.07s
                        Total time: 34000.74s
                               ETA: 972246.2s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.128s, learning 0.174s)
               Value function loss: 203954.5746
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3089.65
               Mean episode length: 69.61
                  Mean reward/step: 49.45
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0859
Mean episode consecutive_successes: 14.8622
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 16.30s
                        Total time: 34017.05s
                               ETA: 972414.5s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.683s, learning 0.256s)
               Value function loss: 215041.7805
                    Surrogate loss: -0.0049
             Mean action noise std: 0.71
                       Mean reward: 3773.90
               Mean episode length: 71.09
                  Mean reward/step: 53.03
       Mean episode length/episode: 7.23
            Mean episode successes: 4.0840
Mean episode consecutive_successes: 14.8317
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 16.94s
                        Total time: 34033.98s
                               ETA: 972600.8s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.685s, learning 0.260s)
               Value function loss: 205617.2461
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 3597.22
               Mean episode length: 69.92
                  Mean reward/step: 54.30
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2388
Mean episode consecutive_successes: 14.8055
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 16.94s
                        Total time: 34050.93s
                               ETA: 972787.3s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 947 steps/s (collection: 17.098s, learning 0.196s)
               Value function loss: 204460.5609
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 3585.83
               Mean episode length: 70.90
                  Mean reward/step: 55.65
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4990
Mean episode consecutive_successes: 14.7403
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 17.29s
                        Total time: 34068.22s
                               ETA: 972983.6s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.687s, learning 0.226s)
               Value function loss: 210582.0301
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 3690.97
               Mean episode length: 71.16
                  Mean reward/step: 54.18
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4185
Mean episode consecutive_successes: 14.7447
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 16.91s
                        Total time: 34085.13s
                               ETA: 973168.9s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.040s, learning 0.227s)
               Value function loss: 202462.2867
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 4371.81
               Mean episode length: 71.17
                  Mean reward/step: 54.59
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4468
Mean episode consecutive_successes: 14.8768
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 17.27s
                        Total time: 34102.40s
                               ETA: 973364.1s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.397s, learning 0.171s)
               Value function loss: 190444.6879
                    Surrogate loss: -0.0156
             Mean action noise std: 0.71
                       Mean reward: 3398.50
               Mean episode length: 71.04
                  Mean reward/step: 52.54
       Mean episode length/episode: 7.22
            Mean episode successes: 4.0825
Mean episode consecutive_successes: 14.8861
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 16.57s
                        Total time: 34118.97s
                               ETA: 973539.4s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.755s, learning 0.168s)
               Value function loss: 209709.0816
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 4420.28
               Mean episode length: 69.27
                  Mean reward/step: 53.20
       Mean episode length/episode: 7.34
            Mean episode successes: 4.2583
Mean episode consecutive_successes: 14.9368
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 16.92s
                        Total time: 34135.89s
                               ETA: 973724.6s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.528s, learning 0.168s)
               Value function loss: 221095.6004
                    Surrogate loss: -0.0119
             Mean action noise std: 0.71
                       Mean reward: 3253.58
               Mean episode length: 68.96
                  Mean reward/step: 53.05
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1880
Mean episode consecutive_successes: 14.8704
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 16.70s
                        Total time: 34152.59s
                               ETA: 973903.2s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.775s, learning 0.157s)
               Value function loss: 233438.1746
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 3826.43
               Mean episode length: 71.56
                  Mean reward/step: 54.10
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2593
Mean episode consecutive_successes: 14.9009
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 16.93s
                        Total time: 34169.52s
                               ETA: 974088.5s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 948 steps/s (collection: 17.048s, learning 0.221s)
               Value function loss: 220607.0980
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 3683.48
               Mean episode length: 70.92
                  Mean reward/step: 54.96
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3809
Mean episode consecutive_successes: 14.9162
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 17.27s
                        Total time: 34186.79s
                               ETA: 974283.2s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.645s, learning 0.308s)
               Value function loss: 206848.9371
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 3625.74
               Mean episode length: 71.10
                  Mean reward/step: 54.15
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4346
Mean episode consecutive_successes: 14.9203
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 16.95s
                        Total time: 34203.74s
                               ETA: 974468.8s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.816s, learning 0.296s)
               Value function loss: 226117.2563
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 3508.08
               Mean episode length: 70.70
                  Mean reward/step: 55.51
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3887
Mean episode consecutive_successes: 14.9856
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 17.11s
                        Total time: 34220.86s
                               ETA: 974658.8s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.631s, learning 0.165s)
               Value function loss: 227183.7004
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 3725.70
               Mean episode length: 70.72
                  Mean reward/step: 53.90
       Mean episode length/episode: 7.28
            Mean episode successes: 4.3940
Mean episode consecutive_successes: 15.0157
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 16.80s
                        Total time: 34237.65s
                               ETA: 974839.7s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.550s, learning 0.164s)
               Value function loss: 232455.6297
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 3818.95
               Mean episode length: 71.26
                  Mean reward/step: 53.03
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2974
Mean episode consecutive_successes: 14.9942
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 16.71s
                        Total time: 34254.37s
                               ETA: 975018.1s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.522s, learning 0.216s)
               Value function loss: 207401.3359
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 3896.07
               Mean episode length: 70.84
                  Mean reward/step: 54.36
       Mean episode length/episode: 7.22
            Mean episode successes: 4.1802
Mean episode consecutive_successes: 15.0586
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 16.74s
                        Total time: 34271.10s
                               ETA: 975197.1s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.610s, learning 0.155s)
               Value function loss: 198387.8676
                    Surrogate loss: -0.0109
             Mean action noise std: 0.71
                       Mean reward: 3249.28
               Mean episode length: 69.72
                  Mean reward/step: 55.82
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4155
Mean episode consecutive_successes: 14.9414
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 16.76s
                        Total time: 34287.87s
                               ETA: 975376.8s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.717s, learning 0.155s)
               Value function loss: 187997.4629
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 3811.62
               Mean episode length: 69.57
                  Mean reward/step: 55.26
       Mean episode length/episode: 7.34
            Mean episode successes: 4.4736
Mean episode consecutive_successes: 15.0562
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 16.87s
                        Total time: 34304.74s
                               ETA: 975559.3s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 952 steps/s (collection: 17.039s, learning 0.161s)
               Value function loss: 195721.1461
                    Surrogate loss: 0.0004
             Mean action noise std: 0.71
                       Mean reward: 4306.40
               Mean episode length: 70.56
                  Mean reward/step: 55.97
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3770
Mean episode consecutive_successes: 15.1725
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 17.20s
                        Total time: 34321.94s
                               ETA: 975751.1s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.470s, learning 0.260s)
               Value function loss: 209785.7258
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 4173.26
               Mean episode length: 70.45
                  Mean reward/step: 55.77
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2466
Mean episode consecutive_successes: 15.2368
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 16.73s
                        Total time: 34338.67s
                               ETA: 975929.4s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.860s, learning 0.205s)
               Value function loss: 191157.3141
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 3786.97
               Mean episode length: 71.24
                  Mean reward/step: 57.86
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4580
Mean episode consecutive_successes: 15.1992
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 17.07s
                        Total time: 34355.73s
                               ETA: 976117.2s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.617s, learning 0.157s)
               Value function loss: 209678.8059
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 3548.05
               Mean episode length: 70.45
                  Mean reward/step: 57.79
       Mean episode length/episode: 7.33
            Mean episode successes: 4.7080
Mean episode consecutive_successes: 15.1226
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 16.77s
                        Total time: 34372.51s
                               ETA: 976296.5s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 964 steps/s (collection: 16.823s, learning 0.167s)
               Value function loss: 249499.0316
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: 4124.55
               Mean episode length: 71.19
                  Mean reward/step: 57.47
       Mean episode length/episode: 7.23
            Mean episode successes: 4.5986
Mean episode consecutive_successes: 15.2315
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 16.99s
                        Total time: 34389.50s
                               ETA: 976481.8s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.012s, learning 0.237s)
               Value function loss: 205407.4754
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 4264.66
               Mean episode length: 71.38
                  Mean reward/step: 54.30
       Mean episode length/episode: 7.23
            Mean episode successes: 4.4648
Mean episode consecutive_successes: 15.2822
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 17.25s
                        Total time: 34406.75s
                               ETA: 976674.4s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.597s, learning 0.165s)
               Value function loss: 195706.5195
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 4051.03
               Mean episode length: 70.49
                  Mean reward/step: 54.50
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4282
Mean episode consecutive_successes: 15.2638
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 16.76s
                        Total time: 34423.51s
                               ETA: 976853.1s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.846s, learning 0.207s)
               Value function loss: 209717.1434
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 4390.01
               Mean episode length: 72.39
                  Mean reward/step: 55.15
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1035
Mean episode consecutive_successes: 15.4616
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 17.05s
                        Total time: 34440.56s
                               ETA: 977039.8s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 967 steps/s (collection: 16.779s, learning 0.161s)
               Value function loss: 195062.7848
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 3311.29
               Mean episode length: 68.83
                  Mean reward/step: 55.57
       Mean episode length/episode: 7.34
            Mean episode successes: 4.3730
Mean episode consecutive_successes: 15.3761
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 16.94s
                        Total time: 34457.50s
                               ETA: 977223.3s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.307s, learning 0.157s)
               Value function loss: 201561.2109
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 4164.50
               Mean episode length: 71.86
                  Mean reward/step: 57.16
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5044
Mean episode consecutive_successes: 15.4217
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 16.46s
                        Total time: 34473.97s
                               ETA: 977393.1s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.260s, learning 0.161s)
               Value function loss: 210066.2621
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 4097.53
               Mean episode length: 71.82
                  Mean reward/step: 57.38
       Mean episode length/episode: 7.31
            Mean episode successes: 4.6514
Mean episode consecutive_successes: 15.4399
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 16.42s
                        Total time: 34490.39s
                               ETA: 977561.6s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.849s, learning 0.311s)
               Value function loss: 228131.7082
                    Surrogate loss: -0.0121
             Mean action noise std: 0.71
                       Mean reward: 4818.74
               Mean episode length: 72.69
                  Mean reward/step: 59.82
       Mean episode length/episode: 7.25
            Mean episode successes: 4.6431
Mean episode consecutive_successes: 15.5692
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 17.16s
                        Total time: 34507.55s
                               ETA: 977750.9s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.795s, learning 0.166s)
               Value function loss: 211569.7055
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 4083.84
               Mean episode length: 71.41
                  Mean reward/step: 58.60
       Mean episode length/episode: 7.33
            Mean episode successes: 4.6177
Mean episode consecutive_successes: 15.7137
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 16.96s
                        Total time: 34524.51s
                               ETA: 977934.5s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.585s, learning 0.167s)
               Value function loss: 227879.2590
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 3873.37
               Mean episode length: 70.79
                  Mean reward/step: 55.50
       Mean episode length/episode: 7.24
            Mean episode successes: 4.5659
Mean episode consecutive_successes: 15.6488
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 16.75s
                        Total time: 34541.26s
                               ETA: 978112.1s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.861s, learning 0.164s)
               Value function loss: 216815.3184
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 3591.56
               Mean episode length: 71.43
                  Mean reward/step: 54.70
       Mean episode length/episode: 7.23
            Mean episode successes: 4.4165
Mean episode consecutive_successes: 15.6646
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 17.02s
                        Total time: 34558.28s
                               ETA: 978297.2s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.859s, learning 0.171s)
               Value function loss: 208572.0617
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 3722.88
               Mean episode length: 70.43
                  Mean reward/step: 55.56
       Mean episode length/episode: 7.17
            Mean episode successes: 4.3765
Mean episode consecutive_successes: 15.5246
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 17.03s
                        Total time: 34575.31s
                               ETA: 978482.4s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 975 steps/s (collection: 16.617s, learning 0.172s)
               Value function loss: 220737.6621
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 3903.73
               Mean episode length: 71.12
                  Mean reward/step: 54.40
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2002
Mean episode consecutive_successes: 15.6652
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 16.79s
                        Total time: 34592.10s
                               ETA: 978660.7s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.510s, learning 0.169s)
               Value function loss: 209658.0250
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 4032.51
               Mean episode length: 70.24
                  Mean reward/step: 53.04
       Mean episode length/episode: 7.34
            Mean episode successes: 4.3188
Mean episode consecutive_successes: 15.6911
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 16.68s
                        Total time: 34608.78s
                               ETA: 978835.7s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.100s, learning 0.158s)
               Value function loss: 235126.1641
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 3950.83
               Mean episode length: 69.49
                  Mean reward/step: 54.14
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2354
Mean episode consecutive_successes: 15.6467
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 17.26s
                        Total time: 34626.04s
                               ETA: 979027.0s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 1110 steps/s (collection: 14.571s, learning 0.180s)
               Value function loss: 256510.8117
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 3867.33
               Mean episode length: 68.91
                  Mean reward/step: 53.76
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2612
Mean episode consecutive_successes: 15.6411
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 14.75s
                        Total time: 34640.79s
                               ETA: 979147.3s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.350s, learning 0.170s)
               Value function loss: 228267.2242
                    Surrogate loss: -0.0109
             Mean action noise std: 0.71
                       Mean reward: 3747.15
               Mean episode length: 68.91
                  Mean reward/step: 55.86
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2773
Mean episode consecutive_successes: 15.5821
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 8.52s
                        Total time: 34649.31s
                               ETA: 979091.4s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.567s, learning 0.171s)
               Value function loss: 193328.9797
                    Surrogate loss: -0.0086
             Mean action noise std: 0.71
                       Mean reward: 3363.56
               Mean episode length: 68.27
                  Mean reward/step: 56.26
       Mean episode length/episode: 7.32
            Mean episode successes: 4.5137
Mean episode consecutive_successes: 15.4793
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 8.74s
                        Total time: 34658.05s
                               ETA: 979041.8s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.324s, learning 0.180s)
               Value function loss: 223167.6051
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 3273.18
               Mean episode length: 70.99
                  Mean reward/step: 57.91
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5820
Mean episode consecutive_successes: 15.5503
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 8.50s
                        Total time: 34666.55s
                               ETA: 978985.5s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.407s, learning 0.184s)
               Value function loss: 205352.2324
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 3680.83
               Mean episode length: 71.55
                  Mean reward/step: 56.41
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7271
Mean episode consecutive_successes: 15.4972
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 8.59s
                        Total time: 34675.14s
                               ETA: 978931.7s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.795s, learning 0.162s)
               Value function loss: 209401.6684
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 3871.48
               Mean episode length: 70.67
                  Mean reward/step: 56.01
       Mean episode length/episode: 7.13
            Mean episode successes: 4.3896
Mean episode consecutive_successes: 15.4657
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 8.96s
                        Total time: 34684.10s
                               ETA: 978888.3s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.672s, learning 0.271s)
               Value function loss: 208533.0875
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 3920.73
               Mean episode length: 69.90
                  Mean reward/step: 52.84
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1582
Mean episode consecutive_successes: 15.6318
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 8.94s
                        Total time: 34693.05s
                               ETA: 978844.6s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.526s, learning 0.168s)
               Value function loss: 224590.0090
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 4148.08
               Mean episode length: 70.11
                  Mean reward/step: 50.33
       Mean episode length/episode: 7.32
            Mean episode successes: 4.0776
Mean episode consecutive_successes: 15.6206
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 8.69s
                        Total time: 34701.74s
                               ETA: 978793.8s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.358s, learning 0.175s)
               Value function loss: 243026.1367
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 3436.77
               Mean episode length: 71.65
                  Mean reward/step: 49.37
       Mean episode length/episode: 7.34
            Mean episode successes: 4.1172
Mean episode consecutive_successes: 15.5225
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 8.53s
                        Total time: 34710.27s
                               ETA: 978738.5s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.440s, learning 0.202s)
               Value function loss: 229607.3844
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 3636.58
               Mean episode length: 71.01
                  Mean reward/step: 52.47
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1138
Mean episode consecutive_successes: 15.4898
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 8.64s
                        Total time: 34718.91s
                               ETA: 978686.3s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.415s, learning 0.193s)
               Value function loss: 221077.1836
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 3856.35
               Mean episode length: 71.14
                  Mean reward/step: 55.40
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2666
Mean episode consecutive_successes: 15.4369
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 8.61s
                        Total time: 34727.52s
                               ETA: 978633.1s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.688s, learning 0.163s)
               Value function loss: 245076.6445
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 3459.90
               Mean episode length: 69.54
                  Mean reward/step: 53.20
       Mean episode length/episode: 7.24
            Mean episode successes: 4.1934
Mean episode consecutive_successes: 15.3587
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 8.85s
                        Total time: 34736.37s
                               ETA: 978586.9s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.229s, learning 0.270s)
               Value function loss: 211978.4066
                    Surrogate loss: -0.0120
             Mean action noise std: 0.71
                       Mean reward: 3372.71
               Mean episode length: 70.08
                  Mean reward/step: 54.49
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4351
Mean episode consecutive_successes: 15.2210
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 8.50s
                        Total time: 34744.87s
                               ETA: 978530.7s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.404s, learning 0.175s)
               Value function loss: 229188.5406
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 4057.08
               Mean episode length: 72.06
                  Mean reward/step: 56.00
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4556
Mean episode consecutive_successes: 15.2985
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 8.58s
                        Total time: 34753.45s
                               ETA: 978476.8s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.188s, learning 0.168s)
               Value function loss: 224590.7719
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 3958.55
               Mean episode length: 71.14
                  Mean reward/step: 53.48
       Mean episode length/episode: 7.20
            Mean episode successes: 4.2739
Mean episode consecutive_successes: 15.2958
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 8.36s
                        Total time: 34761.81s
                               ETA: 978416.7s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.169s, learning 0.280s)
               Value function loss: 217617.3387
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 3582.58
               Mean episode length: 69.57
                  Mean reward/step: 52.22
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1973
Mean episode consecutive_successes: 15.2205
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 8.45s
                        Total time: 34770.26s
                               ETA: 978359.2s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.722s, learning 0.168s)
               Value function loss: 197223.1328
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 3858.43
               Mean episode length: 70.16
                  Mean reward/step: 51.75
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1646
Mean episode consecutive_successes: 15.2431
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 8.89s
                        Total time: 34779.15s
                               ETA: 978314.2s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.632s, learning 0.187s)
               Value function loss: 178399.4156
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 4301.02
               Mean episode length: 70.66
                  Mean reward/step: 52.24
       Mean episode length/episode: 7.35
            Mean episode successes: 4.0391
Mean episode consecutive_successes: 15.3608
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 8.82s
                        Total time: 34787.96s
                               ETA: 978267.1s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.599s, learning 0.159s)
               Value function loss: 188161.5270
                    Surrogate loss: -0.0002
             Mean action noise std: 0.71
                       Mean reward: 4180.19
               Mean episode length: 72.69
                  Mean reward/step: 52.62
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0562
Mean episode consecutive_successes: 15.3045
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 8.76s
                        Total time: 34796.72s
                               ETA: 978218.4s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.600s, learning 0.196s)
               Value function loss: 188224.4125
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 3394.79
               Mean episode length: 70.22
                  Mean reward/step: 55.16
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4053
Mean episode consecutive_successes: 15.2133
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 8.80s
                        Total time: 34805.52s
                               ETA: 978170.8s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.688s, learning 0.188s)
               Value function loss: 198596.9027
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 4269.19
               Mean episode length: 71.75
                  Mean reward/step: 54.25
       Mean episode length/episode: 7.23
            Mean episode successes: 4.2271
Mean episode consecutive_successes: 15.2803
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 8.88s
                        Total time: 34814.40s
                               ETA: 978125.5s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.363s, learning 0.184s)
               Value function loss: 199491.9410
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 2965.10
               Mean episode length: 70.28
                  Mean reward/step: 52.96
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2593
Mean episode consecutive_successes: 15.1626
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 8.55s
                        Total time: 34822.94s
                               ETA: 978070.9s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 1778 steps/s (collection: 8.911s, learning 0.303s)
               Value function loss: 218048.9062
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 4324.09
               Mean episode length: 70.97
                  Mean reward/step: 55.42
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4468
Mean episode consecutive_successes: 15.1802
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 9.21s
                        Total time: 34832.16s
                               ETA: 978035.1s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.324s, learning 0.172s)
               Value function loss: 207492.9312
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 4590.48
               Mean episode length: 73.34
                  Mean reward/step: 56.69
       Mean episode length/episode: 7.31
            Mean episode successes: 4.6006
Mean episode consecutive_successes: 15.2799
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 8.50s
                        Total time: 34840.65s
                               ETA: 977979.1s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.596s, learning 0.268s)
               Value function loss: 210493.2547
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 3270.57
               Mean episode length: 71.23
                  Mean reward/step: 53.43
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2158
Mean episode consecutive_successes: 15.2539
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 8.86s
                        Total time: 34849.52s
                               ETA: 977933.5s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.568s, learning 0.172s)
               Value function loss: 232727.8184
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 3871.58
               Mean episode length: 71.42
                  Mean reward/step: 52.73
       Mean episode length/episode: 7.22
            Mean episode successes: 4.0371
Mean episode consecutive_successes: 15.2196
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 8.74s
                        Total time: 34858.26s
                               ETA: 977884.5s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.751s, learning 0.163s)
               Value function loss: 201792.4496
                    Surrogate loss: -0.0148
             Mean action noise std: 0.71
                       Mean reward: 3731.44
               Mean episode length: 68.63
                  Mean reward/step: 53.00
       Mean episode length/episode: 7.29
            Mean episode successes: 4.2295
Mean episode consecutive_successes: 15.1939
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 8.91s
                        Total time: 34867.17s
                               ETA: 977840.3s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.434s, learning 0.291s)
               Value function loss: 226064.3977
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 4239.46
               Mean episode length: 71.60
                  Mean reward/step: 54.09
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0913
Mean episode consecutive_successes: 15.3351
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 8.73s
                        Total time: 34875.90s
                               ETA: 977790.9s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.741s, learning 0.221s)
               Value function loss: 225490.8289
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 3643.47
               Mean episode length: 70.34
                  Mean reward/step: 54.58
       Mean episode length/episode: 7.33
            Mean episode successes: 4.2856
Mean episode consecutive_successes: 15.2483
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 8.96s
                        Total time: 34884.86s
                               ETA: 977748.2s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.688s, learning 0.164s)
               Value function loss: 222366.7527
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 3843.12
               Mean episode length: 70.34
                  Mean reward/step: 57.15
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4990
Mean episode consecutive_successes: 15.2175
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 8.85s
                        Total time: 34893.71s
                               ETA: 977702.3s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.549s, learning 0.295s)
               Value function loss: 195777.9691
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 3746.51
               Mean episode length: 71.31
                  Mean reward/step: 57.10
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5640
Mean episode consecutive_successes: 15.2681
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 8.84s
                        Total time: 34902.55s
                               ETA: 977656.3s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.529s, learning 0.160s)
               Value function loss: 210847.4672
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 4178.87
               Mean episode length: 71.08
                  Mean reward/step: 56.78
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4492
Mean episode consecutive_successes: 15.3470
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 8.69s
                        Total time: 34911.24s
                               ETA: 977605.9s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.525s, learning 0.323s)
               Value function loss: 199104.6773
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 3583.35
               Mean episode length: 70.91
                  Mean reward/step: 55.01
       Mean episode length/episode: 7.33
            Mean episode successes: 4.5898
Mean episode consecutive_successes: 15.2686
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 8.85s
                        Total time: 34920.09s
                               ETA: 977560.1s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.512s, learning 0.176s)
               Value function loss: 222661.6328
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 4413.26
               Mean episode length: 72.44
                  Mean reward/step: 55.04
       Mean episode length/episode: 7.24
            Mean episode successes: 4.4009
Mean episode consecutive_successes: 15.3961
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 8.69s
                        Total time: 34928.78s
                               ETA: 977509.7s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.557s, learning 0.176s)
               Value function loss: 200790.8418
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 3500.61
               Mean episode length: 70.32
                  Mean reward/step: 54.02
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4111
Mean episode consecutive_successes: 15.3589
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 8.73s
                        Total time: 34937.51s
                               ETA: 977460.7s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.228s, learning 0.174s)
               Value function loss: 213721.8293
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 4241.15
               Mean episode length: 70.11
                  Mean reward/step: 52.65
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1582
Mean episode consecutive_successes: 15.4854
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 8.40s
                        Total time: 34945.91s
                               ETA: 977402.4s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.759s, learning 0.159s)
               Value function loss: 206568.2715
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 3460.31
               Mean episode length: 69.75
                  Mean reward/step: 52.82
       Mean episode length/episode: 7.31
            Mean episode successes: 4.1494
Mean episode consecutive_successes: 15.4074
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 8.92s
                        Total time: 34954.83s
                               ETA: 977358.5s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.576s, learning 0.158s)
               Value function loss: 215436.2242
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 4035.18
               Mean episode length: 72.53
                  Mean reward/step: 54.81
       Mean episode length/episode: 7.35
            Mean episode successes: 4.3530
Mean episode consecutive_successes: 15.4205
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 8.73s
                        Total time: 34963.57s
                               ETA: 977309.6s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.435s, learning 0.343s)
               Value function loss: 220326.0809
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 3530.38
               Mean episode length: 70.08
                  Mean reward/step: 56.43
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3853
Mean episode consecutive_successes: 15.3239
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 8.78s
                        Total time: 34972.34s
                               ETA: 977261.9s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.661s, learning 0.196s)
               Value function loss: 228170.2629
                    Surrogate loss: -0.0109
             Mean action noise std: 0.71
                       Mean reward: 4065.44
               Mean episode length: 70.31
                  Mean reward/step: 55.42
       Mean episode length/episode: 7.32
            Mean episode successes: 4.5327
Mean episode consecutive_successes: 15.4201
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 8.86s
                        Total time: 34981.20s
                               ETA: 977216.4s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.531s, learning 0.215s)
               Value function loss: 232137.6078
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 3850.39
               Mean episode length: 69.71
                  Mean reward/step: 55.74
       Mean episode length/episode: 7.18
            Mean episode successes: 4.3389
Mean episode consecutive_successes: 15.4191
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 8.75s
                        Total time: 34989.95s
                               ETA: 977167.9s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.672s, learning 0.253s)
               Value function loss: 187867.3305
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 3520.29
               Mean episode length: 70.51
                  Mean reward/step: 54.30
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3286
Mean episode consecutive_successes: 15.4491
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 8.92s
                        Total time: 34998.87s
                               ETA: 977124.3s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.325s, learning 0.159s)
               Value function loss: 213530.2707
                    Surrogate loss: 0.0011
             Mean action noise std: 0.71
                       Mean reward: 3239.53
               Mean episode length: 69.63
                  Mean reward/step: 55.11
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5552
Mean episode consecutive_successes: 15.3418
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 8.48s
                        Total time: 35007.35s
                               ETA: 977068.5s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.580s, learning 0.169s)
               Value function loss: 211681.3531
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 3675.36
               Mean episode length: 70.57
                  Mean reward/step: 53.81
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2773
Mean episode consecutive_successes: 15.3807
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 8.75s
                        Total time: 35016.10s
                               ETA: 977020.1s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.529s, learning 0.168s)
               Value function loss: 220387.2453
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 3979.17
               Mean episode length: 71.54
                  Mean reward/step: 53.14
       Mean episode length/episode: 7.23
            Mean episode successes: 4.1772
Mean episode consecutive_successes: 15.3551
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 8.70s
                        Total time: 35024.80s
                               ETA: 976970.3s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.606s, learning 0.186s)
               Value function loss: 215500.5316
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 3700.23
               Mean episode length: 69.91
                  Mean reward/step: 52.08
       Mean episode length/episode: 7.26
            Mean episode successes: 4.0117
Mean episode consecutive_successes: 15.4094
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 8.79s
                        Total time: 35033.59s
                               ETA: 976923.2s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.602s, learning 0.160s)
               Value function loss: 212698.4824
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 3255.59
               Mean episode length: 67.21
                  Mean reward/step: 51.95
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1294
Mean episode consecutive_successes: 15.3593
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 8.76s
                        Total time: 35042.36s
                               ETA: 976875.2s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.330s, learning 0.312s)
               Value function loss: 197837.1883
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 3529.41
               Mean episode length: 69.74
                  Mean reward/step: 52.56
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0020
Mean episode consecutive_successes: 15.3831
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 8.64s
                        Total time: 35051.00s
                               ETA: 976823.9s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.428s, learning 0.167s)
               Value function loss: 200731.3211
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 3077.93
               Mean episode length: 70.89
                  Mean reward/step: 54.38
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2964
Mean episode consecutive_successes: 15.2069
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 8.60s
                        Total time: 35059.59s
                               ETA: 976771.4s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.858s, learning 0.177s)
               Value function loss: 208421.3164
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 3791.31
               Mean episode length: 70.71
                  Mean reward/step: 54.98
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4141
Mean episode consecutive_successes: 15.1711
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 9.04s
                        Total time: 35068.63s
                               ETA: 976731.1s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.376s, learning 0.276s)
               Value function loss: 214582.9508
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 4118.71
               Mean episode length: 70.39
                  Mean reward/step: 53.92
       Mean episode length/episode: 7.24
            Mean episode successes: 4.3110
Mean episode consecutive_successes: 15.1775
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 8.65s
                        Total time: 35077.28s
                               ETA: 976680.1s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.581s, learning 0.158s)
               Value function loss: 224085.1023
                    Surrogate loss: -0.0119
             Mean action noise std: 0.71
                       Mean reward: 3625.58
               Mean episode length: 70.97
                  Mean reward/step: 56.97
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5435
Mean episode consecutive_successes: 15.1704
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 8.74s
                        Total time: 35086.02s
                               ETA: 976631.7s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.523s, learning 0.160s)
               Value function loss: 212670.8910
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 4324.26
               Mean episode length: 70.99
                  Mean reward/step: 55.26
       Mean episode length/episode: 7.21
            Mean episode successes: 4.4043
Mean episode consecutive_successes: 15.2268
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 8.68s
                        Total time: 35094.70s
                               ETA: 976581.6s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.288s, learning 0.228s)
               Value function loss: 205584.8648
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 3893.53
               Mean episode length: 70.91
                  Mean reward/step: 54.53
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3447
Mean episode consecutive_successes: 15.2030
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 8.52s
                        Total time: 35103.22s
                               ETA: 976527.0s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.274s, learning 0.217s)
               Value function loss: 194141.8410
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 4562.55
               Mean episode length: 71.62
                  Mean reward/step: 55.57
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3315
Mean episode consecutive_successes: 15.2823
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 8.49s
                        Total time: 35111.71s
                               ETA: 976471.7s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.559s, learning 0.158s)
               Value function loss: 195508.1871
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 4601.71
               Mean episode length: 70.94
                  Mean reward/step: 55.02
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4072
Mean episode consecutive_successes: 15.3658
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 8.72s
                        Total time: 35120.43s
                               ETA: 976422.7s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.562s, learning 0.159s)
               Value function loss: 205015.3574
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 3793.49
               Mean episode length: 70.42
                  Mean reward/step: 55.98
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4741
Mean episode consecutive_successes: 15.3361
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 8.72s
                        Total time: 35129.15s
                               ETA: 976373.8s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.320s, learning 0.166s)
               Value function loss: 219249.9781
                    Surrogate loss: -0.0131
             Mean action noise std: 0.71
                       Mean reward: 4344.42
               Mean episode length: 71.41
                  Mean reward/step: 55.80
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4741
Mean episode consecutive_successes: 15.3847
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 8.49s
                        Total time: 35137.63s
                               ETA: 976318.4s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.650s, learning 0.207s)
               Value function loss: 208503.3262
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 4214.88
               Mean episode length: 69.36
                  Mean reward/step: 58.53
       Mean episode length/episode: 7.31
            Mean episode successes: 4.6470
Mean episode consecutive_successes: 15.4232
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 8.86s
                        Total time: 35146.49s
                               ETA: 976273.3s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.310s, learning 0.186s)
               Value function loss: 217355.7617
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 3346.20
               Mean episode length: 71.96
                  Mean reward/step: 56.21
       Mean episode length/episode: 7.24
            Mean episode successes: 4.5239
Mean episode consecutive_successes: 15.3860
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 8.50s
                        Total time: 35154.98s
                               ETA: 976218.3s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.602s, learning 0.234s)
               Value function loss: 208306.7039
                    Surrogate loss: -0.0133
             Mean action noise std: 0.71
                       Mean reward: 4015.59
               Mean episode length: 70.79
                  Mean reward/step: 56.62
       Mean episode length/episode: 7.36
            Mean episode successes: 4.5864
Mean episode consecutive_successes: 15.5434
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 8.84s
                        Total time: 35163.82s
                               ETA: 976172.7s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.532s, learning 0.187s)
               Value function loss: 231814.1680
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 3971.47
               Mean episode length: 71.36
                  Mean reward/step: 54.85
       Mean episode length/episode: 7.26
            Mean episode successes: 4.5952
Mean episode consecutive_successes: 15.4570
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 8.72s
                        Total time: 35172.54s
                               ETA: 976123.9s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.519s, learning 0.285s)
               Value function loss: 215543.6098
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 4238.79
               Mean episode length: 71.32
                  Mean reward/step: 54.42
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3105
Mean episode consecutive_successes: 15.6276
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 8.80s
                        Total time: 35181.34s
                               ETA: 976077.5s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.363s, learning 0.159s)
               Value function loss: 221737.5426
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 3547.26
               Mean episode length: 70.24
                  Mean reward/step: 51.59
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1436
Mean episode consecutive_successes: 15.6147
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 8.52s
                        Total time: 35189.86s
                               ETA: 976023.2s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.600s, learning 0.309s)
               Value function loss: 216716.9590
                    Surrogate loss: -0.0109
             Mean action noise std: 0.71
                       Mean reward: 3258.35
               Mean episode length: 68.17
                  Mean reward/step: 53.31
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2349
Mean episode consecutive_successes: 15.4721
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 8.91s
                        Total time: 35198.77s
                               ETA: 975979.8s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.349s, learning 0.159s)
               Value function loss: 237724.2992
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 3941.62
               Mean episode length: 71.69
                  Mean reward/step: 53.37
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2656
Mean episode consecutive_successes: 15.4417
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 8.51s
                        Total time: 35207.28s
                               ETA: 975925.2s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.877s, learning 0.177s)
               Value function loss: 217704.4371
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 3958.91
               Mean episode length: 71.22
                  Mean reward/step: 52.98
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2710
Mean episode consecutive_successes: 15.4439
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 9.05s
                        Total time: 35216.34s
                               ETA: 975885.8s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.719s, learning 0.171s)
               Value function loss: 224096.7199
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3703.01
               Mean episode length: 70.88
                  Mean reward/step: 54.82
       Mean episode length/episode: 7.32
            Mean episode successes: 4.3599
Mean episode consecutive_successes: 15.4764
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 8.89s
                        Total time: 35225.23s
                               ETA: 975841.9s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.105s, learning 0.394s)
               Value function loss: 223261.8371
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 3600.52
               Mean episode length: 70.31
                  Mean reward/step: 56.17
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4316
Mean episode consecutive_successes: 15.4203
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 8.50s
                        Total time: 35233.72s
                               ETA: 975787.1s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.738s, learning 0.263s)
               Value function loss: 238759.1770
                    Surrogate loss: -0.0101
             Mean action noise std: 0.71
                       Mean reward: 4007.31
               Mean episode length: 69.44
                  Mean reward/step: 55.29
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3521
Mean episode consecutive_successes: 15.4604
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 9.00s
                        Total time: 35242.72s
                               ETA: 975746.3s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.828s, learning 0.160s)
               Value function loss: 264290.7383
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 3830.90
               Mean episode length: 70.80
                  Mean reward/step: 56.38
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4702
Mean episode consecutive_successes: 15.3981
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 8.99s
                        Total time: 35251.71s
                               ETA: 975705.1s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.605s, learning 0.160s)
               Value function loss: 268213.6934
                    Surrogate loss: -0.0088
             Mean action noise std: 0.71
                       Mean reward: 4111.38
               Mean episode length: 70.87
                  Mean reward/step: 54.59
       Mean episode length/episode: 7.24
            Mean episode successes: 4.4736
Mean episode consecutive_successes: 15.3550
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 8.76s
                        Total time: 35260.48s
                               ETA: 975657.8s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.289s, learning 0.294s)
               Value function loss: 231620.2332
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 3971.13
               Mean episode length: 70.96
                  Mean reward/step: 52.62
       Mean episode length/episode: 7.23
            Mean episode successes: 4.2876
Mean episode consecutive_successes: 15.4160
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 8.58s
                        Total time: 35269.06s
                               ETA: 975605.5s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.600s, learning 0.174s)
               Value function loss: 275838.8461
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 3830.85
               Mean episode length: 70.26
                  Mean reward/step: 54.59
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3379
Mean episode consecutive_successes: 15.4375
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 8.77s
                        Total time: 35277.83s
                               ETA: 975558.5s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.541s, learning 0.338s)
               Value function loss: 262005.9422
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 4491.48
               Mean episode length: 71.06
                  Mean reward/step: 54.44
       Mean episode length/episode: 7.29
            Mean episode successes: 4.1021
Mean episode consecutive_successes: 15.5922
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 8.88s
                        Total time: 35286.71s
                               ETA: 975514.4s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.137s, learning 0.155s)
               Value function loss: 195216.6789
                    Surrogate loss: 0.0042
             Mean action noise std: 0.71
                       Mean reward: 3429.95
               Mean episode length: 69.87
                  Mean reward/step: 53.92
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2124
Mean episode consecutive_successes: 15.5321
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 8.29s
                        Total time: 35295.01s
                               ETA: 975454.1s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.308s, learning 0.275s)
               Value function loss: 204613.7621
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 3687.01
               Mean episode length: 69.02
                  Mean reward/step: 57.80
       Mean episode length/episode: 7.34
            Mean episode successes: 4.5806
Mean episode consecutive_successes: 15.4111
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 8.58s
                        Total time: 35303.59s
                               ETA: 975401.9s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.300s, learning 0.205s)
               Value function loss: 208465.9418
                    Surrogate loss: -0.0119
             Mean action noise std: 0.71
                       Mean reward: 3059.67
               Mean episode length: 70.31
                  Mean reward/step: 58.78
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6562
Mean episode consecutive_successes: 15.3874
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 8.50s
                        Total time: 35312.09s
                               ETA: 975347.5s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.164s, learning 0.158s)
               Value function loss: 225044.0004
                    Surrogate loss: -0.0061
             Mean action noise std: 0.71
                       Mean reward: 3943.97
               Mean episode length: 70.63
                  Mean reward/step: 56.51
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6616
Mean episode consecutive_successes: 15.4222
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 8.32s
                        Total time: 35320.42s
                               ETA: 975288.2s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.476s, learning 0.170s)
               Value function loss: 213891.5512
                    Surrogate loss: -0.0050
             Mean action noise std: 0.71
                       Mean reward: 4114.94
               Mean episode length: 72.23
                  Mean reward/step: 53.84
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5654
Mean episode consecutive_successes: 15.4540
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 8.65s
                        Total time: 35329.06s
                               ETA: 975237.7s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.601s, learning 0.169s)
               Value function loss: 206939.5180
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 4186.30
               Mean episode length: 71.55
                  Mean reward/step: 52.30
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3564
Mean episode consecutive_successes: 15.5220
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 8.77s
                        Total time: 35337.83s
                               ETA: 975190.8s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.584s, learning 0.158s)
               Value function loss: 206707.4469
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 3249.71
               Mean episode length: 69.68
                  Mean reward/step: 51.29
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0986
Mean episode consecutive_successes: 15.5459
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 8.74s
                        Total time: 35346.57s
                               ETA: 975143.1s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.750s, learning 0.177s)
               Value function loss: 205845.4297
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 3995.08
               Mean episode length: 69.48
                  Mean reward/step: 53.61
       Mean episode length/episode: 7.22
            Mean episode successes: 4.1318
Mean episode consecutive_successes: 15.4963
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 8.93s
                        Total time: 35355.50s
                               ETA: 975100.5s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.449s, learning 0.322s)
               Value function loss: 208264.6660
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 3405.89
               Mean episode length: 71.11
                  Mean reward/step: 54.22
       Mean episode length/episode: 7.31
            Mean episode successes: 4.2266
Mean episode consecutive_successes: 15.3472
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 8.77s
                        Total time: 35364.27s
                               ETA: 975053.6s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.506s, learning 0.189s)
               Value function loss: 218400.1926
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 3960.71
               Mean episode length: 70.58
                  Mean reward/step: 51.66
       Mean episode length/episode: 7.34
            Mean episode successes: 4.2480
Mean episode consecutive_successes: 15.4065
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 8.70s
                        Total time: 35372.97s
                               ETA: 975004.7s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.162s, learning 0.164s)
               Value function loss: 223513.7059
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 3454.44
               Mean episode length: 69.02
                  Mean reward/step: 53.60
       Mean episode length/episode: 7.23
            Mean episode successes: 4.2524
Mean episode consecutive_successes: 15.3228
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 8.33s
                        Total time: 35381.29s
                               ETA: 974945.6s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.545s, learning 0.162s)
               Value function loss: 249366.3984
                    Surrogate loss: -0.0160
             Mean action noise std: 0.71
                       Mean reward: 3993.55
               Mean episode length: 70.89
                  Mean reward/step: 57.45
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5625
Mean episode consecutive_successes: 15.2968
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 8.71s
                        Total time: 35390.00s
                               ETA: 974897.0s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.521s, learning 0.211s)
               Value function loss: 361536.5242
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 3844.30
               Mean episode length: 72.59
                  Mean reward/step: 57.54
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7173
Mean episode consecutive_successes: 15.2942
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 8.73s
                        Total time: 35398.73s
                               ETA: 974849.1s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.461s, learning 0.169s)
               Value function loss: 217411.8730
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 3687.10
               Mean episode length: 69.35
                  Mean reward/step: 57.72
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5786
Mean episode consecutive_successes: 15.4036
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 8.63s
                        Total time: 35407.36s
                               ETA: 974798.5s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.104s, learning 0.160s)
               Value function loss: 206778.9031
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 3752.15
               Mean episode length: 69.69
                  Mean reward/step: 58.91
       Mean episode length/episode: 7.23
            Mean episode successes: 4.9004
Mean episode consecutive_successes: 15.2770
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 8.26s
                        Total time: 35415.63s
                               ETA: 974737.8s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1812 steps/s (collection: 8.780s, learning 0.260s)
               Value function loss: 203450.7789
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 3924.35
               Mean episode length: 68.48
                  Mean reward/step: 55.55
       Mean episode length/episode: 7.31
            Mean episode successes: 4.8477
Mean episode consecutive_successes: 15.3580
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 9.04s
                        Total time: 35424.67s
                               ETA: 974698.5s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.191s, learning 0.162s)
               Value function loss: 240671.5285
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 4485.01
               Mean episode length: 70.89
                  Mean reward/step: 52.17
       Mean episode length/episode: 7.19
            Mean episode successes: 4.4419
Mean episode consecutive_successes: 15.4446
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 8.35s
                        Total time: 35433.02s
                               ETA: 974640.3s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.254s, learning 0.177s)
               Value function loss: 197819.1059
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 4514.90
               Mean episode length: 72.43
                  Mean reward/step: 50.49
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1128
Mean episode consecutive_successes: 15.5085
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 8.43s
                        Total time: 35441.45s
                               ETA: 974584.3s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1799 steps/s (collection: 8.939s, learning 0.165s)
               Value function loss: 203840.6785
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 4165.01
               Mean episode length: 69.70
                  Mean reward/step: 51.09
       Mean episode length/episode: 7.28
            Mean episode successes: 4.0503
Mean episode consecutive_successes: 15.4504
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 9.10s
                        Total time: 35450.55s
                               ETA: 974546.8s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.454s, learning 0.166s)
               Value function loss: 218194.2934
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 3808.59
               Mean episode length: 71.22
                  Mean reward/step: 50.22
       Mean episode length/episode: 7.29
            Mean episode successes: 4.0713
Mean episode consecutive_successes: 15.3757
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 8.62s
                        Total time: 35459.17s
                               ETA: 974496.1s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.615s, learning 0.164s)
               Value function loss: 262844.1148
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 4264.96
               Mean episode length: 71.94
                  Mean reward/step: 50.24
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1895
Mean episode consecutive_successes: 15.3124
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 8.78s
                        Total time: 35467.95s
                               ETA: 974449.7s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.615s, learning 0.173s)
               Value function loss: 256174.7020
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 3647.59
               Mean episode length: 69.76
                  Mean reward/step: 53.00
       Mean episode length/episode: 7.22
            Mean episode successes: 4.1318
Mean episode consecutive_successes: 15.1659
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 8.79s
                        Total time: 35476.74s
                               ETA: 974403.5s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.503s, learning 0.222s)
               Value function loss: 209191.5039
                    Surrogate loss: -0.0057
             Mean action noise std: 0.71
                       Mean reward: 3788.94
               Mean episode length: 70.99
                  Mean reward/step: 54.43
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4097
Mean episode consecutive_successes: 15.0862
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 8.72s
                        Total time: 35485.46s
                               ETA: 974355.7s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.419s, learning 0.193s)
               Value function loss: 221337.7887
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 3456.15
               Mean episode length: 71.49
                  Mean reward/step: 54.46
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6147
Mean episode consecutive_successes: 15.0203
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 8.61s
                        Total time: 35494.08s
                               ETA: 974304.8s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.846s, learning 0.162s)
               Value function loss: 221296.5301
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3792.60
               Mean episode length: 69.32
                  Mean reward/step: 53.07
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4824
Mean episode consecutive_successes: 15.0642
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 9.01s
                        Total time: 35503.08s
                               ETA: 974264.8s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.541s, learning 0.174s)
               Value function loss: 221141.4523
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 4359.52
               Mean episode length: 71.02
                  Mean reward/step: 52.73
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2334
Mean episode consecutive_successes: 15.2135
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 8.71s
                        Total time: 35511.80s
                               ETA: 974216.7s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.399s, learning 0.165s)
               Value function loss: 224010.1672
                    Surrogate loss: -0.0083
             Mean action noise std: 0.71
                       Mean reward: 3621.04
               Mean episode length: 71.08
                  Mean reward/step: 53.13
       Mean episode length/episode: 7.27
            Mean episode successes: 4.1953
Mean episode consecutive_successes: 15.1587
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 8.56s
                        Total time: 35520.36s
                               ETA: 974164.6s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.053s, learning 0.229s)
               Value function loss: 209881.4156
                    Surrogate loss: -0.0058
             Mean action noise std: 0.71
                       Mean reward: 3666.36
               Mean episode length: 70.18
                  Mean reward/step: 53.65
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2974
Mean episode consecutive_successes: 15.1109
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 8.28s
                        Total time: 35528.64s
                               ETA: 974104.8s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.359s, learning 0.189s)
               Value function loss: 211159.8188
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 3788.82
               Mean episode length: 71.60
                  Mean reward/step: 55.38
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3462
Mean episode consecutive_successes: 15.0911
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 8.55s
                        Total time: 35537.19s
                               ETA: 974052.2s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.437s, learning 0.257s)
               Value function loss: 205058.1773
                    Surrogate loss: -0.0099
             Mean action noise std: 0.71
                       Mean reward: 3867.46
               Mean episode length: 72.22
                  Mean reward/step: 53.17
       Mean episode length/episode: 7.34
            Mean episode successes: 4.4453
Mean episode consecutive_successes: 15.0410
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 8.69s
                        Total time: 35545.89s
                               ETA: 974003.7s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.728s, learning 0.314s)
               Value function loss: 203421.3848
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 3850.86
               Mean episode length: 70.86
                  Mean reward/step: 56.21
       Mean episode length/episode: 7.25
            Mean episode successes: 4.5645
Mean episode consecutive_successes: 15.0121
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 9.04s
                        Total time: 35554.93s
                               ETA: 973964.8s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 1784 steps/s (collection: 8.915s, learning 0.266s)
               Value function loss: 224651.3258
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 4622.00
               Mean episode length: 71.25
                  Mean reward/step: 57.96
       Mean episode length/episode: 7.33
            Mean episode successes: 4.7896
Mean episode consecutive_successes: 15.0835
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 9.18s
                        Total time: 35564.11s
                               ETA: 973929.7s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.579s, learning 0.247s)
               Value function loss: 225479.8070
                    Surrogate loss: -0.0152
             Mean action noise std: 0.71
                       Mean reward: 4364.41
               Mean episode length: 70.90
                  Mean reward/step: 58.37
       Mean episode length/episode: 7.31
            Mean episode successes: 4.7817
Mean episode consecutive_successes: 15.0962
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 8.83s
                        Total time: 35572.94s
                               ETA: 973884.8s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.578s, learning 0.212s)
               Value function loss: 268111.6023
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 4183.92
               Mean episode length: 70.43
                  Mean reward/step: 57.49
       Mean episode length/episode: 7.23
            Mean episode successes: 4.6968
Mean episode consecutive_successes: 15.1590
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 8.79s
                        Total time: 35581.73s
                               ETA: 973839.0s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1787 steps/s (collection: 8.771s, learning 0.397s)
               Value function loss: 218088.1133
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 4304.67
               Mean episode length: 71.61
                  Mean reward/step: 56.17
       Mean episode length/episode: 7.35
            Mean episode successes: 4.8506
Mean episode consecutive_successes: 15.2403
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 9.17s
                        Total time: 35590.89s
                               ETA: 973803.6s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.653s, learning 0.205s)
               Value function loss: 197689.2336
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 4141.33
               Mean episode length: 71.06
                  Mean reward/step: 53.83
       Mean episode length/episode: 7.23
            Mean episode successes: 4.5435
Mean episode consecutive_successes: 15.3275
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 8.86s
                        Total time: 35599.75s
                               ETA: 973759.7s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.479s, learning 0.412s)
               Value function loss: 220519.5875
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 3866.09
               Mean episode length: 70.66
                  Mean reward/step: 53.13
       Mean episode length/episode: 7.27
            Mean episode successes: 4.5283
Mean episode consecutive_successes: 15.2933
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 8.89s
                        Total time: 35608.64s
                               ETA: 973716.7s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.730s, learning 0.255s)
               Value function loss: 234453.3379
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 4494.66
               Mean episode length: 71.14
                  Mean reward/step: 53.45
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3882
Mean episode consecutive_successes: 15.3886
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 8.99s
                        Total time: 35617.63s
                               ETA: 973676.4s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.633s, learning 0.228s)
               Value function loss: 192089.5379
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 3618.80
               Mean episode length: 69.99
                  Mean reward/step: 53.69
       Mean episode length/episode: 7.22
            Mean episode successes: 4.2178
Mean episode consecutive_successes: 15.3539
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 8.86s
                        Total time: 35626.49s
                               ETA: 973632.6s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.407s, learning 0.346s)
               Value function loss: 203678.4891
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 4146.08
               Mean episode length: 69.97
                  Mean reward/step: 51.84
       Mean episode length/episode: 7.30
            Mean episode successes: 4.2705
Mean episode consecutive_successes: 15.3192
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 8.75s
                        Total time: 35635.24s
                               ETA: 973585.9s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.612s, learning 0.242s)
               Value function loss: 234136.4820
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 3469.95
               Mean episode length: 69.91
                  Mean reward/step: 52.18
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3428
Mean episode consecutive_successes: 15.2217
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 8.85s
                        Total time: 35644.10s
                               ETA: 973542.0s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 1806 steps/s (collection: 8.775s, learning 0.294s)
               Value function loss: 273807.8758
                    Surrogate loss: -0.0137
             Mean action noise std: 0.71
                       Mean reward: 3655.79
               Mean episode length: 70.77
                  Mean reward/step: 53.39
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3071
Mean episode consecutive_successes: 15.2355
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 9.07s
                        Total time: 35653.17s
                               ETA: 973504.0s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.528s, learning 0.211s)
               Value function loss: 255375.7422
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 3869.34
               Mean episode length: 71.16
                  Mean reward/step: 56.36
       Mean episode length/episode: 7.21
            Mean episode successes: 4.2974
Mean episode consecutive_successes: 15.2119
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 8.74s
                        Total time: 35661.90s
                               ETA: 973457.0s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.436s, learning 0.169s)
               Value function loss: 254206.8781
                    Surrogate loss: -0.0103
             Mean action noise std: 0.71
                       Mean reward: 3705.93
               Mean episode length: 69.13
                  Mean reward/step: 56.99
       Mean episode length/episode: 7.35
            Mean episode successes: 4.6489
Mean episode consecutive_successes: 15.1669
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 8.61s
                        Total time: 35670.51s
                               ETA: 973406.3s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.500s, learning 0.166s)
               Value function loss: 292370.8281
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 4160.51
               Mean episode length: 72.90
                  Mean reward/step: 57.83
       Mean episode length/episode: 7.30
            Mean episode successes: 4.7568
Mean episode consecutive_successes: 15.2040
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 8.67s
                        Total time: 35679.18s
                               ETA: 973357.4s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.373s, learning 0.352s)
               Value function loss: 219676.7660
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 3778.39
               Mean episode length: 70.89
                  Mean reward/step: 57.18
       Mean episode length/episode: 7.21
            Mean episode successes: 4.6655
Mean episode consecutive_successes: 15.2354
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 8.73s
                        Total time: 35687.90s
                               ETA: 973310.1s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.470s, learning 0.210s)
               Value function loss: 176138.7965
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 4314.92
               Mean episode length: 72.27
                  Mean reward/step: 54.87
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6968
Mean episode consecutive_successes: 15.3166
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 8.68s
                        Total time: 35696.58s
                               ETA: 973261.5s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.329s, learning 0.173s)
               Value function loss: 177180.8102
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 4276.90
               Mean episode length: 71.30
                  Mean reward/step: 57.22
       Mean episode length/episode: 7.21
            Mean episode successes: 4.5137
Mean episode consecutive_successes: 15.3739
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 8.50s
                        Total time: 35705.08s
                               ETA: 973208.2s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.264s, learning 0.179s)
               Value function loss: 172348.4758
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 4446.77
               Mean episode length: 70.80
                  Mean reward/step: 54.70
       Mean episode length/episode: 7.33
            Mean episode successes: 4.5874
Mean episode consecutive_successes: 15.4302
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 8.44s
                        Total time: 35713.53s
                               ETA: 973153.3s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.334s, learning 0.188s)
               Value function loss: 187448.7207
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 3627.83
               Mean episode length: 70.49
                  Mean reward/step: 52.76
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3877
Mean episode consecutive_successes: 15.4299
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 8.52s
                        Total time: 35722.05s
                               ETA: 973100.5s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.687s, learning 0.166s)
               Value function loss: 196562.2445
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 2930.48
               Mean episode length: 70.62
                  Mean reward/step: 56.90
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6592
Mean episode consecutive_successes: 15.2207
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 8.85s
                        Total time: 35730.90s
                               ETA: 973056.8s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.448s, learning 0.182s)
               Value function loss: 218003.8758
                    Surrogate loss: -0.0123
             Mean action noise std: 0.71
                       Mean reward: 4553.18
               Mean episode length: 72.42
                  Mean reward/step: 56.87
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5830
Mean episode consecutive_successes: 15.4968
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 8.63s
                        Total time: 35739.53s
                               ETA: 973007.0s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.429s, learning 0.173s)
               Value function loss: 231859.3109
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 3633.09
               Mean episode length: 69.74
                  Mean reward/step: 54.13
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3740
Mean episode consecutive_successes: 15.6127
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 8.60s
                        Total time: 35748.13s
                               ETA: 972956.5s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.545s, learning 0.172s)
               Value function loss: 297983.7035
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 3726.41
               Mean episode length: 70.91
                  Mean reward/step: 54.05
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5859
Mean episode consecutive_successes: 15.4989
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 8.72s
                        Total time: 35756.85s
                               ETA: 972909.1s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.571s, learning 0.175s)
               Value function loss: 205868.6105
                    Surrogate loss: 0.0005
             Mean action noise std: 0.71
                       Mean reward: 4123.67
               Mean episode length: 70.67
                  Mean reward/step: 54.73
       Mean episode length/episode: 7.24
            Mean episode successes: 4.5532
Mean episode consecutive_successes: 15.5139
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 8.75s
                        Total time: 35765.60s
                               ETA: 972862.6s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.383s, learning 0.163s)
               Value function loss: 154858.9285
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 3905.47
               Mean episode length: 70.28
                  Mean reward/step: 54.50
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3223
Mean episode consecutive_successes: 15.6415
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 8.55s
                        Total time: 35774.14s
                               ETA: 972810.6s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.467s, learning 0.159s)
               Value function loss: 173652.7043
                    Surrogate loss: -0.0182
             Mean action noise std: 0.71
                       Mean reward: 3864.01
               Mean episode length: 71.26
                  Mean reward/step: 54.50
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4683
Mean episode consecutive_successes: 15.5033
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 8.63s
                        Total time: 35782.77s
                               ETA: 972760.8s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.525s, learning 0.202s)
               Value function loss: 174870.8773
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 3633.18
               Mean episode length: 71.00
                  Mean reward/step: 51.29
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3579
Mean episode consecutive_successes: 15.4868
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 8.73s
                        Total time: 35791.50s
                               ETA: 972713.8s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.662s, learning 0.190s)
               Value function loss: 192053.4660
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 3608.27
               Mean episode length: 70.15
                  Mean reward/step: 51.87
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2588
Mean episode consecutive_successes: 15.5040
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 8.85s
                        Total time: 35800.35s
                               ETA: 972670.2s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.211s, learning 0.228s)
               Value function loss: 198615.4102
                    Surrogate loss: -0.0170
             Mean action noise std: 0.71
                       Mean reward: 3861.95
               Mean episode length: 72.63
                  Mean reward/step: 52.56
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2637
Mean episode consecutive_successes: 15.4962
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 8.44s
                        Total time: 35808.79s
                               ETA: 972615.5s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 1810 steps/s (collection: 8.873s, learning 0.177s)
               Value function loss: 194856.7945
                    Surrogate loss: -0.0126
             Mean action noise std: 0.71
                       Mean reward: 4366.30
               Mean episode length: 70.78
                  Mean reward/step: 53.57
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2661
Mean episode consecutive_successes: 15.4891
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 9.05s
                        Total time: 35817.84s
                               ETA: 972577.3s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.751s, learning 0.202s)
               Value function loss: 196720.1777
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 2670.45
               Mean episode length: 68.44
                  Mean reward/step: 53.72
       Mean episode length/episode: 7.28
            Mean episode successes: 4.2817
Mean episode consecutive_successes: 15.3120
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 8.95s
                        Total time: 35826.79s
                               ETA: 972536.5s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.512s, learning 0.174s)
               Value function loss: 204494.9723
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 3949.03
               Mean episode length: 72.17
                  Mean reward/step: 53.31
       Mean episode length/episode: 7.32
            Mean episode successes: 4.4336
Mean episode consecutive_successes: 15.2732
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 8.69s
                        Total time: 35835.48s
                               ETA: 972488.5s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.552s, learning 0.176s)
               Value function loss: 209315.4750
                    Surrogate loss: -0.0109
             Mean action noise std: 0.71
                       Mean reward: 4219.50
               Mean episode length: 70.47
                  Mean reward/step: 53.70
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4121
Mean episode consecutive_successes: 15.2957
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 8.73s
                        Total time: 35844.20s
                               ETA: 972441.7s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.883s, learning 0.205s)
               Value function loss: 204259.5312
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 3873.54
               Mean episode length: 70.95
                  Mean reward/step: 55.59
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4443
Mean episode consecutive_successes: 15.2531
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 9.09s
                        Total time: 35853.29s
                               ETA: 972404.6s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.534s, learning 0.178s)
               Value function loss: 200374.8840
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 3350.40
               Mean episode length: 70.57
                  Mean reward/step: 53.47
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4443
Mean episode consecutive_successes: 15.2379
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 8.71s
                        Total time: 35862.01s
                               ETA: 972357.4s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.240s, learning 0.171s)
               Value function loss: 201743.4395
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 4377.24
               Mean episode length: 72.13
                  Mean reward/step: 51.76
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2988
Mean episode consecutive_successes: 15.3425
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 8.41s
                        Total time: 35870.42s
                               ETA: 972302.0s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.432s, learning 0.248s)
               Value function loss: 193883.2484
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 2858.51
               Mean episode length: 69.11
                  Mean reward/step: 51.22
       Mean episode length/episode: 7.26
            Mean episode successes: 4.2598
Mean episode consecutive_successes: 15.2142
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 8.68s
                        Total time: 35879.10s
                               ETA: 972253.9s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.339s, learning 0.227s)
               Value function loss: 224861.6727
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 4417.07
               Mean episode length: 71.91
                  Mean reward/step: 54.46
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3613
Mean episode consecutive_successes: 15.2247
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 8.57s
                        Total time: 35887.66s
                               ETA: 972202.8s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.722s, learning 0.171s)
               Value function loss: 198328.7543
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 4211.49
               Mean episode length: 72.00
                  Mean reward/step: 54.18
       Mean episode length/episode: 7.32
            Mean episode successes: 4.2163
Mean episode consecutive_successes: 15.4253
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 8.89s
                        Total time: 35896.55s
                               ETA: 972160.5s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.613s, learning 0.190s)
               Value function loss: 204616.0410
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 3447.63
               Mean episode length: 70.21
                  Mean reward/step: 53.82
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4443
Mean episode consecutive_successes: 15.2276
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 8.80s
                        Total time: 35905.36s
                               ETA: 972115.9s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.696s, learning 0.193s)
               Value function loss: 215198.0824
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 3505.47
               Mean episode length: 70.97
                  Mean reward/step: 54.56
       Mean episode length/episode: 7.28
            Mean episode successes: 4.4893
Mean episode consecutive_successes: 15.1693
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 8.89s
                        Total time: 35914.25s
                               ETA: 972073.5s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.546s, learning 0.325s)
               Value function loss: 218462.7324
                    Surrogate loss: -0.0063
             Mean action noise std: 0.71
                       Mean reward: 4162.02
               Mean episode length: 71.10
                  Mean reward/step: 54.58
       Mean episode length/episode: 7.36
            Mean episode successes: 4.7285
Mean episode consecutive_successes: 15.1615
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 8.87s
                        Total time: 35923.12s
                               ETA: 972030.8s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 1824 steps/s (collection: 8.689s, learning 0.292s)
               Value function loss: 207520.5773
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 3908.61
               Mean episode length: 70.52
                  Mean reward/step: 52.50
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3833
Mean episode consecutive_successes: 15.2728
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 8.98s
                        Total time: 35932.10s
                               ETA: 971991.0s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.165s, learning 0.179s)
               Value function loss: 205127.2848
                    Surrogate loss: -0.0138
             Mean action noise std: 0.71
                       Mean reward: 3791.90
               Mean episode length: 70.97
                  Mean reward/step: 52.54
       Mean episode length/episode: 7.35
            Mean episode successes: 4.5352
Mean episode consecutive_successes: 15.1894
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 8.34s
                        Total time: 35940.44s
                               ETA: 971934.0s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.319s, learning 0.316s)
               Value function loss: 217261.8887
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 3990.75
               Mean episode length: 69.64
                  Mean reward/step: 53.74
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2998
Mean episode consecutive_successes: 15.2023
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 8.63s
                        Total time: 35949.08s
                               ETA: 971884.9s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.365s, learning 0.166s)
               Value function loss: 247284.0449
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 3310.39
               Mean episode length: 71.15
                  Mean reward/step: 55.03
       Mean episode length/episode: 7.35
            Mean episode successes: 4.6299
Mean episode consecutive_successes: 15.0741
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 8.53s
                        Total time: 35957.61s
                               ETA: 971833.0s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.601s, learning 0.189s)
               Value function loss: 253445.0969
                    Surrogate loss: 0.0031
             Mean action noise std: 0.71
                       Mean reward: 3588.33
               Mean episode length: 71.37
                  Mean reward/step: 54.58
       Mean episode length/episode: 7.22
            Mean episode successes: 4.5151
Mean episode consecutive_successes: 15.1169
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 8.79s
                        Total time: 35966.40s
                               ETA: 971788.1s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.531s, learning 0.177s)
               Value function loss: 211869.1914
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 3796.49
               Mean episode length: 70.63
                  Mean reward/step: 54.30
       Mean episode length/episode: 7.25
            Mean episode successes: 4.2803
Mean episode consecutive_successes: 15.2442
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 8.71s
                        Total time: 35975.11s
                               ETA: 971741.0s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.656s, learning 0.198s)
               Value function loss: 199944.6121
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 4141.82
               Mean episode length: 70.77
                  Mean reward/step: 55.41
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4106
Mean episode consecutive_successes: 15.2383
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 8.85s
                        Total time: 35983.96s
                               ETA: 971697.9s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.567s, learning 0.157s)
               Value function loss: 210421.9328
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 3454.89
               Mean episode length: 70.46
                  Mean reward/step: 54.12
       Mean episode length/episode: 7.21
            Mean episode successes: 4.3101
Mean episode consecutive_successes: 15.1711
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 8.72s
                        Total time: 35992.68s
                               ETA: 971651.3s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.538s, learning 0.170s)
               Value function loss: 204653.8926
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 3594.26
               Mean episode length: 69.65
                  Mean reward/step: 51.63
       Mean episode length/episode: 7.28
            Mean episode successes: 4.3149
Mean episode consecutive_successes: 15.1599
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 8.71s
                        Total time: 36001.39s
                               ETA: 971604.3s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.513s, learning 0.171s)
               Value function loss: 200036.9844
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 3866.84
               Mean episode length: 71.15
                  Mean reward/step: 53.93
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3491
Mean episode consecutive_successes: 15.1874
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 8.68s
                        Total time: 36010.08s
                               ETA: 971556.7s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.643s, learning 0.162s)
               Value function loss: 191624.5613
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 3056.97
               Mean episode length: 70.41
                  Mean reward/step: 54.77
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4829
Mean episode consecutive_successes: 15.1339
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 8.80s
                        Total time: 36018.88s
                               ETA: 971512.3s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.297s, learning 0.179s)
               Value function loss: 202612.7324
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 3926.84
               Mean episode length: 71.67
                  Mean reward/step: 54.67
       Mean episode length/episode: 7.31
            Mean episode successes: 4.5732
Mean episode consecutive_successes: 15.1743
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 8.48s
                        Total time: 36027.36s
                               ETA: 971459.1s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 1773 steps/s (collection: 9.054s, learning 0.182s)
               Value function loss: 195811.8617
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 2906.33
               Mean episode length: 70.08
                  Mean reward/step: 57.31
       Mean episode length/episode: 7.25
            Mean episode successes: 4.6963
Mean episode consecutive_successes: 15.0623
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 9.24s
                        Total time: 36036.59s
                               ETA: 971426.5s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.375s, learning 0.184s)
               Value function loss: 206954.1781
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 4111.34
               Mean episode length: 71.07
                  Mean reward/step: 57.39
       Mean episode length/episode: 7.30
            Mean episode successes: 4.7832
Mean episode consecutive_successes: 15.1744
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 8.56s
                        Total time: 36045.15s
                               ETA: 971375.5s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.285s, learning 0.171s)
               Value function loss: 227977.0652
                    Surrogate loss: -0.0065
             Mean action noise std: 0.71
                       Mean reward: 4256.56
               Mean episode length: 71.20
                  Mean reward/step: 54.73
       Mean episode length/episode: 7.32
            Mean episode successes: 4.6763
Mean episode consecutive_successes: 15.2893
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 8.46s
                        Total time: 36053.61s
                               ETA: 971321.9s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.749s, learning 0.157s)
               Value function loss: 210588.4980
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 3923.87
               Mean episode length: 70.85
                  Mean reward/step: 51.34
       Mean episode length/episode: 7.24
            Mean episode successes: 4.4561
Mean episode consecutive_successes: 15.2813
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 8.91s
                        Total time: 36062.51s
                               ETA: 971280.3s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.598s, learning 0.189s)
               Value function loss: 212626.5645
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 3806.71
               Mean episode length: 70.47
                  Mean reward/step: 51.44
       Mean episode length/episode: 7.27
            Mean episode successes: 4.3306
Mean episode consecutive_successes: 15.2046
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 8.79s
                        Total time: 36071.30s
                               ETA: 971235.6s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.325s, learning 0.186s)
               Value function loss: 194636.8898
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 3673.26
               Mean episode length: 70.67
                  Mean reward/step: 50.68
       Mean episode length/episode: 7.25
            Mean episode successes: 4.1597
Mean episode consecutive_successes: 15.2493
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 8.51s
                        Total time: 36079.81s
                               ETA: 971183.5s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.507s, learning 0.176s)
               Value function loss: 196942.3805
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 3711.05
               Mean episode length: 70.13
                  Mean reward/step: 50.74
       Mean episode length/episode: 7.30
            Mean episode successes: 4.1875
Mean episode consecutive_successes: 15.2552
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 8.68s
                        Total time: 36088.49s
                               ETA: 971136.0s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.533s, learning 0.165s)
               Value function loss: 214642.0438
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 2692.39
               Mean episode length: 70.04
                  Mean reward/step: 53.02
       Mean episode length/episode: 7.26
            Mean episode successes: 4.1660
Mean episode consecutive_successes: 15.1134
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 8.70s
                        Total time: 36097.19s
                               ETA: 971089.0s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.369s, learning 0.178s)
               Value function loss: 205640.1262
                    Surrogate loss: -0.0128
             Mean action noise std: 0.71
                       Mean reward: 3505.13
               Mean episode length: 69.93
                  Mean reward/step: 55.02
       Mean episode length/episode: 7.27
            Mean episode successes: 4.2944
Mean episode consecutive_successes: 15.0949
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 8.55s
                        Total time: 36105.74s
                               ETA: 971037.9s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.300s, learning 0.180s)
               Value function loss: 238120.4141
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 3530.22
               Mean episode length: 70.79
                  Mean reward/step: 56.73
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4556
Mean episode consecutive_successes: 15.0171
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 8.48s
                        Total time: 36114.22s
                               ETA: 970985.1s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.600s, learning 0.275s)
               Value function loss: 240240.5699
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 3827.68
               Mean episode length: 69.87
                  Mean reward/step: 55.88
       Mean episode length/episode: 7.30
            Mean episode successes: 4.5981
Mean episode consecutive_successes: 15.0357
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 8.87s
                        Total time: 36123.09s
                               ETA: 970942.8s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.588s, learning 0.263s)
               Value function loss: 214053.0008
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 3691.27
               Mean episode length: 70.75
                  Mean reward/step: 57.41
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8735
Mean episode consecutive_successes: 15.0366
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 8.85s
                        Total time: 36131.94s
                               ETA: 970900.0s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.536s, learning 0.171s)
               Value function loss: 223182.9547
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 5012.54
               Mean episode length: 71.63
                  Mean reward/step: 58.13
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6592
Mean episode consecutive_successes: 15.2985
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 8.71s
                        Total time: 36140.65s
                               ETA: 970853.3s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.155s, learning 0.174s)
               Value function loss: 208252.8754
                    Surrogate loss: -0.0102
             Mean action noise std: 0.71
                       Mean reward: 3495.79
               Mean episode length: 69.72
                  Mean reward/step: 53.39
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4766
Mean episode consecutive_successes: 15.3704
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 8.33s
                        Total time: 36148.98s
                               ETA: 970796.4s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.567s, learning 0.177s)
               Value function loss: 224649.1609
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 3546.28
               Mean episode length: 71.41
                  Mean reward/step: 53.57
       Mean episode length/episode: 7.23
            Mean episode successes: 4.4595
Mean episode consecutive_successes: 15.3032
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 8.74s
                        Total time: 36157.72s
                               ETA: 970750.8s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.433s, learning 0.235s)
               Value function loss: 291444.0281
                    Surrogate loss: -0.0109
             Mean action noise std: 0.71
                       Mean reward: 4160.95
               Mean episode length: 70.35
                  Mean reward/step: 55.83
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4453
Mean episode consecutive_successes: 15.3675
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 8.67s
                        Total time: 36166.39s
                               ETA: 970703.1s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.446s, learning 0.157s)
               Value function loss: 267823.4914
                    Surrogate loss: -0.0056
             Mean action noise std: 0.71
                       Mean reward: 4505.90
               Mean episode length: 72.55
                  Mean reward/step: 55.88
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3066
Mean episode consecutive_successes: 15.5058
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 8.60s
                        Total time: 36174.99s
                               ETA: 970653.7s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.498s, learning 0.255s)
               Value function loss: 198093.6801
                    Surrogate loss: -0.0120
             Mean action noise std: 0.71
                       Mean reward: 3860.61
               Mean episode length: 70.61
                  Mean reward/step: 57.48
       Mean episode length/episode: 7.39
            Mean episode successes: 4.5474
Mean episode consecutive_successes: 15.5277
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 8.75s
                        Total time: 36183.75s
                               ETA: 970608.4s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.553s, learning 0.158s)
               Value function loss: 177848.8551
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 3621.65
               Mean episode length: 71.23
                  Mean reward/step: 58.57
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6294
Mean episode consecutive_successes: 15.5021
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 8.71s
                        Total time: 36192.46s
                               ETA: 970562.0s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.674s, learning 0.222s)
               Value function loss: 178075.4664
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 4319.41
               Mean episode length: 71.48
                  Mean reward/step: 59.98
       Mean episode length/episode: 7.25
            Mean episode successes: 4.7002
Mean episode consecutive_successes: 15.5481
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 8.90s
                        Total time: 36201.36s
                               ETA: 970520.5s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.581s, learning 0.174s)
               Value function loss: 188346.7930
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 4078.46
               Mean episode length: 70.39
                  Mean reward/step: 60.97
       Mean episode length/episode: 7.27
            Mean episode successes: 4.8784
Mean episode consecutive_successes: 15.6135
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 8.75s
                        Total time: 36210.11s
                               ETA: 970475.2s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.577s, learning 0.282s)
               Value function loss: 182565.9375
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 4269.35
               Mean episode length: 71.12
                  Mean reward/step: 60.40
       Mean episode length/episode: 7.28
            Mean episode successes: 4.9009
Mean episode consecutive_successes: 15.7277
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 8.86s
                        Total time: 36218.97s
                               ETA: 970432.8s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.754s, learning 0.171s)
               Value function loss: 208358.3445
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 3771.43
               Mean episode length: 71.02
                  Mean reward/step: 59.08
       Mean episode length/episode: 7.29
            Mean episode successes: 4.9658
Mean episode consecutive_successes: 15.7518
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 8.93s
                        Total time: 36227.89s
                               ETA: 970392.2s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.454s, learning 0.174s)
               Value function loss: 207803.8270
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 4435.38
               Mean episode length: 71.85
                  Mean reward/step: 57.83
       Mean episode length/episode: 7.24
            Mean episode successes: 4.6948
Mean episode consecutive_successes: 15.8752
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 8.63s
                        Total time: 36236.52s
                               ETA: 970343.6s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.181s, learning 0.191s)
               Value function loss: 216283.0871
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 3841.20
               Mean episode length: 71.28
                  Mean reward/step: 56.24
       Mean episode length/episode: 7.26
            Mean episode successes: 4.5894
Mean episode consecutive_successes: 15.8623
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 8.37s
                        Total time: 36244.89s
                               ETA: 970288.2s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.460s, learning 0.302s)
               Value function loss: 224464.7453
                    Surrogate loss: -0.0037
             Mean action noise std: 0.71
                       Mean reward: 4747.29
               Mean episode length: 71.20
                  Mean reward/step: 53.52
       Mean episode length/episode: 7.30
            Mean episode successes: 4.3828
Mean episode consecutive_successes: 15.9507
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 8.76s
                        Total time: 36253.66s
                               ETA: 970243.3s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.771s, learning 0.192s)
               Value function loss: 202613.0945
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 3765.88
               Mean episode length: 71.90
                  Mean reward/step: 53.47
       Mean episode length/episode: 7.36
            Mean episode successes: 4.4414
Mean episode consecutive_successes: 15.9317
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 8.96s
                        Total time: 36262.62s
                               ETA: 970203.7s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.602s, learning 0.161s)
               Value function loss: 210692.6719
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 3949.16
               Mean episode length: 70.95
                  Mean reward/step: 58.28
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6626
Mean episode consecutive_successes: 15.9133
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 8.76s
                        Total time: 36271.38s
                               ETA: 970158.8s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.358s, learning 0.168s)
               Value function loss: 271722.4207
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 3893.99
               Mean episode length: 70.56
                  Mean reward/step: 58.30
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6235
Mean episode consecutive_successes: 15.9321
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 8.53s
                        Total time: 36279.91s
                               ETA: 970107.6s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.454s, learning 0.171s)
               Value function loss: 229312.7445
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 4094.06
               Mean episode length: 71.15
                  Mean reward/step: 57.52
       Mean episode length/episode: 7.32
            Mean episode successes: 4.8135
Mean episode consecutive_successes: 15.8683
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 8.63s
                        Total time: 36288.53s
                               ETA: 970059.1s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.548s, learning 0.174s)
               Value function loss: 245962.0648
                    Surrogate loss: -0.0125
             Mean action noise std: 0.71
                       Mean reward: 4079.44
               Mean episode length: 71.74
                  Mean reward/step: 58.71
       Mean episode length/episode: 7.30
            Mean episode successes: 4.9229
Mean episode consecutive_successes: 15.8687
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 8.72s
                        Total time: 36297.26s
                               ETA: 970013.2s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1821 steps/s (collection: 8.823s, learning 0.172s)
               Value function loss: 279392.3609
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 3572.69
               Mean episode length: 68.77
                  Mean reward/step: 59.20
       Mean episode length/episode: 7.23
            Mean episode successes: 4.9468
Mean episode consecutive_successes: 15.8797
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 8.99s
                        Total time: 36306.25s
                               ETA: 969974.6s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.353s, learning 0.170s)
               Value function loss: 300091.3879
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 4205.59
               Mean episode length: 72.09
                  Mean reward/step: 60.44
       Mean episode length/episode: 7.26
            Mean episode successes: 4.9170
Mean episode consecutive_successes: 15.9651
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 8.52s
                        Total time: 36314.77s
                               ETA: 969923.4s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.166s, learning 0.184s)
               Value function loss: 242297.8051
                    Surrogate loss: 0.0226
             Mean action noise std: 0.71
                       Mean reward: 4642.51
               Mean episode length: 72.67
                  Mean reward/step: 56.23
       Mean episode length/episode: 7.25
            Mean episode successes: 4.5640
Mean episode consecutive_successes: 16.1204
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 8.35s
                        Total time: 36323.12s
                               ETA: 969867.6s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.458s, learning 0.195s)
               Value function loss: 182525.8016
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 4141.79
               Mean episode length: 71.31
                  Mean reward/step: 54.65
       Mean episode length/episode: 7.33
            Mean episode successes: 4.4751
Mean episode consecutive_successes: 16.1075
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 8.65s
                        Total time: 36331.78s
                               ETA: 969820.0s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.404s, learning 0.166s)
               Value function loss: 181256.2930
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 3792.45
               Mean episode length: 69.02
                  Mean reward/step: 53.46
       Mean episode length/episode: 7.29
            Mean episode successes: 4.3862
Mean episode consecutive_successes: 16.0620
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 8.57s
                        Total time: 36340.35s
                               ETA: 969770.1s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.521s, learning 0.179s)
               Value function loss: 205018.4793
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 4076.46
               Mean episode length: 70.51
                  Mean reward/step: 55.19
       Mean episode length/episode: 7.31
            Mean episode successes: 4.4849
Mean episode consecutive_successes: 16.0535
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 8.70s
                        Total time: 36349.05s
                               ETA: 969723.8s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 1836 steps/s (collection: 8.750s, learning 0.169s)
               Value function loss: 211798.0746
                    Surrogate loss: -0.0119
             Mean action noise std: 0.71
                       Mean reward: 4047.96
               Mean episode length: 69.52
                  Mean reward/step: 56.52
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3555
Mean episode consecutive_successes: 16.0462
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 8.92s
                        Total time: 36357.97s
                               ETA: 969683.3s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.703s, learning 0.174s)
               Value function loss: 206478.2691
                    Surrogate loss: -0.0112
             Mean action noise std: 0.71
                       Mean reward: 3761.28
               Mean episode length: 70.26
                  Mean reward/step: 56.98
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5337
Mean episode consecutive_successes: 15.9083
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 8.88s
                        Total time: 36366.84s
                               ETA: 969641.7s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.601s, learning 0.175s)
               Value function loss: 220213.3266
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 4001.29
               Mean episode length: 70.46
                  Mean reward/step: 58.12
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8862
Mean episode consecutive_successes: 15.7837
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 8.78s
                        Total time: 36375.62s
                               ETA: 969597.4s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.279s, learning 0.343s)
               Value function loss: 218303.8621
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 4115.52
               Mean episode length: 69.72
                  Mean reward/step: 57.37
       Mean episode length/episode: 7.32
            Mean episode successes: 4.9023
Mean episode consecutive_successes: 15.8570
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 8.62s
                        Total time: 36384.24s
                               ETA: 969549.0s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.452s, learning 0.161s)
               Value function loss: 238802.6742
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 3986.33
               Mean episode length: 70.32
                  Mean reward/step: 57.53
       Mean episode length/episode: 7.19
            Mean episode successes: 4.7759
Mean episode consecutive_successes: 15.8108
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 8.61s
                        Total time: 36392.85s
                               ETA: 969500.4s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.478s, learning 0.242s)
               Value function loss: 225917.3887
                    Surrogate loss: -0.0089
             Mean action noise std: 0.71
                       Mean reward: 4126.74
               Mean episode length: 70.77
                  Mean reward/step: 56.18
       Mean episode length/episode: 7.23
            Mean episode successes: 4.6025
Mean episode consecutive_successes: 15.8441
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 8.72s
                        Total time: 36401.57s
                               ETA: 969454.7s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.451s, learning 0.164s)
               Value function loss: 255174.5781
                    Surrogate loss: -0.0046
             Mean action noise std: 0.71
                       Mean reward: 4567.16
               Mean episode length: 71.51
                  Mean reward/step: 58.30
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6230
Mean episode consecutive_successes: 15.9142
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 8.62s
                        Total time: 36410.19s
                               ETA: 969406.2s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.565s, learning 0.171s)
               Value function loss: 288070.6859
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 3545.64
               Mean episode length: 69.87
                  Mean reward/step: 56.41
       Mean episode length/episode: 7.35
            Mean episode successes: 4.7393
Mean episode consecutive_successes: 15.8885
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 8.74s
                        Total time: 36418.93s
                               ETA: 969361.0s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.334s, learning 0.245s)
               Value function loss: 199577.6516
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 4038.79
               Mean episode length: 71.59
                  Mean reward/step: 57.86
       Mean episode length/episode: 7.31
            Mean episode successes: 4.9312
Mean episode consecutive_successes: 15.8158
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 8.58s
                        Total time: 36427.50s
                               ETA: 969311.5s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.479s, learning 0.158s)
               Value function loss: 244631.5578
                    Surrogate loss: -0.0092
             Mean action noise std: 0.71
                       Mean reward: 4810.34
               Mean episode length: 72.85
                  Mean reward/step: 59.10
       Mean episode length/episode: 7.26
            Mean episode successes: 4.8535
Mean episode consecutive_successes: 15.9765
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 8.64s
                        Total time: 36436.14s
                               ETA: 969263.7s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.558s, learning 0.159s)
               Value function loss: 173458.3199
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 3877.48
               Mean episode length: 69.59
                  Mean reward/step: 57.65
       Mean episode length/episode: 7.24
            Mean episode successes: 4.7236
Mean episode consecutive_successes: 15.9852
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 8.72s
                        Total time: 36444.86s
                               ETA: 969218.0s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.414s, learning 0.168s)
               Value function loss: 180448.9066
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 3841.29
               Mean episode length: 71.14
                  Mean reward/step: 58.43
       Mean episode length/episode: 7.35
            Mean episode successes: 4.9077
Mean episode consecutive_successes: 15.9995
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 8.58s
                        Total time: 36453.44s
                               ETA: 969168.8s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.576s, learning 0.168s)
               Value function loss: 209417.6324
                    Surrogate loss: -0.0076
             Mean action noise std: 0.71
                       Mean reward: 3407.65
               Mean episode length: 69.91
                  Mean reward/step: 61.27
       Mean episode length/episode: 7.31
            Mean episode successes: 5.2808
Mean episode consecutive_successes: 15.9026
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 8.74s
                        Total time: 36462.19s
                               ETA: 969123.9s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.461s, learning 0.168s)
               Value function loss: 244378.6562
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 4930.04
               Mean episode length: 71.75
                  Mean reward/step: 59.62
       Mean episode length/episode: 7.24
            Mean episode successes: 4.8730
Mean episode consecutive_successes: 16.1984
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 8.63s
                        Total time: 36470.81s
                               ETA: 969075.9s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.413s, learning 0.175s)
               Value function loss: 201953.0340
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 4538.47
               Mean episode length: 72.42
                  Mean reward/step: 57.89
       Mean episode length/episode: 7.26
            Mean episode successes: 4.7773
Mean episode consecutive_successes: 16.2167
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 8.59s
                        Total time: 36479.40s
                               ETA: 969026.9s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 1846 steps/s (collection: 8.707s, learning 0.168s)
               Value function loss: 216524.7938
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 4356.14
               Mean episode length: 70.48
                  Mean reward/step: 56.30
       Mean episode length/episode: 7.37
            Mean episode successes: 4.6650
Mean episode consecutive_successes: 16.3217
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 8.87s
                        Total time: 36488.28s
                               ETA: 968985.5s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.615s, learning 0.175s)
               Value function loss: 188146.3004
                    Surrogate loss: -0.0135
             Mean action noise std: 0.71
                       Mean reward: 3962.52
               Mean episode length: 69.96
                  Mean reward/step: 54.67
       Mean episode length/episode: 7.24
            Mean episode successes: 4.6040
Mean episode consecutive_successes: 16.2463
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 8.79s
                        Total time: 36497.07s
                               ETA: 968941.8s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.128s, learning 0.229s)
               Value function loss: 190110.2500
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 4489.73
               Mean episode length: 71.64
                  Mean reward/step: 54.20
       Mean episode length/episode: 7.27
            Mean episode successes: 4.4922
Mean episode consecutive_successes: 16.3010
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 8.36s
                        Total time: 36505.42s
                               ETA: 968886.7s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.755s, learning 0.175s)
               Value function loss: 194247.6477
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 3962.75
               Mean episode length: 70.23
                  Mean reward/step: 56.08
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5234
Mean episode consecutive_successes: 16.2073
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 8.93s
                        Total time: 36514.35s
                               ETA: 968846.9s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.413s, learning 0.169s)
               Value function loss: 201515.9398
                    Surrogate loss: -0.0073
             Mean action noise std: 0.71
                       Mean reward: 3522.23
               Mean episode length: 69.26
                  Mean reward/step: 56.52
       Mean episode length/episode: 7.22
            Mean episode successes: 4.5020
Mean episode consecutive_successes: 16.1533
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 8.58s
                        Total time: 36522.94s
                               ETA: 968797.8s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.348s, learning 0.170s)
               Value function loss: 192003.6539
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 4109.16
               Mean episode length: 70.94
                  Mean reward/step: 55.45
       Mean episode length/episode: 7.33
            Mean episode successes: 4.6587
Mean episode consecutive_successes: 16.1159
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 8.52s
                        Total time: 36531.45s
                               ETA: 968747.0s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.162s)
               Value function loss: 195838.9859
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 3582.78
               Mean episode length: 70.53
                  Mean reward/step: 53.84
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6226
Mean episode consecutive_successes: 16.0586
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 8.37s
                        Total time: 36539.83s
                               ETA: 968692.4s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.539s, learning 0.173s)
               Value function loss: 203262.6035
                    Surrogate loss: -0.0167
             Mean action noise std: 0.71
                       Mean reward: 4221.56
               Mean episode length: 70.93
                  Mean reward/step: 54.25
       Mean episode length/episode: 7.25
            Mean episode successes: 4.6265
Mean episode consecutive_successes: 16.0123
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 8.71s
                        Total time: 36548.54s
                               ETA: 968646.8s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.472s, learning 0.163s)
               Value function loss: 202996.8637
                    Surrogate loss: -0.0132
             Mean action noise std: 0.71
                       Mean reward: 3790.96
               Mean episode length: 71.11
                  Mean reward/step: 52.50
       Mean episode length/episode: 7.23
            Mean episode successes: 4.2656
Mean episode consecutive_successes: 15.9864
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 8.64s
                        Total time: 36557.17s
                               ETA: 968599.2s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.583s, learning 0.161s)
               Value function loss: 206642.2070
                    Surrogate loss: -0.0118
             Mean action noise std: 0.71
                       Mean reward: 3605.41
               Mean episode length: 70.68
                  Mean reward/step: 53.81
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2710
Mean episode consecutive_successes: 15.9118
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 8.74s
                        Total time: 36565.92s
                               ETA: 968554.6s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.451s, learning 0.189s)
               Value function loss: 219323.8262
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 4018.36
               Mean episode length: 72.35
                  Mean reward/step: 55.62
       Mean episode length/episode: 7.35
            Mean episode successes: 4.4609
Mean episode consecutive_successes: 15.7842
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 8.64s
                        Total time: 36574.56s
                               ETA: 968507.2s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.678s, learning 0.211s)
               Value function loss: 259056.8219
                    Surrogate loss: -0.0105
             Mean action noise std: 0.71
                       Mean reward: 3853.50
               Mean episode length: 70.48
                  Mean reward/step: 56.28
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5537
Mean episode consecutive_successes: 15.7815
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 8.89s
                        Total time: 36583.45s
                               ETA: 968466.3s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.740s, learning 0.169s)
               Value function loss: 256302.5699
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 4183.11
               Mean episode length: 70.24
                  Mean reward/step: 56.31
       Mean episode length/episode: 7.33
            Mean episode successes: 4.7588
Mean episode consecutive_successes: 15.7797
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 8.91s
                        Total time: 36592.36s
                               ETA: 968426.1s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.535s, learning 0.165s)
               Value function loss: 308483.1063
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 3903.33
               Mean episode length: 70.71
                  Mean reward/step: 56.23
       Mean episode length/episode: 7.22
            Mean episode successes: 4.5493
Mean episode consecutive_successes: 15.7268
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 8.70s
                        Total time: 36601.06s
                               ETA: 968380.3s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 1837 steps/s (collection: 8.741s, learning 0.173s)
               Value function loss: 225224.9625
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 3830.93
               Mean episode length: 70.38
                  Mean reward/step: 55.95
       Mean episode length/episode: 7.31
            Mean episode successes: 4.5200
Mean episode consecutive_successes: 15.7960
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 8.91s
                        Total time: 36609.97s
                               ETA: 968340.2s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.577s, learning 0.272s)
               Value function loss: 221294.2687
                    Surrogate loss: 0.0022
             Mean action noise std: 0.71
                       Mean reward: 3370.11
               Mean episode length: 69.70
                  Mean reward/step: 54.32
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5566
Mean episode consecutive_successes: 15.7220
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 8.85s
                        Total time: 36618.82s
                               ETA: 968298.5s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 1160 steps/s (collection: 13.894s, learning 0.224s)
               Value function loss: 208452.6875
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 3363.57
               Mean episode length: 71.64
                  Mean reward/step: 52.44
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4497
Mean episode consecutive_successes: 15.6584
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 14.12s
                        Total time: 36632.94s
                               ETA: 968396.0s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.340s, learning 0.199s)
               Value function loss: 210103.2227
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 4734.99
               Mean episode length: 71.64
                  Mean reward/step: 51.36
       Mean episode length/episode: 7.24
            Mean episode successes: 4.2910
Mean episode consecutive_successes: 15.7003
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 16.54s
                        Total time: 36649.48s
                               ETA: 968557.4s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 962 steps/s (collection: 16.846s, learning 0.170s)
               Value function loss: 195397.5203
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 4077.05
               Mean episode length: 71.52
                  Mean reward/step: 52.30
       Mean episode length/episode: 7.33
            Mean episode successes: 4.1802
Mean episode consecutive_successes: 15.7287
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 17.02s
                        Total time: 36666.49s
                               ETA: 968731.3s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 959 steps/s (collection: 16.900s, learning 0.172s)
               Value function loss: 195686.3168
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 3788.78
               Mean episode length: 70.46
                  Mean reward/step: 53.69
       Mean episode length/episode: 7.31
            Mean episode successes: 4.1831
Mean episode consecutive_successes: 15.7354
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 17.07s
                        Total time: 36683.57s
                               ETA: 968906.7s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 956 steps/s (collection: 16.961s, learning 0.170s)
               Value function loss: 186176.3223
                    Surrogate loss: -0.0130
             Mean action noise std: 0.71
                       Mean reward: 3312.91
               Mean episode length: 70.91
                  Mean reward/step: 57.33
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5288
Mean episode consecutive_successes: 15.5912
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 17.13s
                        Total time: 36700.70s
                               ETA: 969083.4s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.476s, learning 0.305s)
               Value function loss: 223099.4254
                    Surrogate loss: -0.0091
             Mean action noise std: 0.71
                       Mean reward: 3835.56
               Mean episode length: 70.32
                  Mean reward/step: 59.10
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7183
Mean episode consecutive_successes: 15.5457
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 16.78s
                        Total time: 36717.48s
                               ETA: 969250.9s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.816s, learning 0.222s)
               Value function loss: 224381.4031
                    Surrogate loss: -0.0094
             Mean action noise std: 0.71
                       Mean reward: 4258.91
               Mean episode length: 71.17
                  Mean reward/step: 61.09
       Mean episode length/episode: 7.29
            Mean episode successes: 5.0737
Mean episode consecutive_successes: 15.5240
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 17.04s
                        Total time: 36734.51s
                               ETA: 969425.0s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 957 steps/s (collection: 16.758s, learning 0.351s)
               Value function loss: 206145.3492
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 3813.26
               Mean episode length: 70.34
                  Mean reward/step: 59.12
       Mean episode length/episode: 7.28
            Mean episode successes: 5.1577
Mean episode consecutive_successes: 15.4914
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 17.11s
                        Total time: 36751.62s
                               ETA: 969600.8s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 951 steps/s (collection: 17.038s, learning 0.177s)
               Value function loss: 236704.5949
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 3857.73
               Mean episode length: 69.56
                  Mean reward/step: 59.67
       Mean episode length/episode: 7.29
            Mean episode successes: 4.9131
Mean episode consecutive_successes: 15.7094
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 17.21s
                        Total time: 36768.84s
                               ETA: 969779.4s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 943 steps/s (collection: 17.037s, learning 0.327s)
               Value function loss: 233516.8039
                    Surrogate loss: -0.0096
             Mean action noise std: 0.71
                       Mean reward: 4559.59
               Mean episode length: 72.03
                  Mean reward/step: 58.28
       Mean episode length/episode: 7.32
            Mean episode successes: 4.9409
Mean episode consecutive_successes: 15.8593
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 17.36s
                        Total time: 36786.20s
                               ETA: 969961.8s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.765s, learning 0.185s)
               Value function loss: 224284.6020
                    Surrogate loss: -0.0142
             Mean action noise std: 0.71
                       Mean reward: 4240.22
               Mean episode length: 72.72
                  Mean reward/step: 55.84
       Mean episode length/episode: 7.30
            Mean episode successes: 4.7305
Mean episode consecutive_successes: 15.9801
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 16.95s
                        Total time: 36803.15s
                               ETA: 970133.1s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 943 steps/s (collection: 17.131s, learning 0.242s)
               Value function loss: 226708.3895
                    Surrogate loss: -0.0075
             Mean action noise std: 0.71
                       Mean reward: 4409.19
               Mean episode length: 70.25
                  Mean reward/step: 58.12
       Mean episode length/episode: 7.26
            Mean episode successes: 4.5244
Mean episode consecutive_successes: 16.0841
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 17.37s
                        Total time: 36820.53s
                               ETA: 970315.6s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 965 steps/s (collection: 16.729s, learning 0.240s)
               Value function loss: 206745.9133
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 3973.95
               Mean episode length: 71.39
                  Mean reward/step: 56.90
       Mean episode length/episode: 7.25
            Mean episode successes: 4.3745
Mean episode consecutive_successes: 16.0918
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 16.97s
                        Total time: 36837.50s
                               ETA: 970487.2s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 955 steps/s (collection: 16.931s, learning 0.212s)
               Value function loss: 204491.0609
                    Surrogate loss: -0.0117
             Mean action noise std: 0.71
                       Mean reward: 3995.11
               Mean episode length: 72.37
                  Mean reward/step: 57.10
       Mean episode length/episode: 7.36
            Mean episode successes: 4.6387
Mean episode consecutive_successes: 16.0707
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 17.14s
                        Total time: 36854.64s
                               ETA: 970663.3s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.386s, learning 0.164s)
               Value function loss: 206651.6359
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 4170.03
               Mean episode length: 70.95
                  Mean reward/step: 55.76
       Mean episode length/episode: 7.25
            Mean episode successes: 4.7217
Mean episode consecutive_successes: 16.0056
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 16.55s
                        Total time: 36871.19s
                               ETA: 970823.7s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.466s, learning 0.358s)
               Value function loss: 189517.7398
                    Surrogate loss: -0.0051
             Mean action noise std: 0.71
                       Mean reward: 4251.35
               Mean episode length: 71.95
                  Mean reward/step: 56.93
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7075
Mean episode consecutive_successes: 16.0097
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 16.82s
                        Total time: 36888.01s
                               ETA: 970991.3s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.687s, learning 0.163s)
               Value function loss: 200843.9031
                    Surrogate loss: 0.0054
             Mean action noise std: 0.71
                       Mean reward: 4288.49
               Mean episode length: 69.79
                  Mean reward/step: 57.08
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4702
Mean episode consecutive_successes: 16.1388
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 16.85s
                        Total time: 36904.86s
                               ETA: 971159.4s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.543s, learning 0.267s)
               Value function loss: 195108.8754
                    Surrogate loss: -0.0114
             Mean action noise std: 0.71
                       Mean reward: 3782.48
               Mean episode length: 70.59
                  Mean reward/step: 58.39
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6597
Mean episode consecutive_successes: 16.0410
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 16.81s
                        Total time: 36921.67s
                               ETA: 971326.3s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 978 steps/s (collection: 16.564s, learning 0.173s)
               Value function loss: 207549.9848
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 3152.29
               Mean episode length: 69.92
                  Mean reward/step: 55.54
       Mean episode length/episode: 7.26
            Mean episode successes: 4.5728
Mean episode consecutive_successes: 15.9539
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 16.74s
                        Total time: 36938.41s
                               ETA: 971491.2s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 971 steps/s (collection: 16.709s, learning 0.164s)
               Value function loss: 205387.6027
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 4341.37
               Mean episode length: 71.40
                  Mean reward/step: 56.81
       Mean episode length/episode: 7.33
            Mean episode successes: 4.8335
Mean episode consecutive_successes: 15.9222
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 16.87s
                        Total time: 36955.28s
                               ETA: 971659.6s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 960 steps/s (collection: 16.866s, learning 0.192s)
               Value function loss: 210217.7012
                    Surrogate loss: -0.0187
             Mean action noise std: 0.71
                       Mean reward: 4756.10
               Mean episode length: 72.96
                  Mean reward/step: 56.08
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6079
Mean episode consecutive_successes: 15.9900
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 17.06s
                        Total time: 36972.34s
                               ETA: 971832.8s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.172s, learning 0.172s)
               Value function loss: 226781.7656
                    Surrogate loss: -0.0119
             Mean action noise std: 0.71
                       Mean reward: 4450.04
               Mean episode length: 71.55
                  Mean reward/step: 56.46
       Mean episode length/episode: 7.29
            Mean episode successes: 4.4731
Mean episode consecutive_successes: 16.0719
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 16.34s
                        Total time: 36988.68s
                               ETA: 971987.1s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.612s, learning 0.282s)
               Value function loss: 218627.7578
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 3875.09
               Mean episode length: 68.94
                  Mean reward/step: 56.64
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6387
Mean episode consecutive_successes: 15.9450
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 16.89s
                        Total time: 37005.58s
                               ETA: 972155.8s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.023s, learning 0.298s)
               Value function loss: 225782.4316
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 4763.34
               Mean episode length: 72.86
                  Mean reward/step: 58.27
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8174
Mean episode consecutive_successes: 16.0235
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 16.32s
                        Total time: 37021.90s
                               ETA: 972309.3s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.463s, learning 0.200s)
               Value function loss: 206359.5465
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 4312.37
               Mean episode length: 71.75
                  Mean reward/step: 57.58
       Mean episode length/episode: 7.30
            Mean episode successes: 4.6689
Mean episode consecutive_successes: 16.1143
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 16.66s
                        Total time: 37038.56s
                               ETA: 972471.7s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 983 steps/s (collection: 16.381s, learning 0.272s)
               Value function loss: 222851.0238
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 3220.68
               Mean episode length: 70.87
                  Mean reward/step: 55.88
       Mean episode length/episode: 7.27
            Mean episode successes: 4.7803
Mean episode consecutive_successes: 15.9903
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 16.65s
                        Total time: 37055.22s
                               ETA: 972633.8s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.412s, learning 0.202s)
               Value function loss: 282031.2594
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 3968.95
               Mean episode length: 70.90
                  Mean reward/step: 56.09
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6426
Mean episode consecutive_successes: 16.1429
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 16.61s
                        Total time: 37071.83s
                               ETA: 972794.7s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.605s, learning 0.156s)
               Value function loss: 307967.9828
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 3591.87
               Mean episode length: 72.47
                  Mean reward/step: 53.71
       Mean episode length/episode: 7.22
            Mean episode successes: 4.3711
Mean episode consecutive_successes: 16.1105
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 16.76s
                        Total time: 37088.59s
                               ETA: 972959.4s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 968 steps/s (collection: 16.718s, learning 0.195s)
               Value function loss: 267728.5238
                    Surrogate loss: -0.0087
             Mean action noise std: 0.71
                       Mean reward: 3779.29
               Mean episode length: 71.37
                  Mean reward/step: 54.59
       Mean episode length/episode: 7.26
            Mean episode successes: 4.3574
Mean episode consecutive_successes: 16.0310
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 16.91s
                        Total time: 37105.50s
                               ETA: 973127.9s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.084s, learning 0.175s)
               Value function loss: 204098.0340
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 4315.34
               Mean episode length: 72.43
                  Mean reward/step: 54.95
       Mean episode length/episode: 7.31
            Mean episode successes: 4.3550
Mean episode consecutive_successes: 16.1073
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 16.26s
                        Total time: 37121.76s
                               ETA: 973279.3s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.497s, learning 0.185s)
               Value function loss: 183666.5750
                    Surrogate loss: 0.0073
             Mean action noise std: 0.71
                       Mean reward: 3868.54
               Mean episode length: 70.22
                  Mean reward/step: 56.23
       Mean episode length/episode: 7.35
            Mean episode successes: 4.6079
Mean episode consecutive_successes: 15.9687
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 16.68s
                        Total time: 37138.44s
                               ETA: 973441.6s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 952 steps/s (collection: 16.971s, learning 0.234s)
               Value function loss: 187241.6531
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 3791.49
               Mean episode length: 71.03
                  Mean reward/step: 55.65
       Mean episode length/episode: 7.30
            Mean episode successes: 4.6558
Mean episode consecutive_successes: 15.9409
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 17.20s
                        Total time: 37155.65s
                               ETA: 973617.5s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 949 steps/s (collection: 17.073s, learning 0.178s)
               Value function loss: 202130.0992
                    Surrogate loss: -0.0155
             Mean action noise std: 0.71
                       Mean reward: 4220.14
               Mean episode length: 72.28
                  Mean reward/step: 58.04
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7178
Mean episode consecutive_successes: 16.0234
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 17.25s
                        Total time: 37172.90s
                               ETA: 973794.5s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.217s, learning 0.252s)
               Value function loss: 194513.4977
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 4131.88
               Mean episode length: 70.78
                  Mean reward/step: 57.07
       Mean episode length/episode: 7.27
            Mean episode successes: 4.5801
Mean episode consecutive_successes: 16.0965
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 16.47s
                        Total time: 37189.37s
                               ETA: 973951.0s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 961 steps/s (collection: 16.890s, learning 0.157s)
               Value function loss: 211338.5266
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 3640.17
               Mean episode length: 70.39
                  Mean reward/step: 56.43
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6836
Mean episode consecutive_successes: 16.0886
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 17.05s
                        Total time: 37206.42s
                               ETA: 974122.4s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.490s, learning 0.227s)
               Value function loss: 246231.3430
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 4321.07
               Mean episode length: 70.71
                  Mean reward/step: 58.36
       Mean episode length/episode: 7.27
            Mean episode successes: 4.6665
Mean episode consecutive_successes: 16.1264
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 16.72s
                        Total time: 37223.13s
                               ETA: 974285.1s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.595s, learning 0.173s)
               Value function loss: 282059.2070
                    Surrogate loss: -0.0069
             Mean action noise std: 0.71
                       Mean reward: 3217.75
               Mean episode length: 71.53
                  Mean reward/step: 55.24
       Mean episode length/episode: 7.23
            Mean episode successes: 4.5903
Mean episode consecutive_successes: 16.0766
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 16.77s
                        Total time: 37239.90s
                               ETA: 974449.1s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 972 steps/s (collection: 16.677s, learning 0.164s)
               Value function loss: 254155.9344
                    Surrogate loss: -0.0071
             Mean action noise std: 0.71
                       Mean reward: 3734.32
               Mean episode length: 71.96
                  Mean reward/step: 53.31
       Mean episode length/episode: 7.25
            Mean episode successes: 4.4473
Mean episode consecutive_successes: 16.0161
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 16.84s
                        Total time: 37256.74s
                               ETA: 974614.9s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.377s, learning 0.346s)
               Value function loss: 235558.5453
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 4445.43
               Mean episode length: 71.82
                  Mean reward/step: 54.68
       Mean episode length/episode: 7.30
            Mean episode successes: 4.4019
Mean episode consecutive_successes: 16.0563
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 8.72s
                        Total time: 37265.47s
                               ETA: 974568.3s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.586s, learning 0.181s)
               Value function loss: 243598.1012
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 3818.07
               Mean episode length: 69.25
                  Mean reward/step: 58.09
       Mean episode length/episode: 7.36
            Mean episode successes: 4.7148
Mean episode consecutive_successes: 16.0401
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 8.77s
                        Total time: 37274.23s
                               ETA: 974522.9s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.462s, learning 0.234s)
               Value function loss: 229334.1500
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 4585.31
               Mean episode length: 72.09
                  Mean reward/step: 58.68
       Mean episode length/episode: 7.23
            Mean episode successes: 4.6626
Mean episode consecutive_successes: 16.0535
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 8.70s
                        Total time: 37282.93s
                               ETA: 974475.6s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.462s, learning 0.183s)
               Value function loss: 238694.6180
                    Surrogate loss: -0.0129
             Mean action noise std: 0.71
                       Mean reward: 3867.17
               Mean episode length: 71.34
                  Mean reward/step: 58.08
       Mean episode length/episode: 7.34
            Mean episode successes: 4.8306
Mean episode consecutive_successes: 16.0257
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 8.64s
                        Total time: 37291.57s
                               ETA: 974427.0s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.432s, learning 0.340s)
               Value function loss: 241429.4777
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 4046.85
               Mean episode length: 70.50
                  Mean reward/step: 59.71
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8438
Mean episode consecutive_successes: 15.9983
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 8.77s
                        Total time: 37300.35s
                               ETA: 974381.7s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 1843 steps/s (collection: 8.726s, learning 0.162s)
               Value function loss: 214523.2703
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 4136.48
               Mean episode length: 70.73
                  Mean reward/step: 59.33
       Mean episode length/episode: 7.28
            Mean episode successes: 4.9478
Mean episode consecutive_successes: 15.9253
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 8.89s
                        Total time: 37309.23s
                               ETA: 974339.5s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.870s, learning 0.176s)
               Value function loss: 231122.7289
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 3536.30
               Mean episode length: 70.91
                  Mean reward/step: 58.90
       Mean episode length/episode: 7.21
            Mean episode successes: 4.8877
Mean episode consecutive_successes: 15.9470
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 9.05s
                        Total time: 37318.28s
                               ETA: 974301.5s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 1811 steps/s (collection: 8.870s, learning 0.175s)
               Value function loss: 247458.0836
                    Surrogate loss: -0.0054
             Mean action noise std: 0.71
                       Mean reward: 4201.60
               Mean episode length: 71.24
                  Mean reward/step: 56.61
       Mean episode length/episode: 7.34
            Mean episode successes: 4.7998
Mean episode consecutive_successes: 16.0481
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 9.05s
                        Total time: 37327.32s
                               ETA: 974263.4s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.170s)
               Value function loss: 245837.0883
                    Surrogate loss: -0.0141
             Mean action noise std: 0.71
                       Mean reward: 3399.19
               Mean episode length: 70.80
                  Mean reward/step: 55.59
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6802
Mean episode consecutive_successes: 16.0333
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 8.54s
                        Total time: 37335.87s
                               ETA: 974212.3s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.577s, learning 0.157s)
               Value function loss: 246969.1148
                    Surrogate loss: -0.0082
             Mean action noise std: 0.71
                       Mean reward: 4555.22
               Mean episode length: 71.11
                  Mean reward/step: 55.13
       Mean episode length/episode: 7.28
            Mean episode successes: 4.5215
Mean episode consecutive_successes: 16.1551
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 8.73s
                        Total time: 37344.60s
                               ETA: 974166.1s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.481s, learning 0.167s)
               Value function loss: 228540.3230
                    Surrogate loss: -0.0079
             Mean action noise std: 0.71
                       Mean reward: 4677.06
               Mean episode length: 70.88
                  Mean reward/step: 58.07
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5576
Mean episode consecutive_successes: 16.2570
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 8.65s
                        Total time: 37353.25s
                               ETA: 974117.8s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.654s, learning 0.168s)
               Value function loss: 218444.2820
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 4209.00
               Mean episode length: 72.88
                  Mean reward/step: 59.62
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6084
Mean episode consecutive_successes: 16.2074
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 8.82s
                        Total time: 37362.07s
                               ETA: 974074.0s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.524s, learning 0.172s)
               Value function loss: 218154.0574
                    Surrogate loss: -0.0042
             Mean action noise std: 0.70
                       Mean reward: 4052.19
               Mean episode length: 72.05
                  Mean reward/step: 60.00
       Mean episode length/episode: 7.36
            Mean episode successes: 4.9536
Mean episode consecutive_successes: 16.1451
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 8.70s
                        Total time: 37370.77s
                               ETA: 974026.9s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1826 steps/s (collection: 8.690s, learning 0.279s)
               Value function loss: 222927.1785
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 3440.06
               Mean episode length: 71.06
                  Mean reward/step: 63.55
       Mean episode length/episode: 7.29
            Mean episode successes: 5.3364
Mean episode consecutive_successes: 16.0267
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 8.97s
                        Total time: 37379.74s
                               ETA: 973986.9s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.299s, learning 0.295s)
               Value function loss: 284118.2711
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 4497.27
               Mean episode length: 71.94
                  Mean reward/step: 63.65
       Mean episode length/episode: 7.21
            Mean episode successes: 5.0962
Mean episode consecutive_successes: 16.1712
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 8.59s
                        Total time: 37388.33s
                               ETA: 973937.2s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.438s, learning 0.186s)
               Value function loss: 269079.6414
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 4742.47
               Mean episode length: 71.80
                  Mean reward/step: 61.41
       Mean episode length/episode: 7.33
            Mean episode successes: 5.2109
Mean episode consecutive_successes: 16.3617
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 8.62s
                        Total time: 37396.96s
                               ETA: 973888.3s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 1832 steps/s (collection: 8.660s, learning 0.280s)
               Value function loss: 237831.7086
                    Surrogate loss: -0.0052
             Mean action noise std: 0.71
                       Mean reward: 4237.24
               Mean episode length: 71.15
                  Mean reward/step: 60.28
       Mean episode length/episode: 7.29
            Mean episode successes: 5.0244
Mean episode consecutive_successes: 16.4241
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 8.94s
                        Total time: 37405.90s
                               ETA: 973847.7s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.406s, learning 0.167s)
               Value function loss: 223797.2687
                    Surrogate loss: -0.0080
             Mean action noise std: 0.70
                       Mean reward: 4227.53
               Mean episode length: 71.24
                  Mean reward/step: 61.16
       Mean episode length/episode: 7.35
            Mean episode successes: 5.1772
Mean episode consecutive_successes: 16.4587
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 8.57s
                        Total time: 37414.47s
                               ETA: 973797.5s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.570s, learning 0.189s)
               Value function loss: 227612.6633
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 4710.19
               Mean episode length: 72.73
                  Mean reward/step: 57.61
       Mean episode length/episode: 7.24
            Mean episode successes: 4.8521
Mean episode consecutive_successes: 16.6656
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 8.76s
                        Total time: 37423.23s
                               ETA: 973752.2s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.658s, learning 0.205s)
               Value function loss: 223345.1359
                    Surrogate loss: 0.0195
             Mean action noise std: 0.70
                       Mean reward: 4376.54
               Mean episode length: 70.92
                  Mean reward/step: 55.27
       Mean episode length/episode: 7.26
            Mean episode successes: 4.7515
Mean episode consecutive_successes: 16.5877
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 8.86s
                        Total time: 37432.09s
                               ETA: 973709.6s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.612s, learning 0.169s)
               Value function loss: 227880.4008
                    Surrogate loss: -0.0038
             Mean action noise std: 0.70
                       Mean reward: 4671.75
               Mean episode length: 71.28
                  Mean reward/step: 57.66
       Mean episode length/episode: 7.32
            Mean episode successes: 4.8022
Mean episode consecutive_successes: 16.5974
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 8.78s
                        Total time: 37440.87s
                               ETA: 973664.9s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 1765 steps/s (collection: 9.090s, learning 0.188s)
               Value function loss: 232836.3668
                    Surrogate loss: -0.0082
             Mean action noise std: 0.70
                       Mean reward: 4057.86
               Mean episode length: 70.54
                  Mean reward/step: 57.25
       Mean episode length/episode: 7.26
            Mean episode successes: 4.4863
Mean episode consecutive_successes: 16.6773
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 9.28s
                        Total time: 37450.15s
                               ETA: 973633.1s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.627s, learning 0.197s)
               Value function loss: 242905.6586
                    Surrogate loss: 0.0014
             Mean action noise std: 0.70
                       Mean reward: 3354.46
               Mean episode length: 69.68
                  Mean reward/step: 57.45
       Mean episode length/episode: 7.32
            Mean episode successes: 4.5483
Mean episode consecutive_successes: 16.6109
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 8.82s
                        Total time: 37458.97s
                               ETA: 973589.6s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.340s, learning 0.219s)
               Value function loss: 231992.4176
                    Surrogate loss: -0.0155
             Mean action noise std: 0.70
                       Mean reward: 4085.23
               Mean episode length: 72.32
                  Mean reward/step: 58.95
       Mean episode length/episode: 7.23
            Mean episode successes: 4.7524
Mean episode consecutive_successes: 16.4564
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 8.56s
                        Total time: 37467.53s
                               ETA: 973539.2s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.303s, learning 0.179s)
               Value function loss: 212947.7555
                    Surrogate loss: -0.0105
             Mean action noise std: 0.70
                       Mean reward: 3795.87
               Mean episode length: 70.14
                  Mean reward/step: 58.72
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7578
Mean episode consecutive_successes: 16.3934
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 8.48s
                        Total time: 37476.02s
                               ETA: 973486.8s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.491s, learning 0.190s)
               Value function loss: 233887.1418
                    Surrogate loss: -0.0106
             Mean action noise std: 0.70
                       Mean reward: 4451.73
               Mean episode length: 70.16
                  Mean reward/step: 59.86
       Mean episode length/episode: 7.20
            Mean episode successes: 4.8457
Mean episode consecutive_successes: 16.3257
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 8.68s
                        Total time: 37484.70s
                               ETA: 973439.5s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.506s, learning 0.233s)
               Value function loss: 226462.3531
                    Surrogate loss: -0.0077
             Mean action noise std: 0.70
                       Mean reward: 4470.97
               Mean episode length: 73.01
                  Mean reward/step: 60.76
       Mean episode length/episode: 7.38
            Mean episode successes: 4.9463
Mean episode consecutive_successes: 16.4828
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 8.74s
                        Total time: 37493.44s
                               ETA: 973393.9s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.485s, learning 0.190s)
               Value function loss: 261318.1648
                    Surrogate loss: -0.0035
             Mean action noise std: 0.70
                       Mean reward: 4494.12
               Mean episode length: 71.84
                  Mean reward/step: 60.59
       Mean episode length/episode: 7.30
            Mean episode successes: 5.0073
Mean episode consecutive_successes: 16.5724
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 8.67s
                        Total time: 37502.11s
                               ETA: 973346.5s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.545s, learning 0.162s)
               Value function loss: 298739.0789
                    Surrogate loss: -0.0054
             Mean action noise std: 0.70
                       Mean reward: 4316.94
               Mean episode length: 71.15
                  Mean reward/step: 60.94
       Mean episode length/episode: 7.24
            Mean episode successes: 4.9399
Mean episode consecutive_successes: 16.6433
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 8.71s
                        Total time: 37510.82s
                               ETA: 973300.1s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.181s, learning 0.205s)
               Value function loss: 237448.9430
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 4419.58
               Mean episode length: 71.93
                  Mean reward/step: 60.45
       Mean episode length/episode: 7.33
            Mean episode successes: 4.9590
Mean episode consecutive_successes: 16.8055
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 8.39s
                        Total time: 37519.20s
                               ETA: 973245.3s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.714s, learning 0.169s)
               Value function loss: 254612.9520
                    Surrogate loss: -0.0159
             Mean action noise std: 0.70
                       Mean reward: 4110.19
               Mean episode length: 69.25
                  Mean reward/step: 59.15
       Mean episode length/episode: 7.23
            Mean episode successes: 4.7471
Mean episode consecutive_successes: 16.8340
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 8.88s
                        Total time: 37528.09s
                               ETA: 973203.4s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.433s, learning 0.195s)
               Value function loss: 278459.8187
                    Surrogate loss: -0.0114
             Mean action noise std: 0.70
                       Mean reward: 4183.28
               Mean episode length: 70.46
                  Mean reward/step: 58.43
       Mean episode length/episode: 7.33
            Mean episode successes: 4.8521
Mean episode consecutive_successes: 16.8007
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 8.63s
                        Total time: 37536.71s
                               ETA: 973155.0s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.684s, learning 0.168s)
               Value function loss: 338220.2352
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 3717.23
               Mean episode length: 70.08
                  Mean reward/step: 59.19
       Mean episode length/episode: 7.22
            Mean episode successes: 4.8755
Mean episode consecutive_successes: 16.7494
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 8.85s
                        Total time: 37545.57s
                               ETA: 973112.4s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.399s, learning 0.183s)
               Value function loss: 331129.3109
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 4427.42
               Mean episode length: 71.45
                  Mean reward/step: 57.98
       Mean episode length/episode: 7.23
            Mean episode successes: 4.7939
Mean episode consecutive_successes: 16.7316
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 8.58s
                        Total time: 37554.15s
                               ETA: 973062.8s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.591s, learning 0.176s)
               Value function loss: 256166.6445
                    Surrogate loss: 0.0097
             Mean action noise std: 0.70
                       Mean reward: 4429.02
               Mean episode length: 70.44
                  Mean reward/step: 58.49
       Mean episode length/episode: 7.26
            Mean episode successes: 4.6450
Mean episode consecutive_successes: 16.7439
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 8.77s
                        Total time: 37562.92s
                               ETA: 973018.0s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.624s, learning 0.181s)
               Value function loss: 203853.0750
                    Surrogate loss: 0.0196
             Mean action noise std: 0.70
                       Mean reward: 4156.15
               Mean episode length: 70.67
                  Mean reward/step: 57.52
       Mean episode length/episode: 7.29
            Mean episode successes: 4.5791
Mean episode consecutive_successes: 16.7219
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 8.81s
                        Total time: 37571.72s
                               ETA: 972974.2s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 1815 steps/s (collection: 8.717s, learning 0.309s)
               Value function loss: 212349.4672
                    Surrogate loss: -0.0052
             Mean action noise std: 0.70
                       Mean reward: 3448.03
               Mean episode length: 69.85
                  Mean reward/step: 58.95
       Mean episode length/episode: 7.32
            Mean episode successes: 4.8281
Mean episode consecutive_successes: 16.5901
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 9.03s
                        Total time: 37580.75s
                               ETA: 972936.1s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.649s, learning 0.192s)
               Value function loss: 211690.4777
                    Surrogate loss: -0.0165
             Mean action noise std: 0.70
                       Mean reward: 5005.86
               Mean episode length: 71.85
                  Mean reward/step: 62.42
       Mean episode length/episode: 7.28
            Mean episode successes: 4.9775
Mean episode consecutive_successes: 16.6968
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 8.84s
                        Total time: 37589.59s
                               ETA: 972893.3s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.356s, learning 0.206s)
               Value function loss: 228822.5773
                    Surrogate loss: -0.0090
             Mean action noise std: 0.70
                       Mean reward: 4354.81
               Mean episode length: 70.70
                  Mean reward/step: 62.23
       Mean episode length/episode: 7.29
            Mean episode successes: 5.0918
Mean episode consecutive_successes: 16.6610
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 8.56s
                        Total time: 37598.15s
                               ETA: 972843.3s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 1780 steps/s (collection: 8.794s, learning 0.406s)
               Value function loss: 219667.7289
                    Surrogate loss: -0.0173
             Mean action noise std: 0.70
                       Mean reward: 4780.58
               Mean episode length: 71.96
                  Mean reward/step: 63.77
       Mean episode length/episode: 7.23
            Mean episode successes: 5.1812
Mean episode consecutive_successes: 16.6978
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 9.20s
                        Total time: 37607.35s
                               ETA: 972809.8s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.609s, learning 0.169s)
               Value function loss: 260360.9125
                    Surrogate loss: -0.0057
             Mean action noise std: 0.70
                       Mean reward: 4847.89
               Mean episode length: 71.58
                  Mean reward/step: 61.47
       Mean episode length/episode: 7.27
            Mean episode successes: 5.0425
Mean episode consecutive_successes: 16.7722
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 8.78s
                        Total time: 37616.13s
                               ETA: 972765.4s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.616s, learning 0.174s)
               Value function loss: 250706.2484
                    Surrogate loss: -0.0068
             Mean action noise std: 0.70
                       Mean reward: 4298.86
               Mean episode length: 70.67
                  Mean reward/step: 59.87
       Mean episode length/episode: 7.29
            Mean episode successes: 5.1206
Mean episode consecutive_successes: 16.7736
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 8.79s
                        Total time: 37624.92s
                               ETA: 972721.3s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.342s, learning 0.273s)
               Value function loss: 253052.8531
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 4661.58
               Mean episode length: 72.73
                  Mean reward/step: 59.81
       Mean episode length/episode: 7.27
            Mean episode successes: 4.8125
Mean episode consecutive_successes: 16.8802
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 8.61s
                        Total time: 37633.53s
                               ETA: 972672.7s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.628s, learning 0.177s)
               Value function loss: 234901.4980
                    Surrogate loss: -0.0067
             Mean action noise std: 0.70
                       Mean reward: 4087.14
               Mean episode length: 71.34
                  Mean reward/step: 58.63
       Mean episode length/episode: 7.34
            Mean episode successes: 5.0508
Mean episode consecutive_successes: 16.7751
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 8.81s
                        Total time: 37642.34s
                               ETA: 972629.1s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.300s, learning 0.166s)
               Value function loss: 231075.1098
                    Surrogate loss: -0.0084
             Mean action noise std: 0.70
                       Mean reward: 4813.07
               Mean episode length: 72.63
                  Mean reward/step: 59.96
       Mean episode length/episode: 7.28
            Mean episode successes: 4.7710
Mean episode consecutive_successes: 16.9817
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 8.47s
                        Total time: 37650.80s
                               ETA: 972576.7s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.156s, learning 0.198s)
               Value function loss: 260757.3613
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 3735.87
               Mean episode length: 70.40
                  Mean reward/step: 59.38
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8911
Mean episode consecutive_successes: 16.9035
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 8.35s
                        Total time: 37659.16s
                               ETA: 972521.5s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.404s, learning 0.198s)
               Value function loss: 288759.7984
                    Surrogate loss: -0.0094
             Mean action noise std: 0.70
                       Mean reward: 4388.23
               Mean episode length: 71.80
                  Mean reward/step: 60.34
       Mean episode length/episode: 7.32
            Mean episode successes: 4.8921
Mean episode consecutive_successes: 16.9807
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 8.60s
                        Total time: 37667.76s
                               ETA: 972472.7s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.298s, learning 0.173s)
               Value function loss: 252330.5586
                    Surrogate loss: -0.0047
             Mean action noise std: 0.70
                       Mean reward: 3976.33
               Mean episode length: 70.90
                  Mean reward/step: 59.87
       Mean episode length/episode: 7.29
            Mean episode successes: 5.0122
Mean episode consecutive_successes: 16.9168
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 8.47s
                        Total time: 37676.23s
                               ETA: 972420.5s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.333s, learning 0.316s)
               Value function loss: 242783.4262
                    Surrogate loss: -0.0019
             Mean action noise std: 0.70
                       Mean reward: 4312.05
               Mean episode length: 71.90
                  Mean reward/step: 61.32
       Mean episode length/episode: 7.29
            Mean episode successes: 5.0337
Mean episode consecutive_successes: 16.9123
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 8.65s
                        Total time: 37684.88s
                               ETA: 972373.0s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.620s, learning 0.192s)
               Value function loss: 247609.4070
                    Surrogate loss: -0.0064
             Mean action noise std: 0.70
                       Mean reward: 4236.58
               Mean episode length: 70.59
                  Mean reward/step: 60.33
       Mean episode length/episode: 7.22
            Mean episode successes: 4.9585
Mean episode consecutive_successes: 16.9217
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 8.81s
                        Total time: 37693.69s
                               ETA: 972329.6s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.745s, learning 0.182s)
               Value function loss: 252770.3898
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 4382.80
               Mean episode length: 72.02
                  Mean reward/step: 59.46
       Mean episode length/episode: 7.33
            Mean episode successes: 5.1143
Mean episode consecutive_successes: 16.9130
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 8.93s
                        Total time: 37702.62s
                               ETA: 972289.3s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.424s, learning 0.302s)
               Value function loss: 288904.6156
                    Surrogate loss: 0.0107
             Mean action noise std: 0.70
                       Mean reward: 3920.57
               Mean episode length: 70.62
                  Mean reward/step: 58.40
       Mean episode length/episode: 7.22
            Mean episode successes: 4.9067
Mean episode consecutive_successes: 16.8733
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 8.73s
                        Total time: 37711.35s
                               ETA: 972243.7s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.566s, learning 0.157s)
               Value function loss: 269326.6148
                    Surrogate loss: -0.0093
             Mean action noise std: 0.70
                       Mean reward: 4199.16
               Mean episode length: 71.43
                  Mean reward/step: 59.12
       Mean episode length/episode: 7.32
            Mean episode successes: 5.0415
Mean episode consecutive_successes: 16.8111
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 8.72s
                        Total time: 37720.07s
                               ETA: 972198.2s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.403s, learning 0.278s)
               Value function loss: 292356.4469
                    Surrogate loss: -0.0052
             Mean action noise std: 0.70
                       Mean reward: 4237.40
               Mean episode length: 71.87
                  Mean reward/step: 57.38
       Mean episode length/episode: 7.29
            Mean episode successes: 4.7427
Mean episode consecutive_successes: 16.8885
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 8.68s
                        Total time: 37728.75s
                               ETA: 972151.5s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 1768 steps/s (collection: 8.943s, learning 0.320s)
               Value function loss: 261786.3379
                    Surrogate loss: -0.0080
             Mean action noise std: 0.70
                       Mean reward: 3906.84
               Mean episode length: 71.51
                  Mean reward/step: 56.95
       Mean episode length/episode: 7.33
            Mean episode successes: 4.8887
Mean episode consecutive_successes: 16.8287
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 9.26s
                        Total time: 37738.01s
                               ETA: 972119.9s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.512s, learning 0.171s)
               Value function loss: 303470.4742
                    Surrogate loss: -0.0066
             Mean action noise std: 0.70
                       Mean reward: 4436.80
               Mean episode length: 71.17
                  Mean reward/step: 57.47
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8506
Mean episode consecutive_successes: 16.7867
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 8.68s
                        Total time: 37746.70s
                               ETA: 972073.3s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.514s, learning 0.172s)
               Value function loss: 317920.7117
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 4542.94
               Mean episode length: 72.15
                  Mean reward/step: 60.33
       Mean episode length/episode: 7.33
            Mean episode successes: 4.9165
Mean episode consecutive_successes: 16.9250
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 8.69s
                        Total time: 37755.38s
                               ETA: 972026.9s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 1845 steps/s (collection: 8.712s, learning 0.166s)
               Value function loss: 260432.8977
                    Surrogate loss: -0.0066
             Mean action noise std: 0.70
                       Mean reward: 3585.67
               Mean episode length: 70.15
                  Mean reward/step: 59.33
       Mean episode length/episode: 7.26
            Mean episode successes: 4.8374
Mean episode consecutive_successes: 16.8652
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 8.88s
                        Total time: 37764.26s
                               ETA: 971985.4s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 1842 steps/s (collection: 8.724s, learning 0.168s)
               Value function loss: 250099.9137
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 4047.48
               Mean episode length: 72.37
                  Mean reward/step: 57.25
       Mean episode length/episode: 7.25
            Mean episode successes: 4.6772
Mean episode consecutive_successes: 16.8332
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 8.89s
                        Total time: 37773.15s
                               ETA: 971944.3s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.368s, learning 0.172s)
               Value function loss: 231820.5094
                    Surrogate loss: -0.0036
             Mean action noise std: 0.70
                       Mean reward: 3567.18
               Mean episode length: 71.32
                  Mean reward/step: 54.54
       Mean episode length/episode: 7.32
            Mean episode successes: 4.8223
Mean episode consecutive_successes: 16.7188
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 8.54s
                        Total time: 37781.69s
                               ETA: 971894.2s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.161s, learning 0.213s)
               Value function loss: 254200.5875
                    Surrogate loss: -0.0026
             Mean action noise std: 0.70
                       Mean reward: 4426.49
               Mean episode length: 73.25
                  Mean reward/step: 56.28
       Mean episode length/episode: 7.26
            Mean episode successes: 4.7261
Mean episode consecutive_successes: 16.7068
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 8.37s
                        Total time: 37790.07s
                               ETA: 971839.8s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.603s, learning 0.267s)
               Value function loss: 242351.0207
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 3835.30
               Mean episode length: 71.93
                  Mean reward/step: 56.31
       Mean episode length/episode: 7.23
            Mean episode successes: 4.5532
Mean episode consecutive_successes: 16.6122
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 8.87s
                        Total time: 37798.94s
                               ETA: 971798.2s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.230s, learning 0.191s)
               Value function loss: 237592.4539
                    Surrogate loss: -0.0055
             Mean action noise std: 0.70
                       Mean reward: 4668.26
               Mean episode length: 72.52
                  Mean reward/step: 61.12
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6343
Mean episode consecutive_successes: 16.7305
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 8.42s
                        Total time: 37807.36s
                               ETA: 971745.0s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 1813 steps/s (collection: 8.875s, learning 0.160s)
               Value function loss: 222196.2848
                    Surrogate loss: -0.0168
             Mean action noise std: 0.70
                       Mean reward: 3172.59
               Mean episode length: 71.15
                  Mean reward/step: 60.25
       Mean episode length/episode: 7.30
            Mean episode successes: 4.8276
Mean episode consecutive_successes: 16.5932
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 9.04s
                        Total time: 37816.39s
                               ETA: 971707.7s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.747s, learning 0.188s)
               Value function loss: 220803.0711
                    Surrogate loss: -0.0157
             Mean action noise std: 0.70
                       Mean reward: 4241.76
               Mean episode length: 70.88
                  Mean reward/step: 60.15
       Mean episode length/episode: 7.30
            Mean episode successes: 5.1323
Mean episode consecutive_successes: 16.5306
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 8.93s
                        Total time: 37825.33s
                               ETA: 971667.8s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.584s, learning 0.175s)
               Value function loss: 230454.8129
                    Surrogate loss: -0.0114
             Mean action noise std: 0.70
                       Mean reward: 4722.61
               Mean episode length: 72.15
                  Mean reward/step: 62.64
       Mean episode length/episode: 7.31
            Mean episode successes: 5.3359
Mean episode consecutive_successes: 16.5229
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 8.76s
                        Total time: 37834.09s
                               ETA: 971623.4s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.654s, learning 0.284s)
               Value function loss: 223509.0227
                    Surrogate loss: -0.0178
             Mean action noise std: 0.70
                       Mean reward: 4311.98
               Mean episode length: 71.53
                  Mean reward/step: 63.11
       Mean episode length/episode: 7.28
            Mean episode successes: 5.2231
Mean episode consecutive_successes: 16.5755
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 8.94s
                        Total time: 37843.02s
                               ETA: 971583.6s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.653s, learning 0.167s)
               Value function loss: 231468.3535
                    Surrogate loss: -0.0127
             Mean action noise std: 0.70
                       Mean reward: 4414.89
               Mean episode length: 71.93
                  Mean reward/step: 61.94
       Mean episode length/episode: 7.29
            Mean episode successes: 5.1797
Mean episode consecutive_successes: 16.7348
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 8.82s
                        Total time: 37851.85s
                               ETA: 971540.8s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.358s, learning 0.176s)
               Value function loss: 240419.2527
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 4255.57
               Mean episode length: 69.00
                  Mean reward/step: 59.01
       Mean episode length/episode: 7.28
            Mean episode successes: 5.0483
Mean episode consecutive_successes: 16.7358
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 8.53s
                        Total time: 37860.38s
                               ETA: 971490.7s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 1849 steps/s (collection: 8.602s, learning 0.258s)
               Value function loss: 256050.8297
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 4727.00
               Mean episode length: 71.53
                  Mean reward/step: 59.35
       Mean episode length/episode: 7.26
            Mean episode successes: 4.9531
Mean episode consecutive_successes: 16.7774
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 8.86s
                        Total time: 37869.24s
                               ETA: 971448.9s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.293s, learning 0.164s)
               Value function loss: 231038.5117
                    Surrogate loss: -0.0078
             Mean action noise std: 0.70
                       Mean reward: 3761.41
               Mean episode length: 71.57
                  Mean reward/step: 57.92
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6514
Mean episode consecutive_successes: 16.8636
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 8.46s
                        Total time: 37877.70s
                               ETA: 971396.9s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 1822 steps/s (collection: 8.810s, learning 0.179s)
               Value function loss: 245405.7586
                    Surrogate loss: -0.0074
             Mean action noise std: 0.70
                       Mean reward: 4484.43
               Mean episode length: 71.30
                  Mean reward/step: 57.33
       Mean episode length/episode: 7.37
            Mean episode successes: 4.6704
Mean episode consecutive_successes: 16.8875
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 8.99s
                        Total time: 37886.68s
                               ETA: 971358.5s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.678s, learning 0.172s)
               Value function loss: 231470.2859
                    Surrogate loss: -0.0103
             Mean action noise std: 0.70
                       Mean reward: 4627.84
               Mean episode length: 72.07
                  Mean reward/step: 58.81
       Mean episode length/episode: 7.31
            Mean episode successes: 4.7583
Mean episode consecutive_successes: 16.8558
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 8.85s
                        Total time: 37895.53s
                               ETA: 971316.5s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.432s, learning 0.262s)
               Value function loss: 259182.8570
                    Surrogate loss: -0.0140
             Mean action noise std: 0.70
                       Mean reward: 4281.56
               Mean episode length: 70.84
                  Mean reward/step: 60.04
       Mean episode length/episode: 7.34
            Mean episode successes: 4.9780
Mean episode consecutive_successes: 16.8657
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 8.69s
                        Total time: 37904.23s
                               ETA: 971270.6s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.344s, learning 0.177s)
               Value function loss: 268399.2289
                    Surrogate loss: 0.0014
             Mean action noise std: 0.70
                       Mean reward: 3611.81
               Mean episode length: 71.77
                  Mean reward/step: 60.73
       Mean episode length/episode: 7.30
            Mean episode successes: 4.9668
Mean episode consecutive_successes: 16.8496
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 8.52s
                        Total time: 37912.75s
                               ETA: 971220.3s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.230s, learning 0.167s)
               Value function loss: 172958.0973
                    Surrogate loss: -0.0096
             Mean action noise std: 0.70
                       Mean reward: 3780.93
               Mean episode length: 71.16
                  Mean reward/step: 60.79
       Mean episode length/episode: 7.26
            Mean episode successes: 5.0376
Mean episode consecutive_successes: 16.8379
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 8.40s
                        Total time: 37921.15s
                               ETA: 971166.8s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.509s, learning 0.267s)
               Value function loss: 173608.8566
                    Surrogate loss: 0.0050
             Mean action noise std: 0.70
                       Mean reward: 3981.37
               Mean episode length: 70.87
                  Mean reward/step: 58.94
       Mean episode length/episode: 7.28
            Mean episode successes: 5.0908
Mean episode consecutive_successes: 16.7571
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 8.78s
                        Total time: 37929.92s
                               ETA: 971123.0s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.270s, learning 0.178s)
               Value function loss: 178970.3148
                    Surrogate loss: -0.0089
             Mean action noise std: 0.70
                       Mean reward: 4289.34
               Mean episode length: 71.71
                  Mean reward/step: 59.60
       Mean episode length/episode: 7.23
            Mean episode successes: 5.0049
Mean episode consecutive_successes: 16.8260
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 8.45s
                        Total time: 37938.37s
                               ETA: 971070.9s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.693s, learning 0.160s)
               Value function loss: 194143.7828
                    Surrogate loss: -0.0064
             Mean action noise std: 0.70
                       Mean reward: 4142.31
               Mean episode length: 69.59
                  Mean reward/step: 58.68
       Mean episode length/episode: 7.26
            Mean episode successes: 4.8770
Mean episode consecutive_successes: 16.8472
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 8.85s
                        Total time: 37947.22s
                               ETA: 971029.1s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.502s, learning 0.173s)
               Value function loss: 213115.5723
                    Surrogate loss: -0.0062
             Mean action noise std: 0.70
                       Mean reward: 4236.49
               Mean episode length: 71.32
                  Mean reward/step: 56.09
       Mean episode length/episode: 7.23
            Mean episode successes: 4.6367
Mean episode consecutive_successes: 16.8778
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 8.68s
                        Total time: 37955.90s
                               ETA: 970982.9s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.670s, learning 0.282s)
               Value function loss: 205141.4699
                    Surrogate loss: -0.0071
             Mean action noise std: 0.70
                       Mean reward: 3574.60
               Mean episode length: 69.51
                  Mean reward/step: 55.41
       Mean episode length/episode: 7.28
            Mean episode successes: 4.7339
Mean episode consecutive_successes: 16.7286
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 8.95s
                        Total time: 37964.85s
                               ETA: 970943.7s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.566s, learning 0.169s)
               Value function loss: 186736.3156
                    Surrogate loss: -0.0090
             Mean action noise std: 0.70
                       Mean reward: 4154.64
               Mean episode length: 71.49
                  Mean reward/step: 56.03
       Mean episode length/episode: 7.34
            Mean episode successes: 4.7383
Mean episode consecutive_successes: 16.7979
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 8.73s
                        Total time: 37973.58s
                               ETA: 970899.0s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.529s, learning 0.172s)
               Value function loss: 207455.4254
                    Surrogate loss: -0.0089
             Mean action noise std: 0.70
                       Mean reward: 3927.60
               Mean episode length: 69.51
                  Mean reward/step: 59.06
       Mean episode length/episode: 7.31
            Mean episode successes: 4.8354
Mean episode consecutive_successes: 16.6757
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 8.70s
                        Total time: 37982.28s
                               ETA: 970853.4s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.682s, learning 0.160s)
               Value function loss: 240835.3930
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 4444.92
               Mean episode length: 71.10
                  Mean reward/step: 61.20
       Mean episode length/episode: 7.27
            Mean episode successes: 4.9668
Mean episode consecutive_successes: 16.7285
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 8.84s
                        Total time: 37991.13s
                               ETA: 970811.5s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.334s, learning 0.243s)
               Value function loss: 247440.4449
                    Surrogate loss: 0.0082
             Mean action noise std: 0.70
                       Mean reward: 3766.14
               Mean episode length: 70.40
                  Mean reward/step: 59.87
       Mean episode length/episode: 7.33
            Mean episode successes: 5.0654
Mean episode consecutive_successes: 16.6248
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 8.58s
                        Total time: 37999.70s
                               ETA: 970762.8s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.370s, learning 0.192s)
               Value function loss: 199463.9891
                    Surrogate loss: -0.0146
             Mean action noise std: 0.70
                       Mean reward: 4835.45
               Mean episode length: 72.46
                  Mean reward/step: 61.32
       Mean episode length/episode: 7.23
            Mean episode successes: 5.0449
Mean episode consecutive_successes: 16.7157
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 8.56s
                        Total time: 38008.27s
                               ETA: 970713.8s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.171s)
               Value function loss: 211976.7805
                    Surrogate loss: -0.0043
             Mean action noise std: 0.70
                       Mean reward: 4244.22
               Mean episode length: 71.42
                  Mean reward/step: 60.55
       Mean episode length/episode: 7.33
            Mean episode successes: 5.0518
Mean episode consecutive_successes: 16.7049
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 8.54s
                        Total time: 38016.81s
                               ETA: 970664.3s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.185s, learning 0.204s)
               Value function loss: 209509.7152
                    Surrogate loss: -0.0145
             Mean action noise std: 0.70
                       Mean reward: 4875.79
               Mean episode length: 72.16
                  Mean reward/step: 62.42
       Mean episode length/episode: 7.30
            Mean episode successes: 5.2319
Mean episode consecutive_successes: 16.7358
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 8.39s
                        Total time: 38025.20s
                               ETA: 970610.9s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.385s, learning 0.183s)
               Value function loss: 222300.4852
                    Surrogate loss: -0.0166
             Mean action noise std: 0.70
                       Mean reward: 3828.96
               Mean episode length: 70.31
                  Mean reward/step: 59.62
       Mean episode length/episode: 7.19
            Mean episode successes: 4.8110
Mean episode consecutive_successes: 16.7851
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 8.57s
                        Total time: 38033.77s
                               ETA: 970562.1s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.672s, learning 0.178s)
               Value function loss: 225110.7141
                    Surrogate loss: -0.0075
             Mean action noise std: 0.70
                       Mean reward: 4922.07
               Mean episode length: 73.46
                  Mean reward/step: 58.40
       Mean episode length/episode: 7.24
            Mean episode successes: 4.6909
Mean episode consecutive_successes: 16.8777
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 8.85s
                        Total time: 38042.62s
                               ETA: 970520.4s

################################################################################
                    [1m Learning iteration 3772/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.652s, learning 0.172s)
               Value function loss: 205487.9930
                    Surrogate loss: -0.0150
             Mean action noise std: 0.70
                       Mean reward: 4407.16
               Mean episode length: 71.38
                  Mean reward/step: 59.93
       Mean episode length/episode: 7.33
            Mean episode successes: 5.0273
Mean episode consecutive_successes: 16.7764
--------------------------------------------------------------------------------
                   Total timesteps: 61816832
                    Iteration time: 8.82s
                        Total time: 38051.44s
                               ETA: 970478.2s

################################################################################
                    [1m Learning iteration 3773/100000 [0m                    

                       Computation: 1820 steps/s (collection: 8.738s, learning 0.261s)
               Value function loss: 212227.2379
                    Surrogate loss: -0.0145
             Mean action noise std: 0.70
                       Mean reward: 3823.44
               Mean episode length: 71.03
                  Mean reward/step: 62.65
       Mean episode length/episode: 7.39
            Mean episode successes: 5.2788
Mean episode consecutive_successes: 16.7330
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 9.00s
                        Total time: 38060.44s
                               ETA: 970440.4s

################################################################################
                    [1m Learning iteration 3774/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.193s, learning 0.188s)
               Value function loss: 237094.9691
                    Surrogate loss: -0.0147
             Mean action noise std: 0.70
                       Mean reward: 4235.35
               Mean episode length: 71.57
                  Mean reward/step: 61.71
       Mean episode length/episode: 7.27
            Mean episode successes: 5.1084
Mean episode consecutive_successes: 16.9058
--------------------------------------------------------------------------------
                   Total timesteps: 61849600
                    Iteration time: 8.38s
                        Total time: 38068.82s
                               ETA: 970386.9s

################################################################################
                    [1m Learning iteration 3775/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.424s, learning 0.294s)
               Value function loss: 243158.8719
                    Surrogate loss: -0.0103
             Mean action noise std: 0.70
                       Mean reward: 4389.50
               Mean episode length: 71.14
                  Mean reward/step: 63.14
       Mean episode length/episode: 7.30
            Mean episode successes: 5.2930
Mean episode consecutive_successes: 16.8686
--------------------------------------------------------------------------------
                   Total timesteps: 61865984
                    Iteration time: 8.72s
                        Total time: 38077.54s
                               ETA: 970342.0s

################################################################################
                    [1m Learning iteration 3776/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.376s, learning 0.170s)
               Value function loss: 245135.9656
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 4568.00
               Mean episode length: 71.73
                  Mean reward/step: 63.12
       Mean episode length/episode: 7.24
            Mean episode successes: 5.1982
Mean episode consecutive_successes: 16.9990
--------------------------------------------------------------------------------
                   Total timesteps: 61882368
                    Iteration time: 8.55s
                        Total time: 38086.09s
                               ETA: 970292.7s

################################################################################
                    [1m Learning iteration 3777/100000 [0m                    

                       Computation: 1773 steps/s (collection: 9.040s, learning 0.197s)
               Value function loss: 216048.0316
                    Surrogate loss: -0.0083
             Mean action noise std: 0.70
                       Mean reward: 4061.85
               Mean episode length: 70.90
                  Mean reward/step: 63.82
       Mean episode length/episode: 7.32
            Mean episode successes: 5.4229
Mean episode consecutive_successes: 17.0011
--------------------------------------------------------------------------------
                   Total timesteps: 61898752
                    Iteration time: 9.24s
                        Total time: 38095.32s
                               ETA: 970261.1s

################################################################################
                    [1m Learning iteration 3778/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.392s, learning 0.163s)
               Value function loss: 240519.4082
                    Surrogate loss: -0.0085
             Mean action noise std: 0.70
                       Mean reward: 3751.47
               Mean episode length: 71.49
                  Mean reward/step: 61.96
       Mean episode length/episode: 7.20
            Mean episode successes: 5.2471
Mean episode consecutive_successes: 16.9670
--------------------------------------------------------------------------------
                   Total timesteps: 61915136
                    Iteration time: 8.56s
                        Total time: 38103.88s
                               ETA: 970212.1s

################################################################################
                    [1m Learning iteration 3779/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.297s, learning 0.341s)
               Value function loss: 236373.9285
                    Surrogate loss: -0.0095
             Mean action noise std: 0.70
                       Mean reward: 4246.32
               Mean episode length: 71.06
                  Mean reward/step: 63.89
       Mean episode length/episode: 7.22
            Mean episode successes: 5.1826
Mean episode consecutive_successes: 16.9995
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 8.64s
                        Total time: 38112.52s
                               ETA: 970165.2s

################################################################################
                    [1m Learning iteration 3780/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.339s, learning 0.159s)
               Value function loss: 217479.3457
                    Surrogate loss: -0.0103
             Mean action noise std: 0.70
                       Mean reward: 4079.53
               Mean episode length: 71.82
                  Mean reward/step: 62.33
       Mean episode length/episode: 7.24
            Mean episode successes: 4.9736
Mean episode consecutive_successes: 17.0849
--------------------------------------------------------------------------------
                   Total timesteps: 61947904
                    Iteration time: 8.50s
                        Total time: 38121.02s
                               ETA: 970114.8s

################################################################################
                    [1m Learning iteration 3781/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.219s, learning 0.173s)
               Value function loss: 237857.2023
                    Surrogate loss: -0.0036
             Mean action noise std: 0.70
                       Mean reward: 4456.94
               Mean episode length: 71.28
                  Mean reward/step: 63.36
       Mean episode length/episode: 7.32
            Mean episode successes: 5.1953
Mean episode consecutive_successes: 17.1617
--------------------------------------------------------------------------------
                   Total timesteps: 61964288
                    Iteration time: 8.39s
                        Total time: 38129.41s
                               ETA: 970061.7s

################################################################################
                    [1m Learning iteration 3782/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.276s, learning 0.186s)
               Value function loss: 217132.2152
                    Surrogate loss: -0.0103
             Mean action noise std: 0.70
                       Mean reward: 4416.95
               Mean episode length: 71.27
                  Mean reward/step: 65.48
       Mean episode length/episode: 7.36
            Mean episode successes: 5.4795
Mean episode consecutive_successes: 17.2476
--------------------------------------------------------------------------------
                   Total timesteps: 61980672
                    Iteration time: 8.46s
                        Total time: 38137.87s
                               ETA: 970010.5s

################################################################################
                    [1m Learning iteration 3783/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.414s, learning 0.271s)
               Value function loss: 245375.2215
                    Surrogate loss: -0.0141
             Mean action noise std: 0.70
                       Mean reward: 5016.57
               Mean episode length: 73.31
                  Mean reward/step: 64.83
       Mean episode length/episode: 7.33
            Mean episode successes: 5.5771
Mean episode consecutive_successes: 17.3843
--------------------------------------------------------------------------------
                   Total timesteps: 61997056
                    Iteration time: 8.68s
                        Total time: 38146.55s
                               ETA: 969964.9s

################################################################################
                    [1m Learning iteration 3784/100000 [0m                    

                       Computation: 1773 steps/s (collection: 8.934s, learning 0.307s)
               Value function loss: 252122.2219
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 5145.39
               Mean episode length: 71.47
                  Mean reward/step: 66.77
       Mean episode length/episode: 7.21
            Mean episode successes: 5.3242
Mean episode consecutive_successes: 17.6168
--------------------------------------------------------------------------------
                   Total timesteps: 62013440
                    Iteration time: 9.24s
                        Total time: 38155.79s
                               ETA: 969933.4s

################################################################################
                    [1m Learning iteration 3785/100000 [0m                    

                       Computation: 1802 steps/s (collection: 8.694s, learning 0.394s)
               Value function loss: 241056.7746
                    Surrogate loss: -0.0154
             Mean action noise std: 0.70
                       Mean reward: 4945.73
               Mean episode length: 72.66
                  Mean reward/step: 65.82
       Mean episode length/episode: 7.36
            Mean episode successes: 5.4790
Mean episode consecutive_successes: 17.7181
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 9.09s
                        Total time: 38164.88s
                               ETA: 969898.1s

################################################################################
                    [1m Learning iteration 3786/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.592s, learning 0.174s)
               Value function loss: 286394.0695
                    Surrogate loss: -0.0162
             Mean action noise std: 0.70
                       Mean reward: 4647.28
               Mean episode length: 71.37
                  Mean reward/step: 64.38
       Mean episode length/episode: 7.26
            Mean episode successes: 5.5347
Mean episode consecutive_successes: 17.6935
--------------------------------------------------------------------------------
                   Total timesteps: 62046208
                    Iteration time: 8.77s
                        Total time: 38173.65s
                               ETA: 969854.6s

################################################################################
                    [1m Learning iteration 3787/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.505s, learning 0.305s)
               Value function loss: 313390.2469
                    Surrogate loss: -0.0095
             Mean action noise std: 0.70
                       Mean reward: 4290.24
               Mean episode length: 71.50
                  Mean reward/step: 61.90
       Mean episode length/episode: 7.24
            Mean episode successes: 5.2734
Mean episode consecutive_successes: 17.7128
--------------------------------------------------------------------------------
                   Total timesteps: 62062592
                    Iteration time: 8.81s
                        Total time: 38182.46s
                               ETA: 969812.3s

################################################################################
                    [1m Learning iteration 3788/100000 [0m                    

                       Computation: 1825 steps/s (collection: 8.796s, learning 0.181s)
               Value function loss: 300594.1531
                    Surrogate loss: -0.0081
             Mean action noise std: 0.70
                       Mean reward: 4330.83
               Mean episode length: 72.99
                  Mean reward/step: 59.72
       Mean episode length/episode: 7.29
            Mean episode successes: 5.1636
Mean episode consecutive_successes: 17.7754
--------------------------------------------------------------------------------
                   Total timesteps: 62078976
                    Iteration time: 8.98s
                        Total time: 38191.44s
                               ETA: 969774.2s

################################################################################
                    [1m Learning iteration 3789/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.605s, learning 0.183s)
               Value function loss: 238216.6805
                    Surrogate loss: -0.0044
             Mean action noise std: 0.70
                       Mean reward: 4057.50
               Mean episode length: 72.52
                  Mean reward/step: 57.32
       Mean episode length/episode: 7.22
            Mean episode successes: 4.7827
Mean episode consecutive_successes: 17.7522
--------------------------------------------------------------------------------
                   Total timesteps: 62095360
                    Iteration time: 8.79s
                        Total time: 38200.22s
                               ETA: 969731.3s

################################################################################
                    [1m Learning iteration 3790/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.398s, learning 0.207s)
               Value function loss: 187060.8547
                    Surrogate loss: -0.0201
             Mean action noise std: 0.70
                       Mean reward: 4650.33
               Mean episode length: 71.78
                  Mean reward/step: 57.68
       Mean episode length/episode: 7.28
            Mean episode successes: 4.6519
Mean episode consecutive_successes: 17.7530
--------------------------------------------------------------------------------
                   Total timesteps: 62111744
                    Iteration time: 8.61s
                        Total time: 38208.83s
                               ETA: 969683.8s

################################################################################
                    [1m Learning iteration 3791/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.524s, learning 0.175s)
               Value function loss: 214240.1836
                    Surrogate loss: -0.0109
             Mean action noise std: 0.70
                       Mean reward: 4152.39
               Mean episode length: 71.66
                  Mean reward/step: 60.39
       Mean episode length/episode: 7.42
            Mean episode successes: 4.8281
Mean episode consecutive_successes: 17.7211
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 8.70s
                        Total time: 38217.53s
                               ETA: 969638.7s

################################################################################
                    [1m Learning iteration 3792/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.556s, learning 0.163s)
               Value function loss: 215644.7305
                    Surrogate loss: -0.0112
             Mean action noise std: 0.70
                       Mean reward: 4412.83
               Mean episode length: 72.13
                  Mean reward/step: 63.21
       Mean episode length/episode: 7.37
            Mean episode successes: 5.1670
Mean episode consecutive_successes: 17.6795
--------------------------------------------------------------------------------
                   Total timesteps: 62144512
                    Iteration time: 8.72s
                        Total time: 38226.25s
                               ETA: 969594.2s

################################################################################
                    [1m Learning iteration 3793/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.654s, learning 0.182s)
               Value function loss: 237970.1512
                    Surrogate loss: -0.0076
             Mean action noise std: 0.70
                       Mean reward: 4147.67
               Mean episode length: 72.60
                  Mean reward/step: 64.01
       Mean episode length/episode: 7.26
            Mean episode successes: 5.1724
Mean episode consecutive_successes: 17.6117
--------------------------------------------------------------------------------
                   Total timesteps: 62160896
                    Iteration time: 8.84s
                        Total time: 38235.08s
                               ETA: 969552.6s

################################################################################
                    [1m Learning iteration 3794/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.881s, learning 0.174s)
               Value function loss: 217603.3406
                    Surrogate loss: -0.0123
             Mean action noise std: 0.70
                       Mean reward: 4459.46
               Mean episode length: 70.32
                  Mean reward/step: 66.61
       Mean episode length/episode: 7.28
            Mean episode successes: 5.4150
Mean episode consecutive_successes: 17.5292
--------------------------------------------------------------------------------
                   Total timesteps: 62177280
                    Iteration time: 9.06s
                        Total time: 38244.14s
                               ETA: 969516.6s

################################################################################
                    [1m Learning iteration 3795/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.115s, learning 0.160s)
               Value function loss: 231372.8777
                    Surrogate loss: -0.0159
             Mean action noise std: 0.70
                       Mean reward: 4361.83
               Mean episode length: 70.79
                  Mean reward/step: 67.11
       Mean episode length/episode: 7.26
            Mean episode successes: 5.5752
Mean episode consecutive_successes: 17.5191
--------------------------------------------------------------------------------
                   Total timesteps: 62193664
                    Iteration time: 8.28s
                        Total time: 38252.41s
                               ETA: 969460.8s

################################################################################
                    [1m Learning iteration 3796/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.531s, learning 0.277s)
               Value function loss: 264380.4258
                    Surrogate loss: -0.0135
             Mean action noise std: 0.70
                       Mean reward: 5282.97
               Mean episode length: 71.60
                  Mean reward/step: 61.29
       Mean episode length/episode: 7.27
            Mean episode successes: 5.3721
Mean episode consecutive_successes: 17.6167
--------------------------------------------------------------------------------
                   Total timesteps: 62210048
                    Iteration time: 8.81s
                        Total time: 38261.22s
                               ETA: 969418.6s

################################################################################
                    [1m Learning iteration 3797/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.387s, learning 0.159s)
               Value function loss: 252747.3344
                    Surrogate loss: 0.0116
             Mean action noise std: 0.70
                       Mean reward: 4272.88
               Mean episode length: 71.71
                  Mean reward/step: 58.81
       Mean episode length/episode: 7.21
            Mean episode successes: 5.1460
Mean episode consecutive_successes: 17.5820
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 8.55s
                        Total time: 38269.77s
                               ETA: 969369.7s

################################################################################
                    [1m Learning iteration 3798/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.545s, learning 0.174s)
               Value function loss: 227233.6754
                    Surrogate loss: 0.0041
             Mean action noise std: 0.70
                       Mean reward: 4092.57
               Mean episode length: 71.96
                  Mean reward/step: 60.32
       Mean episode length/episode: 7.27
            Mean episode successes: 4.9761
Mean episode consecutive_successes: 17.5603
--------------------------------------------------------------------------------
                   Total timesteps: 62242816
                    Iteration time: 8.72s
                        Total time: 38278.48s
                               ETA: 969325.3s

################################################################################
                    [1m Learning iteration 3799/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.346s, learning 0.212s)
               Value function loss: 223621.4430
                    Surrogate loss: -0.0103
             Mean action noise std: 0.70
                       Mean reward: 4781.04
               Mean episode length: 72.06
                  Mean reward/step: 60.57
       Mean episode length/episode: 7.29
            Mean episode successes: 4.6372
Mean episode consecutive_successes: 17.7395
--------------------------------------------------------------------------------
                   Total timesteps: 62259200
                    Iteration time: 8.56s
                        Total time: 38287.04s
                               ETA: 969276.8s

################################################################################
                    [1m Learning iteration 3800/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.323s, learning 0.170s)
               Value function loss: 203341.2836
                    Surrogate loss: -0.0148
             Mean action noise std: 0.70
                       Mean reward: 3811.76
               Mean episode length: 71.11
                  Mean reward/step: 61.51
       Mean episode length/episode: 7.37
            Mean episode successes: 4.9375
Mean episode consecutive_successes: 17.6628
--------------------------------------------------------------------------------
                   Total timesteps: 62275584
                    Iteration time: 8.49s
                        Total time: 38295.54s
                               ETA: 969226.7s

################################################################################
                    [1m Learning iteration 3801/100000 [0m                    

                       Computation: 1839 steps/s (collection: 8.736s, learning 0.169s)
               Value function loss: 227092.1293
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 3465.23
               Mean episode length: 69.36
                  Mean reward/step: 63.18
       Mean episode length/episode: 7.33
            Mean episode successes: 5.3506
Mean episode consecutive_successes: 17.5412
--------------------------------------------------------------------------------
                   Total timesteps: 62291968
                    Iteration time: 8.91s
                        Total time: 38304.44s
                               ETA: 969187.0s

################################################################################
                    [1m Learning iteration 3802/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.367s, learning 0.268s)
               Value function loss: 250741.0734
                    Surrogate loss: -0.0124
             Mean action noise std: 0.70
                       Mean reward: 4402.33
               Mean episode length: 71.85
                  Mean reward/step: 65.86
       Mean episode length/episode: 7.26
            Mean episode successes: 5.5200
Mean episode consecutive_successes: 17.4223
--------------------------------------------------------------------------------
                   Total timesteps: 62308352
                    Iteration time: 8.63s
                        Total time: 38313.08s
                               ETA: 969140.5s

################################################################################
                    [1m Learning iteration 3803/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.350s, learning 0.176s)
               Value function loss: 295336.1621
                    Surrogate loss: -0.0072
             Mean action noise std: 0.70
                       Mean reward: 4273.95
               Mean episode length: 70.57
                  Mean reward/step: 62.62
       Mean episode length/episode: 7.27
            Mean episode successes: 5.3672
Mean episode consecutive_successes: 17.5123
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 8.53s
                        Total time: 38321.60s
                               ETA: 969091.2s

################################################################################
                    [1m Learning iteration 3804/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.678s, learning 0.255s)
               Value function loss: 233754.9520
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 5041.10
               Mean episode length: 72.53
                  Mean reward/step: 63.69
       Mean episode length/episode: 7.29
            Mean episode successes: 5.2954
Mean episode consecutive_successes: 17.6614
--------------------------------------------------------------------------------
                   Total timesteps: 62341120
                    Iteration time: 8.93s
                        Total time: 38330.53s
                               ETA: 969052.3s

################################################################################
                    [1m Learning iteration 3805/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.673s, learning 0.174s)
               Value function loss: 206363.4188
                    Surrogate loss: -0.0135
             Mean action noise std: 0.70
                       Mean reward: 3046.82
               Mean episode length: 70.15
                  Mean reward/step: 61.50
       Mean episode length/episode: 7.26
            Mean episode successes: 5.3506
Mean episode consecutive_successes: 17.5335
--------------------------------------------------------------------------------
                   Total timesteps: 62357504
                    Iteration time: 8.85s
                        Total time: 38339.38s
                               ETA: 969011.2s

################################################################################
                    [1m Learning iteration 3806/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.517s, learning 0.185s)
               Value function loss: 223638.0590
                    Surrogate loss: -0.0061
             Mean action noise std: 0.70
                       Mean reward: 4582.24
               Mean episode length: 71.71
                  Mean reward/step: 61.36
       Mean episode length/episode: 7.26
            Mean episode successes: 5.0762
Mean episode consecutive_successes: 17.6378
--------------------------------------------------------------------------------
                   Total timesteps: 62373888
                    Iteration time: 8.70s
                        Total time: 38348.08s
                               ETA: 968966.5s

################################################################################
                    [1m Learning iteration 3807/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.489s, learning 0.169s)
               Value function loss: 222247.8449
                    Surrogate loss: -0.0079
             Mean action noise std: 0.70
                       Mean reward: 3927.55
               Mean episode length: 71.09
                  Mean reward/step: 61.79
       Mean episode length/episode: 7.27
            Mean episode successes: 5.1294
Mean episode consecutive_successes: 17.5205
--------------------------------------------------------------------------------
                   Total timesteps: 62390272
                    Iteration time: 8.66s
                        Total time: 38356.74s
                               ETA: 968920.7s

################################################################################
                    [1m Learning iteration 3808/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.020s, learning 0.164s)
               Value function loss: 222645.9043
                    Surrogate loss: -0.0109
             Mean action noise std: 0.70
                       Mean reward: 4505.29
               Mean episode length: 71.98
                  Mean reward/step: 64.19
       Mean episode length/episode: 7.28
            Mean episode successes: 5.1602
Mean episode consecutive_successes: 17.6046
--------------------------------------------------------------------------------
                   Total timesteps: 62406656
                    Iteration time: 8.18s
                        Total time: 38364.93s
                               ETA: 968862.9s

################################################################################
                    [1m Learning iteration 3809/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.552s, learning 0.168s)
               Value function loss: 237490.1105
                    Surrogate loss: -0.0074
             Mean action noise std: 0.70
                       Mean reward: 4842.80
               Mean episode length: 71.75
                  Mean reward/step: 63.96
       Mean episode length/episode: 7.35
            Mean episode successes: 5.2095
Mean episode consecutive_successes: 17.6873
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 8.72s
                        Total time: 38373.65s
                               ETA: 968818.7s

################################################################################
                    [1m Learning iteration 3810/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.317s, learning 0.223s)
               Value function loss: 209072.4973
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 4671.96
               Mean episode length: 71.23
                  Mean reward/step: 64.23
       Mean episode length/episode: 7.36
            Mean episode successes: 5.4126
Mean episode consecutive_successes: 17.7821
--------------------------------------------------------------------------------
                   Total timesteps: 62439424
                    Iteration time: 8.54s
                        Total time: 38382.19s
                               ETA: 968770.0s

################################################################################
                    [1m Learning iteration 3811/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.331s, learning 0.165s)
               Value function loss: 236600.9289
                    Surrogate loss: -0.0068
             Mean action noise std: 0.70
                       Mean reward: 4454.69
               Mean episode length: 71.57
                  Mean reward/step: 65.13
       Mean episode length/episode: 7.32
            Mean episode successes: 5.5684
Mean episode consecutive_successes: 17.8278
--------------------------------------------------------------------------------
                   Total timesteps: 62455808
                    Iteration time: 8.50s
                        Total time: 38390.68s
                               ETA: 968720.2s

################################################################################
                    [1m Learning iteration 3812/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.484s, learning 0.260s)
               Value function loss: 240892.7844
                    Surrogate loss: -0.0089
             Mean action noise std: 0.70
                       Mean reward: 4261.29
               Mean episode length: 70.42
                  Mean reward/step: 67.48
       Mean episode length/episode: 7.18
            Mean episode successes: 5.4551
Mean episode consecutive_successes: 17.7862
--------------------------------------------------------------------------------
                   Total timesteps: 62472192
                    Iteration time: 8.74s
                        Total time: 38399.42s
                               ETA: 968676.6s

################################################################################
                    [1m Learning iteration 3813/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.329s, learning 0.199s)
               Value function loss: 211398.9980
                    Surrogate loss: -0.0030
             Mean action noise std: 0.70
                       Mean reward: 4500.23
               Mean episode length: 72.34
                  Mean reward/step: 66.60
       Mean episode length/episode: 7.28
            Mean episode successes: 5.5283
Mean episode consecutive_successes: 17.8299
--------------------------------------------------------------------------------
                   Total timesteps: 62488576
                    Iteration time: 8.53s
                        Total time: 38407.95s
                               ETA: 968627.6s

################################################################################
                    [1m Learning iteration 3814/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.268s, learning 0.199s)
               Value function loss: 208295.0148
                    Surrogate loss: -0.0138
             Mean action noise std: 0.70
                       Mean reward: 4101.26
               Mean episode length: 72.73
                  Mean reward/step: 64.28
       Mean episode length/episode: 7.27
            Mean episode successes: 5.5781
Mean episode consecutive_successes: 17.7360
--------------------------------------------------------------------------------
                   Total timesteps: 62504960
                    Iteration time: 8.47s
                        Total time: 38416.42s
                               ETA: 968577.1s

################################################################################
                    [1m Learning iteration 3815/100000 [0m                    

                       Computation: 1831 steps/s (collection: 8.756s, learning 0.193s)
               Value function loss: 243892.8672
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 4272.50
               Mean episode length: 72.90
                  Mean reward/step: 63.49
       Mean episode length/episode: 7.27
            Mean episode successes: 5.4404
Mean episode consecutive_successes: 17.9062
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 8.95s
                        Total time: 38425.37s
                               ETA: 968538.8s

################################################################################
                    [1m Learning iteration 3816/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.792s, learning 0.195s)
               Value function loss: 316344.9016
                    Surrogate loss: -0.0105
             Mean action noise std: 0.70
                       Mean reward: 4650.43
               Mean episode length: 71.22
                  Mean reward/step: 63.26
       Mean episode length/episode: 7.22
            Mean episode successes: 5.1997
Mean episode consecutive_successes: 17.9786
--------------------------------------------------------------------------------
                   Total timesteps: 62537728
                    Iteration time: 8.99s
                        Total time: 38434.35s
                               ETA: 968501.4s

################################################################################
                    [1m Learning iteration 3817/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.363s, learning 0.161s)
               Value function loss: 587146.5062
                    Surrogate loss: -0.0057
             Mean action noise std: 0.70
                       Mean reward: 4617.77
               Mean episode length: 72.29
                  Mean reward/step: 63.38
       Mean episode length/episode: 7.28
            Mean episode successes: 5.1357
Mean episode consecutive_successes: 17.9872
--------------------------------------------------------------------------------
                   Total timesteps: 62554112
                    Iteration time: 8.52s
                        Total time: 38442.88s
                               ETA: 968452.4s

################################################################################
                    [1m Learning iteration 3818/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.182s, learning 0.181s)
               Value function loss: 257788.2520
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 4436.96
               Mean episode length: 70.74
                  Mean reward/step: 63.01
       Mean episode length/episode: 7.36
            Mean episode successes: 5.2397
Mean episode consecutive_successes: 18.0340
--------------------------------------------------------------------------------
                   Total timesteps: 62570496
                    Iteration time: 8.36s
                        Total time: 38451.24s
                               ETA: 968399.4s

################################################################################
                    [1m Learning iteration 3819/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.417s, learning 0.175s)
               Value function loss: 216423.0012
                    Surrogate loss: 0.0246
             Mean action noise std: 0.70
                       Mean reward: 4433.51
               Mean episode length: 70.37
                  Mean reward/step: 63.08
       Mean episode length/episode: 7.37
            Mean episode successes: 5.2744
Mean episode consecutive_successes: 18.1545
--------------------------------------------------------------------------------
                   Total timesteps: 62586880
                    Iteration time: 8.59s
                        Total time: 38459.83s
                               ETA: 968352.1s

################################################################################
                    [1m Learning iteration 3820/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.562s, learning 0.237s)
               Value function loss: 224432.0086
                    Surrogate loss: -0.0059
             Mean action noise std: 0.70
                       Mean reward: 4779.77
               Mean episode length: 70.90
                  Mean reward/step: 65.21
       Mean episode length/episode: 7.26
            Mean episode successes: 5.3877
Mean episode consecutive_successes: 18.1469
--------------------------------------------------------------------------------
                   Total timesteps: 62603264
                    Iteration time: 8.80s
                        Total time: 38468.63s
                               ETA: 968310.1s

################################################################################
                    [1m Learning iteration 3821/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.460s, learning 0.168s)
               Value function loss: 209038.9441
                    Surrogate loss: -0.0088
             Mean action noise std: 0.70
                       Mean reward: 4252.81
               Mean episode length: 71.98
                  Mean reward/step: 65.89
       Mean episode length/episode: 7.28
            Mean episode successes: 5.3496
Mean episode consecutive_successes: 18.1864
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 8.63s
                        Total time: 38477.26s
                               ETA: 968263.8s

################################################################################
                    [1m Learning iteration 3822/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.208s, learning 0.208s)
               Value function loss: 215861.9773
                    Surrogate loss: -0.0081
             Mean action noise std: 0.70
                       Mean reward: 5168.65
               Mean episode length: 72.52
                  Mean reward/step: 65.38
       Mean episode length/episode: 7.25
            Mean episode successes: 5.3452
Mean episode consecutive_successes: 18.2061
--------------------------------------------------------------------------------
                   Total timesteps: 62636032
                    Iteration time: 8.42s
                        Total time: 38485.68s
                               ETA: 968212.2s

################################################################################
                    [1m Learning iteration 3823/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.537s, learning 0.253s)
               Value function loss: 204123.0570
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 3527.20
               Mean episode length: 71.90
                  Mean reward/step: 64.43
       Mean episode length/episode: 7.29
            Mean episode successes: 5.2407
Mean episode consecutive_successes: 18.1520
--------------------------------------------------------------------------------
                   Total timesteps: 62652416
                    Iteration time: 8.79s
                        Total time: 38494.47s
                               ETA: 968170.0s

################################################################################
                    [1m Learning iteration 3824/100000 [0m                    

                       Computation: 1841 steps/s (collection: 8.692s, learning 0.206s)
               Value function loss: 194295.1738
                    Surrogate loss: -0.0098
             Mean action noise std: 0.70
                       Mean reward: 4793.52
               Mean episode length: 72.81
                  Mean reward/step: 62.31
       Mean episode length/episode: 7.34
            Mean episode successes: 5.4795
Mean episode consecutive_successes: 18.1032
--------------------------------------------------------------------------------
                   Total timesteps: 62668800
                    Iteration time: 8.90s
                        Total time: 38503.36s
                               ETA: 968130.6s

################################################################################
                    [1m Learning iteration 3825/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.549s, learning 0.250s)
               Value function loss: 219733.2699
                    Surrogate loss: -0.0119
             Mean action noise std: 0.70
                       Mean reward: 4355.05
               Mean episode length: 72.07
                  Mean reward/step: 60.60
       Mean episode length/episode: 7.26
            Mean episode successes: 5.1294
Mean episode consecutive_successes: 18.1082
--------------------------------------------------------------------------------
                   Total timesteps: 62685184
                    Iteration time: 8.80s
                        Total time: 38512.16s
                               ETA: 968088.7s

################################################################################
                    [1m Learning iteration 3826/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.375s, learning 0.215s)
               Value function loss: 278345.4266
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 5140.71
               Mean episode length: 72.25
                  Mean reward/step: 60.20
       Mean episode length/episode: 7.28
            Mean episode successes: 4.8535
Mean episode consecutive_successes: 18.2431
--------------------------------------------------------------------------------
                   Total timesteps: 62701568
                    Iteration time: 8.59s
                        Total time: 38520.75s
                               ETA: 968041.5s

################################################################################
                    [1m Learning iteration 3827/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.052s, learning 0.167s)
               Value function loss: 275990.0691
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 4127.09
               Mean episode length: 71.42
                  Mean reward/step: 61.37
       Mean episode length/episode: 7.31
            Mean episode successes: 4.9038
Mean episode consecutive_successes: 18.1782
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 8.22s
                        Total time: 38528.97s
                               ETA: 967985.1s

################################################################################
                    [1m Learning iteration 3828/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.259s, learning 0.182s)
               Value function loss: 192443.8281
                    Surrogate loss: -0.0093
             Mean action noise std: 0.70
                       Mean reward: 3813.45
               Mean episode length: 69.42
                  Mean reward/step: 63.13
       Mean episode length/episode: 7.36
            Mean episode successes: 5.0981
Mean episode consecutive_successes: 18.1188
--------------------------------------------------------------------------------
                   Total timesteps: 62734336
                    Iteration time: 8.44s
                        Total time: 38537.41s
                               ETA: 967934.2s

################################################################################
                    [1m Learning iteration 3829/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.528s, learning 0.167s)
               Value function loss: 211499.9301
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 3988.89
               Mean episode length: 70.98
                  Mean reward/step: 65.72
       Mean episode length/episode: 7.29
            Mean episode successes: 5.3516
Mean episode consecutive_successes: 18.0249
--------------------------------------------------------------------------------
                   Total timesteps: 62750720
                    Iteration time: 8.70s
                        Total time: 38546.11s
                               ETA: 967889.8s

################################################################################
                    [1m Learning iteration 3830/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.464s, learning 0.189s)
               Value function loss: 210371.8289
                    Surrogate loss: -0.0119
             Mean action noise std: 0.70
                       Mean reward: 4418.26
               Mean episode length: 72.30
                  Mean reward/step: 66.61
       Mean episode length/episode: 7.38
            Mean episode successes: 5.7070
Mean episode consecutive_successes: 17.9914
--------------------------------------------------------------------------------
                   Total timesteps: 62767104
                    Iteration time: 8.65s
                        Total time: 38554.76s
                               ETA: 967844.3s

################################################################################
                    [1m Learning iteration 3831/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.440s, learning 0.176s)
               Value function loss: 231154.1367
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 4504.96
               Mean episode length: 72.07
                  Mean reward/step: 66.70
       Mean episode length/episode: 7.19
            Mean episode successes: 5.6338
Mean episode consecutive_successes: 17.9273
--------------------------------------------------------------------------------
                   Total timesteps: 62783488
                    Iteration time: 8.62s
                        Total time: 38563.38s
                               ETA: 967797.9s

################################################################################
                    [1m Learning iteration 3832/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.260s, learning 0.159s)
               Value function loss: 248643.0105
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 4806.62
               Mean episode length: 73.65
                  Mean reward/step: 63.64
       Mean episode length/episode: 7.27
            Mean episode successes: 5.3198
Mean episode consecutive_successes: 18.0597
--------------------------------------------------------------------------------
                   Total timesteps: 62799872
                    Iteration time: 8.42s
                        Total time: 38571.80s
                               ETA: 967746.5s

################################################################################
                    [1m Learning iteration 3833/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.623s, learning 0.215s)
               Value function loss: 210000.2141
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 4576.98
               Mean episode length: 70.43
                  Mean reward/step: 61.87
       Mean episode length/episode: 7.27
            Mean episode successes: 5.2822
Mean episode consecutive_successes: 18.0376
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 8.84s
                        Total time: 38580.63s
                               ETA: 967705.7s

################################################################################
                    [1m Learning iteration 3834/100000 [0m                    

                       Computation: 1840 steps/s (collection: 8.736s, learning 0.169s)
               Value function loss: 208311.8992
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 4662.39
               Mean episode length: 71.22
                  Mean reward/step: 59.23
       Mean episode length/episode: 7.23
            Mean episode successes: 4.9224
Mean episode consecutive_successes: 18.0967
--------------------------------------------------------------------------------
                   Total timesteps: 62832640
                    Iteration time: 8.90s
                        Total time: 38589.54s
                               ETA: 967666.6s

################################################################################
                    [1m Learning iteration 3835/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.605s, learning 0.196s)
               Value function loss: 215337.5715
                    Surrogate loss: -0.0090
             Mean action noise std: 0.70
                       Mean reward: 4733.33
               Mean episode length: 72.77
                  Mean reward/step: 60.33
       Mean episode length/episode: 7.35
            Mean episode successes: 4.9287
Mean episode consecutive_successes: 18.1373
--------------------------------------------------------------------------------
                   Total timesteps: 62849024
                    Iteration time: 8.80s
                        Total time: 38598.34s
                               ETA: 967624.9s

################################################################################
                    [1m Learning iteration 3836/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.642s, learning 0.169s)
               Value function loss: 194695.3824
                    Surrogate loss: -0.0098
             Mean action noise std: 0.70
                       Mean reward: 4523.32
               Mean episode length: 72.72
                  Mean reward/step: 65.63
       Mean episode length/episode: 7.36
            Mean episode successes: 5.1733
Mean episode consecutive_successes: 18.1663
--------------------------------------------------------------------------------
                   Total timesteps: 62865408
                    Iteration time: 8.81s
                        Total time: 38607.15s
                               ETA: 967583.5s

################################################################################
                    [1m Learning iteration 3837/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.568s, learning 0.283s)
               Value function loss: 203397.3039
                    Surrogate loss: -0.0151
             Mean action noise std: 0.70
                       Mean reward: 4365.75
               Mean episode length: 72.51
                  Mean reward/step: 64.57
       Mean episode length/episode: 7.31
            Mean episode successes: 5.2485
Mean episode consecutive_successes: 18.1619
--------------------------------------------------------------------------------
                   Total timesteps: 62881792
                    Iteration time: 8.85s
                        Total time: 38616.00s
                               ETA: 967543.1s

################################################################################
                    [1m Learning iteration 3838/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.426s, learning 0.189s)
               Value function loss: 239086.8582
                    Surrogate loss: -0.0111
             Mean action noise std: 0.70
                       Mean reward: 4078.94
               Mean episode length: 71.49
                  Mean reward/step: 66.83
       Mean episode length/episode: 7.28
            Mean episode successes: 5.4878
Mean episode consecutive_successes: 18.0735
--------------------------------------------------------------------------------
                   Total timesteps: 62898176
                    Iteration time: 8.61s
                        Total time: 38624.61s
                               ETA: 967496.8s

################################################################################
                    [1m Learning iteration 3839/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.612s, learning 0.165s)
               Value function loss: 262258.7211
                    Surrogate loss: -0.0026
             Mean action noise std: 0.70
                       Mean reward: 4236.79
               Mean episode length: 71.14
                  Mean reward/step: 67.76
       Mean episode length/episode: 7.31
            Mean episode successes: 5.6865
Mean episode consecutive_successes: 18.0882
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 8.78s
                        Total time: 38633.39s
                               ETA: 967454.6s

################################################################################
                    [1m Learning iteration 3840/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.580s, learning 0.171s)
               Value function loss: 257411.5125
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 4598.46
               Mean episode length: 71.84
                  Mean reward/step: 65.79
       Mean episode length/episode: 7.19
            Mean episode successes: 5.5405
Mean episode consecutive_successes: 18.1205
--------------------------------------------------------------------------------
                   Total timesteps: 62930944
                    Iteration time: 8.75s
                        Total time: 38642.14s
                               ETA: 967411.7s

################################################################################
                    [1m Learning iteration 3841/100000 [0m                    

                       Computation: 1823 steps/s (collection: 8.809s, learning 0.176s)
               Value function loss: 284162.9398
                    Surrogate loss: -0.0122
             Mean action noise std: 0.70
                       Mean reward: 5001.02
               Mean episode length: 72.25
                  Mean reward/step: 62.28
       Mean episode length/episode: 7.23
            Mean episode successes: 5.1934
Mean episode consecutive_successes: 18.1775
--------------------------------------------------------------------------------
                   Total timesteps: 62947328
                    Iteration time: 8.98s
                        Total time: 38651.13s
                               ETA: 967374.7s

################################################################################
                    [1m Learning iteration 3842/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.171s)
               Value function loss: 250072.3062
                    Surrogate loss: -0.0024
             Mean action noise std: 0.70
                       Mean reward: 4640.42
               Mean episode length: 71.35
                  Mean reward/step: 61.50
       Mean episode length/episode: 7.23
            Mean episode successes: 5.1641
Mean episode consecutive_successes: 18.1259
--------------------------------------------------------------------------------
                   Total timesteps: 62963712
                    Iteration time: 8.54s
                        Total time: 38659.67s
                               ETA: 967326.7s

################################################################################
                    [1m Learning iteration 3843/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.510s, learning 0.221s)
               Value function loss: 236511.2703
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 5058.80
               Mean episode length: 71.99
                  Mean reward/step: 60.57
       Mean episode length/episode: 7.32
            Mean episode successes: 5.0010
Mean episode consecutive_successes: 18.2106
--------------------------------------------------------------------------------
                   Total timesteps: 62980096
                    Iteration time: 8.73s
                        Total time: 38668.40s
                               ETA: 967283.4s

################################################################################
                    [1m Learning iteration 3844/100000 [0m                    

                       Computation: 1838 steps/s (collection: 8.744s, learning 0.168s)
               Value function loss: 219985.4352
                    Surrogate loss: -0.0119
             Mean action noise std: 0.70
                       Mean reward: 4538.78
               Mean episode length: 72.30
                  Mean reward/step: 58.69
       Mean episode length/episode: 7.34
            Mean episode successes: 4.7695
Mean episode consecutive_successes: 18.2346
--------------------------------------------------------------------------------
                   Total timesteps: 62996480
                    Iteration time: 8.91s
                        Total time: 38677.31s
                               ETA: 967244.7s

################################################################################
                    [1m Learning iteration 3845/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.620s, learning 0.192s)
               Value function loss: 239165.1543
                    Surrogate loss: -0.0063
             Mean action noise std: 0.70
                       Mean reward: 4274.30
               Mean episode length: 71.70
                  Mean reward/step: 63.00
       Mean episode length/episode: 7.34
            Mean episode successes: 5.1538
Mean episode consecutive_successes: 18.0663
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 8.81s
                        Total time: 38686.12s
                               ETA: 967203.4s

################################################################################
                    [1m Learning iteration 3846/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.659s, learning 0.177s)
               Value function loss: 277795.5906
                    Surrogate loss: -0.0117
             Mean action noise std: 0.70
                       Mean reward: 5000.10
               Mean episode length: 71.37
                  Mean reward/step: 63.74
       Mean episode length/episode: 7.30
            Mean episode successes: 5.2285
Mean episode consecutive_successes: 18.1240
--------------------------------------------------------------------------------
                   Total timesteps: 63029248
                    Iteration time: 8.84s
                        Total time: 38694.96s
                               ETA: 967162.8s

################################################################################
                    [1m Learning iteration 3847/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.431s, learning 0.313s)
               Value function loss: 305843.3844
                    Surrogate loss: -0.0034
             Mean action noise std: 0.70
                       Mean reward: 4815.57
               Mean episode length: 71.87
                  Mean reward/step: 62.47
       Mean episode length/episode: 7.32
            Mean episode successes: 5.2578
Mean episode consecutive_successes: 18.1066
--------------------------------------------------------------------------------
                   Total timesteps: 63045632
                    Iteration time: 8.74s
                        Total time: 38703.70s
                               ETA: 967119.9s

################################################################################
                    [1m Learning iteration 3848/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.664s, learning 0.169s)
               Value function loss: 307637.3125
                    Surrogate loss: -0.0092
             Mean action noise std: 0.70
                       Mean reward: 4292.85
               Mean episode length: 71.52
                  Mean reward/step: 62.75
       Mean episode length/episode: 7.28
            Mean episode successes: 5.1533
Mean episode consecutive_successes: 18.0734
--------------------------------------------------------------------------------
                   Total timesteps: 63062016
                    Iteration time: 8.83s
                        Total time: 38712.54s
                               ETA: 967079.2s

################################################################################
                    [1m Learning iteration 3849/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.503s, learning 0.171s)
               Value function loss: 205328.0781
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 4465.66
               Mean episode length: 72.49
                  Mean reward/step: 64.07
       Mean episode length/episode: 7.34
            Mean episode successes: 5.1890
Mean episode consecutive_successes: 18.1178
--------------------------------------------------------------------------------
                   Total timesteps: 63078400
                    Iteration time: 8.67s
                        Total time: 38721.21s
                               ETA: 967034.6s

################################################################################
                    [1m Learning iteration 3850/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.418s, learning 0.174s)
               Value function loss: 208378.9715
                    Surrogate loss: -0.0171
             Mean action noise std: 0.70
                       Mean reward: 3705.79
               Mean episode length: 71.20
                  Mean reward/step: 62.14
       Mean episode length/episode: 7.23
            Mean episode successes: 5.0000
Mean episode consecutive_successes: 18.0196
--------------------------------------------------------------------------------
                   Total timesteps: 63094784
                    Iteration time: 8.59s
                        Total time: 38729.80s
                               ETA: 966988.0s

################################################################################
                    [1m Learning iteration 3851/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.544s, learning 0.174s)
               Value function loss: 208994.1488
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 4606.85
               Mean episode length: 71.23
                  Mean reward/step: 59.87
       Mean episode length/episode: 7.29
            Mean episode successes: 4.9526
Mean episode consecutive_successes: 18.0084
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 8.72s
                        Total time: 38738.52s
                               ETA: 966944.5s

################################################################################
                    [1m Learning iteration 3852/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.714s, learning 0.169s)
               Value function loss: 217499.3602
                    Surrogate loss: -0.0140
             Mean action noise std: 0.70
                       Mean reward: 4360.61
               Mean episode length: 71.91
                  Mean reward/step: 59.93
       Mean episode length/episode: 7.27
            Mean episode successes: 4.8955
Mean episode consecutive_successes: 17.8991
--------------------------------------------------------------------------------
                   Total timesteps: 63127552
                    Iteration time: 8.88s
                        Total time: 38747.41s
                               ETA: 966905.1s

################################################################################
                    [1m Learning iteration 3853/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.570s, learning 0.238s)
               Value function loss: 224443.9133
                    Surrogate loss: -0.0093
             Mean action noise std: 0.70
                       Mean reward: 4181.70
               Mean episode length: 70.97
                  Mean reward/step: 60.81
       Mean episode length/episode: 7.29
            Mean episode successes: 4.8730
Mean episode consecutive_successes: 17.8429
--------------------------------------------------------------------------------
                   Total timesteps: 63143936
                    Iteration time: 8.81s
                        Total time: 38756.21s
                               ETA: 966863.9s

################################################################################
                    [1m Learning iteration 3854/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.402s, learning 0.184s)
               Value function loss: 254099.1809
                    Surrogate loss: -0.0045
             Mean action noise std: 0.70
                       Mean reward: 3743.64
               Mean episode length: 70.64
                  Mean reward/step: 61.63
       Mean episode length/episode: 7.28
            Mean episode successes: 5.0146
Mean episode consecutive_successes: 17.7644
--------------------------------------------------------------------------------
                   Total timesteps: 63160320
                    Iteration time: 8.59s
                        Total time: 38764.80s
                               ETA: 966817.2s

################################################################################
                    [1m Learning iteration 3855/100000 [0m                    

                       Computation: 1853 steps/s (collection: 8.662s, learning 0.177s)
               Value function loss: 253471.7859
                    Surrogate loss: 0.0094
             Mean action noise std: 0.70
                       Mean reward: 4454.56
               Mean episode length: 71.37
                  Mean reward/step: 65.13
       Mean episode length/episode: 7.35
            Mean episode successes: 5.3262
Mean episode consecutive_successes: 17.8522
--------------------------------------------------------------------------------
                   Total timesteps: 63176704
                    Iteration time: 8.84s
                        Total time: 38773.64s
                               ETA: 966776.8s

################################################################################
                    [1m Learning iteration 3856/100000 [0m                    

                       Computation: 1835 steps/s (collection: 8.758s, learning 0.171s)
               Value function loss: 232065.4719
                    Surrogate loss: -0.0128
             Mean action noise std: 0.70
                       Mean reward: 4760.99
               Mean episode length: 72.00
                  Mean reward/step: 65.88
       Mean episode length/episode: 7.30
            Mean episode successes: 5.3545
Mean episode consecutive_successes: 17.9188
--------------------------------------------------------------------------------
                   Total timesteps: 63193088
                    Iteration time: 8.93s
                        Total time: 38782.57s
                               ETA: 966738.7s

################################################################################
                    [1m Learning iteration 3857/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.484s, learning 0.188s)
               Value function loss: 229731.5113
                    Surrogate loss: -0.0060
             Mean action noise std: 0.70
                       Mean reward: 4588.74
               Mean episode length: 70.29
                  Mean reward/step: 66.17
       Mean episode length/episode: 7.29
            Mean episode successes: 5.5020
Mean episode consecutive_successes: 17.9408
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 8.67s
                        Total time: 38791.24s
                               ETA: 966694.2s

################################################################################
                    [1m Learning iteration 3858/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.690s, learning 0.173s)
               Value function loss: 221782.8852
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 4397.21
               Mean episode length: 72.38
                  Mean reward/step: 66.64
       Mean episode length/episode: 7.36
            Mean episode successes: 5.6553
Mean episode consecutive_successes: 17.9894
--------------------------------------------------------------------------------
                   Total timesteps: 63225856
                    Iteration time: 8.86s
                        Total time: 38800.10s
                               ETA: 966654.4s

################################################################################
                    [1m Learning iteration 3859/100000 [0m                    

                       Computation: 1834 steps/s (collection: 8.545s, learning 0.387s)
               Value function loss: 267723.0609
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 4090.16
               Mean episode length: 70.76
                  Mean reward/step: 65.74
       Mean episode length/episode: 7.20
            Mean episode successes: 5.6968
Mean episode consecutive_successes: 17.8684
--------------------------------------------------------------------------------
                   Total timesteps: 63242240
                    Iteration time: 8.93s
                        Total time: 38809.03s
                               ETA: 966616.4s

################################################################################
                    [1m Learning iteration 3860/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.504s, learning 0.192s)
               Value function loss: 426275.3195
                    Surrogate loss: -0.0117
             Mean action noise std: 0.70
                       Mean reward: 4787.83
               Mean episode length: 72.07
                  Mean reward/step: 64.25
       Mean episode length/episode: 7.33
            Mean episode successes: 5.6147
Mean episode consecutive_successes: 17.9818
--------------------------------------------------------------------------------
                   Total timesteps: 63258624
                    Iteration time: 8.70s
                        Total time: 38817.73s
                               ETA: 966572.5s

################################################################################
                    [1m Learning iteration 3861/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.108s, learning 0.177s)
               Value function loss: 296477.5000
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 4958.51
               Mean episode length: 72.53
                  Mean reward/step: 63.33
       Mean episode length/episode: 7.23
            Mean episode successes: 5.3164
Mean episode consecutive_successes: 18.0441
--------------------------------------------------------------------------------
                   Total timesteps: 63275008
                    Iteration time: 8.28s
                        Total time: 38826.01s
                               ETA: 966518.4s

################################################################################
                    [1m Learning iteration 3862/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.587s, learning 0.162s)
               Value function loss: 165457.6953
                    Surrogate loss: 0.0007
             Mean action noise std: 0.70
                       Mean reward: 4671.81
               Mean episode length: 71.05
                  Mean reward/step: 61.64
       Mean episode length/episode: 7.25
            Mean episode successes: 4.9995
Mean episode consecutive_successes: 18.1785
--------------------------------------------------------------------------------
                   Total timesteps: 63291392
                    Iteration time: 8.75s
                        Total time: 38834.76s
                               ETA: 966475.9s

################################################################################
                    [1m Learning iteration 3863/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.487s, learning 0.194s)
               Value function loss: 167022.3496
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 3914.07
               Mean episode length: 70.79
                  Mean reward/step: 60.26
       Mean episode length/episode: 7.32
            Mean episode successes: 5.0830
Mean episode consecutive_successes: 18.0953
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 8.68s
                        Total time: 38843.44s
                               ETA: 966431.7s

################################################################################
                    [1m Learning iteration 3864/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.483s, learning 0.175s)
               Value function loss: 173250.8637
                    Surrogate loss: -0.0037
             Mean action noise std: 0.70
                       Mean reward: 4074.27
               Mean episode length: 71.61
                  Mean reward/step: 62.25
       Mean episode length/episode: 7.39
            Mean episode successes: 5.3896
Mean episode consecutive_successes: 18.0309
--------------------------------------------------------------------------------
                   Total timesteps: 63324160
                    Iteration time: 8.66s
                        Total time: 38852.10s
                               ETA: 966387.0s

################################################################################
                    [1m Learning iteration 3865/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.423s, learning 0.171s)
               Value function loss: 192488.6574
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 4806.26
               Mean episode length: 72.27
                  Mean reward/step: 64.22
       Mean episode length/episode: 7.26
            Mean episode successes: 5.2554
Mean episode consecutive_successes: 18.1518
--------------------------------------------------------------------------------
                   Total timesteps: 63340544
                    Iteration time: 8.59s
                        Total time: 38860.70s
                               ETA: 966340.7s

################################################################################
                    [1m Learning iteration 3866/100000 [0m                    

                       Computation: 1827 steps/s (collection: 8.802s, learning 0.161s)
               Value function loss: 218980.8109
                    Surrogate loss: -0.0038
             Mean action noise std: 0.70
                       Mean reward: 4075.57
               Mean episode length: 71.53
                  Mean reward/step: 66.84
       Mean episode length/episode: 7.31
            Mean episode successes: 5.3965
Mean episode consecutive_successes: 18.1102
--------------------------------------------------------------------------------
                   Total timesteps: 63356928
                    Iteration time: 8.96s
                        Total time: 38869.66s
                               ETA: 966303.6s

################################################################################
                    [1m Learning iteration 3867/100000 [0m                    

                       Computation: 1830 steps/s (collection: 8.784s, learning 0.165s)
               Value function loss: 208206.8363
                    Surrogate loss: -0.0084
             Mean action noise std: 0.70
                       Mean reward: 4850.71
               Mean episode length: 71.72
                  Mean reward/step: 68.11
       Mean episode length/episode: 7.33
            Mean episode successes: 5.5728
Mean episode consecutive_successes: 18.1001
--------------------------------------------------------------------------------
                   Total timesteps: 63373312
                    Iteration time: 8.95s
                        Total time: 38878.61s
                               ETA: 966266.1s

################################################################################
                    [1m Learning iteration 3868/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.162s)
               Value function loss: 222192.7000
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 4498.22
               Mean episode length: 71.77
                  Mean reward/step: 65.64
       Mean episode length/episode: 7.20
            Mean episode successes: 5.4878
Mean episode consecutive_successes: 18.1155
--------------------------------------------------------------------------------
                   Total timesteps: 63389696
                    Iteration time: 8.43s
                        Total time: 38887.04s
                               ETA: 966215.8s

################################################################################
                    [1m Learning iteration 3869/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.332s, learning 0.210s)
               Value function loss: 216762.2559
                    Surrogate loss: -0.0091
             Mean action noise std: 0.70
                       Mean reward: 5276.43
               Mean episode length: 71.68
                  Mean reward/step: 63.84
       Mean episode length/episode: 7.28
            Mean episode successes: 5.2544
Mean episode consecutive_successes: 18.3224
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 8.54s
                        Total time: 38895.58s
                               ETA: 966168.2s

################################################################################
                    [1m Learning iteration 3870/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.454s, learning 0.286s)
               Value function loss: 211585.0621
                    Surrogate loss: -0.0099
             Mean action noise std: 0.70
                       Mean reward: 3924.83
               Mean episode length: 72.27
                  Mean reward/step: 62.10
       Mean episode length/episode: 7.26
            Mean episode successes: 5.1948
Mean episode consecutive_successes: 18.2094
--------------------------------------------------------------------------------
                   Total timesteps: 63422464
                    Iteration time: 8.74s
                        Total time: 38904.32s
                               ETA: 966125.7s

################################################################################
                    [1m Learning iteration 3871/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.468s, learning 0.248s)
               Value function loss: 221839.4617
                    Surrogate loss: 0.0107
             Mean action noise std: 0.70
                       Mean reward: 4342.28
               Mean episode length: 71.70
                  Mean reward/step: 60.02
       Mean episode length/episode: 7.29
            Mean episode successes: 4.9380
Mean episode consecutive_successes: 18.3021
--------------------------------------------------------------------------------
                   Total timesteps: 63438848
                    Iteration time: 8.72s
                        Total time: 38913.04s
                               ETA: 966082.5s

################################################################################
                    [1m Learning iteration 3872/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.266s, learning 0.305s)
               Value function loss: 196660.6090
                    Surrogate loss: -0.0125
             Mean action noise std: 0.70
                       Mean reward: 3920.28
               Mean episode length: 72.30
                  Mean reward/step: 61.70
       Mean episode length/episode: 7.28
            Mean episode successes: 4.9585
Mean episode consecutive_successes: 18.1800
--------------------------------------------------------------------------------
                   Total timesteps: 63455232
                    Iteration time: 8.57s
                        Total time: 38921.61s
                               ETA: 966035.7s

################################################################################
                    [1m Learning iteration 3873/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.625s, learning 0.311s)
               Value function loss: 207439.9953
                    Surrogate loss: -0.0136
             Mean action noise std: 0.70
                       Mean reward: 4149.40
               Mean episode length: 70.81
                  Mean reward/step: 64.40
       Mean episode length/episode: 7.31
            Mean episode successes: 5.1729
Mean episode consecutive_successes: 18.1477
--------------------------------------------------------------------------------
                   Total timesteps: 63471616
                    Iteration time: 8.94s
                        Total time: 38930.55s
                               ETA: 965998.1s

################################################################################
                    [1m Learning iteration 3874/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.367s, learning 0.174s)
               Value function loss: 222974.3258
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 4617.43
               Mean episode length: 72.03
                  Mean reward/step: 65.49
       Mean episode length/episode: 7.30
            Mean episode successes: 5.3652
Mean episode consecutive_successes: 18.1563
--------------------------------------------------------------------------------
                   Total timesteps: 63488000
                    Iteration time: 8.54s
                        Total time: 38939.09s
                               ETA: 965950.6s

################################################################################
                    [1m Learning iteration 3875/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.521s, learning 0.172s)
               Value function loss: 198158.7086
                    Surrogate loss: -0.0117
             Mean action noise std: 0.70
                       Mean reward: 3643.23
               Mean episode length: 70.34
                  Mean reward/step: 66.68
       Mean episode length/episode: 7.29
            Mean episode successes: 5.6792
Mean episode consecutive_successes: 17.9595
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 8.69s
                        Total time: 38947.78s
                               ETA: 965906.9s

################################################################################
                    [1m Learning iteration 3876/100000 [0m                    

                       Computation: 1859 steps/s (collection: 8.583s, learning 0.226s)
               Value function loss: 216346.1082
                    Surrogate loss: -0.0105
             Mean action noise std: 0.70
                       Mean reward: 5248.93
               Mean episode length: 72.51
                  Mean reward/step: 66.21
       Mean episode length/episode: 7.27
            Mean episode successes: 5.4814
Mean episode consecutive_successes: 18.2115
--------------------------------------------------------------------------------
                   Total timesteps: 63520768
                    Iteration time: 8.81s
                        Total time: 38956.59s
                               ETA: 965866.2s

################################################################################
                    [1m Learning iteration 3877/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.705s, learning 0.162s)
               Value function loss: 272015.3934
                    Surrogate loss: -0.0072
             Mean action noise std: 0.70
                       Mean reward: 4507.37
               Mean episode length: 71.78
                  Mean reward/step: 67.05
       Mean episode length/episode: 7.34
            Mean episode successes: 5.6880
Mean episode consecutive_successes: 18.2248
--------------------------------------------------------------------------------
                   Total timesteps: 63537152
                    Iteration time: 8.87s
                        Total time: 38965.46s
                               ETA: 965826.9s

################################################################################
                    [1m Learning iteration 3878/100000 [0m                    

                       Computation: 1855 steps/s (collection: 8.642s, learning 0.188s)
               Value function loss: 252515.8188
                    Surrogate loss: 0.0164
             Mean action noise std: 0.70
                       Mean reward: 4459.80
               Mean episode length: 71.77
                  Mean reward/step: 64.70
       Mean episode length/episode: 7.21
            Mean episode successes: 5.4497
Mean episode consecutive_successes: 18.2275
--------------------------------------------------------------------------------
                   Total timesteps: 63553536
                    Iteration time: 8.83s
                        Total time: 38974.29s
                               ETA: 965786.6s

################################################################################
                    [1m Learning iteration 3879/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.308s, learning 0.170s)
               Value function loss: 220957.6422
                    Surrogate loss: -0.0077
             Mean action noise std: 0.70
                       Mean reward: 3991.50
               Mean episode length: 71.50
                  Mean reward/step: 63.26
       Mean episode length/episode: 7.28
            Mean episode successes: 5.2954
Mean episode consecutive_successes: 18.2046
--------------------------------------------------------------------------------
                   Total timesteps: 63569920
                    Iteration time: 8.48s
                        Total time: 38982.76s
                               ETA: 965737.7s

################################################################################
                    [1m Learning iteration 3880/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.473s, learning 0.284s)
               Value function loss: 227078.1797
                    Surrogate loss: -0.0071
             Mean action noise std: 0.70
                       Mean reward: 4414.50
               Mean episode length: 71.01
                  Mean reward/step: 62.66
       Mean episode length/episode: 7.30
            Mean episode successes: 5.2378
Mean episode consecutive_successes: 18.2256
--------------------------------------------------------------------------------
                   Total timesteps: 63586304
                    Iteration time: 8.76s
                        Total time: 38991.52s
                               ETA: 965695.7s

################################################################################
                    [1m Learning iteration 3881/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.630s, learning 0.167s)
               Value function loss: 228499.1387
                    Surrogate loss: -0.0133
             Mean action noise std: 0.70
                       Mean reward: 4452.25
               Mean episode length: 70.43
                  Mean reward/step: 62.28
       Mean episode length/episode: 7.25
            Mean episode successes: 5.1260
Mean episode consecutive_successes: 18.2034
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 8.80s
                        Total time: 39000.32s
                               ETA: 965654.7s

################################################################################
                    [1m Learning iteration 3882/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.514s, learning 0.167s)
               Value function loss: 232028.9203
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 4899.88
               Mean episode length: 71.56
                  Mean reward/step: 64.08
       Mean episode length/episode: 7.29
            Mean episode successes: 5.0894
Mean episode consecutive_successes: 18.2323
--------------------------------------------------------------------------------
                   Total timesteps: 63619072
                    Iteration time: 8.68s
                        Total time: 39009.00s
                               ETA: 965610.9s

################################################################################
                    [1m Learning iteration 3883/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.625s, learning 0.202s)
               Value function loss: 222771.3082
                    Surrogate loss: -0.0088
             Mean action noise std: 0.70
                       Mean reward: 4379.28
               Mean episode length: 70.51
                  Mean reward/step: 64.68
       Mean episode length/episode: 7.42
            Mean episode successes: 5.4512
Mean episode consecutive_successes: 18.1531
--------------------------------------------------------------------------------
                   Total timesteps: 63635456
                    Iteration time: 8.83s
                        Total time: 39017.83s
                               ETA: 965570.6s

################################################################################
                    [1m Learning iteration 3884/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.508s, learning 0.194s)
               Value function loss: 234904.0063
                    Surrogate loss: -0.0148
             Mean action noise std: 0.70
                       Mean reward: 5387.58
               Mean episode length: 71.46
                  Mean reward/step: 66.36
       Mean episode length/episode: 7.29
            Mean episode successes: 5.3501
Mean episode consecutive_successes: 18.3071
--------------------------------------------------------------------------------
                   Total timesteps: 63651840
                    Iteration time: 8.70s
                        Total time: 39026.53s
                               ETA: 965527.3s

################################################################################
                    [1m Learning iteration 3885/100000 [0m                    

                       Computation: 1809 steps/s (collection: 8.851s, learning 0.205s)
               Value function loss: 213793.5809
                    Surrogate loss: -0.0080
             Mean action noise std: 0.70
                       Mean reward: 4614.05
               Mean episode length: 70.44
                  Mean reward/step: 67.71
       Mean episode length/episode: 7.29
            Mean episode successes: 5.5566
Mean episode consecutive_successes: 18.3027
--------------------------------------------------------------------------------
                   Total timesteps: 63668224
                    Iteration time: 9.06s
                        Total time: 39035.58s
                               ETA: 965492.8s

################################################################################
                    [1m Learning iteration 3886/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.320s, learning 0.165s)
               Value function loss: 217236.6977
                    Surrogate loss: -0.0051
             Mean action noise std: 0.70
                       Mean reward: 4454.01
               Mean episode length: 71.11
                  Mean reward/step: 67.44
       Mean episode length/episode: 7.31
            Mean episode successes: 5.7388
Mean episode consecutive_successes: 18.2908
--------------------------------------------------------------------------------
                   Total timesteps: 63684608
                    Iteration time: 8.48s
                        Total time: 39044.07s
                               ETA: 965444.2s

################################################################################
                    [1m Learning iteration 3887/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.418s, learning 0.253s)
               Value function loss: 228928.5625
                    Surrogate loss: -0.0102
             Mean action noise std: 0.70
                       Mean reward: 4496.99
               Mean episode length: 71.39
                  Mean reward/step: 68.31
       Mean episode length/episode: 7.21
            Mean episode successes: 5.6187
Mean episode consecutive_successes: 18.2643
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 8.67s
                        Total time: 39052.74s
                               ETA: 965400.2s

################################################################################
                    [1m Learning iteration 3888/100000 [0m                    

                       Computation: 1818 steps/s (collection: 8.774s, learning 0.238s)
               Value function loss: 237522.4273
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 4934.38
               Mean episode length: 70.28
                  Mean reward/step: 67.07
       Mean episode length/episode: 7.28
            Mean episode successes: 5.7842
Mean episode consecutive_successes: 18.2959
--------------------------------------------------------------------------------
                   Total timesteps: 63717376
                    Iteration time: 9.01s
                        Total time: 39061.75s
                               ETA: 965364.6s

################################################################################
                    [1m Learning iteration 3889/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.599s, learning 0.164s)
               Value function loss: 239529.0164
                    Surrogate loss: -0.0026
             Mean action noise std: 0.70
                       Mean reward: 4513.52
               Mean episode length: 70.10
                  Mean reward/step: 65.42
       Mean episode length/episode: 7.23
            Mean episode successes: 5.3745
Mean episode consecutive_successes: 18.4200
--------------------------------------------------------------------------------
                   Total timesteps: 63733760
                    Iteration time: 8.76s
                        Total time: 39070.51s
                               ETA: 965322.9s

################################################################################
                    [1m Learning iteration 3890/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.322s, learning 0.165s)
               Value function loss: 257738.1117
                    Surrogate loss: -0.0062
             Mean action noise std: 0.70
                       Mean reward: 5206.26
               Mean episode length: 72.75
                  Mean reward/step: 64.19
       Mean episode length/episode: 7.24
            Mean episode successes: 5.1562
Mean episode consecutive_successes: 18.5588
--------------------------------------------------------------------------------
                   Total timesteps: 63750144
                    Iteration time: 8.49s
                        Total time: 39079.00s
                               ETA: 965274.4s

################################################################################
                    [1m Learning iteration 3891/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.441s, learning 0.174s)
               Value function loss: 247224.5855
                    Surrogate loss: -0.0122
             Mean action noise std: 0.70
                       Mean reward: 4890.40
               Mean episode length: 71.96
                  Mean reward/step: 61.98
       Mean episode length/episode: 7.30
            Mean episode successes: 5.0342
Mean episode consecutive_successes: 18.6353
--------------------------------------------------------------------------------
                   Total timesteps: 63766528
                    Iteration time: 8.62s
                        Total time: 39087.62s
                               ETA: 965229.1s

################################################################################
                    [1m Learning iteration 3892/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.641s, learning 0.182s)
               Value function loss: 228242.3410
                    Surrogate loss: -0.0067
             Mean action noise std: 0.70
                       Mean reward: 4682.24
               Mean episode length: 71.96
                  Mean reward/step: 61.23
       Mean episode length/episode: 7.45
            Mean episode successes: 5.2646
Mean episode consecutive_successes: 18.6671
--------------------------------------------------------------------------------
                   Total timesteps: 63782912
                    Iteration time: 8.82s
                        Total time: 39096.44s
                               ETA: 965189.0s

################################################################################
                    [1m Learning iteration 3893/100000 [0m                    

                       Computation: 1844 steps/s (collection: 8.713s, learning 0.167s)
               Value function loss: 296498.3258
                    Surrogate loss: -0.0046
             Mean action noise std: 0.70
                       Mean reward: 4244.83
               Mean episode length: 72.04
                  Mean reward/step: 65.02
       Mean episode length/episode: 7.25
            Mean episode successes: 5.3374
Mean episode consecutive_successes: 18.5170
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 8.88s
                        Total time: 39105.32s
                               ETA: 965150.2s

################################################################################
                    [1m Learning iteration 3894/100000 [0m                    

                       Computation: 1788 steps/s (collection: 8.976s, learning 0.183s)
               Value function loss: 337402.1117
                    Surrogate loss: -0.0080
             Mean action noise std: 0.70
                       Mean reward: 4657.73
               Mean episode length: 71.63
                  Mean reward/step: 68.38
       Mean episode length/episode: 7.27
            Mean episode successes: 5.6152
Mean episode consecutive_successes: 18.4571
--------------------------------------------------------------------------------
                   Total timesteps: 63815680
                    Iteration time: 9.16s
                        Total time: 39114.48s
                               ETA: 965118.4s

################################################################################
                    [1m Learning iteration 3895/100000 [0m                    

                       Computation: 1833 steps/s (collection: 8.627s, learning 0.310s)
               Value function loss: 224861.6672
                    Surrogate loss: 0.0006
             Mean action noise std: 0.70
                       Mean reward: 4630.37
               Mean episode length: 71.43
                  Mean reward/step: 69.57
       Mean episode length/episode: 7.22
            Mean episode successes: 5.4277
Mean episode consecutive_successes: 18.4525
--------------------------------------------------------------------------------
                   Total timesteps: 63832064
                    Iteration time: 8.94s
                        Total time: 39123.42s
                               ETA: 965081.1s

################################################################################
                    [1m Learning iteration 3896/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.574s, learning 0.163s)
               Value function loss: 202759.7445
                    Surrogate loss: -0.0118
             Mean action noise std: 0.70
                       Mean reward: 4365.60
               Mean episode length: 72.15
                  Mean reward/step: 67.55
       Mean episode length/episode: 7.30
            Mean episode successes: 5.7021
Mean episode consecutive_successes: 18.3836
--------------------------------------------------------------------------------
                   Total timesteps: 63848448
                    Iteration time: 8.74s
                        Total time: 39132.15s
                               ETA: 965038.8s

################################################################################
                    [1m Learning iteration 3897/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.437s, learning 0.216s)
               Value function loss: 225539.0254
                    Surrogate loss: -0.0061
             Mean action noise std: 0.70
                       Mean reward: 4975.65
               Mean episode length: 71.85
                  Mean reward/step: 65.15
       Mean episode length/episode: 7.26
            Mean episode successes: 5.5210
Mean episode consecutive_successes: 18.5017
--------------------------------------------------------------------------------
                   Total timesteps: 63864832
                    Iteration time: 8.65s
                        Total time: 39140.81s
                               ETA: 964994.6s

################################################################################
                    [1m Learning iteration 3898/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.461s, learning 0.191s)
               Value function loss: 207925.6699
                    Surrogate loss: -0.0094
             Mean action noise std: 0.70
                       Mean reward: 5088.64
               Mean episode length: 72.39
                  Mean reward/step: 64.83
       Mean episode length/episode: 7.28
            Mean episode successes: 5.4160
Mean episode consecutive_successes: 18.5595
--------------------------------------------------------------------------------
                   Total timesteps: 63881216
                    Iteration time: 8.65s
                        Total time: 39149.46s
                               ETA: 964950.3s

################################################################################
                    [1m Learning iteration 3899/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.414s, learning 0.197s)
               Value function loss: 208750.9863
                    Surrogate loss: -0.0140
             Mean action noise std: 0.70
                       Mean reward: 5178.75
               Mean episode length: 72.22
                  Mean reward/step: 65.77
       Mean episode length/episode: 7.32
            Mean episode successes: 5.2583
Mean episode consecutive_successes: 18.6932
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 8.61s
                        Total time: 39158.07s
                               ETA: 964905.0s

################################################################################
                    [1m Learning iteration 3900/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.645s, learning 0.157s)
               Value function loss: 205234.8633
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 4655.68
               Mean episode length: 69.57
                  Mean reward/step: 63.66
       Mean episode length/episode: 7.27
            Mean episode successes: 5.1738
Mean episode consecutive_successes: 18.6327
--------------------------------------------------------------------------------
                   Total timesteps: 63913984
                    Iteration time: 8.80s
                        Total time: 39166.87s
                               ETA: 964864.5s

################################################################################
                    [1m Learning iteration 3901/100000 [0m                    

                       Computation: 1847 steps/s (collection: 8.698s, learning 0.169s)
               Value function loss: 193744.5406
                    Surrogate loss: -0.0080
             Mean action noise std: 0.70
                       Mean reward: 4055.81
               Mean episode length: 70.46
                  Mean reward/step: 62.61
       Mean episode length/episode: 7.29
            Mean episode successes: 5.1826
Mean episode consecutive_successes: 18.4856
--------------------------------------------------------------------------------
                   Total timesteps: 63930368
                    Iteration time: 8.87s
                        Total time: 39175.74s
                               ETA: 964825.5s

################################################################################
                    [1m Learning iteration 3902/100000 [0m                    

                       Computation: 1829 steps/s (collection: 8.696s, learning 0.257s)
               Value function loss: 208823.7961
                    Surrogate loss: -0.0040
             Mean action noise std: 0.70
                       Mean reward: 4808.66
               Mean episode length: 70.15
                  Mean reward/step: 65.47
       Mean episode length/episode: 7.33
            Mean episode successes: 5.3403
Mean episode consecutive_successes: 18.5174
--------------------------------------------------------------------------------
                   Total timesteps: 63946752
                    Iteration time: 8.95s
                        Total time: 39184.69s
                               ETA: 964788.8s

################################################################################
                    [1m Learning iteration 3903/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.469s, learning 0.183s)
               Value function loss: 210624.5832
                    Surrogate loss: -0.0057
             Mean action noise std: 0.70
                       Mean reward: 5032.72
               Mean episode length: 71.12
                  Mean reward/step: 65.67
       Mean episode length/episode: 7.23
            Mean episode successes: 5.2935
Mean episode consecutive_successes: 18.5497
--------------------------------------------------------------------------------
                   Total timesteps: 63963136
                    Iteration time: 8.65s
                        Total time: 39193.34s
                               ETA: 964744.6s

################################################################################
                    [1m Learning iteration 3904/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.484s, learning 0.163s)
               Value function loss: 227979.4199
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 4695.09
               Mean episode length: 72.54
                  Mean reward/step: 66.41
       Mean episode length/episode: 7.28
            Mean episode successes: 5.3076
Mean episode consecutive_successes: 18.5420
--------------------------------------------------------------------------------
                   Total timesteps: 63979520
                    Iteration time: 8.65s
                        Total time: 39201.99s
                               ETA: 964700.3s

################################################################################
                    [1m Learning iteration 3905/100000 [0m                    

                       Computation: 1828 steps/s (collection: 8.665s, learning 0.295s)
               Value function loss: 223853.4289
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 4708.67
               Mean episode length: 71.84
                  Mean reward/step: 63.66
       Mean episode length/episode: 7.33
            Mean episode successes: 5.4448
Mean episode consecutive_successes: 18.4965
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 8.96s
                        Total time: 39210.95s
                               ETA: 964663.7s

################################################################################
                    [1m Learning iteration 3906/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.191s, learning 0.202s)
               Value function loss: 299916.1914
                    Surrogate loss: -0.0096
             Mean action noise std: 0.70
                       Mean reward: 5263.66
               Mean episode length: 72.54
                  Mean reward/step: 61.73
       Mean episode length/episode: 7.30
            Mean episode successes: 5.2661
Mean episode consecutive_successes: 18.5630
--------------------------------------------------------------------------------
                   Total timesteps: 64012288
                    Iteration time: 8.39s
                        Total time: 39219.34s
                               ETA: 964613.1s

################################################################################
                    [1m Learning iteration 3907/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.806s, learning 0.178s)
               Value function loss: 261695.1086
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 4668.78
               Mean episode length: 73.29
                  Mean reward/step: 61.81
       Mean episode length/episode: 7.26
            Mean episode successes: 5.1807
Mean episode consecutive_successes: 18.4685
--------------------------------------------------------------------------------
                   Total timesteps: 64028672
                    Iteration time: 15.98s
                        Total time: 39235.33s
                               ETA: 964749.3s

################################################################################
                    [1m Learning iteration 3908/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.135s, learning 0.162s)
               Value function loss: 286148.3234
                    Surrogate loss: -0.0125
             Mean action noise std: 0.70
                       Mean reward: 4179.27
               Mean episode length: 70.78
                  Mean reward/step: 61.89
       Mean episode length/episode: 7.28
            Mean episode successes: 4.9951
Mean episode consecutive_successes: 18.4358
--------------------------------------------------------------------------------
                   Total timesteps: 64045056
                    Iteration time: 16.30s
                        Total time: 39251.62s
                               ETA: 964893.1s

################################################################################
                    [1m Learning iteration 3909/100000 [0m                    

                       Computation: 970 steps/s (collection: 16.557s, learning 0.321s)
               Value function loss: 231432.3730
                    Surrogate loss: -0.0040
             Mean action noise std: 0.70
                       Mean reward: 3484.73
               Mean episode length: 69.68
                  Mean reward/step: 62.38
       Mean episode length/episode: 7.21
            Mean episode successes: 4.9966
Mean episode consecutive_successes: 18.2495
--------------------------------------------------------------------------------
                   Total timesteps: 64061440
                    Iteration time: 16.88s
                        Total time: 39268.50s
                               ETA: 965051.0s

################################################################################
                    [1m Learning iteration 3910/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.635s, learning 0.170s)
               Value function loss: 208094.9426
                    Surrogate loss: 0.0008
             Mean action noise std: 0.70
                       Mean reward: 4446.97
               Mean episode length: 71.31
                  Mean reward/step: 61.54
       Mean episode length/episode: 7.31
            Mean episode successes: 5.0474
Mean episode consecutive_successes: 18.2039
--------------------------------------------------------------------------------
                   Total timesteps: 64077824
                    Iteration time: 16.81s
                        Total time: 39285.31s
                               ETA: 965207.1s

################################################################################
                    [1m Learning iteration 3911/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.615s, learning 0.162s)
               Value function loss: 189339.7707
                    Surrogate loss: -0.0139
             Mean action noise std: 0.70
                       Mean reward: 4191.71
               Mean episode length: 70.72
                  Mean reward/step: 64.74
       Mean episode length/episode: 7.38
            Mean episode successes: 5.4004
Mean episode consecutive_successes: 18.2117
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 16.78s
                        Total time: 39302.08s
                               ETA: 965362.5s

################################################################################
                    [1m Learning iteration 3912/100000 [0m                    

                       Computation: 966 steps/s (collection: 16.732s, learning 0.223s)
               Value function loss: 207895.3746
                    Surrogate loss: -0.0122
             Mean action noise std: 0.70
                       Mean reward: 4531.56
               Mean episode length: 70.55
                  Mean reward/step: 66.60
       Mean episode length/episode: 7.20
            Mean episode successes: 5.3022
Mean episode consecutive_successes: 18.1624
--------------------------------------------------------------------------------
                   Total timesteps: 64110592
                    Iteration time: 16.96s
                        Total time: 39319.04s
                               ETA: 965522.1s
